<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">

  <script>
    (function(){
        if(''){
            if (prompt('请输入密码') !== ''){
                alert('密码错误');
                history.back();
            }
        }
    })();
</script>







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="TensorFlow,CNN,">





  <link rel="alternate" href="/atom.xml" title="When Art Meets Technology" type="application/atom+xml">






<meta name="description" content="愿你每天欢喜多于悲，孤独有人陪…   读前一览： CNN 经典模型之 LeNet &amp;amp;&amp;amp; AlexNet 解读与 TensorFlow 实现。">
<meta name="keywords" content="TensorFlow,CNN">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络系列之从 LeNet 到 AlexNet">
<meta property="og:url" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/index.html">
<meta property="og:site_name" content="When Art Meets Technology">
<meta property="og:description" content="愿你每天欢喜多于悲，孤独有人陪…   读前一览： CNN 经典模型之 LeNet &amp;amp;&amp;amp; AlexNet 解读与 TensorFlow 实现。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/CNN_connection.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/CNN_parameters_sharing.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/CNN_pooling.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/LeNet-5.jpg">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/conv_fp.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/conv_pool_fp.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/conv_pool_connection.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/f6_fcnn.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/LeNet-5_3.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/ReLU.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/Relu_tanh.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/data_augmentation.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/CNN_pooling.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/LRN.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/Dropout.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet.jpg">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_extend.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_1.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_1_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_2.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_2_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_3.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_3_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_4.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_4_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_5.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_5_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_6.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_6_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_7.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_7_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_8.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_with_8_pic.png">
<meta property="og:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/AlexNet_pic.jpeg">
<meta property="og:updated_time" content="2019-06-19T09:06:05.047Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷积神经网络系列之从 LeNet 到 AlexNet">
<meta name="twitter:description" content="愿你每天欢喜多于悲，孤独有人陪…   读前一览： CNN 经典模型之 LeNet &amp;amp;&amp;amp; AlexNet 解读与 TensorFlow 实现。">
<meta name="twitter:image" content="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/CNN_connection.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/">






  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "7e6ff6a0"
    });
  daovoice('update');
  </script>

  <title>卷积神经网络系列之从 LeNet 到 AlexNet | When Art Meets Technology</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  
    <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">When Art Meets Technology</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="TheMusicIsLoud">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="When Art Meets Technology">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">卷积神经网络系列之从 LeNet 到 AlexNet</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-03T13:06:15+08:00">
                2018-07-03
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-06-19T17:06:05+08:00">
                2019-06-19
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Convlutional-Neural-Networks/" itemprop="url" rel="index">
                    <span itemprop="name">Convlutional Neural Networks</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/" class="leancloud_visitors" data-flag-title="卷积神经网络系列之从 LeNet 到 AlexNet">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读热度&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
                 <span>次</span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  8.2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  30
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <center> 愿你每天欢喜多于悲，孤独有人陪… </center>

<p><strong>读前一览：</strong></p>
<p>CNN 经典模型之 LeNet &amp;&amp; AlexNet 解读与 TensorFlow 实现。</p>
<a id="more"></a>
<p>本博文系列是对刘昕博士的 <a href="https://mp.weixin.qq.com/s?__biz=MzI1NTE4NTUwOQ==&amp;mid=2650324619&amp;idx=1&amp;sn=ca1aed9e42d8f020d0971e62148e13be&amp;scene=1&amp;srcid=0503De6zpYN01gagUvn0Ht8D#wechat_redirect" target="_blank" rel="noopener">《CNN 的近期进展与实用技巧》</a> 的一个扩充性资料，全面详细地介绍了 CNN 进化史各个阶段的里程碑成果：</p>
<p>卷积神经网络系列之：<a href="">大话卷积神经网络（CNN）</a> ；</p>
<p>卷积神经网络系列之：<a href="">CNN 进化史</a> ；</p>
<p>卷积神经网络系列之：<a href="">从 LeNet 到 AlexNet</a> ；</p>
<p>卷积神经网络系列之：<a href="">CNN 演变之路之网络结构加深</a> ；</p>
<p><strong>声明：</strong>本系列博文只供学习和交流使用，禁止转载，侵权请联系删除！</p>
<hr>
<h3 id="从-LeNet-到-AlexNet"><a href="#从-LeNet-到-AlexNet" class="headerlink" title="从 LeNet 到 AlexNet"></a>从 LeNet 到 AlexNet</h3><p>在之前的章节中，已经介绍了卷积神经网络（CNN）的技术原理，细节部分就不再重复了。在此首先来简单回顾一下 CNN 的几个特点：局部感知、参数共享、池化。</p>
<h4 id="1-CNN回顾"><a href="#1-CNN回顾" class="headerlink" title="1. CNN回顾"></a>1. CNN回顾</h4><h5 id="1-1-局部感知"><a href="#1-1-局部感知" class="headerlink" title="1.1 局部感知"></a>1.1 局部感知</h5><p>人类对外界的认知一般是从局部到全局、从片面到全面。</p>
<p>类似的，在机器识别图像时也没有必要把整张图像按像素全部都连接到神经网络中，在图像中也是局部周边的像素联系比较紧密，而距离较远的像素则相关性较弱，因此可以采用局部连接的模式（将图像分块连接，这样能大大减少模型的参数），如下图所示：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/CNN_connection.png" alt="avatar"></p>
<hr>
<h5 id="1-2-参数（权值）共享"><a href="#1-2-参数（权值）共享" class="headerlink" title="1.2 参数（权值）共享"></a>1.2 参数（权值）共享</h5><p>每张自然图像（人物、山水、建筑等）都有其固有特性，也就是说，图像其中一部分的统计特性与其它部分是接近的。这也意味着这一部分学习的特征也能用在另一部分上，能使用同样的学习特征。</p>
<p>还有，图像中局部特征不应随着其位置的变化而变化，也就是说，位于不同位置的相同局部特征应该对应相同过滤器权重参数。</p>
<p>因此，在局部连接中隐藏层的每一个神经元连接的局部图像的权值参数（例如<strong>5×5</strong>），将这些权值参数共享给其它剩下的神经元使用，那么此时不管隐藏层有多少个神经元，需要训练的参数就是这个局部图像的权限参数（例如5×5），也就是卷积核的大小，这样大大减少了训练参数。如下图：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./CNN_parameters_sharing.png" alt="avatar"></p>
<hr>
<h5 id="1-3-池化"><a href="#1-3-池化" class="headerlink" title="1.3 池化"></a>1.3 池化</h5><p>随着模型网络不断加深，卷积核越来越多，要训练的参数还是很多，而且直接拿卷积核提取的特征直接训练也容易出现过拟合的现象。</p>
<p>回想一下，之所以对图像使用卷积提取特征是因为图像具有一种“静态性”的属性，因此，一个很自然的想法就是对不同位置区域提取出具有代表性的特征（进行聚合统计，例如最大值、平均值等），这种聚合的操作就叫做池化，池化的过程通常也被称为降采样，如下图：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./CNN_pooling.png" alt="avatar"></p>
<hr>
<h4 id="2-一切的开始：LeNet"><a href="#2-一切的开始：LeNet" class="headerlink" title="2. 一切的开始：LeNet"></a>2. 一切的开始：LeNet</h4><p>回顾了卷积神经网络（CNN）上面的三个特点后，下面来介绍一下 CNN 的经典模型：LeNet-5（最早将卷积神经网络运用于 MNIST 手写数字识别的网络）。</p>
<p>LeNet-5 诞生于 1998 年，是最早的卷积神经网络之一， 是由大佬 Yann LeCun 完成，推动了深度学习领域的发展。</p>
<p>当时，没有 GPU 帮助训练模型，甚至 CPU 的速度也很慢，因此，LeNet-5 通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是当前许多经典神经网络架构的起点，给这个领域带来了许多灵感。</p>
<hr>
<h5 id="2-1-LeNet-5-整体网络架构"><a href="#2-1-LeNet-5-整体网络架构" class="headerlink" title="2.1 LeNet-5 整体网络架构"></a>2.1 LeNet-5 整体网络架构</h5><p>下图是广为流传 LeNet-5 的网络结构，麻雀虽小，但五脏俱全：卷积层、Pooling 层、全连接层，这些都是现代 CNN 网络的基本组件。</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./LeNet-5.jpg" alt="avatar"></p>
<blockquote>
<p>输入尺寸：32 × 32<br>卷积层：2 个<br>降采样层（池化层）：2个<br>全连接层：3 个<br>输出：10 个类别（数字 0-9 的概率）</p>
</blockquote>
<p>LeNet-5 由 7 层 CNN（不包含输入层）组成，上图中输入的原始图像大小是 <strong>32×32​</strong> 像素（INPUT），卷积层用 <strong>Ci</strong> 表示，子采样层（池化层：pooling）用 <strong>Si</strong> 表示，全连接层用 <strong>Fi</strong> 表示。下面逐层介绍其作用和示意图上方的数字含义：</p>
<h6 id="0-–-gt-INPUT-32-×-32"><a href="#0-–-gt-INPUT-32-×-32" class="headerlink" title="0 –&gt; INPUT ( 32 × 32 )"></a>0 –&gt; INPUT ( 32 × 32 )</h6><p>输入图像 Size 为: <strong>32×32​</strong>。</p>
<p>注意，本层不算 LeNet-5 的网络结构。传统意义上，不将输入层视为网络层次结构之一。</p>
<hr>
<h6 id="1-–-gt-C1（卷积层）：6-28×28"><a href="#1-–-gt-C1（卷积层）：6-28×28" class="headerlink" title="1 –&gt; C1（卷积层）：6@28×28"></a>1 –&gt; C1（卷积层）：6@28×28</h6><p>该层使用了 <strong>6​</strong> 个卷积核（过滤器深度），每个卷积核尺寸大小为 <strong>5×5​</strong>，这样就得到了 ​<strong>6​</strong> 个 feature map（特征图）。</p>
<p>–&gt; 1_1）特征图大小</p>
<p>每个卷积核 （<strong>5×5​</strong>） 与原始的输入图像 （<strong>32×32</strong>） 进行卷积，这样得到的 feature map（特征图）大小为：<strong>(32−5+1) × (32−5+1) = 28×28</strong>。</p>
<p>卷积过程如下图所示：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./conv_fp.png" alt="avatar"></p>
<p>卷积核与输入图像按卷积核大小逐个区域进行匹配计算，匹配后原始输入图像的尺寸将变小。而且，没有使用填充，边缘部分卷积核无法越出界，只能匹配一次。</p>
<p>如上图，匹配计算后的尺寸变为 <strong>Cr×Cc=（Ir-Kr+1）×（Ic-Kc+1）</strong> ，其中 <code>Cr</code>、<code>Cc</code>，<code>Ir</code>、<code>Ic</code>，<code>Kr</code>、<code>Kc</code>分别表示卷积后结果图像、输入图像以及卷积核的尺寸的大小。</p>
<p>–&gt; 1_2）参数个数</p>
<p>由于参数（权值）共享的原因，对于每个卷积核（对应输出单位节点矩阵上的一个神经元）均使用相同的参数，因此，参数个数为 <strong>（5×5+1）×6= 156</strong> ，其中 ​<strong>5×5</strong> 为卷积核参数，<strong>+1</strong> 为偏置参数。</p>
<p>–&gt; 1_3）和上一层的连接数</p>
<p>卷积后的图像大小为 <strong>28×28</strong>，因此每个特征图有 <strong>28×28</strong> 个神经元，每个卷积数的连接数为 <strong>（5×5+1）× 6</strong>，因此，该层的连接数为 <strong>（5×5+1）×6×28×28 = 122304</strong>。</p>
<hr>
<h6 id="2-–-gt-S2（下采样层，也称池化层）：6-14×14"><a href="#2-–-gt-S2（下采样层，也称池化层）：6-14×14" class="headerlink" title="2 –&gt; S2（下采样层，也称池化层）：6@14×14"></a>2 –&gt; S2（下采样层，也称池化层）：6@14×14</h6><p>采样方式：局部 <strong>2 × 2</strong> 区域中的 4 个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过 <strong>sigmoid</strong> 激活函数。</p>
<p>–&gt; 2_1）特征图大小</p>
<p>这一层主要是做池化或者特征映射（特征降维），池化单元为 <strong>2×2</strong>，因此，<strong>6</strong> 个特征图的大小经池化后即变为<strong>14×14​</strong>（<strong>S2</strong> 中每个特征图的大小是 <strong>C1</strong> 中特征图大小的 <strong>1/4</strong>，长宽各减少 <strong>1/2</strong>）。</p>
<p>回顾本文刚开始讲到的池化操作，LeNet 模型池化单元之间是 <strong>没有重叠的</strong>，在池化区域内进行聚合统计后得到新的特征值。因此经 <strong>2×2</strong> 池化后，每两行两列重新算出一个特征值出来，相当于图像大小减半，因此卷积后的 <strong>28×28</strong> 图像经 <strong>2×2</strong> 池化后就变为 <strong>14×14</strong>。</p>
<p>这一层的计算过程是：<strong>2×2</strong> 单元里的值相加，然后再乘以训练参数$w$，再加上一个偏置参数 $b$（每一个特征图共享相同的 $w$ 和 $b$)，然后取 <strong>sigmoid</strong> 值（S 函数：0~1区间），作为对应的该单元的值。卷积操作与池化的示意图如下：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./conv_pool_fp.png" alt="avatar"></p>
<p>–&gt; 2_2）参数个数</p>
<p>S2 层由于每个特征图都共享相同的 w 和 b 这两个参数，因此需要 <strong>(1 + 1)×6=12</strong> 个参数</p>
<p>–&gt; 2_3）和上一层的连接数</p>
<p>下采样之后的图像大小为 <strong>14×14</strong>，因此 S2 层的每个特征图有 <strong>14×14</strong> 个神经元，每个池化单元连接数为  <strong>2×2+1</strong>（1 为偏置量），因此，该层的连接数为 <strong>（2×2+1）×14×14×6 = 5880</strong> 。</p>
<hr>
<h6 id="3-–-gt-C3（卷积层）：16-10×10"><a href="#3-–-gt-C3（卷积层）：16-10×10" class="headerlink" title="3 –&gt; C3（卷积层）：16@10×10"></a>3 –&gt; C3（卷积层）：16@10×10</h6><p>C3 层有 <strong>16</strong> 个卷积核，卷积模板大小为 <strong>5×5</strong>。这样就得到了 <strong>16</strong> 个 feature map（特征图）。</p>
<p>–&gt; 3_1）特征图大小</p>
<p>与 C1 层的分析类似，C3 层的特征图大小为 <strong>（14-5+1）×（14-5+1）= 10×10</strong>​</p>
<p>–&gt; 3_2）参数个数</p>
<p>需要注意的是，实际中，<strong>C3 与 S2 并不是全连接而是部分连接！！！</strong></p>
<p>C3 中的每个 feature map 是连接到 S2 中的所有 6 个或者几个 feature  map，表示本层的 feature map 是上一层提取到的 feature  map 的不同组合。</p>
<p>存在的一个方式是：C3 的前 6 个特征图与 S2 中 3 个相邻的特征图子集为输入。接下来 6 个特征图与 S2 中 4 个相邻特征图子集为输入。然后的 3 个与不相邻的 4 个特征图子集为输入。最后一个将 S2 中所有特征图为输入。如下表所示：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./conv_pool_connection.png" alt="avatar"></p>
<p>–&gt; 详细说明：C3 的前 6 个 feature map（对应上图第一个红框的6列）与 S2 层相连的 3 个 feature map 相连接（上图第一个红框）；后面 6 个 feature map 与 S2 层相连的 4 个 feature map 相连接（上图第二个红框）；后面 3 个 feature map 与 S2 层部分不相连的 4 个 feature map 相连接；最后一个 feature map 与 S2 层的所有 feature map 相连。</p>
<p>卷积核大小依然为 <strong>5 × 5</strong>，所以总共有 <strong>（5×5×3+1）× 6 +（5×5×4+1）× 9 + 5×5×6+1 = 1516</strong> 。</p>
<p>–&gt; 3_3）和上一层的连接数</p>
<p>卷积后的特征图大小为<strong>10×10</strong>，参数数量为<strong>1516</strong>（每个单位节点矩阵的连接数），因此总连接数为 <strong>1516×10×10= 151600</strong>。</p>
<p>–&gt; 为什么采用上述这样的组合了？论文中说有两个原因：</p>
<ol>
<li>进一步减少参数；</li>
<li>这种不对称的组合连接的方式有利于提取多种组合特征。</li>
</ol>
<hr>
<h6 id="4-–-gt-S4（下采样层，也称池化层）：16-5×5"><a href="#4-–-gt-S4（下采样层，也称池化层）：16-5×5" class="headerlink" title="4 –&gt; S4（下采样层，也称池化层）：16@5×5"></a>4 –&gt; S4（下采样层，也称池化层）：16@5×5</h6><p>采样方式：局部 <strong>2 × 2</strong> 区域中的4 个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过 sigmoid。</p>
<p>–&gt; 4_1）特征图大小</p>
<p>与 S2 的分析类似，池化单元大小为 <strong>2×2</strong>，该层与 C3 一样共有 $16$ 个特征图，因此，每个特征图的大小为 <strong>5×5</strong>。</p>
<p>–&gt; 4_2）参数个数</p>
<p>与 S2 的计算类似，所需要参数个数为 <strong>16×2 = 32</strong>。</p>
<p>–&gt; 4_3）和上一层的连接数</p>
<p>连接数为 <strong>（2×2+1）×5×5×16 = 2000​</strong>。</p>
<hr>
<h6 id="5-–-gt-C5层（全卷积层）：120"><a href="#5-–-gt-C5层（全卷积层）：120" class="headerlink" title="5 –&gt; C5层（全卷积层）：120"></a>5 –&gt; C5层（全卷积层）：120</h6><p>–&gt; 5_1）特征图大小</p>
<p>该层使用 $120$ 个卷积核，每个卷积核的大小仍为 <strong>5×5</strong>，因此有 <strong>120</strong> 个特征图。由于 S4 层的大小为 <strong>5×5</strong>，而该层的卷积核尺寸也使用的是 <strong>5×5</strong>，因此特征图大小为<strong>（5-5+1）×（5-5+1）= 1×1</strong>。这样该层就刚好变成了全连接。</p>
<p>注意，在好多材料中你会发现：该层比较特殊，可以用全卷积层，或全连接层实现，实现不同，但功能相同。</p>
<p>所以，后续的 TensorFlow 实现中我们会用全连接层来代替。</p>
<p>–&gt; 5_2）参数个数</p>
<p>与前面的分析类似，本层的参数数目为 <strong>120×（5×5×16+1） = 48120</strong>$。</p>
<p>–&gt; 5_3）和上一层的连接数</p>
<p>由于该层的特征图大小刚好为 <strong>1×1</strong>，因此连接数为 <strong>48120×1×1=48120</strong>。</p>
<hr>
<h6 id="6-–-gt-F6层（全连接层）：84"><a href="#6-–-gt-F6层（全连接层）：84" class="headerlink" title="6 –&gt; F6层（全连接层）：84"></a>6 –&gt; F6层（全连接层）：84</h6><p>–&gt; 6_1）特征图大小</p>
<p>该层有 <strong>84</strong> 个特征图，特征图大小与 C5 一样都是 <strong>1×1</strong>，与 C5 层全连接。</p>
<p>–&gt; 6_2）参数个数</p>
<p>由于是全连接，参数数量为 <strong>（120+1）× 84=10164</strong>。跟经典神经网络一样，F6 层计算输入向量和权重向量之间的点积，再加上一个偏置，然后将其传递给 <strong>sigmoid</strong> 函数得出结果。</p>
<p>–&gt; 6_3）和上一层的连接数</p>
<p>由于是全连接，连接数与参数数量一样，也是 <strong>10164</strong>。</p>
<p><strong>详细说明：</strong></p>
<p>6 层是全连接层。F6 层有 84 个节点，对应于一个 7x12 的比特图（-1 表示白色，1 表示黑色），这样每个符号的比特图的黑白色就对应于一个编码。ASCII 编码图如下：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./f6_fcnn.png" alt="avatar"></p>
<hr>
<h6 id="7-–-gt-OUTPUT层（输出层）：10"><a href="#7-–-gt-OUTPUT层（输出层）：10" class="headerlink" title="7 –&gt; OUTPUT层（输出层）：10"></a>7 –&gt; OUTPUT层（输出层）：10</h6><p>Output 层也是全连接层，共有 10 个节点，分别代表数字 0 到 9。如果第 i 个节点的值为 0，则表示网络识别的结果是数字 i。</p>
<p>输出层采用的是径向基函数（Radial Basis Function）的网络连接方式。假设 x 是上一层的输入，y 是 RBF 的输出，则 RBF 输出的计算方式是：</p>
<p>$$ y_i = \sum_{j}(x_j + w_{ij})^2 $$</p>
<p>上式中的 $w_{ij}​$ 的值由 i 的比特图编码确定，i 从 0 到 9，j 取值从 0 到 7×12-1。RBF 输出的值越接近于 0，表示当前网络输入的识别结果与字符 i 越接近，即越接近于 i 的 ASCII 编码图。</p>
<p>–&gt; 7_2）参数个数</p>
<p>由于是全连接，参数数量为 <strong>10 × 84=840</strong>。</p>
<p>–&gt; 7_3）和上一层的连接数</p>
<p>由于是全连接，连接数与参数数量一样，也是 <strong>840​</strong>。</p>
<hr>
<h5 id="2-2-LeNet-5-中字符-“3”-的识别"><a href="#2-2-LeNet-5-中字符-“3”-的识别" class="headerlink" title="2.2 LeNet-5 中字符 “3” 的识别"></a>2.2 LeNet-5 中字符 “3” 的识别</h5><p>通过以上介绍，已经了解了 LeNet 各层网络的结构、特征图大小、参数数量、连接数量等信息，下图是识别数字 3 的过程，可对照上面介绍各个层的功能进行一一回顾：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./LeNet-5_3.png" alt="avatar"></p>
<p>Yann LeCun 在 1998 年发表了关于LeNet的经典论文 《Gradient-Based Learning Applied to Document Recognition 》（基于梯度学习在文档识别中的应用），可用于参考。</p>
<hr>
<h5 id="2-3-LetNet-5-的-TensorFlow-实现"><a href="#2-3-LetNet-5-的-TensorFlow-实现" class="headerlink" title="2.3 LetNet-5 的 TensorFlow 实现"></a>2.3 LetNet-5 的 TensorFlow 实现</h5><p>附一个<strong>类似</strong>于 LetNet-5 网络结构进行 <a href="https://www.orangeshare.cn/TensorFlow/TensorFlow-系列之经典卷积神经网络模型%EF%BC%88LeNet-5-Inception-v3%EF%BC%89/" target="_blank" rel="noopener">MNIST 手写数字识别的 TensorFlow 实现</a> 的链接。</p>
<p>为什么不实现原有 LetNet-5 结构？</p>
<p>你可以这么想，LetNet-5 尽管说是开山之作，但网络中仍然存在有一些的“糟粕”，目前来说，是象征（致敬 LeNet，致敬大佬），但一般不会用。</p>
<p>你会使用前面的 CNN 卷积网络知识实现类似的（通用卷积神经网络架构）即可。</p>
<hr>
<h4 id="3-王者回归：AlexNet"><a href="#3-王者回归：AlexNet" class="headerlink" title="3. 王者回归：AlexNet"></a>3. 王者回归：AlexNet</h4><p>时隔 十几年之久，<strong>2012 年</strong> Alex Krizhevsky、Ilya Sutskever 在多伦多大学 Geoff Hinton 的实验室设计出了一个深层的卷积神经网络 AlexNet，夺得了 2012 年 ImageNet LSVRC 的冠军，且准确率远超第二名（Top5 错误率为 15.3%，第二名为 26.2%），引起了很大的轰动。</p>
<p> <strong>AlexNet 可以说是具有历史意义的一个网络结构 </strong> ，在此之前，<strong>深度学习已经沉寂了很长时间。</strong>自 2012 年 AlexNet 诞生之后，后面的 ImageNet 冠军都是用卷积神经网络（CNN）来做的，并且层次越来越深，<strong>使得 CNN 成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。</strong></p>
<h5 id="3-1-AlexNet-模型的特点"><a href="#3-1-AlexNet-模型的特点" class="headerlink" title="3.1 AlexNet 模型的特点"></a>3.1 AlexNet 模型的特点</h5><p>AlexNet 之所以能够成功，深度学习之所以能够重回历史舞台，原因在于：</p>
<ul>
<li>非线性激活函数：ReLU</li>
<li>防止过拟合的方法：Dropout，Data augmentation（数据扩充）</li>
<li>大数据训练：百万级 ImageNet 图像数据</li>
<li>其他：多 GPU 实现，LRN 归一化层的使用</li>
</ul>
<p>下面我们将开始详细介绍一下 AlexNet 的一些细节：</p>
<hr>
<h6 id="3-1-1-使用-ReLU-激活函数"><a href="#3-1-1-使用-ReLU-激活函数" class="headerlink" title="3.1.1 使用 ReLU 激活函数"></a>3.1.1 使用 ReLU 激活函数</h6><p>传统的神经网络普遍使用 Sigmoid 或者 tanh 等非线性函数作为激励函数，然而它们容易出现梯度弥散或梯度饱和的情况。</p>
<p>以 Sigmoid 函数为例: Sigmoids saturate and kill gradients, 当输入的值非常大或者非常小的时候，这些神经元的梯度接近于 0（梯度饱和现象），如果输入的初始值很大的话，梯度在反向传播时因为需要乘上一个 Sigmoid 导数，会造成梯度越来越小，导致网络变的很难学习。详见博客：<a href="https://my.oschina.net/u/876354/blog/1624376" target="_blank" rel="noopener">深度学习中常用的激励函数</a> ）。</p>
<p>在 AlexNet 中，使用了 ReLU （Rectified Linear Units）激励函数，该函数的公式为：f(x)=max(0,x)，当输入信号 <0 时，输出都是="" 0，当输入信号="">0 时，输出等于输入，如下图所示：</0></p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./ReLU.png" alt="avatar"></p>
<p>使用 ReLU 替代 Sigmoid/tanh，主要是因为它是 linear，而且 non-saturating（因为ReLU的导数始终是1），相比于 sigmoid/tanh，ReLU 只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算。计算量大大减少，收敛速度会比 Sigmoid/tanh 快很多，如下图所示：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./Relu_tanh.png" alt="avatar"></p>
<hr>
<h6 id="3-1-2-数据扩充（Data-augmentation）"><a href="#3-1-2-数据扩充（Data-augmentation）" class="headerlink" title="3.1.2 数据扩充（Data augmentation）"></a>3.1.2 数据扩充（Data augmentation）</h6><p>有一种观点认为神经网络是靠数据喂出来的，如果能够增加训练数据，提供海量数据进行训练，则能够有效提升算法的准确率，因为这样可以避免过拟合，从而可以进一步增大、加深网络结构。</p>
<p>而当训练数据有限时，可以通过一些变换从已有的训练数据集中生成一些新的数据，以快速地扩充训练数据。<br>其中，最简单、通用的图像数据变形的方式：水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换，如下图所示：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./data_augmentation.png" alt="avatar"></p>
<p>AlexNet 在训练时，在数据扩充（data augmentation）这样处理：</p>
<p>1）随机裁剪，对 256×256 的图片进行随机裁剪到 224×224，然后进行水平翻转，相当于将样本数量增加了（（256-224）^2）×2=2048倍；</p>
<p>2）测试的时候，对左上、右上、左下、右下、中间分别做了5次裁剪，然后翻转，共10个裁剪，之后对结果求平均。作者认为，如果不做随机裁剪，大网络基本上都过拟合；</p>
<p>3）对 RGB 空间做 PCA（主成分分析），然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换，结果使错误率又下降了1%。</p>
<hr>
<h6 id="3-1-3-重叠池化-Overlapping-Pooling"><a href="#3-1-3-重叠池化-Overlapping-Pooling" class="headerlink" title="3.1.3 重叠池化 (Overlapping Pooling)"></a>3.1.3 重叠池化 (Overlapping Pooling)</h6><p>前面 LeNet-5 中，池化（Pooling）是不重叠的，池化区域的窗口大小与步长相同，如下图所示：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./CNN_pooling.png" alt="avatar"></p>
<p>在 AlexNet 中使用的池化（Pooling）却是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的窗口长度。AlexNet 池化的大小为 3×3 的正方形，每次池化移动步长为 2，这样就会出现重叠。重叠池化可以避免过拟合，这个策略贡献了 0.3% 的Top-5错误率。</p>
<hr>
<h6 id="3-1-4-局部归一化（Local-Response-Normalization，简称LRN）"><a href="#3-1-4-局部归一化（Local-Response-Normalization，简称LRN）" class="headerlink" title="3.1.4 局部归一化（Local Response Normalization，简称LRN）"></a>3.1.4 局部归一化（Local Response Normalization，简称LRN）</h6><p>一句话概括：本质上，这个层也是为了防止激活函数的饱和的。</p>
<p>在神经生物学有一个概念叫做“侧抑制”（lateral inhibitio），指的是被激活的神经元抑制相邻神经元。归一化（normalization）的目的是“抑制”，局部归一化就是借鉴了“侧抑制”的思想来实现局部抑制，尤其当使用 ReLU 时这种“侧抑制”很管用，因为 ReLU 的响应结果是无界的（可以非常大），所以需要归一化。使用局部归一化的方案有助于增加泛化能力。</p>
<blockquote>
<p>个人理解原理是通过正则化让激活函数的输入靠近“碗”的中间(避免饱和)，从而获得比较大的导数值。所以从功能上说，跟 ReLU 是重复的。不过作者说，从试验结果看，LRN 操作可以提高网络的泛化能力，将错误率降低了大约 1 个百分点。</p>
</blockquote>
<p>LRN 的公式如下，核心思想就是利用临近的数据做归一化，这个策略贡献了 1.2% 的 Top-5 错误率。</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./LRN.png" alt="avatar"></p>
<hr>
<h6 id="3-1-5-Dropout"><a href="#3-1-5-Dropout" class="headerlink" title="3.1.5 Dropout"></a>3.1.5 Dropout</h6><p>引入 Dropout 主要是为了防止过拟合。在神经网络中 Dropout 通过修改神经网络本身结构来实现。</p>
<p>假设对于某一层的神经元，通过定义以 0.5 的概率，将每个隐层神经元的输出设置为零。以这种方式 “dropped out” 的神经元既不参与前向传播，也不参与反向传播。就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为 0），直至训练结束。</p>
<p>同时，Dropout 也可以看成是一种模型组合。通常结合预先训练好的许多不同模型（迁移学习），来进行预测是一种非常成功的减少测试误差的方式（Ensemble）。但因为每个模型的训练都需要花了好几天时间，因此这种做法对于大型神经网络来说太过昂贵。然而，AlexNet 提出的 Dropout 是一个非常有效的模型组合版本，它在训练中只需要花费 <strong>两倍</strong> 于单模型的时间即可实现模型组合（类似取平均）的效果，非常高效。</p>
<p>Dropout 应该算是 AlexNet 中一个很大的创新，以至于“神经网络之父” Hinton 在后来很长一段时间里的演讲中都拿 Dropout 说事。如下图所示通过 Dropout 实现模型组合：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./Dropout.png" alt="avatar"></p>
<hr>
<h6 id="3-1-6-多-GPU-训练"><a href="#3-1-6-多-GPU-训练" class="headerlink" title="3.1.6 多 GPU 训练"></a>3.1.6 多 GPU 训练</h6><p>AlexNet 当时使用了 GTX580 的 GPU 进行训练，由于单个 GTX 580 GPU 只有 3GB 内存，这限制了在其上训练的网络的最大规模，因此他们在每个 GPU 中放置一半核（或神经元），将网络分布在两个 GPU 上进行并行计算，大大加快了 AlexNet 的训练速度。</p>
<p>目前的 GPU 特别适合跨 GPU 并行化，因为它们能够直接从另一个 GPU 的内存中读出和写入，不需要通过主机内存。</p>
<blockquote>
<p>他们采用的并行方案是：在每个 PU 中放置一半核（或神经元），还有一个额外的技巧：GPU 间的通讯只在某些层进行。</p>
</blockquote>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet.jpg" alt="avatar"></p>
<p>例如，第 3 层的核需要从第2层中所有核映射输入。然而，第 4 层的核只需要从第3层中位于同一 GPU 的那些核映射输入。</p>
<hr>
<h5 id="3-2-AlexNet-整体网络架构解析"><a href="#3-2-AlexNet-整体网络架构解析" class="headerlink" title="3.2 AlexNet 整体网络架构解析"></a>3.2 AlexNet 整体网络架构解析</h5><p>首先给出 AlexNet 网络架构示意图：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_extend.png" alt="avatar"></p>
<p>可以看出，AlexNet 网络结构共有 $8$ 层，前面 $5$ 层是卷积层（其实不完全是卷积层，有些层还加入了池化层，并对数据进行标准化处理。），后面 $3$ 层是全连接层，最后一个全连接层的输出传递给一个 $1000​$ 个节点的 softmax 层，对应 1000 个类标签的分布。</p>
<p>由于 AlexNet 采用了两个 GPU 进行训练，因此，该网络结构图由上下两部分组成：一个 GPU 运行图上方的层，另一个运行图下方的层，两个 GPU 只在特定的层通信。</p>
<p>例如第二、四、五层卷积层的核只和同一个 GPU 上的前一层的核特征图相连，第三层卷积层和第二层所有的核特征图相连接，全连接层中的神经元和前一层中的所有神经元相连接。</p>
<p>下面开始逐层解析 AlexNet 架构：</p>
<hr>
<h6 id="1-–-gt-Conv1"><a href="#1-–-gt-Conv1" class="headerlink" title="1 –&gt; Conv1"></a>1 –&gt; Conv1</h6><p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_1.png" alt="avatar"></p>
<p>该层的处理流程为：卷积–&gt;ReLU–&gt;池化–&gt;归一化，流程图如下：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_1_pic.png" alt="avatar"></p>
<p>1_1）卷积</p>
<p>输入的原始图像大小为 $224×224×3​$（RGB图像），在训练时会经过预处理变为 $227×227×3​$。</p>
<p>在本层使用 $96​$ 个 $11×11×3​$ 的卷积核进行卷积计算，生成新的像素。由于采用了两个 GPU 并行运算，因此，网络结构图中上下两部分分别承担了 $48​$ 个卷积核的运算。</p>
<p>–&gt; 特征图</p>
<p>卷积核沿图像按一定的步长往 x 轴方向、y 轴方向移动计算卷积，然后生成新的特征图，其大小为：</p>
<p>$$floor((img\_size - filter\_size)/stride) +1 = new\_feture\_size$$</p>
<p>其中 $floor$ 表示向下取整，$img\_size$ 为图像大小，$filter\_size$ 为核大小，$stride$ 为步长，$new\_feture\_size$ 为卷积后的特征图大小，这个公式表示图像尺寸减去卷积核尺寸除以步长，再加上被减去的核大小像素对应生成的一个像素，结果就是卷积后特征图的大小。</p>
<p>AlexNet 中本层的卷积移动步长是 $4​$ 个像素，卷积核经移动计算后生成的特征图大小为 $(227-11)/4+1=55​$，即 $55×55​$。</p>
<p>1_2）ReLU</p>
<p>卷积后的 $55×55$ 像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 $55×55×48$ 的像素层数据。</p>
<p>1_3）池化（降采样）</p>
<p>RuLU后的像素层再经过池化运算，池化运算的尺寸为 $3×3​$，步长为 $2​$，则池化后图像的尺寸为 $(55-3)/2+1=27​$，即池化后像素的规模为 $27×27×96​$。</p>
<p>1_4）归一化</p>
<p>池化后的像素层再进行归一化处理，归一化运算的尺寸为 5×5，归一化后的像素规模不变，仍为 $27×27×96​$。</p>
<p>这 96 层像素层（feature map）被分为两组，每组 48 个像素层，分别在一个独立的 GPU 上进行运算。</p>
<hr>
<h6 id="2-–-gt-Conv2"><a href="#2-–-gt-Conv2" class="headerlink" title="2 –&gt; Conv2"></a>2 –&gt; Conv2</h6><p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_2.png" alt="avatar"></p>
<p>该层与第一层类似，处理流程为：卷积–&gt;ReLU–&gt;池化–&gt;归一化，流程图如下：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_2_pic.png" alt="avatar"></p>
<p>2_1）卷积</p>
<p>第二层的输入数据为第一层输出的 $27×27×96$ 的像素层（被分成两组 $27×27×48$ 的 feature map 放在两个不同 GPU 中进行运算）。</p>
<p>–&gt; 特征图</p>
<p>为方便后续处理，在这里每幅像素层的上下左右边缘都被填充了 2 个像素（填充0），即图像的大小变为 $(27+2+2) ×(27+2+2)$。第二层的卷积核大小为 $5×5$，移动步长为 1 个像素，跟第一层第（1）点的计算公式一样，经卷积核计算后的像素层大小变为 $(27+2+2-5)/1+1=27$，即卷积后大小为 $27×27$ 。</p>
<p>本层使用了 $256$ 个 $5×5×48$ 的卷积核，同样也是被分成两组，每组为 $128$ 个，分给两个 GPU 进行卷积运算，结果生成两组 $27×27×128$ 个卷积后的像素层。</p>
<p>2_2）ReLU</p>
<p>这些像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为两组 $27×27×128$ 的像素层。</p>
<p>2_3）池化（降采样）</p>
<p>再经过池化运算的处理，池化运算的尺寸为 $3×3$，步长为 2，池化后图像的尺寸为 $(57-3)/2+1=13$，即池化后像素的规模为 2 组 $13×13×128$ 的像素层。</p>
<p>2_4）归一化</p>
<p>然后再经归一化处理，归一化运算的尺度为 5×5，归一化后的像素层的规模为 2 组 $13×13×128$ 的像素层，分别由 2 个 GPU 进行运算。</p>
<hr>
<h6 id="3-–-gt-Conv3"><a href="#3-–-gt-Conv3" class="headerlink" title="3 –&gt; Conv3"></a>3 –&gt; Conv3</h6><p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_3.png" alt="avatar"></p>
<p>第三层的处理流程为：卷积–&gt;ReLU</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_3_pic.png" alt="avatar"></p>
<p>3_1）卷积</p>
<p>第三层输入数据为第二层输出的 2 组 $13×13×128$ 的像素层。这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 $3×3×256$。</p>
<p>为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后变为 $(13+1+1)×(13+1+1)×128$，分布在两个 GPU 中进行运算。</p>
<p>注意，Conv3 卷积层和 Conv2 所有的特征图相连接，因此，每个 GPU 中的卷积核都能对 2 组 $13×13×128$ 的像素层的所有数据进行卷积运算。如该层的结构图所示，两个 GPU 有通过交叉的虚线连接，也就是说每个 GPU 要处理来自前一层的所有 GPU 的输入。</p>
<p>–&gt; 特征图</p>
<p>本层卷积的步长是 1 个像素，经过卷积运算后的尺寸为 $(13+1+1-3)/1+1=13$，即每个 GPU 中共 $192$ 个卷积核，2 个 GPU 中共有 $13×13×384$ 个卷积后的像素层（feature map）。</p>
<p>3_2）ReLU</p>
<p>卷积后的像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 $13×13×192$ 的像素层，分配给两组 GPU 处理。</p>
<hr>
<h6 id="4-–-gt-Conv4"><a href="#4-–-gt-Conv4" class="headerlink" title="4 –&gt; Conv4"></a>4 –&gt; Conv4</h6><p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_4.png" alt="avatar"></p>
<p>与第三层类似，第四层的处理流程为：卷积–&gt;ReLU</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_4_pic.png" alt="avatar"></p>
<p>4_1）卷积</p>
<p>第四层输入数据为第三层输出的 2 组 $13×13×192$ 的像素层，这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 $3×3×192$（与第三层不同，第四层的 GPU 之间没有虚线连接，也即 GPU 之间没有通信）。</p>
<p>–&gt; 特征图</p>
<p>类似于第三层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 $(13+1+1)×(13+1+1)×192$，分布在两个 GPU 中进行运算。</p>
<p>卷积的移动步长是 1 个像素，经卷积运算后的尺寸为 $(13+1+1-3)/1+1=13$，每个 GPU 中有 $192$ 个卷积核，2个 GPU 卷积后生成 $13×13×384​$ 的像素层（feature map）。</p>
<p>4_2）ReLU</p>
<p>卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 $13×13×192$ 像素层，分配给两个 GPU 处理。</p>
<hr>
<h6 id="5-–-gt-Conv5"><a href="#5-–-gt-Conv5" class="headerlink" title="5 –&gt; Conv5"></a>5 –&gt; Conv5</h6><p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_5.png" alt="avatar"></p>
<p>第五层的处理流程为：卷积–&gt;ReLU–&gt;池化</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_5_pic.png" alt="avatar"></p>
<p>5_1）卷积</p>
<p>第五层输入数据为第四层输出的 2 组 $13×13×192$ 的像素层，这一层中每个 GPU 都有 128 个卷积核，每个卷积核的尺寸是 $3×3×192$ 。</p>
<p>为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 $(13+1+1)×(13+1+1)$ ，2 组像素层数据被送至 2 个不同的 GPU 中进行运算。</p>
<p>卷积的步长是 1 个像素，经卷积后的尺寸为 $(13+1+1-3)/1+1=13$，每个 GPU 中有 $13×13×128$ 个卷积核，2 个 GPU 卷积后生成 $13×13×256​$ 的像素层。</p>
<p>5_2）ReLU</p>
<p>卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 $13×13×128$ 像素层，由两个 GPU 分别处理。</p>
<p>5_3）池化（降采样）</p>
<p>2 组 $13×13×128$ 像素层分别在 2 个不同 GPU 中进行池化运算处理，池化运算的尺寸为 $3×3$，步长为 2，池化后图像的尺寸为 $(13-3)/2+1=6$，即池化后像素的规模为两组 $6×6×128$ 的像素层数据，共有 $6×6×256$ 的像素层数据。</p>
<hr>
<h6 id="6-–-gt-FC6"><a href="#6-–-gt-FC6" class="headerlink" title="6 –&gt; FC6"></a>6 –&gt; FC6</h6><p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_6.png" alt="avatar"></p>
<p>第六层的处理流程为：卷积（全连接）–&gt;ReLU–&gt;Dropout</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_6_pic.png" alt="avatar"></p>
<p>6_1）卷积或全卷积层（全连接）</p>
<p>第六层输入数据是第五层的输出，尺寸为 $6×6×256$。本层共有 4096 个卷积核，每个卷积核的尺寸为 $6×6×256$。由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层也被称为全卷积层（可以被看作全连接层）。</p>
<p>由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层为 $4096×1×1$ ，即有 $4096$ 个神经元。</p>
<p>6_2）ReLU</p>
<p>这 4096 个运算结果通过 ReLU 激活函数生成 $4096$ 个值。</p>
<p>6_3）Dropout</p>
<p>然后再通过 Dropout 运算，输出 $4096$ 个结果值。</p>
<hr>
<h6 id="7-–-gt-FC7"><a href="#7-–-gt-FC7" class="headerlink" title="7 –&gt; FC7"></a>7 –&gt; FC7</h6><p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_7.png" alt="avatar"></p>
<p>第七层的处理流程为：全连接–&gt;ReLU–&gt;Dropout</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_7_pic.png" alt="avatar"></p>
<p>第六层输出的 4096 个数据与第七层的 4096 个神经元进行全连接，然后经 ReLU 进行处理后生成 4096 个数据，再经过 Dropout 处理后输出 $4096$ 个数据。</p>
<hr>
<p>8 –&gt; FC8</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_8.png" alt="avatar"></p>
<p>第八层的处理流程为：全连接</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_with_8_pic.png" alt="avatar"></p>
<p>第七层输出的 4096 个数据与第八层的 1000 个神经元进行全连接，经过训练后输出 $1000$ 个 float 型的值，这就是预测结果。</p>
<hr>
<p>以上就是关于 AlexNet 网络结构图的逐层解析了，看起来挺复杂的。下面是一个 AlexNet 架构简图，看起来就清爽很多啊：</p>
<p><img src="/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/./AlexNet_pic.jpeg" alt="avatar"></p>
<p>–&gt; 图中各卷积层参数（Params）以及连接数（FLOPs）如何计算？</p>
<p>这里我们给出第一层的计算公式:</p>
<p>$Params = 11 × 11 × 3 × 48 × 2 = 34,848 ≈ 35K​$</p>
<p>$FLOPs = Params × 55 × 55 = 34848 × 55 × 55 = 105,415,200 ≈ 105M$</p>
<p>其它层计算类似。</p>
<p>Alex 等人在 2012 年发表了关于 AlexNet 的经典论文 《ImageNet Classification with Deep Convolutional Neural Networks》（基于深度卷积神经网络的 ImageNet 分类），整个过程都有详细的介绍，建议阅读这篇论文，进一步了解细节。链接见： [ <a href="https://github.com/SnailTyan/deep-learning-papers-translation" target="_blank" rel="noopener">https://github.com/SnailTyan/deep-learning-papers-translation</a> ]</p>
<hr>
<h5 id="3-3-AlexNet-的-TensorFlow-实现"><a href="#3-3-AlexNet-的-TensorFlow-实现" class="headerlink" title="3.3 AlexNet 的 TensorFlow 实现"></a>3.3 AlexNet 的 TensorFlow 实现</h5><p>附一个 AlexNet  进行 <a href="https://www.cnblogs.com/vipyoumay/p/7686230.html" target="_blank" rel="noopener">TensorFlow 搭建完整 AlexNet </a> 的链接。</p>

      
    </div>
    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">如果感觉文章对您有较大帮助，请随意打赏。您的鼓励是我保持持续创作的最大动力！</div>
    
</div>
      
    </div>

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/uploads/wechatpay.png" alt="TheMusicIsLoud 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/uploads/alipay.png" alt="TheMusicIsLoud 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    TheMusicIsLoud
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/" title="卷积神经网络系列之从 LeNet 到 AlexNet">http://yoursite.com/Convlutional-Neural-Networks/卷积神经网络系列之从-LeNet-到-AlexNet/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> TensorFlow</a>
          
            <a href="/tags/CNN/" rel="tag"><i class="fa fa-tag"></i> CNN</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Convlutional-Neural-Networks/卷积神经网络系列之-CNN-进化史/" rel="next" title="卷积神经网络系列之 CNN 进化史">
                <i class="fa fa-chevron-left"></i> 卷积神经网络系列之 CNN 进化史
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Convlutional-Neural-Networks/卷积神经网络系列：CNN-演变之路之网络结构加深/" rel="prev" title="卷积神经网络系列：CNN 演变之路之网络结构加深">
                卷积神经网络系列：CNN 演变之路之网络结构加深 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微博</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript">
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80MjA5OC8xODY0NQ=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="TheMusicIsLoud">
            
              <p class="site-author-name" itemprop="name">TheMusicIsLoud</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">75</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">87</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/TheNightIsYoung" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://dev.tencent.com/" title="CloudStudio&&Coding" target="_blank">CloudStudio&&Coding</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#从-LeNet-到-AlexNet"><span class="nav-text">从 LeNet 到 AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-CNN回顾"><span class="nav-text">1. CNN回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-局部感知"><span class="nav-text">1.1 局部感知</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-参数（权值）共享"><span class="nav-text">1.2 参数（权值）共享</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-池化"><span class="nav-text">1.3 池化</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-一切的开始：LeNet"><span class="nav-text">2. 一切的开始：LeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-LeNet-5-整体网络架构"><span class="nav-text">2.1 LeNet-5 整体网络架构</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#0-–-gt-INPUT-32-×-32"><span class="nav-text">0 –&gt; INPUT ( 32 × 32 )</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-–-gt-C1（卷积层）：6-28×28"><span class="nav-text">1 –&gt; C1（卷积层）：6@28×28</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-–-gt-S2（下采样层，也称池化层）：6-14×14"><span class="nav-text">2 –&gt; S2（下采样层，也称池化层）：6@14×14</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-–-gt-C3（卷积层）：16-10×10"><span class="nav-text">3 –&gt; C3（卷积层）：16@10×10</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4-–-gt-S4（下采样层，也称池化层）：16-5×5"><span class="nav-text">4 –&gt; S4（下采样层，也称池化层）：16@5×5</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#5-–-gt-C5层（全卷积层）：120"><span class="nav-text">5 –&gt; C5层（全卷积层）：120</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#6-–-gt-F6层（全连接层）：84"><span class="nav-text">6 –&gt; F6层（全连接层）：84</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#7-–-gt-OUTPUT层（输出层）：10"><span class="nav-text">7 –&gt; OUTPUT层（输出层）：10</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-LeNet-5-中字符-“3”-的识别"><span class="nav-text">2.2 LeNet-5 中字符 “3” 的识别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-LetNet-5-的-TensorFlow-实现"><span class="nav-text">2.3 LetNet-5 的 TensorFlow 实现</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-王者回归：AlexNet"><span class="nav-text">3. 王者回归：AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-AlexNet-模型的特点"><span class="nav-text">3.1 AlexNet 模型的特点</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#3-1-1-使用-ReLU-激活函数"><span class="nav-text">3.1.1 使用 ReLU 激活函数</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-1-2-数据扩充（Data-augmentation）"><span class="nav-text">3.1.2 数据扩充（Data augmentation）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-1-3-重叠池化-Overlapping-Pooling"><span class="nav-text">3.1.3 重叠池化 (Overlapping Pooling)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-1-4-局部归一化（Local-Response-Normalization，简称LRN）"><span class="nav-text">3.1.4 局部归一化（Local Response Normalization，简称LRN）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-1-5-Dropout"><span class="nav-text">3.1.5 Dropout</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-1-6-多-GPU-训练"><span class="nav-text">3.1.6 多 GPU 训练</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-AlexNet-整体网络架构解析"><span class="nav-text">3.2 AlexNet 整体网络架构解析</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-–-gt-Conv1"><span class="nav-text">1 –&gt; Conv1</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-–-gt-Conv2"><span class="nav-text">2 –&gt; Conv2</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-–-gt-Conv3"><span class="nav-text">3 –&gt; Conv3</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4-–-gt-Conv4"><span class="nav-text">4 –&gt; Conv4</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#5-–-gt-Conv5"><span class="nav-text">5 –&gt; Conv5</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#6-–-gt-FC6"><span class="nav-text">6 –&gt; FC6</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#7-–-gt-FC7"><span class="nav-text">7 –&gt; FC7</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-AlexNet-的-TensorFlow-实现"><span class="nav-text">3.3 AlexNet 的 TensorFlow 实现</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TheMusicIsLoud</span>

  
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>

-->


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info//busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      本站总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("L40cS1OTf2nXQmbIANou8HvS-gzGzoHsz", "t0xHBc4DURRDc9MDSKX7vx8c");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
