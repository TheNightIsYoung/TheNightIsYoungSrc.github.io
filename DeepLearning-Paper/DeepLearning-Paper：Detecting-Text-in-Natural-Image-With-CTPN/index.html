<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">

  <script>
    (function(){
        if(''){
            if (prompt('请输入密码') !== ''){
                alert('密码错误');
                history.back();
            }
        }
    })();
</script>







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="DeepLearning Paper,CTPN,">





  <link rel="alternate" href="/atom.xml" title="When Art Meets Technology" type="application/atom+xml">






<meta name="description" content="愿你每天欢喜多于悲，孤独有人陪…   阅读指南 推荐在查看这篇论文之前，大家先去了解图像目标检测与识别项目 Faster Region-CNN（R-CNN）的相关实现原理，CTPN 的提出和实现受 Faster R-CNN 影响很大。熟知 Faster R-CNN 算法原理和实现可以帮助你更快、更准确的理解文中内容。 声明：本文 “ CTPN Paper 中英文对照与细节解读 ” 仅供交流学习，">
<meta name="keywords" content="DeepLearning Paper,CTPN">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearning Paper：Detecting Text in Natural Image With CTPN">
<meta property="og:url" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/index.html">
<meta property="og:site_name" content="When Art Meets Technology">
<meta property="og:description" content="愿你每天欢喜多于悲，孤独有人陪…   阅读指南 推荐在查看这篇论文之前，大家先去了解图像目标检测与识别项目 Faster Region-CNN（R-CNN）的相关实现原理，CTPN 的提出和实现受 Faster R-CNN 影响很大。熟知 Faster R-CNN 算法原理和实现可以帮助你更快、更准确的理解文中内容。 声明：本文 “ CTPN Paper 中英文对照与细节解读 ” 仅供交流学习，">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/text_proposal.png">
<meta property="og:image" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/CTPN_construct.png">
<meta property="og:image" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/RPN_text_proposal.png">
<meta property="og:image" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/anchor.jpg">
<meta property="og:image" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/RNN_text_proposal.png">
<meta property="og:image" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/side_refinement.png">
<meta property="og:updated_time" content="2019-06-13T07:40:15.567Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepLearning Paper：Detecting Text in Natural Image With CTPN">
<meta name="twitter:description" content="愿你每天欢喜多于悲，孤独有人陪…   阅读指南 推荐在查看这篇论文之前，大家先去了解图像目标检测与识别项目 Faster Region-CNN（R-CNN）的相关实现原理，CTPN 的提出和实现受 Faster R-CNN 影响很大。熟知 Faster R-CNN 算法原理和实现可以帮助你更快、更准确的理解文中内容。 声明：本文 “ CTPN Paper 中英文对照与细节解读 ” 仅供交流学习，">
<meta name="twitter:image" content="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/text_proposal.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/">






  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "7e6ff6a0"
    });
  daovoice('update');
  </script>

  <title>DeepLearning Paper：Detecting Text in Natural Image With CTPN | When Art Meets Technology</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  
    <script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">When Art Meets Technology</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="TheMusicIsLoud">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="When Art Meets Technology">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DeepLearning Paper：Detecting Text in Natural Image With CTPN</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-03T12:00:12+08:00">
                2018-09-03
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-06-13T15:40:15+08:00">
                2019-06-13
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/DeepLearning-Paper/" itemprop="url" rel="index">
                    <span itemprop="name">DeepLearning Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/" class="leancloud_visitors" data-flag-title="DeepLearning Paper：Detecting Text in Natural Image With CTPN">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读热度&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
                 <span>次</span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  16.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  73
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <center> 愿你每天欢喜多于悲，孤独有人陪… </center>

<p><strong>阅读指南</strong></p>
<p>推荐在查看这篇论文之前，大家先去了解图像目标检测与识别项目 Faster Region-CNN（R-CNN）的相关实现原理，CTPN 的提出和实现受 Faster R-CNN 影响很大。熟知 Faster R-CNN 算法原理和实现可以帮助你更快、更准确的理解文中内容。</p>
<p><strong>声明：</strong>本文 “ CTPN Paper 中英文对照与细节解读 ” 仅供交流学习，如有侵权请联系删除，谢谢！</p>
<a id="more"></a>
<hr>
<h2 id="Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network"><a href="#Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network" class="headerlink" title="Detecting Text in Natural Image with Connectionist Text Proposal Network"></a>Detecting Text in Natural Image with Connectionist Text Proposal Network</h2><p><strong>Paper Name： </strong>使用连接文本提议网络实现自然场景文本检测</p>
<p><strong>Paper Authors：</strong>Zhi Tian, Weilin Huang, Tong He, Pan He, and Yu Qiao, …</p>
<p><strong>Release Time：</strong>12 Sep 2016</p>
<hr>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin. The CTPN is computationally efficient with 0.14s/image, by using the very deep VGG16 model [27]. Online demo is available at: <a href="http://textdet.com/" target="_blank" rel="noopener">http://textdet.com/</a>.</p>
<p><strong>Keywords</strong></p>
<p>Scene text detection, convolutional network, recurrent neural network, anchor mechanism</p>
<p><strong>|↓↓↓ 中文对照 ↓↓↓|</strong></p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>我们提出了一种新颖的 <strong>连接文本提议网络（CTPN）</strong>，它能够准确定位自然图像中的文本行。CTPN 直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。我们开发了一种 <strong>垂直锚点机制</strong>，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。 <strong>序列提议</strong> 通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。这使得 CTPN 可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。CTPN 在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。它在 ICDAR 2013 和 2015 的基准数据集上达到了 0.88 和 0.61 的 F-measure，大大超过了最近的结果 [8，35]。通过使用非常深的 VGG16 模型 [27]，CTPN 的计算效率为 <strong>0.14s</strong> 每张图像。在线演示获取地址：<a href="http://textdet.com/。" target="_blank" rel="noopener">http://textdet.com/。</a></p>
<p><strong>关键词</strong></p>
<p>场景文本检测，卷积网络，循环神经网络，锚点机制</p>
<hr>
<p>|↓↓↓↓↓↓↓↓↓↓ Get Start ↓↓↓↓↓↓↓↓↓↓|</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>Reading text in natural image has recently attracted increasing attention in computer vision [8,14,15,10,35,11,9,1,28,32]. This is due to its numerous practical applications such as image OCR, multi-language translation, image retrieval, etc. It includes two sub tasks: text detection and recognition. This work focus on the detection task [14,1,28,32], which is more challenging than recognition task carried out on a well-cropped word image [15,9]. Large variance of text patterns and highly cluttered background pose main challenge of accurate text localization.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>在自然图像中阅读文本最近在计算机视觉中引起越来越多的关注 [8，14，15，10，35，11，9，1，28，32]。这是由于它的许多实际应用，如：图像 OCR（Optical Character Recognition），多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。这项工作的重点是检测任务 [14，1，28，32]，这是比在一个良好的裁剪字图像 [15，9] 进行的识别任务更具有挑战性。多变的文本模式和高度杂乱的背景构成了精确文本定位的主要挑战。</p>
<blockquote>
<p><strong>STR 背景与技术难点：</strong></p>
<p>目前图像文本检测和识别领域中（包括两个关键子任务：文本检测和识别），传统的扫描文档图像识别技术（OCR）已经很成熟。自然场景文本识别（Scene Text Recognition，STR）作为检测与识别自然场景图片中的文字信息备受关注。</p>
<p><strong>而相较于识别任务，STR 任务的重点是检测任务。</strong>然而由于其多样的文本模式（多语言、多样式文本）、高度杂乱的背景（干扰纹理：非文字区域有近似文字的纹理）或者自然因素（文本区域变形、残缺、模糊等）等的干扰，自然场景图像中的文字检测与识别任务，其难度远大于扫描文档图像或良好的裁剪字图像中的文字识别。</p>
</blockquote>
<p>–&gt; 大佬们对于传统的 STR 文本检测任务存在问题的思考：</p>
<p>Current approaches for text detection mostly employ a bottom-up pipeline [28,1,14,32,33]. They commonly start from low-level character or stroke detection, which is typically followed by a number of subsequent steps: non-text component filtering, text line construction and text line verification. These multi-step bottom-up approaches are generally complicated with less robustness and reliability. Their performance heavily rely on the results of character detection, and connected-components methods or sliding-window methods have been proposed. These methods commonly explore low-level features (e.g., based on SWT [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background. However, they are not robust by identifying individual strokes or characters separately, without context information. For example, it is more confident for people to identify a sequence of characters than an individual one, especially when a character is extremely ambiguous. These limitations often result in a large number of non-text components in character detection, causing main difficulties for handling them in following steps. Furthermore, these false detections are easily accumulated sequentially in bottom-up pipeline, as pointed out in [28]. To address these problems, we exploit strong deep features for detecting text information directly in convolutional maps. We develop text anchor mechanism that accurately predicts text locations in fine scale. Then, an in-network recurrent architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>目前的文本检测方法大多采用自下而上的流程 [28，1，14，32，33]。它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。它们的性能 <strong>很大程度上依赖于字符检测的结果</strong>，并且已经提出了<strong>连接组件方法或滑动窗口方法。</strong>这些方法通常探索低级特征（例如，基于 SWT [3，13]，MSER [14，33，23] 或 HoG [28] ）来区分候选文本和背景。但是，如果没有上下文信息，他们不能鲁棒的单独识别各个笔划或字符。例如，相比单个字符人们更信任一个字符序列，特别是当一个字符非常模糊时。这些限制在字符检测中通常会导致大量非文本组件，在后续步骤中的主要困难是处理它们。此外，正如 [28] 所指出的，这些误检很容易在自下而上的过程中连续累积。为了解决这些问题，我们利用强大的深度特征直接在卷积映射中检测文本信息。我们开发的文本锚点机制能在细粒度上精确预测文本位置。然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</p>
<blockquote>
<p><strong>STR 文本检测问题总结：</strong></p>
<p>上面提到 STR 任务的重点难点是文本检测任务。对于文本检测，大佬们指出时下（传统）文本检测大多采用 bottom-up（自底向上）的检测方法（先检测低级别字符或笔画，再构建成文本行/线（文本线：是一个由字符、局部字符、多字符组成序列 Sequence）。这种自底向上检测方法的存在很明显的缺陷：（1）它们的性能很大程度依赖于字符检查结果，当字符检测中产生误检时，这些误检很容易在自下而上的过程中累积，产生更多的误检；（2）没有考虑上下文信息，不能鲁棒的单独识别各个笔划或字符，可靠性差；（3）检测后需要多个后续的子模块来构建文本行/线，模型比较复杂。这些因素导致了模型性能受限。</p>
</blockquote>
<blockquote>
<p><strong>针对上述问题，提出的检测策略：</strong></p>
<p>基于以上问题的思考，受益于基于深度学习的一般目标检测最新进展 Faster R-CNN 系统。大佬们提出的是一种 Top-down（自上而下）的文本检测方法（先检测细粒度文本区域，再将其连接成文本行/线）。大佬们提出：使用图像文本信息的深度卷积特征来取代低级特征，使用文本锚点机制在细粒度精度上预测文本区域，并通过使用一种网内循环（RNN）架构探索其上下文信息，按顺序连接这些细粒度文本提议。</p>
</blockquote>
<hr>
<p>–&gt; 既然受益于基于深度学习的一般目标检测任务，那么，先来看其最新进展（Faster R-CNN 系统）：</p>
<p>Deep Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps. Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on generic object detection. However, it is difficult to apply these general object detection systems directly to scene text detection, which generally requires a higher localization accuracy. In generic object detection, each object has a well-defined closed boundary [2], while such a well-defined boundary may not exist in text, since a text line or word is composed of a number of separate characters or strokes. For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it. By contrast, reading text comprehensively is a fine-grained recognition task which requires a correct detection that covers a full region of a text line or word. Therefore, text detection generally requires a more accurate localization, leading to a different evaluation standard, e.g., the Wolf’s standard [30] which is commonly employed by text benchmarks [19,21].</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>深度卷积神经网络（CNN）最近已经基本实现了一般物体检测 [25，5，6]。最先进的方法是 Faster Region-CNN（R-CNN）系统 [25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成 <strong>高质量类别不可知的目标提议</strong>。然后将 RPN 提议输入 Faster R-CNN[5] 模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。然而，很难将这些通用目标检测系统直接应用于场景文本检测，这通常需要更高的定位精度。在通用目标检测中，每个目标都有一个明确的封闭边界 [2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL 标准 [4]）之间的重叠 &gt; 0.5，因为人们可以容易地从目标的主要部分识别它。相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的 Wolf 标准 [19，21]。</p>
<p>–&gt; 大佬们发现，一般目标检测算法对于文本目标检测存在问题：</p>
<blockquote>
<p><strong>Faster Region-CNN（R-CNN）对于图像文本区域检测的不适用：</strong></p>
<p>同属于图像目标检测，从一般物体区域目标检测到文本区域目标检测，大佬们借鉴 Faster R-CNN 系统的设计想法。但若直接采用基于 Faster Region-CNN 等通用物体检测框架的算法都会面临一个问题：<strong>如何生成好的 text region proposal ?</strong>（不同于一般物体检测的松散定义，文本检测通常需要更准确的定义），这实际上比较难解决。</p>
</blockquote>
<hr>
<p>–&gt; 大佬们提出的基于深度学习的 STR 文本检测方案简要思路如下：</p>
<p>In this work, we fill this gap by extending the RPN architecture [25] to accurate text line localization. We present several technical developments that tailor generic object detection model elegantly towards our problem. We strive for a further step by proposing an in-network recurrent mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>在这项工作中，我们通过将 RPN 架构 [25] 扩展到准确的文本行定义（细粒度文本区域提议）来填补这个空白。我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的 CNN 检测模型进行进一步的后处理。</p>
<p>–&gt; 垂直锚点机制和网内循环实现良好的文本区域检测：</p>
<blockquote>
<p>基于 “ 如何生成好的 text region proposal ？ ”，大佬们将 Faster R-CNN 中的 RPN 架构扩展到精准的文本行定义（将 RPN 锚点机制生成一般目标区域提议扩展成文本垂直锚点机制（细粒度文本区域提议））。大佬们的思路是检测一个一个小的，固定宽度的文本段（文本提议），然后在后续处理部分再将这些小的文本段连接起来获得文本行。检测到的文本段的示意图如下图所示：</p>
</blockquote>
<p><img src="/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/./text_proposal.png" alt="avatar"></p>
<p>–&gt; 大佬们提出的文本检测方案与传统 STR 文本检测方案区别：</p>
<blockquote>
<p>可以发现，与传统文本检测方法相比，就如我们前面提到的：使用深度卷积特征替换了低级特征；直接通过文本垂直锚点机制给出细粒度文本区域提议，不再做字符检测；使用网内循环编码文本上下文信息使得检测的文本行更鲁棒。</p>
</blockquote>
<hr>
<p>–&gt; 给出 Paper 的贡献：提出一种 Top-down（自上而下）的新颖的文本检测方案（CTPN）</p>
<h4 id="1-1-Contributions"><a href="#1-1-Contributions" class="headerlink" title="1.1 Contributions"></a>1.1 Contributions</h4><p>We propose a novel Connectionist Text Proposal Network (CTPN) that directly localizes text sequences in convolutional layers. This overcomes a number of main limitations raised by previous bottom-up approaches building on character detection. We leverage the advantages of strong deep convolutional features and sharing computation mechanism, and propose the CTPN architecture which is described in Fig. 1. It makes the following major contributions:</p>
<p>Fig. 1: (a) Architecture of the Connectionist Text Proposal Network (CTPN). We densely slide a 3×3 spatial window through the last convolutional maps (conv5 ) of the VGG16 model [27]. The sequential windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs). The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, y-axis coordinates and side-refinement offsets of k anchors. (b) The CTPN outputs sequential fixed-width fine-scale text proposals. Color of each box indicates the text/non-text score. Only the boxes with positive scores are presented.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h4 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h4><p>我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。这克服了以前的建立在字符检测基础上的自下而上方法带来的一些主要限制。我们利用强深度卷积特性和共享计算机制的优点，提出了如图 1 所示的 CTPN 架构。主要贡献如下：</p>
<p><img src="/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/./CTPN_construct.png" alt="avatar"></p>
<p>图1：（a）连接文本提议网络（CTPN）的架构。我们通过 VGG16 模型 [27] 的最后一个卷积映射（conv5）密集地滑动 3×3 空间窗口。每行的序列窗口通过双向 LSTM（BLSTM）[7] 循环连接，其中每个窗口的卷积特征（3×3×C）被用作 256 维的 BLSTM（包括两个 128 维的 LSTM ）的输入。RNN 层连接到 512 维的全连接层，接着是输出层，联合预测 k 个锚点的文本/非文本分数，y 轴坐标和边缘调整偏移。（b）CTPN 输出连续的固定宽度细粒度文本提议。每个框的颜色表示文本/非文本分数。只显示正分数对应的文本框。</p>
<hr>
<p>–&gt; CTPN 网络架构简要介绍：</p>
<p>First, we cast the problem of text detection into localizing a sequence of fine-scale text proposals. We develop an anchor regression mechanism that jointly predicts vertical location and text/non-text score of each text proposal, resulting in an excellent localization accuracy. This departs from the RPN prediction of a whole object, which is difficult to provide a satisfied localization accuracy.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>首先，我们将文本检测的问题转化为一系列细粒度的文本提议。我们开发了一个（垂直）锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。这背离了整个目标的 RPN 预测，RPN 预测难以提供令人满意的定位精度。</p>
<blockquote>
<p><strong>内容解读：</strong>关于细粒度的文本提议（text region proposal），作者提出了 Vertical Anchor 机制。</p>
<p>基本想法就是去预测文本的竖直方向上的位置，水平方向的位置不预测。与 Faster RCNN 中的 anchor 类似，但是不同的是，Vertical Anchor 的宽度都是固定好的了，本文后面你会看到，本文中 Vertical Anchor 宽度设置为 16 个像素。而高度则从 11 像素到 273 像素变化，总共 10 个 anchor.</p>
</blockquote>
<p>Second, we propose an in-network recurrence mechanism that elegantly connects sequential text proposals in the convolutional feature maps. This connection allows our detector to explore meaningful context information of text line, making it powerful to detect extremely challenging text reliably.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。通过这种连接，我们的检测器可以探索文本行有意义的上下文信息，使其能够可靠地检测极具挑战性的文本。</p>
<blockquote>
<p><strong>内容解读：</strong></p>
<p>关于序列提议，作者将 VGG16 Net conv5_3 卷积层的 feature map 中每行序列窗口的卷积映射（3×3×C）输入到 256 维的 BLSTM（包括两个 128 维的 LSTM ）网络中，用于编码上下文信息。</p>
</blockquote>
<p>Third, both methods are integrated seamlessly to meet the nature of text sequence, resulting in a unified end-to-end trainable model. Our method is able to handle multi-scale and multi-lingual text in a single process, avoiding further post filtering or refinement.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</p>
<blockquote>
<p> 注意，关于上述具体的实现细节可以参见本文第三部分。</p>
</blockquote>
<hr>
<p>–&gt; CTPN 文本检测网络有效性证明：</p>
<p>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015). Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep VGG16 model [27].</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88 的 F-measure 超过了 2013 年 ICDAR 的 [8] 中的 0.83，而 0.64 的 F-measure 超过了 ICDAR2015 上 [35] 中的 0.54 ）。此外，通过使用非常深的 VGG16 模型 [27]，这在计算上是高效的，导致了每张图像 0.14s 的运行时间（在 ICDAR 2013 上）。</p>
<hr>
<h3 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h3><p><strong>Text detection.</strong> Past works in scene text detection have been dominated by bottom-up approaches which are generally built on stroke or character detection. They can be roughly grouped into two categories, connected-components (CCs) based approaches and sliding-window based methods. The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3]. The sliding-window based methods detect character candidates by densely moving a multi-scale window through an image. The character or non-character window is discriminated by a pre-trained classifier, by using manually-designed features [28,29], or recent CNN features [16]. However, both groups of methods commonly suffer from poor performance of character detection, causing accumulated errors in following component filtering and text line construction steps. Furthermore, robustly filtering out non-character components or confidently verifying detected text lines are even difficult themselves [1,33,14]. Another limitation is that the sliding-window methods are computationally expensive, by running a classifier on a huge number of the sliding windows.</p>
<p>–&gt; 这一部分主要提到 Paper 图像文本检测与识别相关技术背景：<strong>（1）文本检测。（2）目标检测。</strong></p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h3 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h3><p><strong>1）文本检测。</strong></p>
<p>过去在场景文本检测中的工作一直以自下而上的方法为主，一般建立在笔画或字符检测上。它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。基于 CC 的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等 [33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。基于滑动窗口的方法通过在图像中密集地滑动多尺度窗口来检测候选字符。字符或非字符窗口通过预先训练的分类器，使用手动设计的特征 [28，29] 或最近的 CNN 特征 [16] 进行区分。然而，这两种方法通常都会受到较差的字符检测性能的影响，导致在接下来的组件过滤和文本行构建步骤中出现累积的错误。此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难 [1，33，14]。另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。</p>
<p><strong>Object detection. </strong>Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. A common strategy is to generate a number of object proposals by employing inexpensive low-level features, and then a strong CNN classifier is applied to further classify and refine the generated proposals. Selective Search (SS) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5]. Recently, Ren et al. [25] proposed a Faster R-CNN system for object detection. They proposed a Region Proposal Network (RPN) that generates high-quality class-agnostic object proposals directly from the convolutional feature maps. The RPN is fast by sharing convolutional computation. However, the RPN proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5]. More importantly, text is different significantly from general objects, making it difficult to directly apply general object detection system to this highly domain-specific task.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p><strong>2）目标检测。</strong></p>
<p>卷积神经网络（CNN）近来在通用目标检测 [25，5，6] 上已经取得了实质的进步。一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强 CNN 分类器来进一步对生成的提议进行分类和细化。生成类别不可知目标提议的选择性搜索（SS）[4] 是目前领先的目标检测系统中应用最广泛的方法之一，如 CNN（R-CNN）[6] 及其扩展 [5]。最近，Ren 等人 [25] 提出了 Faster R-CNN 目标检测系统。他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。通过共享卷积计算 RPN 是快速的。然而，RPN 提议不具有判别性，需要通过额外的成本高昂的 CNN 模型（如 Fast R-CNN 模型 [5]）进一步细化和分类。更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。</p>
<hr>
<p>–&gt; 重点来了…… CTPN 网络架构详解：</p>
<h3 id="3-Connectionist-Text-Proposal-Network"><a href="#3-Connectionist-Text-Proposal-Network" class="headerlink" title="3. Connectionist Text Proposal Network"></a>3. Connectionist Text Proposal Network</h3><p>This section presents details of the Connectionist Text Proposal Network (CTPN). It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent connectionist text proposals, and side-refinement.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h3 id="3-连接文本提议网络"><a href="#3-连接文本提议网络" class="headerlink" title="3. 连接文本提议网络"></a>3. 连接文本提议网络</h3><p>本节介绍连接文本提议网络（CTPN）的细节。它包括三个关键的贡献，使文本定位可靠和准确：（3.1）检测细粒度提议中的文本；（3.2）循环连接文本提议；（3.3）边缘细化。</p>
<blockquote>
<p>通过上文叙述，我们已经大致了解了大佬们提出的连接文本提议网络实现架构。这一部分我们来看其三个关键结构实现细节，包含三部分内容：3.1、3.2 以及 3.3。</p>
</blockquote>
<hr>
<p>–&gt; 第一部分：CTPN 如何检测细粒度提议中的文本</p>
<h4 id="3-1-Detecting-Text-in-Fine-scale-Proposals"><a href="#3-1-Detecting-Text-in-Fine-scale-Proposals" class="headerlink" title="3.1 Detecting Text in Fine-scale Proposals"></a>3.1 Detecting Text in Fine-scale Proposals</h4><p>Similar to Region Proposal Network (RPN) [25], the CTPN is essentially a fully convolutional network that allows an input image of arbitrary size. It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of fine-scale (e.g., fixed 16-pixel width) text proposals, as shown in Fig. 1 (b).</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h4 id="3-1-检测细粒度文本提议"><a href="#3-1-检测细粒度文本提议" class="headerlink" title="3.1 检测细粒度文本提议"></a>3.1 检测细粒度文本提议</h4><p>类似于区域提议网络（RPN）[25]，CTPN 本质上是一个全卷积网络，允许任意大小的输入图像。它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的 16 个像素）文本提议，如图1（b）所示。</p>
<p>We take the very deep 16-layer vggNet (VGG16) [27] as an example to describe our approach, which is readily applicable to other deep models. Architecture of the CTPN is presented in Fig. 1 (a). We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (e.g., the conv5 of the VGG16). The size of conv5 feature maps is determined by the size of input image, while the total stride and receptive field are fixed as 16 and 228 pixels, respectively. Both the total stride and receptive field are fixed by the network architecture. Using a sliding window in the convolutional layer allows it to share convolutional computation, which is the key to reduce computation of the costly sliding-window based methods.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>我们以非常深的 16 层 VGGNet（VGG16）[27] 为例来描述我们的方法，该方法很容易应用于其他深度模型。CTPN 的架构如图 1（a）所示。我们使用一个小的空间窗口 3×3 来滑动最后的卷积层特征映射（例如：VGG16 的 conv5）。conv5 特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为 16 个和 228 个像素。网络架构决定总步长和感受野。在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算量的关键。</p>
<blockquote>
<p>内容解读：这里作者选择 VGG16 Net （conv5_3）作为强深度共享卷积层的 base net，然后选用 3×3 滑动窗口在 conv5_3 的 feature map 上移动。</p>
</blockquote>
<p>–&gt; 以 Anchers 机制为核心的 RPN 区域提议（生成）网络的引入：</p>
<p>Generally, sliding-window methods adopt multi-scale windows to detect objects of different sizes, where one window scale is fixed to objects of similar size. In [25], Ren et al. proposed an efficient anchor regression mechanism that allows the RPN to detect multi-scale objects with a single-scale window. The key insight is that a single window is able to predict objects in a wide range of scales and aspect ratios, by using a number of flexible anchors. We wish to extend this efficient anchor mechanism to our text task. However, text differs from generic objects substantially, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2]. Text is a sequence which does not have an obvious closed boundary. It may include multi-level components, such as stroke, character, word, text line and text region, which are not distinguished clearly between each other. Text detection is defined in word or text line level, so that it may be easy to make an incorrect detection by defining it as a single object, e.g., detecting part of a word. Therefore, directly predicting the location of a text line or word may be difficult or unreliable, making it hard to get a satisfied accuracy. An example is shown in Fig. 2, where the RPN is directly trained for localizing text lines in an image.</p>
<p>Fig. 2: Left: RPN proposals. Right: Fine-scale text proposals.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。在 [25] 中，Ren 等人提出了一种有效的锚点回归机制，允许 RPN 使用单尺度窗口检测多尺度目标。关键的洞察力是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。我们希望将这种有效的锚点机制扩展到我们的文本任务。然而，实质上文本与普通目标不同，（普通目标）它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标 [2]。文本是一个没有明显封闭边界的序列。它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得令人满意的准确性。一个例子如图 2 所示，其中 RPN 直接被用于定位图像中的文本行。</p>
<p><img src="/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/./RPN_text_proposal.png" alt="avatar"></p>
<center>图2：左：RPN提议。右：细粒度的文本提议。</center>

<blockquote>
<p>内容解读：</p>
<p>大佬们分析了文本区域检测以及普通物体目标区域检测的区别，以及 Faster RCNN 中的多尺度描点机制对于生成文本区域提议存在的缺陷。并且通过使用训练 RPN 用于定位图像中的文本行，可视化结果表明与分析结果一致。</p>
</blockquote>
<hr>
<p>–&gt;大佬们提出：使用垂直锚点机制（代替一般目标检测的锚点机制）来构建细粒度文本区域提议的策略:</p>
<p>We look for a unique property of text that is able to generalize well to text components in all levels. We observed that word detection by the RPN is difficult to accurately predict the horizontal sides of words, since each character within a word is isolated or separated, making it confused to find the start and end locations of a word. Obviously, a text line is a sequence which is the main difference between text and generic objects. It is natural to consider a text line as a sequence of fine-scale text proposals, where each proposal generally represents a small part of a text line, e.g., a text piece with 16-pixel width. Each proposal may include a single or multiple strokes, a part of a character, a single or multiple characters, etc. We believe that it would be more accurate to just predict the vertical location of each proposal, by fixing its horizontal location which may be more difficult to predict. This reduces the search space, compared to the RPN which predicts 4 coordinates of an object. We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and y-axis location of each fine-scale proposal. It is also more reliable to detect a general fixed-width text proposal than identifying an isolate character, which is easily confused with part of a character or multiple characters. Furthermore, detecting a text line in a sequence of fixed-width text proposals also works reliably on text of multiple scales and multiple aspect ratios.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>我们寻找文本的独特属性，能够很好地概括各个层次的文本组件。我们观察到由 RPN 进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。显然，文本行是一个序列，它是文本和通用目标之间的主要区别。将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为 16 个像素的文本块。每个提议可能包含单个或多个笔划，字符的一部分，单个或多个字符等。<strong>我们认为，通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测。</strong>与预测目标 4 个坐标的 RPN 相比，这减少了搜索空间。我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。检测一般固定宽度的文本提议比识别分隔的字符更可靠，分隔字符容易与字符或多个字符的一部分混淆。此外，检测一系列固定宽度文本提议中的文本行也可以在多个尺度和多个长宽比的文本上可靠地工作。</p>
<blockquote>
<p>内容解读：前面我们提到，不管是自上而下方法还是自下而上的方法，最终都需要构建成文本行。文本行是一个序列，它是文本和通用目标之间的主要区别。</p>
<p>于是，将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常都代表了文本行的一小部分。大佬们认为通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测，这就是垂直描点机制。</p>
</blockquote>
<hr>
<p>–&gt; 垂直锚点机制解读：</p>
<p>To this end, we design the fine-scale text proposal as follow. Our detector investigates each spatial location in the conv5 densely. A text proposal is defined to have a fixed width of 16 pixels (in the input image). This is equal to move the detector densely through the conv5 maps, where the total stride is exactly 16 pixels. Then we design k vertical anchors to predict y-coordinates for each proposal. The k anchors have a same horizontal location with a fixed width of 16 pixels, but their vertical locations are varied in k different heights. In our experiments, we use ten anchors for each proposal, k=10, whose heights are varied from 11 to 273 pixels (by ÷0.7 each time) in the input image. The explicit vertical coordinates are measured by the height and y-axis center of a proposal bounding box. We compute relative predicted vertical coordinates (v) with respect to the bounding box location of an anchor as, </p>
<p>$$v_c=(c_y-c_y^a)/h^a，v_h=log(h/h^a)$$</p>
<p>$$v_c^☆=(c_y^☆ - c_y^a)/h^a，v_h^☆=log(h^☆/h^a)$$</p>
<p>where v={vc,vh} and v∗={v∗c,v∗h} are the relative predicted coordinates and ground truth coordinates, respectively. cay and ha are the center (y-axis) and height of the anchor box, which can be pre-computed from an input image. cy and h are the predicted y-axis coordinates in the input image, while c∗y and h∗ are the ground truth coordinates. Therefore, each predicted text proposal has a bounding box with size of h×16 (in the input image), as shown in Fig. 1 (b) and Fig. 2 (right). Generally, an text proposal is largely smaller than its effective receptive field which is 228×228.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>为此，我们设计如下的细粒度文本提议。我们的检测器密集地调查了 conv5 中的每个空间位置。文本提议被定义为具有 16 个像素的固定宽度（在输入图像中）。这相当于在 conv5 的映射上密集地移动检测器，其中总步长恰好为 16 个像素。然后，我们设计 k 个垂直锚点来预测每个提议的 y 坐标。k 个锚点具有相同的水平位置，固定宽度为 16 个像素，但其垂直位置在 k 个不同的高度变化。在我们的实验中，我们对每个提议使用十个锚点，k=10，其高度在输入图像中从 11 个像素变化到 273 个像素（每次 ÷0.7）。<strong>明确的垂直坐标是通过提议边界框的高度和 y 轴中心来度量的。</strong>我们计算相对于锚点的边界框位置的相对预测的垂直坐标（v），如下所示：</p>
<p>$$ v_c=(c_y-c_y^a)/h^a，v_h=log(h/h^a) $$</p>
<p>$$v_c^☆=(c_y^☆-c_y^a)/h^a，v_h^☆=log(h^☆/h^a)$$</p>
<p>其中$v={v_c，v_h}​$和$v^☆={v_c^☆，v_h^☆}​$分别是相对预测坐标和实际坐标。$c_y^☆​$ 和 $h^a​$ 是锚盒的中心（y 轴）和高度，可以从输入图像预先计算。$c_y​$ 和 $h​$ 是输入图像中预测的 $y​$ 轴坐标，而 $c_y^☆​$ 和 $h^☆​$ 实际坐标。因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为 h×16 的边界框（在输入图像中）。一般来说，文本提议在很大程度上要比它的有效感受野 228×228 要小。</p>
<blockquote>
<p>内容解读：CTPN 垂直描点机制针对的是水平文字检测。</p>
<p>大佬们采用了一组（10 个）等宽度的 Anchors，用于定位文字位置。Anchor 尺寸：widths=[16]，heights=[11,16,23,33,48,68,97,139,198,283]。由于 CTPN 采用 VGG16 Net 模型提取卷积特征，故 conv5_3 feature map 的宽高都是输入 Image 宽高的 1/16。同时 fc 和 conv5_3 的 width、height 都相等。下面给出 CTPN 垂直描点机制的 Anchor 示意图：</p>
</blockquote>
<p><img src="/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/./anchor.jpg" alt="avatar"></p>
<p>The detection processing is summarised as follow. Given an input image, we have W×H×C conv5 features maps (by using the VGG16 model), where C is the number of feature maps or channels, and W×H is the spatial arrangement. When our detector is sliding a 3×3 window densely through the conv5, each sliding-window takes a convolutional feature of 3×3×C for producing the prediction. For each prediction, the horizontal location (x-coordinates) and k-anchor locations are fixed, which can be pre-computed by mapping the spatial window location in the conv5 onto the input image. Our detector outputs the text/non-text scores and the predicted y-coordinates (v) for k anchors at each window location. The detected text proposals are generated from the anchors having a text/non-text score of &gt;0.7 (with non-maximum suppression). By the designed vertical anchor and fine-scale detection strategy, our detector is able to handle text lines in a wide range of scales and aspect ratios by using a single-scale image. This further reduces its computation, and at the same time, predicting accurate localizations of the text lines. Compared to the RPN or Faster R-CNN system [25], our fine-scale detection provides more detailed supervised information that naturally leads to a more accurate detection.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>检测处理总结如下。给定输入图像，我们有 W×H×C conv5 特征映射（通过使用 VGG16 Net 模型），其中 C 是特征映射或通道的数目，并且 W×H 是空间布置。当我们的检测器通过 conv5 密集地滑动 3×3 窗口时，每个滑动窗口使用 3×3×C 的卷积特征来产生预测。对于每个预测，水平位置（x 轴坐标）和 k 个锚点位置是固定的，可以通过将 conv5 中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出 k 个锚点的文本/非文本分数和预测的 y 轴坐标（v）。检测到的文本提议是从具有 &gt;0.7（具有非极大值抑制）的文本/非文本分数的锚点生成的。通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。与 RPN 或 Faster R-CNN 系统 [25] 相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</p>
<hr>
<p>–&gt; 第二部分：CTPN 如何循环连接细粒度文本提议</p>
<h4 id="3-2-Recurrent-Connectionist-Text-Proposals"><a href="#3-2-Recurrent-Connectionist-Text-Proposals" class="headerlink" title="3.2 Recurrent Connectionist Text Proposals"></a>3.2 Recurrent Connectionist Text Proposals</h4><p>To improve localization accuracy, we split a text line into a sequence of fine-scale text proposals, and predict each of them separately. Obviously, it is not robust to regard each isolated proposal independently. This may lead to a number of false detections on non-text objects which have a similar structure as text patterns, such as windows, bricks, leaves, etc. (referred as text-like outliers in [13]). It is also possible to discard some ambiguous patterns which contain weak text information. Several examples are presented in Fig. 3 (top). Text have strong sequential characteristics where the sequential context information is crucial to make a reliable decision. This has been verified by recent work [9] where a recurrent neural network (RNN) is applied to encode this context information for text recognition. Their results have shown that the sequential context information is greatly facilitate the recognition task on cropped word images.</p>
<p>Fig. 3: Top: CTPN without RNN. Bottom: CTPN with RNN connection.</p>
<p>–&gt; 循环神经网络连接细粒度文本提议的必要性：</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h4 id="3-2-循环连接文本提议"><a href="#3-2-循环连接文本提议" class="headerlink" title="3.2 循环连接文本提议"></a>3.2 循环连接文本提议</h4><p>为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个（细粒度）文本提议。显然，将每个孤立的提议独立考虑并不鲁棒。这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献 [13] 中称为类文本异常值）。还可以丢弃一些含有弱文本信息的模糊模式。图 3 给出了几个例子。文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。最近的工作已经证实了这一点 [9]，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。</p>
<p><img src="/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/./RNN_text_proposal.png" alt="avatar"></p>
<p>图3：上：没有 RNN 的 CTPN。下：有 RNN 连接的 CTPN。</p>
<hr>
<p>–&gt; CTPN 中的网内循环机制解读</p>
<p>Motivated from this work, we believe that this context information may also be of importance for our detection task. Our detector should be able to explore this important context information to make a more reliable decision, when it works on each individual proposal. Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and seamless in-network connection of the fine-scale text proposals. RNN provides a natural choice for encoding this information recurrently using its hidden layers. To this end, we propose to design a RNN layer upon the conv5, which takes the convolutional feature of each window as sequential inputs, and updates its internal state recurrently in the hidden layer, Ht, </p>
<p>$$H_t=\psi(H_{t−1},X_t),t=1,2,…,W)​$$</p>
<p>where Xt∈R3×3×C is the input conv5 feature from t-th sliding-window (3×3). The sliding-window moves densely from left to right, resulting in t=1,2,…,W sequential features for each row. W is the width of the conv5. Ht is a recurrent internal state that is computed jointly from both current input (Xt) and previous states encoded in Ht−1. The recurrence is computed by using a non-linear function φ, which defines exact form of the recurrent model. We exploit the long short-term memory (LSTM) architecture [12] for our RNN layer. The LSTM was proposed specially to address vanishing gradient problem, by introducing three additional multiplicative gates: the input gate, forget gate and output gate. Details can be found in [12]. Hence the internal state in RNN hidden layer accesses the sequential context information scanned by all previous windows through the recurrent connection. We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the recurrent context in both directions, so that the connectionist receipt field is able to cover the whole image width, e.g., 228 × width. We use a 128D hidden layer for each LSTM, resulting in a 256D RNN hidden layer, Ht∈R256.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>受到这项工作的启发，我们认为这种上下文信息对于我们的检测任务也很重要。我们的检测器应该能够探索这些重要的上下文信息，以便在每个单独的提议中都可以做出更可靠的决策。此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。RNN 提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。为此，我们提出在 conv5 上设计一个 RNN 层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t​$，</p>
<p>$$H_t=\psi(H_{t−1},X_t),t=1,2,…,W)​$$</p>
<p>其中，$X_t∈R^{3×3×C}$ 是第 t 个滑动窗口 (3×3) 的输入 conv5 特征。滑动窗口从左向右密集移动，导致每行（产生）的 t=1,2,…,W 序列特征。$H_t$ 是从当前输入 $X_t$ 和以 $H_{t−1}$ 编码的先前状态联合计算的循环内部状态。递归是通过使用非线性函数 φ 来计算的，它定义了循环模型的确切形式。我们利用长短时记忆（LSTM）架构 [12] 作为我们的 RNN 层。通过引入三个附加乘法门：输入门，忘记门和输出门，专门提出了 LSTM 以解决梯度消失问题。细节可以在 [12] 中找到。因此，RNN 隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。我们通过使用双向 LSTM 来进一步扩展 RNN 层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如 $228 × width$ 。我们对每个 LSTM 使用一个 128 维的隐藏层，从而产生 256 维的 RNN 隐藏层 $H_t{\in}R^{256}$。</p>
<p>The internal state in Ht is mapped to the following FC layer, and output layer for computing the predictions of the t-th proposal. Therefore, our integration with the RNN layer is elegant, resulting in an efficient model that is end-to-end trainable without additional cost. The efficiency of the RNN connection is demonstrated in Fig. 3. Obviously, it reduces false detections considerably, and at the same time, recovers many missed text proposals which contain very weak text information.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>H_t 中的内部状态被映射到后面的 FC 层，并且<strong>输出层用于计算第 t 个提议的预测。</strong>因此，我们与 RNN 层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。RNN 连接的功效如图 3 所示。显然，它大大减少了错误检测，同时还能够恢复很多包含非常弱的文本信息的遗漏文本提议。</p>
<hr>
<p>–&gt; 第三部分：进行边缘细化</p>
<h4 id="3-3-Side-refinement"><a href="#3-3-Side-refinement" class="headerlink" title="3.3 Side-refinement"></a>3.3 Side-refinement</h4><p>The fine-scale text proposals are detected accurately and reliably by our CTPN. Text line construction is straightforward by connecting continuous text proposals whose text/non-text score is \(&gt;0.7\). Text lines are constructed as follow. First, we define a paired neighbour (\(B_j\)) for a proposal \(Bi\) as \(B_j−&gt;B_i\), when (i) \(Bj\) is the nearest horizontal distance to \(Bi\), and (ii) this distance is less than 50 pixels, and (iii) their vertical overlap is \(&gt;0.7\). Second, two proposals are grouped into a pair, if \(B_j−&gt;B_i\) and \(B_i−&gt;B_j\). Then a text line is constructed by sequentially connecting the pairs having a same proposal.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h4 id="3-3-边缘细化"><a href="#3-3-边缘细化" class="headerlink" title="3.3 边缘细化"></a>3.3 边缘细化</h4><p>–&gt; 文本行构建方法：细粒度文本提议的连接策略</p>
<p>我们的 CTPN 能够准确可靠地检测细粒度的文本提议。通过连接其文本/非文本分数为 &gt;0.7 的连续文本提议，文本行的构建非常简单。文本行构建如下：首先，我们为提议 $B_i$ 定义一个配对邻居 $B_j$ 作为 $B_j-&gt;B_i$，当（i）$B_j$ 是最接近 $B_i$ 的水平距离，（ii）该距离小于 50 像素，并且（iii）它们的垂直重叠是 &gt;0.7 时。其次，如果有 $B_j-&gt;B_i$ 和 $B_i-&gt;B_j$，则将两个提议分组为一对。然后通过顺序连接具有相同提议的对来构建文本行。</p>
<p>The fine-scale detection and RNN connection are able to predict accurate localizations in vertical direction. In horizontal direction, the image is divided into a sequence of equal 16-pixel width proposals. This may lead to an inaccurate localization when the text proposals in both horizontal sides are not exactly covered by a ground truth text line area, or some side proposals are discarded (e.g., having a low text score), as shown in Fig. 4. This inaccuracy may be not crucial in generic object detection, but should not be ignored in text detection, particularly for those small-scale text lines or words. To address this problem, we propose a side-refinement approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as side-anchor or side-proposal). Similar to the y-coordinate prediction, we compute relative offset as, </p>
<p>$$o = (x_{side}-c_x^a)/w_a，o^☆=(x_{side}^☆−c_{x}^a)/w_a$$</p>
<p>where \(x_{side}\) is the predicted x-coordinate of the nearest horizontal side (e.g., left or right side) to current anchor. \(x_{side}^∗\) is the ground truth (GT) side coordinate in x-axis, which is pre-computed from the GT bounding box and anchor location. \(c_x^a\) is the center of anchor in x-axis. \(w^a\) is the width of anchor, which is fixed, \(w^a=16\) . The side-proposals are defined as the start and end proposals when we connect a sequence of detected fine-scale text proposals into a text line. We only use the offsets of the side-proposals to refine the final text line bounding box. Several detection examples improved by side-refinement are presented in Fig. 4. The side-refinement further improves the localization accuracy, leading to about \(2%\) performance improvements on the SWT and Multi-Lingual datasets. Notice that the offset for side-refinement is predicted simultaneously by our model, as shown in Fig. 1. It is not computed from an additional post-processing step.</p>
<p>Fig.4: CTPN detection with (red box) and without (yellow dashed box) the side-refinement. Color of fine-scale proposal box indicate a text/non-text score.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p><img src="/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/./side_refinement.png" alt="avatar"></p>
<p>图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。细粒度提议边界框的颜色表示文本/非文本分数。</p>
<p>细粒度的检测和 RNN 连接可以预测垂直方向的精确位置。在水平方向上，图像被分成一系列相等的宽度为 16 个像素的提议。如图 4 所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。与 y 坐标预测类似，我们计算相对偏移为：</p>
<p>$$o = (x_{side}-c_x^a)/w_a，o^☆=(x_{side}^☆−c_x^a)/w_a​$$</p>
<p>，其中 $x_{side}$ 是最接近水平边（例如，左边或右边）到当前锚点的预测的 x 坐标。$x_{side}^☆$ 是 x 轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。$c_x^a$ 是 x 轴的锚点的中心。$w^a$ 是固定的锚点宽度，$w^a=16$。当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些边缘提议会被定义为开始和结束提议。我们只使用边缘提议的偏移量来优化最终的文本行边界框。通过边缘细化改进的几个检测示例如图 4 所示。边缘细化进一步提高了定位精度，从而使 SWT 和 Multi-Lingual 数据集上的性能提高了约 \(2\%\)。请注意，我们的模型同时预测了边缘细化的偏移量，如图 1 所示。它不是通过额外的后处理步骤计算的。</p>
<hr>
<h4 id="3-4-Model-Outputs-and-Loss-Functions"><a href="#3-4-Model-Outputs-and-Loss-Functions" class="headerlink" title="3.4 Model Outputs and Loss Functions"></a>3.4 Model Outputs and Loss Functions</h4><p>The proposed CTPN has three outputs which are jointly connected to the last FC layer, as shown in Fig. 1 (a). The three outputs simultaneously predict text/non-text scores (\(s\)), vertical coordinates (\(v=\{v_c,v_h\}\)) in E.q. (2) and side-refinement offset (\(o\)). We explore \(k\) anchors to predict them on each spatial location in the conv5, resulting in \(2k\), \(2k\) and \(k\) parameters in the output layer, respectively.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h4 id="3-4-模型输出与损失函数"><a href="#3-4-模型输出与损失函数" class="headerlink" title="3.4 模型输出与损失函数"></a>3.4 模型输出与损失函数</h4><p>提出的 CTPN 有三个输出共同连接到最后的 FC 层，如图 1（a）所示。这三个输出同时预测公式（2）中的文本/非文本分数（ \(s\) ），垂直坐标（\(v=\{v_c,v_h\}\)）和边缘细化偏移（\(o\)）。我们将探索 \(k\) 个锚点来预测它们在 conv5 中的每个空间位置，从而在输出层分别得到 \(2k\)，\(2k\) 和 \(k\) 个参数。</p>
<p>We employ multi-task learning to jointly optimize model parameters. We introduce three loss functions, \(L_s^{cl}\), \(L_v^{re}\) and \(L_o^{re}\), which compute errors of text/non-text score, coordinate and side-refinement, respectively. With these considerations, we follow the multi-task loss applied in [5,25], and minimize an overall objective function (\(L\)) for an image as, </p>
<p>$$ L(s_i,v_j,o_k)=\frac{1}{N_s}\sum_iL_s^{cl}(s_i,s_i^∗) + \frac{λ_1}{N_v}\sum_{j}L_{v}^{re}(v_j,v_j^∗) + \frac{λ_2}{N_o}\sum_{k}L_o^{re}(o_k,o_k^∗) $$</p>
<p>where each anchor is a training sample, and i is the index of an anchor in a mini-batch. \(s_i\) is the predicted probability of anchor i being a true text. \(s_i^∗=\{0,1\}\) is the ground truth. j is the index of an anchor in the set of valid anchors for y-coordinates regression, which are defined as follow. A valid anchor is a defined positive anchor (\(s_j^∗=1\), described below), or has an Intersection-over-Union (IoU) \(&gt;0.5\) overlap with a ground truth text proposal. \(v_j\) and \(v_j^∗\) are the prediction and ground truth y-coordinates associated with the j-{th} anchor. k is the index of a side-anchor, which is defined as a set of anchors within a horizontal distance (e.g., 32-pixel) to the left or right side of a ground truth text line bounding box. \(o_k\) and \(o_k^∗\) are the predicted and ground truth offsets in x-axis associated to the k-{th} anchor. \(L_s^{cl}\) is the classification loss which we use Softmax loss to distinguish text and non-text. \(L_v^{re}\) and \(L_o^{re}\) are the regression loss. We follow previous work by using the smooth \(L1\) function to compute them [5, 25]. λ1 and λ2 are loss weights to balance different tasks, which are empirically set to 1.0 and 2.0. \(N_s\) \(N_v\) and \(N_o\) are normalization parameters, denoting the total number of anchors used by \(L_s^{cl}\), \(L_v^{re}\) and \(L_o^{re}\), respectively.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p>我们采用多任务学习来联合优化模型参数。我们引入了三种损失函数：\(L_s^{cl}\), \(L_v^{re}\) 和 \(L_o^{re}\)，其分别计算文本/非文本分数，坐标和边缘细化。考虑到这些因素，我们遵循 [5，25] 中应用的多任务损失，并最小化图像的总体目标函数（\(L\)）最小化：</p>
<p>$$ L(s_i,v_j,o_k)=\frac{1}{N_s}\sum_iL_s^{cl}(s_i,s_i^∗) + \frac{λ_1}{N_v}\sum_{j}L_{v}^{re}(v_j,v_j^∗) + \frac{λ_2}{N_o}\sum_{k}L_o^{re}(o_k,o_k^∗) $$</p>
<p>，其中每个锚点都是一个训练样本，i 是一个小批量数据中一个锚点的索引。\(s_i\) 是预测的锚点 i 作为实际文本的预测概率。\(s_i^∗=\{0,1\}\) 是真实值。j 是 y 坐标回归中有效锚点集合中锚点的索引，定义如下。有效的锚点是定义的正锚点（\(s_j^∗=1\)，如下所述），或者与实际文本提议重叠的交并比（IoU）\(&gt;0.5\)。\(v_j\) 和 \(v_j^∗\) 是与第 j 个锚点关联的预测的和真实的 y 坐标。k 是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如 32 个像素）内的一组锚点。\(o_k\) 和 \(o_k^∗\) 是与第 k 个锚点关联的 x 轴的预测和实际偏移量。\(L_s^{cl}\) 是我们使用 Softmax 损失区分文本和非文本的分类损失。\(L_v^{re}\) 和 \(L_o^{re}\) 是回归损失。我们遵循以前的工作，使用平滑 \(L1\) 函数来计算它们 [5，25] 。λ1 和 λ2 是损失权重，用来平衡不同的任务，将它们经验地设置为 1.0 和 2.0。\(N_s\) \(N_v\) 和 \(N_o\) 是标准化参数，表示 \(L_s^{cl}\), \(L_v^{re}\) and \(L_o^{re}\) 分别使用的锚点总数。</p>
<hr>
<h4 id="3-5-Training-and-Implementation-Details"><a href="#3-5-Training-and-Implementation-Details" class="headerlink" title="3.5 Training and Implementation Details"></a>3.5 Training and Implementation Details</h4><p>The CTPN can be trained end-to-end by using the standard back-propagation and stochastic gradient descent (SGD). Similar to RPN [25], training samples are the anchors, whose locations can be pre computed in input image, so that the training labels of each anchor can be computed from corresponding GT box.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h4 id="3-5-训练和实现细节"><a href="#3-5-训练和实现细节" class="headerlink" title="3.5 训练和实现细节"></a>3.5 训练和实现细节</h4><p>通过使用标准的反向传播和随机梯度下降（SGD），可以对 CTPN 进行端对端训练。与 RPN [25] 类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</p>
<p><strong>Training labels</strong>. For text/non-text classification, a binary label is assigned to each positive (text) or negative (non-text) anchor. It is defined by computing the IoU overlap with the GT bounding box (divided by anchor location). A positive anchor is defined as : (i) an anchor that has an &gt;0.7 IoU overlap with any GT box; or (ii) the anchor with the highest IoU overlap with a GT box. By the condition (ii), even a very small text pattern can assign a positive anchor. This is crucial to detect small-scale text patterns, which is one of key advantages of the CTPN. This is different from generic object detection where the impact of condition (ii) may be not significant. The negative anchors are defined as &lt;0.5 IoU overlap with all GT boxes. The training labels for the y-coordinate regression (\(v^∗\)) and offset regression (\(o^∗\)) are computed as E.q. (2) and (4) respectively.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p><strong>训练标签。</strong>对于文本/非文本分类，二值标签分配给每个正（文本）锚点或负（非文本）锚点。它通过计算与实际边界框的 IoU 重叠（除以锚点位置）来定义。正锚点被定义为：（i）与任何实际边界框具有 &gt;0.7 的 IoU 重叠；或者（ii）与实际边界框具有最高 IoU 重叠。通过条件（ii），即使是非常小的文本模式也可以分为正锚点。这对于检测小规模文本模式至关重要，这是 CTPN 的主要优势之一。这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。负锚点定义为与所有实际边界框具有 &lt;0.5 的 IoU 重叠。y 坐标回归（\(v^∗\)）和偏移回归（\(o^∗\)）的训练标签分别按公式（2）和（4）计算。</p>
<hr>
<p><strong>Training data.</strong> In the training process, each mini-batch samples are collected randomly from a single image. The number of anchors for each mini-batch is fixed to \(N_s=128\), with 1:1 ratio for positive and negative samples. A mini-patch is pad with negative samples if the number of positive ones is fewer than 64. Our model was trained on 3,000 natural images, including 229 images from the ICDAR 2013 training set. We collected the other images ourselves and manually labelled them with text line bounding boxes. All self-collected training images are not overlapped with any test image in all benchmarks. The input image is resized by setting its short side to 600 for training, while keeping its original aspect ratio.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p><strong>训练数据。</strong>在训练过程中，每个小批量样本从单张图像中随机收集。每个小批量数据的锚点数量固定为 \(N_s=128\)，正负样本的比例为 1：1。如果正样本的数量少于 64，则会用小图像块填充负样本。我们的模型在 3000 张自然图像上训练，其中包括来自 ICDAR 2013 训练集的 229 张图像。我们自己收集了其他图像，并用文本行边界框进行了手工标注。在所有基准测试集中，所有自我收集的训练图像都不与任何测试图像重叠。为了训练，通过将输入图像的短边设置为 600 来调整输入图像的大小，同时保持其原始长宽比。</p>
<hr>
<p><strong>Implementation Details. </strong>We follow the standard practice, and explore the very deep VGG16 model [27] pre-trained on the ImageNet data [26]. We initialize the new layers (e.g., the RNN and output layers) by using random weights with Gaussian distribution of 0 mean and 0.01 standard deviation. The model was trained end-to-end by fixing the parameters in the first two convolutional layers. We used 0.9 momentum and 0.0005 weight decay. The learning rate was set to 0.001 in the first 16K iterations, followed by another 4K iterations with 0.0001 learning rate. Our model was implemented in Caffe framework [17].</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<p><strong>实现细节。</strong>我们遵循标准实践，并在 ImageNet 数据 [26] 上探索预先训练的非常深的 VGG16 模型 [27]。我们通过使用具有 0 均值和 0.01 标准差的高斯分布的随机权重来初始化新层（例如，RNN 和输出层）。该模型通过固定前两个卷积层中的参数进行端对端的训练。我们使用 0.9 的动量和 0.0005 的重量衰减。在前 16K 次迭代中，学习率被设置为 0.001，随后以 0.0001 的学习率再进行 4K 次迭代。我们的模型在 Caffe 框架 [17] 中实现。</p>
<hr>
<h3 id="4-Experimental-Results-and-Discussions"><a href="#4-Experimental-Results-and-Discussions" class="headerlink" title="4. Experimental Results and Discussions"></a>4. Experimental Results and Discussions</h3><p>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24]. In our experiments, we first verify the efficiency of each proposed component individually, e.g., the fine-scale text proposal detection or in-network recurrent connection. The ICDAR 2013 is used for this component evaluation.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h3 id="4-实验结果和讨论"><a href="#4-实验结果和讨论" class="headerlink" title="4. 实验结果和讨论"></a>4. 实验结果和讨论</h3><p>我们在五个文本检测基准数据集上评估 CTPN，即 ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3] 和 Multilingual[24] 数据集。在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。ICDAR 2013 用于该组件的评估。</p>
<hr>
<h3 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5 Conclusions"></a>5 Conclusions</h3><p>We have presented a Connectionist Text Proposal Network (CTPN) —— an efficient text detector that is end-to-end trainable. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional maps. We develop vertical anchor mechanism that jointly predicts precise location and text/non-text score for each proposal, which is the key to realize accurate localization of text. We propose an in-network RNN layer that connects sequential text proposals elegantly, allowing it to explore meaningful context information. These key technical developments result in a powerful ability to detect highly challenging text, with less false detections. The CTPN is efficient by achieving new state-of-the-art performance on five benchmarks, with 0.14s/image running time.</p>
<p><strong>|↑↑↑ 中文对照 ↓↓↓|</strong></p>
<h3 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h3><p>我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。CTPN 直接在卷积映射的一系列细粒度文本提议中检测文本行。我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。我们提出了一个网内 RNN 层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。这些关键技术的发展带来了检测极具挑战性的文本的强大能力，同时减少了误检。通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为 0.14s，CTPN 是有效的。</p>
<hr>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol>
<li>Busta, M., Neumann, L., Matas, J.: Fastext: Efficient unconstrained scene text detector (2015), in IEEE International Conference on Computer Vision (ICCV)</li>
<li>Cheng, M., Zhang, Z., Lin, W., Torr, P.: Bing: Binarized normed gradients for objectness estimation at 300fps (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</li>
<li>Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke width transform (2010), in IEEE Computer Vision and Pattern Recognition (CVPR)</li>
<li>Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV) 88(2), 303–338 (2010)</li>
<li>Girshick, R.: Fast r-cnn (2015), in IEEE International Conference on Computer Vision (ICCV)</li>
<li>Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation (2014), in IEEE Computer Vision and Pattern Recognition (CVPR)</li>
<li>Graves, A., Schmidhuber, J.: Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks 18(5), 602–610 (2005)</li>
<li>Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
<li>He,P.,Huang,W.,Qiao,Y.,Loy,C.C.,Tang,X.:Readingscenetextindeepconvo- lutional sequences (2016), in The 30th AAAI Conference on Artificial Intelligence (AAAI-16)</li>
<li>He, T., Huang, W., Qiao, Y., Yao, J.: Accurate text localization in natural image with cascaded convolutional text network (2016), arXiv:1603.09423</li>
<li>He, T., Huang, W., Qiao, Y., Yao, J.: Text-attentional convolutional neural net- works for scene text detection. IEEE Trans. Image Processing (TIP) 25, 2529–2541 (2016)</li>
<li>Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Networks 9(8), 1735–1780 (1997)</li>
<li>Huang, W., Lin, Z., Yang, J., Wang, J.: Text localization in natural images using stroke feature transform and text covariance descriptors (2013), in IEEE International Conference on Computer Vision (ICCV)</li>
<li>Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolutional neural networks induced mser trees (2014), in European Conference on Computer Vision (ECCV)</li>
<li>Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild with convolutional neural networks. International Journal of Computer Vision (IJCV) (2016)</li>
<li>Jaderberg, M., Vedaldi, A., Zisserman, A.: Deep features for text spotting (2014), in European Conference on Computer Vision (ECCV)</li>
<li>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding (2014), in ACM International Conference on Multimedia (ACM MM)</li>
<li>Karatzas,D.,Gomez-Bigorda,L.,Nicolaou,A.,Ghosh,S.,Bagdanov,A.,Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S.,Valveny, E.: Icdar 2015 competition on robust reading (2015), in International Conference on Document Analysis and Recognition (ICDAR)</li>
<li>Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R., Mas, J., Mota, D.F., Almazan, J.A., de las Heras., L.P.: Icdar 2013 robust reading competition (2013), in International Conference on Document Analysis and Recognition (ICDAR)</li>
<li>Mao, J., Li, H., Zhou, W., Yan, S., Tian, Q.: Scale based region growing for scene text detection (2013), in ACM International Conference on Multimedia (ACM MM)</li>
<li>Minetto, R., Thome, N., Cord, M., Fabrizio, J., Marcotegui, B.: Snoopertext: A multiresolution system for text detection in complex visual scenes (2010), in IEEE International Conference on Pattern Recognition (ICIP)</li>
<li>Neumann, L., Matas, J.: Efficient scene text localization and recognition with local character refinement (2015), in International Conference on Document Analysis and Recognition (ICDAR)</li>
<li>Neumann, L., Matas, J.: Real-time lexicon-free scene text localization and recognition. In IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) (2015)</li>
<li>Pan, Y., Hou, X., Liu, C.: Hybrid approach to detect and localize texts in natural scene images. IEEE Trans. Image Processing (TIP) 20, 800–813 (2011)</li>
<li>Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks (2015), in Neural Information Processing Systems (NIPS)</li>
<li>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Li, F.: Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV) 115(3), 211–252 (2015)</li>
<li>Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2015), in International Conference on Learning Representation (ICLR)</li>
<li>Tian, S., Pan, Y., Huang, C., Lu, S., Yu, K., Tan, C.L.: Text flow: A unified text detection system in natural scene images (2015), in IEEE International Conference on Computer Vision (ICCV)</li>
<li>Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition (2011), in IEEE International Conference on Computer Vision (ICCV)</li>
<li>Wolf, C., Jolion, J.: Object count / area graphs for the evaluation of object detection and segmentation algorithms. International Journal of Document Analysis 8, 280–296 (2006)</li>
<li>Yao, C., Bai, X., Liu, W.: A unified framework for multioriented text detection and recognition. IEEE Trans. Image Processing (TIP) 23(11), 4737–4749 (2014)</li>
<li>Yin, X.C., Pei, W.Y., Zhang, J., Hao, H.W.: Multi-orientation scene text detection with adaptive clustering. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 37, 1930–1937 (2015)</li>
<li>Yin, X.C., Yin, X., Huang, K., Hao, H.W.: Robust text detection in natural scene images. IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) 36, 970–983 (2014)</li>
<li>Zhang, Z., Shen, W., Yao, C., Bai, X.: Symmetry-based text line detection in natural scenes (2015), in IEEE Computer Vision and Pattern Recognition (CVPR)</li>
<li>Zhang, Z., Zhang, C., Shen, W., Yao, C., Liu, W., Bai, X.: Multi-oriented text de- tection with fully convolutional networks (2016), in IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
</ol>
<hr>

      
    </div>
    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">如果感觉文章对您有较大帮助，请随意打赏。您的鼓励是我保持持续创作的最大动力！</div>
    
</div>
      
    </div>

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/uploads/wechatpay.png" alt="TheMusicIsLoud 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/uploads/alipay.png" alt="TheMusicIsLoud 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    TheMusicIsLoud
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/" title="DeepLearning Paper：Detecting Text in Natural Image With CTPN">http://yoursite.com/DeepLearning-Paper/DeepLearning-Paper：Detecting-Text-in-Natural-Image-With-CTPN/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DeepLearning-Paper/" rel="tag"><i class="fa fa-tag"></i> DeepLearning Paper</a>
          
            <a href="/tags/CTPN/" rel="tag"><i class="fa fa-tag"></i> CTPN</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Jpype/Jpype-安装以及使用指南/" rel="next" title="Jpype 安装以及使用指南">
                <i class="fa fa-chevron-left"></i> Jpype 安装以及使用指南
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/TensorFlow/TensorFlow-基本工作原理/" rel="prev" title="TensorFlow 基本工作原理">
                TensorFlow 基本工作原理 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微博</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript">
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80MjA5OC8xODY0NQ=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/header.jpg" alt="TheMusicIsLoud">
            
              <p class="site-author-name" itemprop="name">TheMusicIsLoud</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">78</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">89</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/TheNightIsYoung" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://dev.tencent.com/" title="CloudStudio&&Coding" target="_blank">CloudStudio&&Coding</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Detecting-Text-in-Natural-Image-with-Connectionist-Text-Proposal-Network"><span class="nav-text">Detecting Text in Natural Image with Connectionist Text Proposal Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#摘要"><span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-引言"><span class="nav-text">1. 引言</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-Contributions"><span class="nav-text">1.1 Contributions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-贡献"><span class="nav-text">1.1 贡献</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Related-Work"><span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-相关工作"><span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Connectionist-Text-Proposal-Network"><span class="nav-text">3. Connectionist Text Proposal Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-连接文本提议网络"><span class="nav-text">3. 连接文本提议网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Detecting-Text-in-Fine-scale-Proposals"><span class="nav-text">3.1 Detecting Text in Fine-scale Proposals</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-检测细粒度文本提议"><span class="nav-text">3.1 检测细粒度文本提议</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Recurrent-Connectionist-Text-Proposals"><span class="nav-text">3.2 Recurrent Connectionist Text Proposals</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-循环连接文本提议"><span class="nav-text">3.2 循环连接文本提议</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Side-refinement"><span class="nav-text">3.3 Side-refinement</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-边缘细化"><span class="nav-text">3.3 边缘细化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-Model-Outputs-and-Loss-Functions"><span class="nav-text">3.4 Model Outputs and Loss Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-模型输出与损失函数"><span class="nav-text">3.4 模型输出与损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-Training-and-Implementation-Details"><span class="nav-text">3.5 Training and Implementation Details</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-训练和实现细节"><span class="nav-text">3.5 训练和实现细节</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Experimental-Results-and-Discussions"><span class="nav-text">4. Experimental Results and Discussions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-实验结果和讨论"><span class="nav-text">4. 实验结果和讨论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Conclusions"><span class="nav-text">5 Conclusions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-结论"><span class="nav-text">5 结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TheMusicIsLoud</span>

  
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>

-->


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info//busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      本站访客数<span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      本站总访问量<span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("L40cS1OTf2nXQmbIANou8HvS-gzGzoHsz", "t0xHBc4DURRDc9MDSKX7vx8c");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
