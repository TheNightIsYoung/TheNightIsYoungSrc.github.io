<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[DeepLearning Paper：Detecting Text in Natural Image With CTPN]]></title>
    <url>%2FDeepLearning-Paper%2FDeepLearning-Paper%EF%BC%9ADetecting-Text-in-Natural-Image-With-CTPN%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 阅读指南 推荐在查看这篇论文之前，大家先去了解图像目标检测与识别项目 Faster Region-CNN（R-CNN）的相关实现原理，CTPN 的提出和实现受 Faster R-CNN 影响很大。熟知 Faster R-CNN 算法原理和实现可以帮助你更快、更准确的理解文中内容。 声明：本文 “ CTPN Paper 中英文对照与细节解读 ” 仅供交流学习。如有转载请注明出处，如有侵权请联系删除，谢谢！ Detecting Text in Natural Image with Connectionist Text Proposal NetworkPaper Name： 使用连接文本提议网络实现自然场景文本检测 Paper Authors：Zhi Tian, Weilin Huang, Tong He, Pan He, and Yu Qiao, … Release Time：12 Sep 2016 AbstractWe propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin. The CTPN is computationally efficient with 0.14s/image, by using the very deep VGG16 model [27]. Online demo is available at: http://textdet.com/. Keywords Scene text detection, convolutional network, recurrent neural network, anchor mechanism |↓↓↓ 中文对照 ↓↓↓| 摘要（译） –&gt; 我们提出了一种新颖的 连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。CTPN 直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。我们开发了一种 垂直锚点机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。 序列提议 通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。这使得 CTPN 可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。CTPN 在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。它在 ICDAR 2013 和 2015 的基准数据集上达到了 0.88 和 0.61 的 F-measure，大大超过了最近的结果 [8，35]。通过使用非常深的 VGG16 模型 [27]，CTPN 的计算效率为 0.14s 每张图像。在线演示获取地址：http://textdet.com/。 关键词 场景文本检测，卷积网络，循环神经网络，锚点机制 |↓↓↓↓↓↓↓↓↓↓ Get Start ↓↓↓↓↓↓↓↓↓↓| 1. IntroductionReading text in natural image has recently attracted increasing attention in computer vision [8,14,15,10,35,11,9,1,28,32]. This is due to its numerous practical applications such as image OCR, multi-language translation, image retrieval, etc. It includes two sub tasks: text detection and recognition. This work focus on the detection task [14,1,28,32], which is more challenging than recognition task carried out on a well-cropped word image [15,9]. Large variance of text patterns and highly cluttered background pose main challenge of accurate text localization. |↑↑↑ 中文对照 ↓↓↓| 1. 引言（译） –&gt; 在自然图像中阅读文本最近在计算机视觉中引起越来越多的关注 [8，14，15，10，35，11，9，1，28，32]。这是由于它的许多实际应用，如：图像 OCR（Optical Character Recognition），多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。这项工作的重点是检测任务 [14，1，28，32]，这是比在一个良好的裁剪字图像 [15，9] 进行的识别任务更具有挑战性。多变的文本模式和高度杂乱的背景构成了精确文本定位的主要挑战。 STR 背景与技术难点： 目前图像文本检测和识别领域中（包括两个关键子任务：文本检测和识别），传统的扫描文档图像识别技术（OCR）已经很成熟。自然场景文本识别（Scene Text Recognition，STR）作为检测与识别自然场景图片中的文字信息备受关注。 而相较于识别任务，STR 任务的重点是文本检测任务。然而由于其多样的文本模式（多语言、多样式文本）、高度杂乱的背景（干扰纹理：非文字区域有近似文字的纹理）或者自然因素（文本区域变形、残缺、模糊等）等的干扰，自然场景图像中的文字检测与识别任务，其难度远大于扫描文档图像或良好的裁剪字图像中的文字识别。 Current approaches for text detection mostly employ a bottom-up pipeline [28,1,14,32,33]. They commonly start from low-level character or stroke detection, which is typically followed by a number of subsequent steps: non-text component filtering, text line construction and text line verification. These multi-step bottom-up approaches are generally complicated with less robustness and reliability. Their performance heavily rely on the results of character detection, and connected-components methods or sliding-window methods have been proposed. These methods commonly explore low-level features (e.g., based on SWT [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background. However, they are not robust by identifying individual strokes or characters separately, without context information. For example, it is more confident for people to identify a sequence of characters than an individual one, especially when a character is extremely ambiguous. These limitations often result in a large number of non-text components in character detection, causing main difficulties for handling them in following steps. Furthermore, these false detections are easily accumulated sequentially in bottom-up pipeline, as pointed out in [28]. To address these problems, we exploit strong deep features for detecting text information directly in convolutional maps. We develop text anchor mechanism that accurately predicts text locations in fine scale. Then, an in-network recurrent architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information. |↑↑↑ 中文对照 ↓↓↓| 目前的文本检测方法大多采用自下而上的流程 [28，1，14，32，33]。它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。它们的性能 很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。这些方法通常探索低级特征（例如，基于 SWT [3，13]，MSER [14，33，23] 或 HoG [28] ）来区分候选文本和背景。但是，如果没有上下文信息，他们不能鲁棒的单独识别各个笔划或字符。例如，相比单个字符人们更信任一个字符序列，特别是当一个字符非常模糊时。这些限制在字符检测中通常会导致大量非文本组件，在后续步骤中的主要困难是处理它们。此外，正如 [28] 所指出的，这些误检很容易在自下而上的过程中连续累积。为了解决这些问题，我们利用强大的深度特征直接在卷积映射中检测文本信息。我们开发的文本锚点机制能在细粒度上精确预测文本位置。然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。 STR 文本检测问题与 Faster Region-CNN（R-CNN）： 上面提到 STR 任务的重点难点是文本检测任务。对于文本检测，大佬们指出时下（传统）文本检测大多采用 bottom-up（自底向上）的检测方法（先检测低级别字符或笔画，再构建成文本行/线（文本线：是一个由字符、局部字符、多字符组成序列 Sequence）。这种自底向上检测方法的存在很明显的缺陷：（1）它们的性能很大程度依赖于字符检查结果，当字符检测中产生误检时，这些误检很容易在自下而上的过程中累积，产生更多的误检；（2）没有考虑上下文信息，不能鲁棒的单独识别各个笔划或字符，可靠性差；（3）检测后需要多个后续的子模块来构建文本行/线，模型比较复杂。这些因素导致了模型性能受限。 基于以上问题的思考，受益于 Faster Region-CNN（R-CNN）系统实现原理。大佬们提出：使用图像文本信息的深度卷积特征来取代低级特征，使用文本锚点机制在细粒度精度上预测文本区域，并通过使用一种网内循环（RNN）架构探索其上下文信息，按顺序连接这些细粒度文本提议。继续往下看，你可以发现其实大佬们提出的是一种 Top-down（自上而下）的文本检测方法（先检测文本区域，再将其连接成文本行/线）。实现细节继续往下看 —–&gt; Deep Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps. Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on generic object detection. However, it is difficult to apply these general object detection systems directly to scene text detection, which generally requires a higher localization accuracy. In generic object detection, each object has a well-defined closed boundary [2], while such a well-defined boundary may not exist in text, since a text line or word is composed of a number of separate characters or strokes. For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it. By contrast, reading text comprehensively is a fine-grained recognition task which requires a correct detection that covers a full region of a text line or word. Therefore, text detection generally requires a more accurate localization, leading to a different evaluation standard, e.g., the Wolf’s standard [30] which is commonly employed by text benchmarks [19,21]. |↑↑↑ 中文对照 ↓↓↓| 深度卷积神经网络（CNN）最近已经基本实现了一般物体检测 [25，5，6]。最先进的方法是 Faster Region-CNN（R-CNN）系统 [25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成 高质量类别不可知的目标提议。然后将 RPN 提议输入 Faster R-CNN[5] 模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。然而，很难将这些通用目标检测系统直接应用于场景文本检测，这通常需要更高的定位精度。在通用目标检测中，每个目标都有一个明确的封闭边界 [2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL 标准 [4]）之间的重叠 &gt; 0.5，因为人们可以容易地从目标的主要部分识别它。相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的 Wolf 标准 [19，21]。 Faster Region-CNN（R-CNN）对于图像文本区域检测的不适用： 同属于图像目标检测，从一般物体区域目标检测到文本区域目标检测，大佬们借鉴 Faster Region-CNN（R-CNN）系统的设计想法。但若直接采用基于 Faster Region-CNN 等通用物体检测框架的算法都会面临一个问题：如何生成好的 text region proposal ?（不同于一般物体检测的松散定义，文本检测通常需要更准确的定义），这实际上比较难解决。 In this work, we fill this gap by extending the RPN architecture [25] to accurate text line localization. We present several technical developments that tailor generic object detection model elegantly towards our problem. We strive for a further step by proposing an in-network recurrent mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model. |↑↑↑ 中文对照 ↓↓↓| 在这项工作中，我们通过将 RPN 架构 [25] 扩展到准确的文本行定义（细粒度文本区域提议）来填补这个空白。我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的 CNN 检测模型进行进一步的后处理。 垂直锚点机制和网内循环: 基于 “ 如何生成好的 text region proposal ？ ”，大佬们将 Faster R-CNN 中的 RPN 架构扩展到精准的文本行定义（将 RPN 锚点机制生成目标区域提议扩展成文本垂直锚点机制（细粒度文本区域提议））。大佬们的思路是检测一个一个小的，固定宽度的文本段，然后在后续处理部分再将这些小的文本段连接起来获得文本行。检测到的文本段的示意图如下图所示： 个人理解： 可以发现，与传统文本检测方法相比，就如我们前面提到的：使用深度卷积特征替换了低级特征；直接通过文本垂直锚点机制给出细粒度文本区域提议，不再做字符检测；使用网内循环编码文本上下文信息使得检测的文本行更鲁棒。 1.1 ContributionsWe propose a novel Connectionist Text Proposal Network (CTPN) that directly localizes text sequences in convolutional layers. This overcomes a number of main limitations raised by previous bottom-up approaches building on character detection. We leverage the advantages of strong deep convolutional features and sharing computation mechanism, and propose the CTPN architecture which is described in Fig. 1. It makes the following major contributions: Fig. 1: (a) Architecture of the Connectionist Text Proposal Network (CTPN). We densely slide a 3×3 spatial window through the last convolutional maps (conv5 ) of the VGG16 model [27]. The sequential windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs). The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, y-axis coordinates and side-refinement offsets of k anchors. (b) The CTPN outputs sequential fixed-width fine-scale text proposals. Color of each box indicates the text/non-text score. Only the boxes with positive scores are presented. |↑↑↑ 中文对照 ↓↓↓| 1.1 贡献（译） –&gt; 我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。这克服了以前的建立在字符检测基础上的自下而上方法带来的一些主要限制。我们利用强深度卷积特性和共享计算机制的优点，提出了如图 1 所示的 CTPN 架构。主要贡献如下： 图1：（a）连接文本提议网络（CTPN）的架构。我们通过 VGG16 模型 [27] 的最后一个卷积映射（conv5）密集地滑动 3×3 空间窗口。每行的序列窗口通过双向 LSTM（BLSTM）[7] 循环连接，其中每个窗口的卷积特征（3×3×C）被用作 256 维的 BLSTM（包括两个 128 维的 LSTM ）的输入。RNN 层连接到 512 维的全连接层，接着是输出层，联合预测 k 个锚点的文本/非文本分数，y 轴坐标和边缘调整偏移。（b）CTPN 输出连续的固定宽度细粒度文本提议。每个框的颜色表示文本/非文本分数。只显示正分数对应的文本框。 First, we cast the problem of text detection into localizing a sequence of fine-scale text proposals. We develop an anchor regression mechanism that jointly predicts vertical location and text/non-text score of each text proposal, resulting in an excellent localization accuracy. This departs from the RPN prediction of a whole object, which is difficult to provide a satisfied localization accuracy. |↑↑↑ 中文对照 ↓↓↓| 首先，我们将文本检测的问题转化为一系列细粒度的文本提议。我们开发了一个（垂直）锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。这背离了整个目标的 RPN 预测，RPN 预测难以提供令人满意的定位精度。 内容解读： 关于细粒度的文本提议（text region proposal），作者提出了 Vertical Anchor 机制。基本想法就是去预测文本的竖直方向上的位置，水平方向的位置不预测。与 Faster RCNN 中的 anchor 类似，但是不同的是，Vertical Anchor 的宽度都是固定好的了，本文后面你会看到，本文中 Vertical Anchor 宽度设置为 16 个像素。而高度则从 11 像素到 273 像素变化，总共 10 个 anchor. Second, we propose an in-network recurrence mechanism that elegantly connects sequential text proposals in the convolutional feature maps. This connection allows our detector to explore meaningful context information of text line, making it powerful to detect extremely challenging text reliably. |↑↑↑ 中文对照 ↓↓↓| 其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。通过这种连接，我们的检测器可以探索文本行有意义的上下文信息，使其能够可靠地检测极具挑战性的文本。 内容解读： 关于序列提议，作者将 VGG16 Net conv5_3 卷积层的 feature map 中每行序列窗口的卷积映射（3×3×C）输入到 256 维的 BLSTM（包括两个 128 维的 LSTM ）网络中，用于编码上下文信息。 Third, both methods are integrated seamlessly to meet the nature of text sequence, resulting in a unified end-to-end trainable model. Our method is able to handle multi-scale and multi-lingual text in a single process, avoiding further post filtering or refinement. |↑↑↑ 中文对照 ↓↓↓| 第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。 Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015). Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep VGG16 model [27]. |↑↑↑ 中文对照 ↓↓↓| 第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88 的 F-measure 超过了 2013 年 ICDAR 的 [8] 中的 0.83，而 0.64 的 F-measure 超过了 ICDAR2015 上[35] 中的 0.54 ）。此外，通过使用非常深的 VGG16 模型 [27]，这在计算上是高效的，导致了每张图像 0.14s 的运行时间（在 ICDAR 2013 上）。 关于上述具体的实现细节可以参见本文第三部分。 2. Related WorkText detection. Past works in scene text detection have been dominated by bottom-up approaches which are generally built on stroke or character detection. They can be roughly grouped into two categories, connected-components (CCs) based approaches and sliding-window based methods. The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3]. The sliding-window based methods detect character candidates by densely moving a multi-scale window through an image. The character or non-character window is discriminated by a pre-trained classifier, by using manually-designed features [28,29], or recent CNN features [16]. However, both groups of methods commonly suffer from poor performance of character detection, causing accumulated errors in following component filtering and text line construction steps. Furthermore, robustly filtering out non-character components or confidently verifying detected text lines are even difficult themselves [1,33,14]. Another limitation is that the sliding-window methods are computationally expensive, by running a classifier on a huge number of the sliding windows. |↑↑↑ 中文对照 ↓↓↓| 2. 相关工作（译） –&gt; 主要内容解读：这一部分主要提到 Paper 图像文本检测与识别相关技术背景：（1）文本检测。（2）目标检测。 文本检测。过去在场景文本检测中的工作一直以自下而上的方法为主，一般建立在笔画或字符检测上。它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。基于 CC 的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等 [33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。基于滑动窗口的方法通过在图像中密集地滑动多尺度窗口来检测候选字符。字符或非字符窗口通过预先训练的分类器，使用手动设计的特征 [28，29] 或最近的 CNN 特征 [16] 进行区分。然而，这两种方法通常都会受到较差的字符检测性能的影响，导致在接下来的组件过滤和文本行构建步骤中出现累积的错误。此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难 [1，33，14]。另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。 Object detection. Convolutional Neural Networks (CNN) have recently advanced general object detection substantially [25,5,6]. A common strategy is to generate a number of object proposals by employing inexpensive low-level features, and then a strong CNN classifier is applied to further classify and refine the generated proposals. Selective Search (SS) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5]. Recently, Ren et al. [25] proposed a Faster R-CNN system for object detection. They proposed a Region Proposal Network (RPN) that generates high-quality class-agnostic object proposals directly from the convolutional feature maps. The RPN is fast by sharing convolutional computation. However, the RPN proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5]. More importantly, text is different significantly from general objects, making it difficult to directly apply general object detection system to this highly domain-specific task. |↑↑↑ 中文对照 ↓↓↓| 目标检测。卷积神经网络（CNN）近来在通用目标检测 [25，5，6] 上已经取得了实质的进步。一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强 CNN 分类器来进一步对生成的提议进行分类和细化。生成类别不可知目标提议的选择性搜索（SS）[4] 是目前领先的目标检测系统中应用最广泛的方法之一，如 CNN（R-CNN）[6] 及其扩展 [5]。最近，Ren 等人 [25] 提出了 Faster R-CNN 目标检测系统。他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。通过共享卷积计算 RPN 是快速的。然而，RPN 提议不具有判别性，需要通过额外的成本高昂的 CNN 模型（如 Fast R-CNN 模型 [5]）进一步细化和分类。更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。 重点来了…… 3. Connectionist Text Proposal NetworkThis section presents details of the Connectionist Text Proposal Network (CTPN). It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent connectionist text proposals, and side-refinement. |↑↑↑ 中文对照 ↓↓↓| 3. 连接文本提议网络（译） –&gt; 本节介绍连接文本提议网络（CTPN）的细节。它包括三个关键的贡献，使文本定位可靠和准确：（3.1）检测细粒度提议中的文本；（3.2）循环连接文本提议；（3.3）边缘细化。 主要内容解读：通过上文叙述，我们已经大致了解了大佬们提出的连接文本提议网络实现架构。这一部分我们来看其三个关键结构实现细节：3.1、3.2 以及 3.3。 3.1 Detecting Text in Fine-scale ProposalsSimilar to Region Proposal Network (RPN) [25], the CTPN is essentially a fully convolutional network that allows an input image of arbitrary size. It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of fine-scale (e.g., fixed 16-pixel width) text proposals, as shown in Fig. 1 (b). |↑↑↑ 中文对照 ↓↓↓| 类似于区域提议网络（RPN）[25]，CTPN 本质上是一个全卷积网络，允许任意大小的输入图像。它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的 16 个像素）文本提议，如图1（b）所示。 We take the very deep 16-layer vggNet (VGG16) [27] as an example to describe our approach, which is readily applicable to other deep models. Architecture of the CTPN is presented in Fig. 1 (a). We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (e.g., the conv5 of the VGG16). The size of conv5 feature maps is determined by the size of input image, while the total stride and receptive field are fixed as 16 and 228 pixels, respectively. Both the total stride and receptive field are fixed by the network architecture. Using a sliding window in the convolutional layer allows it to share convolutional computation, which is the key to reduce computation of the costly sliding-window based methods. |↑↑↑ 中文对照 ↓↓↓| 我们以非常深的 16 层 VGGNet（VGG16）[27] 为例来描述我们的方法，该方法很容易应用于其他深度模型。CTPN 的架构如图 1（a）所示。我们使用一个小的空间窗口 3×3 来滑动最后的卷积层特征映射（例如：VGG16 的 conv5）。conv5 特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为 16 个和 228 个像素。网络架构决定总步长和感受野。在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算量的关键。 内容解读：这里作者选择 VGG16 Net （conv5_3）作为强深度共享卷积层的 base net，然后选用 3×3 滑动窗口在 conv5_3 的 feature map 上移动 Generally, sliding-window methods adopt multi-scale windows to detect objects of different sizes, where one window scale is fixed to objects of similar size. In [25], Ren et al. proposed an efficient anchor regression mechanism that allows the RPN to detect multi-scale objects with a single-scale window. The key insight is that a single window is able to predict objects in a wide range of scales and aspect ratios, by using a number of flexible anchors. We wish to extend this efficient anchor mechanism to our text task. However, text differs from generic objects substantially, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2]. Text is a sequence which does not have an obvious closed boundary. It may include multi-level components, such as stroke, character, word, text line and text region, which are not distinguished clearly between each other. Text detection is defined in word or text line level, so that it may be easy to make an incorrect detection by defining it as a single object, e.g., detecting part of a word. Therefore, directly predicting the location of a text line or word may be difficult or unreliable, making it hard to get a satisfied accuracy. An example is shown in Fig. 2, where the RPN is directly trained for localizing text lines in an image. Fig. 2: Left: RPN proposals. Right: Fine-scale text proposals. |↑↑↑ 中文对照 ↓↓↓| 通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。在 [25] 中，Ren 等人提出了一种有效的锚点回归机制，允许 RPN 使用单尺度窗口检测多尺度目标。关键的洞察力是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。我们希望将这种有效的锚点机制扩展到我们的文本任务。然而，实质上文本与普通目标不同，（普通目标）它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标 [2]。文本是一个没有明显封闭边界的序列。它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得令人满意的准确性。一个例子如图 2 所示，其中 RPN 直接被用于定位图像中的文本行。 图2：左：RPN提议。右：细粒度的文本提议。 内容解读：这一部分我们来解读大佬们提出的通过垂直描点机制来构建细粒度文本区域提议方法。大佬们分析了文本区域检测以及普通物体目标区域检测的区别，以及 Faster RCNN 中的多尺度描点机制对于生成文本区域提议存在的缺陷。并且通过使用训练 RPN 用于定位图像中的文本行，可视化结果表明与分析结果一致。 We look for a unique property of text that is able to generalize well to text components in all levels. We observed that word detection by the RPN is difficult to accurately predict the horizontal sides of words, since each character within a word is isolated or separated, making it confused to find the start and end locations of a word. Obviously, a text line is a sequence which is the main difference between text and generic objects. It is natural to consider a text line as a sequence of fine-scale text proposals, where each proposal generally represents a small part of a text line, e.g., a text piece with 16-pixel width. Each proposal may include a single or multiple strokes, a part of a character, a single or multiple characters, etc. We believe that it would be more accurate to just predict the vertical location of each proposal, by fixing its horizontal location which may be more difficult to predict. This reduces the search space, compared to the RPN which predicts 4 coordinates of an object. We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and y-axis location of each fine-scale proposal. It is also more reliable to detect a general fixed-width text proposal than identifying an isolate character, which is easily confused with part of a character or multiple characters. Furthermore, detecting a text line in a sequence of fixed-width text proposals also works reliably on text of multiple scales and multiple aspect ratios. |↑↑↑ 中文对照 ↓↓↓| 我们寻找文本的独特属性，能够很好地概括各个层次的文本组件。我们观察到由 RPN 进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。显然，文本行是一个序列，它是文本和通用目标之间的主要区别。将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为 16 个像素的文本块。每个提议可能包含单个或多个笔划，字符的一部分，单个或多个字符等。我们认为，通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测。与预测目标 4 个坐标的 RPN 相比，这减少了搜索空间。我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。检测一般固定宽度的文本提议比识别分隔的字符更可靠，分隔字符容易与字符或多个字符的一部分混淆。此外，检测一系列固定宽度文本提议中的文本行也可以在多个尺度和多个长宽比的文本上可靠地工作。 内容解读：前面我们提到，不管是自上而下方法还是自下而上的方法，最终都需要构建成文本行。文本行是一个序列，它是文本和通用目标之间的主要区别。于是将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常都代表了文本行的一小部分。大佬们认为通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测，这就是垂直描点机制。 To this end, we design the fine-scale text proposal as follow. Our detector investigates each spatial location in the conv5 densely. A text proposal is defined to have a fixed width of 16 pixels (in the input image). This is equal to move the detector densely through the conv5 maps, where the total stride is exactly 16 pixels. Then we design k vertical anchors to predict y-coordinates for each proposal. The k anchors have a same horizontal location with a fixed width of 16 pixels, but their vertical locations are varied in k different heights. In our experiments, we use ten anchors for each proposal, k=10, whose heights are varied from 11 to 273 pixels (by ÷0.7 each time) in the input image. The explicit vertical coordinates are measured by the height and y-axis center of a proposal bounding box. We compute relative predicted vertical coordinates (v) with respect to the bounding box location of an anchor as, $$v_c=(c_y-c_y^a)/h^a,v_h=log(h/h^a)​$$ $$v_c^☆=(c_y^☆ - c_y^a)/h^a, v_h^☆=log(h^☆/h^a)$$ where v={vc,vh} and v∗={v∗c,v∗h} are the relative predicted coordinates and ground truth coordinates, respectively. cay and ha are the center (y-axis) and height of the anchor box, which can be pre-computed from an input image. cy and h are the predicted y-axis coordinates in the input image, while c∗y and h∗ are the ground truth coordinates. Therefore, each predicted text proposal has a bounding box with size of h×16 (in the input image), as shown in Fig. 1 (b) and Fig. 2 (right). Generally, an text proposal is largely smaller than its effective receptive field which is 228×228. |↑↑↑ 中文对照 ↓↓↓| 为此，我们设计如下的细粒度文本提议。我们的检测器密集地调查了 conv5 中的每个空间位置。文本提议被定义为具有 16 个像素的固定宽度（在输入图像中）。这相当于在 conv5 的映射上密集地移动检测器，其中总步长恰好为 16 个像素。然后，我们设计 k 个垂直锚点来预测每个提议的 y 坐标。k 个锚点具有相同的水平位置，固定宽度为 16 个像素，但其垂直位置在 k 个不同的高度变化。在我们的实验中，我们对每个提议使用十个锚点，k=10，其高度在输入图像中从 11 个像素变化到 273 个像素（每次 ÷0.7）。明确的垂直坐标是通过提议边界框的高度和 y 轴中心来度量的。我们计算相对于锚点的边界框位置的相对预测的垂直坐标（v），如下所示： $$ v_c=(c_y-c_y^a)/h^a,v_h=log(h/h^a) ​$$ $$v_c^☆=(c_y^☆-c_y^a)/h^a,v_h^☆=log(h^☆/h^a)$$ 其中$v={v_c，v_h}$和$v^☆={v_c^☆，v_h^☆}$分别是相对预测坐标和实际坐标。$c_y^☆$ 和 $h^a$ 是锚盒的中心（y 轴）和高度，可以从输入图像预先计算。$c_y$ 和 $h$ 是输入图像中预测的 $y$ 轴坐标，而 $c_y^☆$ 和 $h^☆$ 实际坐标。因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为 h×16 的边界框（在输入图像中）。一般来说，文本提议在很大程度上要比它的有效感受野 228×228 要小。 内容解读：CTPN 垂直描点机制针对的是水平文字检测。大佬们采用了一组（10 个）等宽度的 Anchors，用于定位文字位置。Anchor尺寸：widths=[16]，heights=[11,16,23,33,48,68,97,139,198,283]。由于 CTPN 采用 VGG16 Net 模型提取卷积特征，故 conv5_3 feature map 的宽高都是输入 Image 宽高的 1/16。同时 fc 和 conv5_3 的 width、height 都相等。下面给出 CTPN 垂直描点机制的 Anchor 示意图： The detection processing is summarised as follow. Given an input image, we have W×H×C conv5 features maps (by using the VGG16 model), where C is the number of feature maps or channels, and W×H is the spatial arrangement. When our detector is sliding a 3×3 window densely through the conv5, each sliding-window takes a convolutional feature of 3×3×C for producing the prediction. For each prediction, the horizontal location (x-coordinates) and k-anchor locations are fixed, which can be pre-computed by mapping the spatial window location in the conv5 onto the input image. Our detector outputs the text/non-text scores and the predicted y-coordinates (v) for k anchors at each window location. The detected text proposals are generated from the anchors having a text/non-text score of &gt;0.7 (with non-maximum suppression). By the designed vertical anchor and fine-scale detection strategy, our detector is able to handle text lines in a wide range of scales and aspect ratios by using a single-scale image. This further reduces its computation, and at the same time, predicting accurate localizations of the text lines. Compared to the RPN or Faster R-CNN system [25], our fine-scale detection provides more detailed supervised information that naturally leads to a more accurate detection. |↑↑↑ 中文对照 ↓↓↓| 检测处理总结如下。给定输入图像，我们有 W×H×C conv5 特征映射（通过使用 VGG16 Net 模型），其中 C 是特征映射或通道的数目，并且 W×H 是空间布置。当我们的检测器通过 conv5 密集地滑动 3×3 窗口时，每个滑动窗口使用 3×3×C 的卷积特征来产生预测。对于每个预测，水平位置（x 轴坐标）和 k 个锚点位置是固定的，可以通过将 conv5 中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出 k 个锚点的文本/非文本分数和预测的 y 轴坐标（v）。检测到的文本提议是从具有 &gt;0.7（具有非极大值抑制）的文本/非文本分数的锚点生成的。通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。与 RPN 或 Faster R-CNN 系统 [25] 相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。]]></content>
      <categories>
        <category>DeepLearning Paper</category>
      </categories>
      <tags>
        <tag>DeepLearning Paper</tag>
        <tag>CTPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习模型移动和嵌入式端落地方案：TFM 实现 TF MNIST 手写数字识别模型移植到 Android 端]]></title>
    <url>%2FTensorFlow%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%A7%BB%E5%8A%A8%E5%92%8C%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%AB%AF%E8%90%BD%E5%9C%B0%E6%96%B9%E6%A1%88%EF%BC%9ATFM-%E5%AE%9E%E7%8E%B0-TF-MNIST-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E7%A7%BB%E6%A4%8D%E5%88%B0-Android-%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 写在前面： 上一篇给出了一个简单的 TensorFlow For Mobile 使用样例，这一篇博文我们利用 TFM 策略来实现 MNIST 手写数字识别模型移植到 Android 手机的全过程。 关于移动和嵌入式平台的深度学习解决方案，这里将其编为一个博文系列，并给出相关文章的索引，仅供参考： 系列博文之一：深度学习模型移动和嵌入式端落地方案：深度学习模型移植到 Android ; 系列博文之二：深度学习模型移动和嵌入式端落地方案：TFM 实现 TF MNIST 手写数字识别模型移植到 Android 端； Blog Abstract这里先给这篇博文来个读前吧，帮助大家更好的了解博文主要内容和结构。 关于本文：推荐在阅读本文之前先进行阅读前面的文章，你会有一个更好的思路。 本文（深度学习模型移植到 Android）采用的策略是 TFM。 本文主要是针对模型移植过程进行说明，对 UI、Android 以及 TF 的细节并不会做过多的说明。 文中如有不恰当之处还请指正，欢迎各位大佬在评论区指出不足之处。 Envs：本系列博文均在以下环境下进行 Demo 测试： Windows10 Ubuntu 16.04 Server TensorFlow Env：Ubuntu 16.04 + TF-GPU 1.6（CUDA 9.0 + cuDNN 7.0） Android Studio 3.x Real Mobile phone envs：SAMSUNG Galaxy S7 TFM 实现 TF MNIST 手写数字识别模型移植到 Android 端本文我们来看 TFM MNIST Demo：利用 TensorFlow 实现一个 MNIST 手写数字识别模型并进行持久化，然后把持久化的模型移植到 Android 端（SAMSUNG Galaxy S7）上运行。 正如官网给出的关于 “Android TensorFlow support” 描述的： 1# Components (a native .so library and a Java JAR) geared towards supporting TensorFlow on Android. 这里，我们首先给出 TFM 移植方案的一般思路： 一般先利用 Bazel 工具把 TensoFlow 编译成 .so 库文件 和 jar 包，进行 Android 相关配置，即可实现 TF 模型的移植。 Bazel 编译成功后，”.so library” 和 “Java JAR” 分别对应的具体文件名称如下： libtensorflow_inference.so libandroid_tensorflow_inference_java.jar 将 1、2 导入 Android 平台后便可以使用其中提供的接口调用 TF Model。 –&gt; 注意： 手动使用 Bazel 编译 TensoFlow 过程中可能遇到很多问题，导致最终编译 .so 库文件 和 jar 包 失败，这是很常见的！！！事实上，不会使用 Bazel 编译也没关系，TensoFlow 官方已经为开发者提供了编译好的 .so库文件 和对应的 jar 包 文件，下载后即可使用。 这里先不要急，后面我会给出上述的两种 “.so library” 和 “Java JAR” 获取方式的具体说明。 1. 预移植环境准备关于预移植环境准备请参见 –&gt; 系列博文之一：深度学习模型移动和嵌入式端落地方案：深度学习模型移植到 Android ; 中【1. 预移植环境准备】。 总结一下，也就是搭建基于 Android Studio IDE 的 Android 开发环境，以及下载用于在 Android 平台调用 TF 模型依赖的 .so library 和 Java JAR 插件。 假设这里我们已经完成了 Android Studio 的安装以及配置，并且依据下载好了 .so library 和 Java JAR 插件，下载地址见 –&gt; 这里。 预移植环境准备完成的话，下面开始正式的 MNIST 手写数字识别模型的移植过程： 2. Moblile Phone 调用 MNIST Model 完成移植过程上面我们给出 TFM 移植方案的一般思路，这里我们来看移动或嵌入式端掉用 TF 模型完成移植的过程简介： 获取训练好的 TF Model，并且明确模型网络节点信息（尤其是输入输出节点），完成模型预测可用验证； 在 Android Studio 中创建 Android 项目，导入 Android 平台调用 TF 模型需要的 jar 包和 so 文件 (负责 TF 模型的解析和运算)，以及 TF Model ，并完成相关的项目配置； 创建并完成调用：定义变量用于储存模型输入输出，并通过 jar 包提供的接口进行 TF 模型的调用。还需要简单注意与 Android UI 页面的关联。 移植测试 下面，我们将根据上述移植过程简介来完成 Moblile Phone 调用 TF Model 完成移植的全过程： 2.1 Prepare Pretrain TF Model先来测试一个最简单的 MNIST Demo：使用单隐藏层的 SoftMax Regression 作为分类器进行 MNIST 的识别，然后将模型持久化成 PB 模型用于完成后续的移植。 1 –&gt; MNIST 手写数字识别模型持久化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import tensorflow as tffrom tensorflow.python.framework import graph_utilfrom tensorflow.examples.tutorials.mnist import input_dataprint('tensortflow:&#123;0&#125;'.format(tf.__version__))mnist = input_data.read_data_sets("MNIST_DATA/", one_hot=True)with tf.name_scope('input'): x = tf.placeholder(tf.float32, [None,784], name='input_x') y_ = tf.placeholder(tf.float32, [None,10], name='input_y')with tf.name_scope('layer'): with tf.name_scope('W'): W = tf.Variable(tf.zeros([784,10]), name='Weights') with tf.name_scope('b'): b = tf.Variable(tf.zeros([10]), name='biases') with tf.name_scope('W_p_b'): Wx_plus_b = tf.add(tf.matmul(x, W), b, name='Wx_plus_b') y = tf.nn.softmax(Wx_plus_b, name='final_result')with tf.name_scope('loss'): loss = -tf.reduce_sum(y_ * tf.log(y))with tf.name_scope('train_step'): train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)sess = tf.InteractiveSession()init = tf.global_variables_initializer()sess.run(init)for step in range(100): batch_xs,batch_ys =mnist.train.next_batch(100) train_step.run(&#123;x:batch_xs,y_:batch_ys&#125;)pre_num = (tf.argmax(y, 1, output_type='int32', name="output"))correct_prediction = tf.equal(pre_num,tf.argmax(y_, 1, output_type='int32'))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))val = accuracy.eval(&#123;x:mnist.test.images,y_:mnist.test.labels&#125;)print('The accuracy of test dataSets：&#123;0&#125;'.format(val))output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def,output_node_names=['output'])with tf.gfile.FastGFile('mnist.pb', mode='wb') as f: f.write(output_graph_def.SerializeToString())sess.close() 运行后会在程序同名目录下生成一个：model.pb （TensorFlow 持久化文件）。 2 –&gt; 明确模型网络节点信息以及验证模型预测可用 测试模型可用样例图片如下： 下面来看具体的验证程序： 12345678910111213141516171819202122232425262728293031323334353637383940import tensorflow as tfimport numpy as npfrom PIL import Imageimport matplotlib.pyplot as pltmodel_path = "./mnist.pb"testImage = Image.open("./data/num_7.jpg")with tf.Graph().as_default() as graph: print("Loading Model &gt;&gt;&gt;&gt;&gt;") with tf.gfile.GFile(model_path, "rb") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name="") print("&lt;&lt;&lt; success!") print("Neural Network Node Information ---&gt;") for op in graph.get_operations(): # print(op) print(op.name, ":", op.values) config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False) with tf.Session(config=config) as sess: tf.global_variables_initializer().run() input_x = sess.graph.get_tensor_by_name("input/input_x:0") output = sess.graph.get_tensor_by_name("output:0") testImage=testImage.convert('L') testImage = testImage.resize((28, 28)) test_input = np.array(testImage) test_input = test_input.reshape(1, 28 * 28) pre_num = sess.run(output, feed_dict = &#123;input_x: test_input&#125;) print('The Result Of Model Prediction [number] is ', pre_num) 输出结果如下： 12345678910111213141516Loading Model &gt;&gt;&gt;&gt;&gt;&lt;&lt;&lt; success!Neural Network Node Information ---&gt;input/input_x : &lt;bound method Operation.values of &lt;tf.Operation 'input/input_x' type=Placeholder&gt;&gt;layer/W/Weights : &lt;bound method Operation.values of &lt;tf.Operation 'layer/W/Weights' type=Const&gt;&gt;layer/W/Weights/read : &lt;bound method Operation.values of &lt;tf.Operation 'layer/W/Weights/read' type=Identity&gt;&gt;layer/b/biases : &lt;bound method Operation.values of &lt;tf.Operation 'layer/b/biases' type=Const&gt;&gt;layer/b/biases/read : &lt;bound method Operation.values of &lt;tf.Operation 'layer/b/biases/read' type=Identity&gt;&gt;layer/W_p_b/MatMul : &lt;bound method Operation.values of &lt;tf.Operation 'layer/W_p_b/MatMul' type=MatMul&gt;&gt;layer/W_p_b/Wx_plus_b : &lt;bound method Operation.values of &lt;tf.Operation 'layer/W_p_b/Wx_plus_b' type=Add&gt;&gt;layer/final_result : &lt;bound method Operation.values of &lt;tf.Operation 'layer/final_result' type=Softmax&gt;&gt;output/dimension : &lt;bound method Operation.values of &lt;tf.Operation 'output/dimension' type=Const&gt;&gt;output : &lt;bound method Operation.values of &lt;tf.Operation 'output' type=ArgMax&gt;&gt;......The Result Of Model Prediction [number] is [7] 由上述输出信息我们知道，MNIST 固化模型中输入（inputName）节点名称为：input/input_x，输出（outputName）节点名称为：output，并且预测模型是可用的。 2.2 Android Project In Android Studio2.2.1 Android Project 配置1 –&gt; Android Studio 中新建一个 Android Project; 2 –&gt; 将 2.1中训练得到的 TF Model：model.pb 文件存放到 ...app/src/main/assets 目录下，若 assets 目录不存在则创建； 3 –&gt; 项目中导入 “.so library” 和 “Java JAR”： 将 libandroid_tensorflow_inference_java.jar 存放到 .../app/libs 目录下； .../app/libs 目录下新建 armeabi 文件夹，并将 libtensorflow_inference.so 放进去。 4 –&gt; 配置 build.gradle（Module：app） 以及 gradle.properties： build.gradle（Module：app） 位于 ../app 目录下。 –&gt; 4.1）在 ../app/build.gradle 文件中的 android 节点追加 soureSets 节点信息，用于指定 jniLibs 的路径： 12345sourceSets &#123; main &#123; jniLibs.srcDirs = ['libs'] &#125; &#125; –&gt; 4.2）在 ../app/build.gradle 中的 dependencies 节点追加如下信息： 1implementation files('libs/libandroid_tensorflow_inference_java.jar') 这里，其实就是增加 libandroid_tensorflow_inference_java.jar 依赖，否则无法解析到 TensorFlow 包，产生报错。还有一种方法就是找到其位置（…/app/libs），右键 “add as Libary” 会自动添加到 dependencies 中。 –&gt; 4.3）并且在 ../app/build.gradle 中的 defaultConfig 节点追加如下信息： 123ndk &#123; abiFilters &quot;armeabi-v7a&quot; &#125; 5）–&gt; 添加 Bitmap 格式 的测试样例图片到 将 Bitmap 格式的测试样例图片存放于 ..app/main/res 目录下，用于后续的模拟器或真机环境测试。 好了，先构建一下看看是否报错，解决一下错误信息，自此关于 Android Studio 中 Android 项目的创建配置就完成。接下来看如何在我们已经创建好的 Android 项目中实现 TF Model 的调用： 2.2.2 模型的调用–&gt; 新建一个 MyTSFMNISTDemo 类（…app/src/main/java/com.xxxx.xxxx/ 下），在这个类里面进行模型的调用，即通过输入获取模型输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import android.content.res.AssetManager;import android.graphics.Bitmap;import android.graphics.Color;import android.graphics.Matrix;import android.util.Log;import org.tensorflow.contrib.android.TensorFlowInferenceInterface;public class MyTSFMNISTDemo &#123; private static final String TAG = "MyTSFMNISTDemo"; private static final String MODEL_FILE = "file:///android_asset/mnist.pb"; // 定义数据的维度: private static final int IN_COL = 1; private static final int IN_ROW = 28*28; private static final int OUT_COL = 1; private static final int OUT_ROW = 1; // 模型中输入变量的名称: private static final String inputName = "input/input_x"; // 模型中输出变量的名称: private static final String outputName = "output"; TensorFlowInferenceInterface inferenceInterface; static &#123; // 加载库文件: System.loadLibrary("tensorflow_inference"); Log.e(TAG,"libtensorflow_inference.so loading successfully."); &#125; MyTSFMNISTDemo(AssetManager assetManager) &#123; // 接口定义: inferenceInterface = new TensorFlowInferenceInterface(assetManager,MODEL_FILE); Log.e(TAG, "Initialize TensorFlowInferenceInterface successfully."); &#125; public int[] getPredict(Bitmap bitmap) &#123; // 需要将图片缩放到 [28*28]，用于初始化样例数据： float[] inputdata = bitmapToFloatArray(bitmap,28, 28); // 将样例数据 feed 给 tensorflow model: inferenceInterface.feed(inputName, inputdata, IN_COL, IN_ROW); // 调用模型进行运算: String[] outputNames = new String[] &#123;outputName&#125;; inferenceInterface.run(outputNames); // 将输出结果物存放到 outputs 中： int[] outputs = new int[OUT_COL*OUT_ROW]; inferenceInterface.fetch(outputName, outputs); return outputs; &#125; /** * Function: 将bitmap 转为（按行优先）一个 float 数组，并且每个像素点都归一化到（0~1）之间。 * @param bitmap bitmap 格式的样例图片 * @param rx 将图片缩放到指定的大小（列）-&gt;28 * @param ry 将图片缩放到指定的大小（行）-&gt;28 * @return 返回归一化后的一维 float 数组 -&gt;28*28 */ public static float[] bitmapToFloatArray(Bitmap bitmap, int rx, int ry)&#123; int height = bitmap.getHeight(); int width = bitmap.getWidth(); // 缩放比例 float scaleWidth = ((float) rx) / width; float scaleHeight = ((float) ry) / height; Matrix matrix = new Matrix(); matrix.postScale(scaleWidth, scaleHeight); bitmap = Bitmap.createBitmap(bitmap, 0, 0, width, height, matrix, true); Log.i(TAG,"bitmap width: " + bitmap.getWidth() + ",height:" + bitmap.getHeight()); Log.i(TAG,"bitmap.getConfig(): " + bitmap.getConfig()); height = bitmap.getHeight(); width = bitmap.getWidth(); float[] result = new float[height * width]; int k = 0; // 行优先 for(int j = 0;j &lt; height;j++)&#123; for (int i = 0;i &lt; width;i++)&#123; int argb = bitmap.getPixel(i,j); int r = Color.red(argb); int g = Color.green(argb); int b = Color.blue(argb); int a = Color.alpha(argb); //由于是灰度图，所以r,g,b分量是相等的。 assert(r==g &amp;&amp; g==b); result[k++] = r / 255.0f; &#125; &#125; return result; &#125;&#125; 注意，不同的神经网络对于输入图片的要求也不一样，有些需要转化成灰度图，有些需要归一化，有些要对尺寸进行裁剪，有些要减去均值。可以依据前面我们给出的用于验证模型预测可用的 Python 脚本将其转化为 Java，并结合 TensorFlowInferenceInterface接口即可完成调用。 在 MainActivity.java 中通过一个单击事件获取预测结果: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import android.graphics.Bitmap;import android.graphics.BitmapFactory;import android.support.v7.app.AppCompatActivity;import android.os.Bundle;import android.util.Log;import android.view.View;import android.widget.ImageView;import android.widget.TextView;public class MainActivity extends AppCompatActivity &#123; private static final String TAG = "MainActivity"; TextView txt; TextView tv; ImageView imageView; Bitmap bitmap; MyTSFMNISTDemo preTF; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); // Example of a call to a native method tv = (TextView) findViewById(R.id.sample_text); txt=(TextView)findViewById(R.id.txt_id); imageView =(ImageView)findViewById(R.id.imageView1); bitmap = BitmapFactory.decodeStream(getClass().getResourceAsStream("/res/drawable/test.bmp")); imageView.setImageBitmap(bitmap); preTF = new MyTSFMNISTDemo(getAssets()); &#125; public void click01(View v)&#123; String res = "The Result Of Prediction："; int[] result = preTF.getPredict(bitmap); for (int i=0;i&lt;result.length;i++)&#123; Log.i(TAG, res + result[i] ); res = res + String.valueOf(result[i])+" "; &#125; txt.setText(res); &#125;&#125; ..app/main/res/layout/activity_main 页面布局文件如下： 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android" android:layout_width="match_parent" android:layout_height="match_parent" android:orientation="vertical" android:paddingBottom="16dp" android:paddingLeft="16dp" android:paddingRight="16dp" android:paddingTop="16dp"&gt; &lt;TextView android:id="@+id/sample_text" android:layout_width="wrap_content" android:layout_height="wrap_content" android:text="Image Recognition Sample" android:layout_gravity="center"/&gt; &lt;Button android:onClick="click01" android:layout_width="match_parent" android:layout_height="wrap_content" android:text="click" /&gt; &lt;TextView android:id="@+id/txt_id" android:layout_width="match_parent" android:layout_height="wrap_content" android:gravity="center" android:text="The Result Of Prediction："/&gt; &lt;ImageView android:id="@+id/imageView1" android:layout_width="wrap_content" android:layout_height="wrap_content" android:layout_gravity="center"/&gt;&lt;/LinearLayout&gt; 构建一下，解决项目报错。自此，我们已经完成了移植的全部过程。项目代码已经上传至 Github 仓库：TSFOnAndroid 。 2.3 真机环境测试通过数据线将手机连接到电脑上，设置 –&gt; (关于手机 –&gt; 版本号 –&gt;) 开发者选项 –&gt; 开启 USB 调试模式）。 真机只要连接没问题，且开启了USB调试模式，就可以在 Android Studio 中通过 Debug ‘app’（Shift F9）或 Run ‘app‘（Shift F10）完成真机中 apk 的安装以及测试了，页面如下：]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>TensorFlow Mobile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习模型移动和嵌入式端落地方案：深度学习模型移植到 Android]]></title>
    <url>%2FTensorFlow%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%A7%BB%E5%8A%A8%E5%92%8C%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%AB%AF%E8%90%BD%E5%9C%B0%E6%96%B9%E6%A1%88%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%A7%BB%E6%A4%8D%E5%88%B0-Android%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 写在前面： 之前做过一个基于图像识别的小程序项目，最近需求变更要将其移植为 APP。由于个人在项目中一直在做算法相关开发维护工作，撇开 UI 部分和 Android 开发部分，重点更关注于深度学习算法模型的移植过程，首要目的就是在移动和嵌入式设备上把 TensorFlow 模型运行起来。 和云端关注点不同的是，移动和嵌入式设备上执行 TensorFlow 注重更低的功耗、更快的速度、以及更小的 size。 关于移动和嵌入式平台的深度学习解决方案，这里将其编为一个博文系列，并给出相关文章的索引，仅供参考： 系列博文之一：深度学习模型移动和嵌入式端落地方案：深度学习模型移植到 Android ; 系列博文之二：深度学习模型移动和嵌入式端落地方案：TFM 实现 TF MNIST 手写数字识别模型移植到 Android 端； TensorFlow For Mobile &amp;&amp; TensorFlow LiteTensorFlow 旨在成为移动和嵌入式平台良好的深度学习解决方案。 通过查阅相关文档，目前 TensorFlow 支持两种良好的在移动和嵌入式设备上部署机器学习应用的解决方案：TensorFlow for Mobile 和 TensorFlow Lite。 TensorFlow for Mobile 官方网址：https://www.tensorflow.org/mobile/mobile_intro?hl=zh-cn ; TensorFlow Lite 官方网址：https://www.tensorflow.org/mobile/tflite/?hl=zh-cn ; TensorFlow 国内网址：https://tensorflow.google.cn/learn 。 TensorFlow For Mobile &amp;&amp; TensorFlow Lite 简单对比下面首先简单给出两者之间的一些差异： TensorFlow For Mobile 比较早出来，比较稳定，并且简单易用，但性能等方面没有针对移动端作过多优化。目前 TensorFlow 官方已不推荐使用，2019 年后将不再提供支持； TensorFlow Lite 是 TensorFlow For Mobile 的进化版，尚在开发阶段，可能存在一些功能尚未补齐。目前官方推荐以及承诺大力开发以及维护的解决方案； 相较于 TensorFlow For Mobile ，在大多数情况下，TensorFlow Lite 拥有跟小的二进制大小，更少的依赖以及更好的性能； 相比 TensorFlow Mobile 是对完整 TensorFlow 的裁减，TensorFlow Lite 是一个新的实现方案。 如何选择？ –&gt; 学习角度： 既然是学习，TensorFlow Mobile &amp;&amp; TensorFlow Lite 两种策略都可以作为学习对象。多一种方案多一种解决思路，后续系列博文中会分别解读这两种解决方案，并进行相关的 Demo 测试。 –&gt; 工程应用： 为了应用，TensorFlow Mobile &amp;&amp; TensorFlow Lite 两者选择其一即可。 TensorFlow Mobile 作为最初启用的移动和嵌入式移植方案，移植过程简单快速，有着较为成熟的文档，但性能相对较低，后续面临的问题是会被 Google 弃用。 TensorFlow Lite 是TensorFlow团队给出的全新的实现方案，目前官方推荐的解决方案（官网有详细的 TensorFlow Lite 说明文档），有更好的性能优化，已经成为目前主流的方法。 Blog Abstract前面是引入，这里先给这篇博文来个读前吧，帮助大家更好的了解博文主要内容和结构。 TensorFlow For Mobile 后续会简称 TFM，TensorFlow Lite 会简称为 TFL。 关于本文：本文（深度学习模型移植到 Android）采用的策略是 TFM，关于 TFL 解决方案请绕道参考后续博文。 本文主要是针对模型移植过程进行说明，对 UI、Android 以及 TF 的细节并不会做过多的说明。 文中如有不恰当之处还请指正，欢迎各位大佬在评论区指出不足之处。 Envs：本系列博文均在以下环境下进行 Demo 测试： Windows10 Ubuntu 16.04 Server TensorFlow Env：Ubuntu 16.04 + TF-GPU 1.6（CUDA 9.0 + cuDNN 7.0） Android Studio 3.x Real Mobile phone envs：SAMSUNG Galaxy S7 TFM 实现 TensorFlow 学得模型移植到 Android 端后文我将给出一个最简单的 TFM Demo：利用 TensorFlow 实现一个求简单的数组相加，并把持久化的模型移植到 Android 端（SAMSUNG Galaxy S7）上运行。 正如官网给出的关于 “Android TensorFlow support” 描述的： 1# Components (a native .so library and a Java JAR) geared towards supporting TensorFlow on Android. 这里，我们首先给出 TFM 移植方案的一般思路： 一般先利用 Bazel 工具把 TensoFlow 编译成 .so 库文件 和 jar 包，进行 Android 相关配置，即可实现 TF 模型的移植。 Bazel 编译成功后，”.so library” 和 “Java JAR” 分别对应的具体文件名称如下： libtensorflow_inference.so libandroid_tensorflow_inference_java.jar 将 1、2 导入 Android 平台后便可以使用其中提供的接口调用 TF Model。 –&gt; 注意： 手动使用 Bazel 编译 TensoFlow 过程中可能遇到很多问题，导致最终编译 .so 库文件 和 jar 包 失败，这是很常见的！！！事实上，不会使用 Bazel 编译也没关系，TensoFlow 官方已经为开发者提供了编译好的 .so库文件 和对应的 jar 包 文件，下载后即可使用。 这里先不要急，后面我会给出上述的两种 “.so library” 和 “Java JAR” 获取方式的具体说明。 1. 预移植环境准备在正式开始移植之前，我们先来完成相关的工具准备： 1.1 Android Envs For Windows 配置关于 Android 开发环境，这里我们使用 Android Studio3.X 来开发我们的第一个 APP（TF 模型移植到 Android Demo）项目。下面开始在 Windows10 下搭建 Android 开发环境： 1.1.1 安装配置 JDK1.8关于 JDK 的详细安装以及配置教程见博文：JDK 下载与安装教程 。 1.1.2 Android Studio 安装与配置关于 Android Studio 环境的详细部署过程见博文：Android Studio 安装与配置 。 Android Envs For Windows 工具环境安装配置完成之后，我们再来补充前面提到的两种 “.so library” 和 “Java JAR” 插件获取方式的具体说明： 1.2 SO library 和 Java JAR开始之前先了解一下关于 TensorFlow 版本问题： –&gt; 重点来了： 前面提过，.so library 和 Java JAR 内部提供了在 Android 平台调用 TF 模型的接口，但随着 TensorFlow 版本的更新，其内部的接口也发生了变化，可以将其分为新旧两个版本线。 –&gt; 旧版本（r1.1 之前）： 12345678/* Tensorflow 接口初始化方法：*/TensorFlowInferenceInterface inferenceInterface = new TensorFlowInferenceInterface();inferenceInterface.initializeTensorFlow(getAssets(), MODEL_FILE);/* TF 模型调用接口：*/inferenceInterface.fillNodeFloat(INPUT_NODE, new int[]&#123;1, HEIGHT, WIDTH, CHANNEL&#125;, inputs); // 将数据 feed 给 tensorflowinferenceInterface.runInference(new String[]&#123;OUTPUT_NODE&#125;); // 进行模型运算inferenceInterface.readNodeFloat(OUTPUT_NODE, outputs); // 将输出存放到 outputs 中 r1.1 版本之前具体接口的实现见：TensorFlowInferenceInterface 。 –&gt; 新版本（Version &gt;= r1.2） 1234567/* Tensorflow 接口初始化方法：*/TensorFlowInferenceInterface inferenceInterface = new TensorFlowInferenceInterface(assetManager,modePath);/* TF 模型调用接口：*/TensorFlowInferenceInterface.feed() // 将数据 feed 给 tensorflowTensorFlowInferenceInterface.run() // 进行模型运算TensorFlowInferenceInterface.fetch() // 将输出存放到 outputs 中 r1.2 之后具体接口的实现见：TensorFlowInferenceInterface 。 注意，对于各个不同 TensorFlow 版本编译的具体接口的查看可以通过修改 Github 分支完成切换。 1.2.1 直接下载编译好的 “.so library” 和 “Java JAR”（推荐）前面提到过 TensoFlow 官方已经为开发者提供了编译好的 .so库文件 和对应的 jar 包 文件，即： libtensorflow_inference.so libandroid_tensorflow_inference_java.jar –&gt; Download 要注意！！！需要根据你的 TensorFlow 版本下载对应的 “.so library” 和 “Java JAR” 。 可以从下面提供的链接中下载到官网提供的已经编译好的 “.so library” 和 “Java JAR” ： For prebuilt libraries, see the nightly Android build artifacts page for a recent build. 网上很多人说 TensorFlow 官网好像已不再提供了…. 幸运的是，这里有资源： –&gt; 新版本： Download –&gt; libtensorflow_inference.so &amp;&amp; libandroid_tensorflow_inference_java.jar 。 –&gt; 旧版本： 旧版本没找到……不过不重要！TensorFlow 2.0 都发布了，你还在使用 r1.1 以下版本也是厉害，大拇指。 1.2.2 手动 Bazel 编译 “.so library” 和 “Java JAR”（可选）如果感觉不自己编难受啊，那还说啥，开始吧…… 详细的构建过程以官方给出的为依据 –&gt; Android TensorFlow support ; 主要步骤如下： How ot Install Bazel？–&gt; https://docs.bazel.build/versions/master/install.html ； （未完）…. 自此，预移植环境准备已经完成，下面开始正式的移植过程： 2. Moblile Phone 调用 TF Model 完成移植过程上面我们给出 TFM 移植方案的一般思路，这里我们来看移动或嵌入式端掉用 TF 模型完成移植的过程简介： 获取训练好的 TF Model，并且明确模型网络节点信息（尤其是输入输出节点），完成模型预测可用验证； 在 Android Studio 中创建 Android 项目，导入 Android 平台调用 TF 模型需要的 jar 包和 so 文件 (负责 TF 模型的解析和运算)，以及 TF Model ，并完成相关的项目配置； 创建并完成调用：定义变量用于储存模型输入输出，并通过 jar 包提供的接口进行 TF 模型的调用。还需要简单注意与 Android UI 页面的关联。 移植测试 下面，我们将根据上述移植过程简介来完成 Moblile Phone 调用 TF Model 完成移植的全过程： 2.1 Prepare Pretrain TF Model先来测试一个最简单的 TFM Demo：输入是一个数组 matrix_1，输出的是数组 matrix_1 自身相加的结果，然后将模型持久化成 PB 模型用于完成后续的移植。 1 –&gt; Matrix 自身相加的 TensorFlow 模型持久化 1234567891011121314151617import tensorflow as tffrom tensorflow.python.framework import graph_util# If error: from tensorflow.python.client import graph_utilmatrix_1 = tf.constant([[4.0,6.0]], name='input')result = tf.add(matrix_1, matrix_1, name='output')sess = tf.Session()sess.run(result)output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, output_node_names=['output'])with tf.gfile.FastGFile('model.pb', mode='wb') as f: f.write(output_graph_def.SerializeToString())sess.close() 运行后会在程序同名目录下生成一个：model.pb TensorFlow 持久化文件。 2 –&gt; 明确模型网络节点信息 需要明确模型中输出（inputName）节点和输出（outputName）节点，为了后续方便进行调用。不清楚的话可以先打印 PB 模型中节点信息用于确定： 1234567891011121314151617import tensorflow as tfmodel_path = &quot;./model.pb&quot;with tf.Graph().as_default() as graph: print(&quot;Loading Model &gt;&gt;&gt;&gt;&gt;&quot;) with tf.gfile.GFile(model_path, &quot;rb&quot;) as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name=&quot;&quot;) print(&quot;&lt;&lt;&lt; success!&quot;) print(&quot;Neural Network Node Information ---&gt;&quot;) for op in graph.get_operations(): print(op.name, &quot;:&quot;, op.values) 输出如下信息： 12345Loading Model &gt;&gt;&gt;&gt;&gt;&lt;&lt;&lt; success!Neural Network Node Information ---&gt;input : &lt;bound method Operation.values of &lt;tf.Operation 'input' type=Const&gt;&gt;output : &lt;bound method Operation.values of &lt;tf.Operation 'output' type=Add&gt;&gt; 我们知道了，输入（inputName）节点名称为：input，输出（outputName）节点名称为：output。 3 –&gt; 验证模型预测可用 在使用持久化模型之前，我们必须先要验证我们模型是可用的： 1234567891011121314151617181920212223242526272829303132333435363738import tensorflow as tfimport numpy as npfrom PIL import Imageimport matplotlib.pyplot as plt#model_path = "./Model/mnist.pb"model_path = "./model.pb"#testImage = Image.open("./data/test_image.jpg")with tf.Graph().as_default() as graph: print("Loading Model &gt;&gt;&gt;&gt;&gt;") with tf.gfile.GFile(model_path, "rb") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) tf.import_graph_def(graph_def, name="") print("&lt;&lt;&lt; success!") print("Neural Network Node Information ---&gt;") for op in graph.get_operations(): # print(op) print(op.name, ":", op.values) config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False) with tf.Session(config=config) as sess: tf.global_variables_initializer().run() input_x = sess.graph.get_tensor_by_name("input:0") output = sess.graph.get_tensor_by_name("output:0") test_matrix = [[2.0, 6.6]] pre_num = sess.run(output, feed_dict=&#123;input_x: test_matrix&#125;) print('The Result Of Model Prediction [number] is ', pre_num) 可以看出，明确节点信息和验证模型预测可用可以放到一起。输出结果如下，说明模型没有问题： 1234567Loading Model &gt;&gt;&gt;&gt;&gt;&lt;&lt;&lt; success!Neural Network Node Information ---&gt;input : &lt;bound method Operation.values of &lt;tf.Operation 'input' type=Const&gt;&gt;output : &lt;bound method Operation.values of &lt;tf.Operation 'output' type=Add&gt;&gt;...The Result Of Model Prediction [number] is [[ 4. 13.2]] 2.2 Android Project In Android Studio2.2.1 Android Project 配置1 –&gt; Android Studio 中新建一个 Android Project; 2 –&gt; 将 2.1中训练得到的 TF Model：model.pb 文件存放到 ...app/src/main/assets 目录下，若 assets 目录不存在则创建； 3 –&gt; 项目中导入 “.so library” 和 “Java JAR”： 将 libandroid_tensorflow_inference_java.jar 存放到 .../app/libs 目录下； .../app/libs 目录下新建 armeabi 文件夹，并将 libtensorflow_inference.so 放进去。 4 –&gt; 配置 build.gradle（Module：app） 以及 gradle.properties： build.gradle（Module：app） 位于 ../app 目录下。 –&gt; 4.1）在 ../app/build.gradle 文件中的 android 节点追加 soureSets 节点信息，用于指定 jniLibs 的路径： 12345sourceSets &#123; main &#123; jniLibs.srcDirs = ['libs'] &#125; &#125; –&gt; 4.2）在 ../app/build.gradle 中的 dependencies 节点追加如下信息： 1implementation files('libs/libandroid_tensorflow_inference_java.jar') 这里，其实就是增加 libandroid_tensorflow_inference_java.jar 依赖，否则无法解析到 TensorFlow 包，产生报错。还有一种方法就是找到其位置（…/app/libs），右键 “add as Libary” 会自动添加到 dependencies 中。 –&gt; 4.3）并且在 ../app/build.gradle 中的 defaultConfig 节点追加如下信息： 123ndk &#123; abiFilters &quot;armeabi&quot; &#125; 好了，先构建一下看看是否报错，解决一下错误信息，自此关于 Android Studio 中 Android 项目的创建配置就完成。接下来看如何在我们已经创建好的 Android 项目中实现 TF Model 的调用： 2.2.2 模型的调用–&gt; 新建一个 MyTSFMatrixDemo 类（…app/src/main/java/com.xxxx.xxxx/ 下），在这个类里面进行模型的调用，即通过输入获取模型输出： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import android.content.res.AssetManager;import android.os.Trace;import android.util.Log;import org.tensorflow.contrib.android.TensorFlowInferenceInterface;public class MyTSFMatrixDemo &#123; private static final String TAG = "MyTSFMatrixDemo"; private static final String MODEL_FILE = "file:///android_asset/model.pb"; // 定义数据的维度 private static final int HEIGHT = 1; private static final int WIDTH = 2; // 模型中输入变量的名称 private static final String inputName = "input"; // 用于模型输入数据存储 private float[] inputs = new float[HEIGHT * WIDTH]; // 模型中输出变量的名称 private static final String outputName = "output"; // 用于模型输出数据存储 private float[] outputs = new float[HEIGHT * WIDTH]; TensorFlowInferenceInterface inferenceInterface; static &#123; // 加载库文件 System.loadLibrary("tensorflow_inference"); Log.e(TAG,"libtensorflow_inference.so loading successfully."); &#125; MyTSFMatrixDemo(AssetManager assetManager) &#123; // 接口定义 inferenceInterface = new TensorFlowInferenceInterface(assetManager,MODEL_FILE); Log.e(TAG, "Initialize TensorFlowInferenceInterface successfully."); &#125; public float[] getAddResult() &#123; // 初始化样例数据： inputs[0] = 1; inputs[1] = 3; // 将样例数据 feed 给 tensorflow model Trace.beginSection("feed"); inferenceInterface.feed(inputName, inputs, WIDTH, HEIGHT); Trace.endSection(); // 调用模型进行运算 Trace.beginSection("run"); String[] outputNames = new String[] &#123;outputName&#125;; inferenceInterface.run(outputNames); Trace.endSection(); // 将输出结果物存放到 outputs 中 Trace.beginSection("fetch"); inferenceInterface.fetch(outputName, outputs); Trace.endSection(); return outputs; &#125;&#125; 在 MainActivity.java 中通过一个单击事件获取预测结果: 12345678910111213141516171819202122232425import android.support.v7.app.AppCompatActivity;import android.os.Bundle;import android.util.Log;import android.view.View;public class MainActivity extends AppCompatActivity &#123; private static final String TAG = "MainActivity"; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); &#125; public void click01(View v)&#123; Log.i(TAG, "click ----------&gt;"); MyTSFMatrixDemo tsfDemo = new MyTSFMatrixDemo(getAssets()); float[] result = tsfDemo.getAddResult(); for (int i=0;i&lt;result.length;i++)&#123; Log.i(TAG, "The result of Matrix elements: " + result[i] ); &#125; &#125;&#125; ..app/main/res/layout/activity_main 页面布局文件如下： 1234567891011121314151617181920&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;android.support.constraint.ConstraintLayout xmlns:android="http://schemas.android.com/apk/res/android" xmlns:tools="http://schemas.android.com/tools" xmlns:app="http://schemas.android.com/apk/res-auto" android:layout_width="match_parent" android:layout_height="match_parent" tools:context=".MainActivity"&gt; &lt;Button android:onClick="click01" android:layout_width="wrap_content" android:layout_height="wrap_content" android:text="click" app:layout_constraintBottom_toBottomOf="parent" app:layout_constraintLeft_toLeftOf="parent" app:layout_constraintRight_toRightOf="parent" app:layout_constraintTop_toTopOf="parent" /&gt;&lt;/android.support.constraint.ConstraintLayout&gt; 构建一下，解决项目报错。自此，我们已经完成了移植的全部过程。项目代码已经上传至 Github 仓库：TSFOnAndroid 。 2.3 测试那么如何对上述移植过程进行测试？Android studo 调试平台有两类：模拟器和真机运行。 2.3.1 真机环境测试通过数据线将手机连接到电脑上，设置 –&gt; (关于手机 –&gt; 版本号 –&gt;) 开发者选项 –&gt; 开启 USB 调试模式）。 真机只要连接没问题，且开启了USB调试模式，就可以在 Android Studio 中通过 Debug ‘app’（Shift F9）或 Run ‘app‘（Shift F10）完成真机中 apk 的安装以及测试了，页面如下： CLICK 之后，Log 信息如下所示： 123456789I/MainActivity: click ----------&gt;E/MyTSFMatrixDemo: libtensorflow_inference.so loading successfully.I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded TensorFlow native methods already loadedI/TensorFlowInferenceInterface: Model load took 1ms, TensorFlow version: 1.1.0-rc2 Successfully loaded model from 'file:///android_asset/model.pb'E/MyTSFMatrixDemo: Initialize TensorFlowInferenceInterface successfully.I/MainActivity: The result of Matrix elements: 2.0 The result of Matrix elements: 6.0 2.3.2 模拟器环境测试推荐 Genymotion 模拟器。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>TensorFlow Mobile &amp;&amp; TensorFlow Lite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android Studio 安装与配置【最新】]]></title>
    <url>%2FAndroid%2FAndroid-Studio-%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 写在前面： Android Studio 是由 Google 退出的用于开发 Android 应用程序的官方 IDE（集成开发环境），基于 Intellij IDEA。本篇博文所作目的主要是记录一下 Android Studio 开发环境的搭建以及配置过程。 先来简单看一下 Android Studio 的特性： 基于 Gradle 的构建支持； 自带布局编辑器，可以让你拖放 UI 组件，并在多个屏幕配置上预览布局等； 内置 SVN、Git 工具支持； 支持丰富的插件，Eclipse 有的，Android Studio 里基本都能找到。 Android Studio3.X 安装与配置这里首先给出 Android Studio 官方网站 提供安装支持。可以发现，Android Studio 对不同的平台（Windows、Mac 以及 Linux）都提供了支持。 下面我们来开始如何在不同平台下安装配置 Android Studio： 1. Windows 平台下安装 Android Studio3.X1.1 安装1.1.1 –&gt; 下载安装包 截至到当前 Windows 平台最新版本安装包为：android-studio-ide-183.5452501-windows.exe。 1.1.2 –&gt; 下载好该安装包之后，点击进行安装，进入欢迎界面 –&gt; Next ，开始安装。 1.1.3 –&gt; 默认选择安装 Android Virtual Device 插件，即询问是否安装 Android 虚拟设备插件？我们选择保持默认即可 –&gt; Next： 注意，安装提醒至少预留 2.6GB 的空闲空间以安装。 1.1.4 –&gt; 自定义程序安装路径，这里我选择 E:\Android Studio，–&gt; Next： 1.1.5 –&gt; 设置启动菜单文件名称，默认为 Android Studio，不用更改。开始安装了 –&gt; Install： 1.1.6 –&gt; 安装进度条已经开始了，等待安装完成。–&gt; Finish： 安装完成后我们选择自动启动 Android Studio。 1.2 快速配置考虑篇幅原因，后面我们将 Android Studio 简称为：AS 1.2.1 AS 配置–&gt; AS 启动后出现如下界面，保持默认就好，–&gt; OK 进入启动界面： –&gt; 首次启动后，会弹出如下界面，询问是否设置代理？–&gt; Cancel 即可 –&gt; 此时会进入AS 的安装向导界面，–&gt; Next： –&gt; 之后询问 AS 安装类型Standard or Custom ？这里我们保持默认即可。随后可以选择自己喜欢的主题Darcula or Light？–&gt; Next 进入如下界面： 这里由于 AS 未检测到系统的中 SDK，询问是否安装 AS SDK 插件？默认会给我们安装最新版本的 SDK（安装路径在 Android SDK Locations 中设置）。当然，如果你之前装过 SDK，也可以将安装选项取消，然后将 SDK 路径指定过去即可。 –&gt; Next 后，进入 AS 默认模拟器设置界面，保持默认即可： –&gt; Next 后，询问是否确认配置，确认没问题的话 –&gt; Finish： –&gt; Finish 后开始安装配置过的组件，等待完成即可（挺慢的~，喝杯咖啡吧）。 –&gt; 安装完各种插件后，就可以进入正式的 AS 欢迎界面了： 自此，AS 安装以及基础配置就完成了。 1.2.2 配置环境变量为了方便使用，接下来我们再在系统变量中手动配置一下 AS 的环境变量： 1）新建系统环境，变量名为：ANDROID_HOME，变量值 ： 1E:\AndroidSDK 2）修改系统变量 Path，追加如下内容： 123%ANDROID_HOME%\tools;%ANDROID_HOME%\platform-tools;%ANDROID_HOME%\build-tools\28.0.3; 配置完环境变量以后，重新打开 AS。 2. 创建第一个 Android 项目这一小节我们来看如何配置 AS 第一次运行环境，并且能成功编译运行一个 APP。以 HelloWorld 为例： AS 欢迎界面如下： –&gt; 点击上图中的 Start a new Android Studio project 新建一个工程，创建一个 Empty Activity –&gt; Next 进入下面的界面： 在这里，我们需要设置 HelloWorld APP 项目的名称、包名、项目路径以及语言。 注意关于 Minimum API Level 的设置，看下面解释： 12# Some devices require additional SDKs, Low API levels target more devices.# By targetting API 15 and later,your app will run on approximately 100% of devices. 下面给出 Minimum API Level 所适应的设备比例： –&gt; Finish 后，一个工程建立完成，第一次建立的工程会发现卡在下面的启动界面： 解释一下：第一次建立工程卡在该界面的时候，是因为要从网上下载 Gradle 构建工具（类似于 Maven，用来进行依赖管理），网速很慢。一种情况就是多等一会，反正我等了 7/8 分钟好了；或者你也可以这里点击取消关闭该界面，采用手动配置 Gradle。 如何手动配置 Gradle？ 在当前用户目录下找到.gradle目录，里面包含了不同版本的Gradle构建工具。例如我的在：C:\Users\guoji\.gradle\wrapper\dists\gradle-5.1.1-all\97z1ksx6lirer3kbvdnh7jtjg，会发现有两个文件： 12gradle-5.1.1-all.zip.lckgradle-5.1.1-all.zip.part 这里的 gradle-5.1.1 指的是版本，它会根据 AS 的版本自动生成，此时我们可以去网上下载一个 gradle-4.1-all.zip 压缩包（–&gt; 戳这里），然后放到该路径下并且进行解压。注意一定要放到这个随机生成的一长串字符的文件夹下面，正常解压后是包含这几个文件： 1234gradle-5.1.1gradle-5.1.1-all.zipgradle-5.1.1-all.zip.lckgradle-5.1.1-all.zip.ok Gradle 构建工具下载以及安装成功后，就开始 –&gt; Build： Build 项目时，碰到一个问题一直在 Gradle Sync Downloading https://...jcenter....，没反应（假死），烦死。原因是国内访问 jcenter 太慢，甚至连接不上，就会报各种关于依赖更新失败的错误。 解决办法： 阿里云提供了 jcenter 的镜像。修改项目下的 build.gradle 配置文件，配置阿里的 jcenter 源： 1234567891011121314151617181920212223242526272829// Top-level build file where you can add configuration options common to all sub-projects/modules.buildscript &#123; repositories &#123; maven&#123; url 'http://maven.aliyun.com/nexus/content/groups/public'&#125; google() jcenter() &#125; dependencies &#123; classpath 'com.android.tools.build:gradle:3.4.0' // NOTE: Do not place your application dependencies here; they belong // in the individual module build.gradle files &#125;&#125;allprojects &#123; repositories &#123; maven&#123; url 'http://maven.aliyun.com/nexus/content/groups/public'&#125; google() jcenter() &#125;&#125;task clean(type: Delete) &#123; delete rootProject.buildDir&#125; 可以了。注意，网络好多教程都是把 google() &amp;&amp; jcenter() 注掉，注掉不行，会报错！！！ –&gt; Gradle 构建完成之后，就可以通过 Build –&gt; Build Bundle(s) / APK(s) –&gt; Build APK(s) 编译打包 apk 文件了。生成的 apk 文件存储在 MyApplication\app\build\outputs\apk\debug 中。 –&gt; 测试 生成 apk 文件之后，导出该 apk 文件到模拟器或者真机上面进行安装就可以运行了。 导入导出的麻烦，这里提供另一种方法： 将手机使用 USB 接到电脑上，打开其开发者模式（具体手机的开启方法见网络）。然后在 AS 上开始 Debug ‘app’（shift + F9），可以发现 AS 会自动检测可以进行 Debug 的设备： 这里，AS 发现一个 Samsung SM-G9350（Android 7.0，API 24）的手机设备，–&gt; OK 后等待即可（弹出其它安装信息暂时先不用管，取消掉就可以）。 等一会发现手机跳转到如下 HelloWorld 页面，说明成功：]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
        <tag>Android Studio</tag>
        <tag>APP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架 Keras 之 Keras 框架结构]]></title>
    <url>%2FKeras%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6-Keras-%E4%B9%8B-Keras-%E6%A1%86%E6%9E%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇 前面，我们知道相较于其它主流深度学习框架，选择 Keras 意味着支持快速实验，支持多个后端，易用而又不失灵活性，具有以上优点完全取决于 Keras 的设计特性。关于其设计特性最重要的一点就是高度模块化的设计思想，快速理解其模块化设计思想可以加深我们对 Keras 框架的整体理解，从而以最短的时间从其它（例如：TensorFlow）深度学习框架转入到 Keras 开发。 1. 初识 Keras 框架结构Keras 的高度模块化理解： Keras 模型（model）可以被理解为由独立的、完全可配置的模块构成的序列或计算图。 模块可以以尽可能少的限制自由组合在一起。具体而言，网络层（layer）、损失函数、优化器、初始化方法、激活函数、正则化方法等等都是独立的可配置的模块，你可以将它们结合起来构建自己的模型。 我们知道，Keras 是构建与第三方深度学习框架（TensorFlow、Theano 等作为后端）上的高级深度学习库。事实上，后端（backend）也是 Keras 框架的一个模块。 1. 1 Keras 模块结构当然，除了上述之外，Keras 还提供一些其它的功能模块，用以帮助开发人员进行快速实验。例如，数据预处理、Scikit-learn API 等。 但 Keras 模块中最为要紧和核心为以下三个模块： backend：后端，对 Tensorflow 、Theano 等进行封装。完成低层的张量运算、计算图编译等； model：模型，是层的有序组合构成的计算图，也是层的“容器”，是 “神经网络” 的整体表示； layer：网络层，神经网络的层本质上规定了一种从输入张量到输出张量的计算规则。 搭建一个神经网络模型，只用上面给出的三个模块内容就可以了。 注意的是，backend（后端）虽然很重要，但其内容多而杂，大部分内容都是被其他 keras 模块调用，而不是被用户直接使用。所以它不是新手马上就应该学的，初学 Keras 不妨先将 backend 放一旁，从 model 和 layers 学起。 这里我们给出一个 Keras 模块结构的思维导图，便于观察与理解： –&gt; 你应该有点感觉，事实上呢？↓↓↓↓↓↓ 下面给一个详细的说明： 1）关于网络层 layer 首先，搭建神经网络模型，需要定义神经网络的前向传播过程。也就是说我们需要使用 Keras 网络层（layers）中支持的不同种类的模块层来组合我们的模型。你可以看到 Keras 支持了几乎所有的常见网络层： 123456789101112核心网络层（Dense ...）卷积层（Convolutional）池化层（Poooling）局部连接层（Locally-connected）循环层（Recurrent）嵌入层（Embedding）融合层（Merge）高级激活层（Advanced Activations）标准化层（Normalization）噪音层（Noise）层包装器（Wrappers）自定义层 来个 Keras 支持的常见网络层思维导图： 2）关于网络配置 除了定义网络层外，Keras 还提供了一组模块用来对神经网络层进行配置： 1234Initializers：初始化策略，规定了网络参数的初始化方法；Regularizers：正则项，提供了一些用于参数正则化的方法；Constraints：约束项，提供了对网络参数进行约束的方法；Activations：激活函数可以通过设置单独的激活层实现，也可以在构造层对象时通过传递 activation 参数实现。 为了训练神经网络，必须为神经网络定义一个神经网络优化的目标和一套参数更新的方式，这部分就是目标函数和优化器配置模块： 12Losses：损失函数，规定了神经网络的优化方向；Optimizers：优化器，规定了神经网络的参数如何更新； 同时，为了方便调试、分析和使用网络，Keras 还提供了下面的配置模块： 123性能评估（Metrics）：评估标准，用于评估当前训练模型的性能；回调函数（Callbacks）：用于训练阶段，查看训练模型的内在状态和统计；可视化（Visualization）：可视化，用于将网络结构绘制出来，以直观观察； 继续上图： 3）关于数据预处理 为了方便使用网络以及处理数据，Keras 提供了下面的模块： 12数据预处理（Preprocessing）：提供了一组用于对文本、图像、序列信号进行预处理的函数 ；工具库（Utils）：常用函数库。大部分 utils 的函数你或许很难用到，但有几个非常重要的； 图来： 4）其它模块 除了上述的模块，Keras 还提供了其它的一些功能模块，用以帮助开发人员进行快速实验： 12常用数据集（Datasets）：提供了一些常用数据库的接口，用户将通过这些接口下载和载入数据集；应用（Applications）：提供了带有预训练权值的深度学习模型，这些模型可以用来进行预测、特征提取和微调（fine-tuning）。 例如，应用模块（keras.applications）中你可以找到在 ImageNet 上预训练过的用于图像分类的模型： Xception：https://keras.io/zh/applications/#xception VGG16：https://keras.io/zh/applications/#vgg16 VGG19：https://keras.io/zh/applications/#vgg19 ResNet50：https://keras.io/zh/applications/#resnet50 InceptionV3：https://keras.io/zh/applications/#inceptionv3 InceptionResNetV2：https://keras.io/zh/applications/#inceptionresnetv2 MobileNet：https://keras.io/zh/applications/#mobilenet DenseNet：https://keras.io/zh/applications/#densenet NASNet：https://keras.io/zh/applications/#nasnet MobileNetV2：https://keras.io/zh/applications/#mobilenetv2 好了，从模块化设计角度解读 Keras 框架就写到这了……。关于 Keras 中各模块的使用具体可以参见其官方文档： 官方文档：https://keras.io/ 中文文档：https://keras.io/zh/ 掌握了高度模块化的设计思想，下面来看如何来快速搭建我们的第一个神经网络： 2. 如何使用 Keras 搭建一个神经网络在使用 Keras 开始搭建神经网络之前，我们先来掌握一些 Keras 中的主要概念： 2.1 符号主义、张量和计算图1 –&gt; 符号主义 Google 一下，你会发现它是一个机器学习的名词，但我们这说的符号主义不是那个东西，这里说的符号主义，指的是使用 符号式编程 的一种方法，与之相对应的是 命令式编程。 我们知道， Keras 的底层库使用 Theano 或 TensorFlow，这两个库也称为 Keras 的后端。无论是 Theano ，TensorFlow 还是 Keras，其实都是一个 “符号式” 的库。所以要说 Theano/Tensorflow/Keras，就不能不提它们的符号主义特性： 事实上，Theano 也好，Tensorflow 也好，不仅仅是为深度学习设计的。假如你有一个与深度学习完全无关的计算任务想运行在 GPU 上，你完全可以通过 Theano/Tensorflow 进行编写和运行。 假如我们要求两个数 a 和 b 的和，通常只要把值赋值给 a 和 b，然后计算 a+b 就可以了。一般会这么写： 123a=3b=5z = a + b 程序运行到第一行，把 3 给了 a，运行到第二行把 5 给了 b 。同理，运行到第三行是计算 a + b 的值然后赋给 z。这呢就是 命令式 的编程思路… 但一些人呢想到：将 a+b 这个计算任务分为三步。（1）声明两个变量 a，b。建立输出变量 z；（2）确立 a，b 和 z 的计算关系，z=a+b；（3）将两个数值 a 和 b 赋值到变量中，计算结果 z。这种“先确定符号以及符号之间的计算关系，然后才放数据进去计算”的方法就是 符号式 编程。 换句话说就是，当我们定义变量或函数时，并不会做真正的数值计算。这类变量或函数的定义中使用数值占位符，只有当给定真正的输入后，才会对这个函数进行编译计算。 –&gt; 符号式编程首先需要定义各种变量，然后建立一个“计算图”（也称为数据流图，它规定了各个变量之间的计算关系。），最后提供输入数据完成最终的运输。 可能你会说，这样做是不是闲的无聊？答案肯定不是，事实上，符号式编程可以优化程序计算速度和占用内存的使用率。 2 –&gt; 张量 在 Keras，theano，Tensorflow 等深度学习中，一般统一采用张量的形式来组织数据，也就是说参与符号运算的那些变量可以统一称作张量。 从功能角度来讲，张量可以看作是向量，矩阵的自然推广，用来表示更加广泛的数据类型。张量的阶数也叫维度。 123450 阶张量：即标量，也就是一个数。1 阶张量：即向量，当我们把一些数有序的排列起来就形成了向量。2 阶张量：即矩阵，如果我们继续把上述的一组向量有序的进行排列就构成了矩阵。3 阶张量：可以看作立方体，我们把一组矩阵按上下一层层排列起来就形成了一个立方体。具有 3 个颜色通道的彩色图片就是一个这样的立方体。4 阶张量：继续排......不要去试图想像4阶张量是什么样子，它就是个数学上的概念。 说到这里，我们不得不提彩色图像的两种数据格式： th 模式或 channels_first 模式：Theano 和 caffe 使用此模式。 tf 模式或 channels_last 模式：TensorFlow 使用此模式。 举例说明： 123456# 对于 100 张 RGB 3通道的 16×32（Height：16；Weight：32）彩色图：th 表示方式：（100,3,16,32）tf 表示方式：（100,16,32,3）# 唯一的区别就是表示通道个数 3 的位置不一样。 一言以蔽之，Keras 等深度学习框架的计算过程，就是建立一个从张量到张量的映射函数，然后再放入真实数据进行计算。对深度学习而言，这个“映射函数”就是一个神经网络，而神经网络中的每个层自然也都是从张量到张量的映射。 2.2 关于 Keras 模型model 是 Keras 的核心数据结构 。前面我们说过，它是层的有序组合构成的计算图，也是层的“容器”，是 “神经网络” 的整体表示。 从结构上来看，Keras 有两种类型的模型：Sequential 顺序模型 和 使用函数式 API 的 Model 类模型 （Functional 函数式模型）。其中，函数式模型应用更为广泛，Sequential 顺序模型可以看作是函数式模型的简略版。 Sequential 顺序模型：网络层的线性堆叠。单输入单输出，一条路通到底，层与层之间只有相邻关系，没有跨层连接。这种模型编译速度快，操作也比较简单； 函数式模型：多输入多输出，层与层之间任意连接。这种模型编译速度慢。 这些模型有两套训练和测试的函数：一套是 fit，evaluate 等；另一套是 fit_generator，evaluate_generator。前者适用于普通情况，后者适用于数据是以迭代器动态生成的情况。迭代器可以在设备内存/显存不足，实时动态优化数据读写进行网络训练，以使用 Keras 的话，Python 的迭代器这一部分是一定要掌握的内容。 对模型而言，最核心的函数有两个： compile()：编译，模型在训练前必须编译，这个函数用于完成指定目标函数，确定优化器等等一系列模型配置功能。这个函数必须指定的参数是优化器和目标函数，经常还需要指定一个metrics来评价模型。 fit() / fit_generator()：用来训练模型，参数较多，是需要重点掌握的函数，对于keras使用者而言，这个函数的每一个参数都需要掌握。 其他的常用的属性和函数请自行在学习过程中使用学习。 当然，关于 Keras 模型这里只是简单的介绍，更详细的模型使用参考官方文档。 好了，有了上面的基础就可以正式开始 Keras 的学习了。撒花……. 先挂一个思维导图，看一下 Keras 搭建了一个神经网络的全部过程： 2.3 Keras 实现线性回归（Linear Regression）先来看如何使用 Keras 完成我们的第一个单层线性回归模型，话不多说，直接开始了… 所有样例均在 Jupyter Notebook 下运行测试 1）首先导入必要模块 123456789import numpy as npimport matplotlib.pyplot as pltimport keras# 从 keras.models 模型模块中导入 Sequential 顺序模型：from keras.models import Sequential# 从 keras.layers 层模块中导入全连接层模块from keras.layers import Dense 2）构建随机数据集，并划分为训练集和测试集 12345678910111213141516## 1. 创建数据集：# 随机产生 200 个样本输入数据：x_inputData = np.random.rand(200)# 添加样本数据噪音：data_noise = np.random.normal(0, 0.05, x_inputData.shape)# 假设我们的真实模型是：Y = 0.5X + 2y_inputData = 0.5 * x_inputData + 2 + data_noise# 显示样本数据集分布：plt.scatter(x_inputData, y_inputData)plt.show()# 按照 4：1 划分训练集和测试集：x_inputData_train, y_inputData_train = x_inputData[:160], y_inputData[:160]x_inputData_test, y_inputData_test = x_inputData[160:], y_inputData[160:] 随机数据集数据分布情况如下： 3）构建模型并进行编译 12345678910111213141516## 2. 开始构建建模：# 定义一个 Keras 模型：# Keras 有两种类型的模型： Sequential 顺序模型和 Funtional 函数式模型# 比较常用的是单输入输出的线性堆叠模型 Sequential：model = Sequential()# 通过 add() 方法向模型中添加一个全连接层：# Dense 为全连接层，第一层需要定义输入的维度;ba# 而第二层无需指定输入，一般第二层会自动将第一层的输出作为输入model.add(Dense(units=1, input_dim=1))# 训练之前需要编译模型：# 通过 compile() 来为模型配置损失函数和优化器# 这里我们选择使用随机梯度下降作为优化方法，均方误差作为损失函数model.compile(optimizer="sgd", loss="mse") 4）训练模型，查看学得的回归模型拟合情况 1234567891011121314151617181920212223242526272829303132## 3. 开始训练：# 打印一下训练之前的权重和偏置值：weights, biases = model.layers[0].get_weights()print("&lt;--- Weights And Biases Display Before training ---&gt;")print("weights: ", weights, "biases: ", biases)print("Begin to train ---&gt;")for step in range(5001): # 调用模型中的 train_on_batch() 函数每次取一个 batch 数据进行模型训练，注意 Keras 中提供了很多用于训练的函数： # 这里由于数据量较少，我们直接全部放入即可 cost = model.train_on_batch(x_inputData_train, y_inputData_train) if step % 500 ==0: print("train cost: ", cost)# 打印一下训练之后的权重和偏置值：weights, biases = model.layers[0].get_weights()print("&lt;--- Weights And Biases Display After training ---&gt;")print("weights: ", weights, "biases: ", biases)## 查看回归模型在训练数据集上的拟合情况：# 获取 x_inputData_train 训练样本数据对应的预测值y_pred = model.predict(x_inputData_train)# 显示模型预测结果plt.scatter(x_inputData, y_inputData)plt.plot(x_inputData_train, y_pred, "r-", lw=3)plt.show() 输入如下： 12345678910111213141516&lt;--- Weights And Biases Display Before training ---&gt;weights: [[ 0.04869342]] biases: [ 0.]Begin to train ---&gt;train cost: 4.91081train cost: 0.00664172train cost: 0.00344313train cost: 0.0024123train cost: 0.00208009train cost: 0.00197302train cost: 0.00193852train cost: 0.0019274train cost: 0.00192381train cost: 0.00192266train cost: 0.00192229&lt;--- Weights And Biases Display After training ---&gt;weights: [[ 0.47959563]] biases: [ 2.01119494] 回归模型拟合情况： 简单可以看到，训练已经收敛，并且训练后权重和偏置已经很接近我们的真实模型了。 5）查看一下测试集上的表现 123456789101112131415# 测试训练好的模型：print("Model Testing ---&gt;")cost = model.evaluate(x_inputData_test, y_inputData_test, batch_size=40)print("test cost: ", cost)## 查看回归模型在测试数据集上的表现：# 获取 x_inputData_train 训练样本数据对应的预测值test_pred = model.predict(x_inputData_test)# 显示回归结果plt.scatter(x_inputData_test, y_inputData_test)plt.plot(x_inputData_test, test_pred, "r-", lw=3)plt.show() 输入如下： 123Model Testing ---&gt;40/40 [==============================] - 0s 40us/steptest cost: 0.00270265294239 回归结果显示如下： 2.4 Keras 实现非线性回归（Nonllinear Regression）上面我们实现了一个线性回归模型，这里我们再来通过一个简单非线性回归来引入 Keras 中的学习率、激活函数等搭建多层非线性回归神经网络模型： 1）设置学习率 首先，我们还是导入必要模块（已将 keras.optimizers 下的 SGD 优化器模块导入），但是构建的随机数据集已经从原来的线性模型输出（0.5 * x_inputData + 2）变为非线性模型输出（np.square）： 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimport keras# 从 keras.models 模型模块中导入 Sequential 顺序模型：from keras.models import Sequential# 从 keras.layers 层模块中导入全连接层模块from keras.layers import Dense# 从 keras.optimizers 优化器模块中导入 SGD 随机梯度下降优化器from keras.optimizers import SGD## 1. 创建数据集：# 随机产生 200 个样本输入数据：x_inputData = np.linspace(-0.5, 0.5, 200)# 添加样本数据噪音：data_nosise = np.random.normal(0, 0.02, x_inputData.shape)# 注意，这里输入 y 已经有原来的线性模型输出通过 np.square() 转变为非线性模型输出了：y_inputData = np.square(x_inputData) + data_nosise# 显示样本数据集分布：plt.scatter(x_inputData, y_inputData)plt.show() 随机数据集数据分布情况如下： 接下来，在线性回归模型的基础上来设置一个适合的学习率，以帮助我们的模型在训练中能够快速收敛： 1234567891011121314151617181920212223242526272829303132333435## 2. 开始构建建模：model = Sequential()model.add(Dense(units=1, input_dim=1))# SGD 默认学习率(0.01)比较小，达到收敛需要的迭代次数太多，这里我们来修改其学习率：sgd = SGD(lr=0.3)model.compile(optimizer=sgd, loss="mse")## 3. 开始训练：# 打印一下训练之前的权重和偏置值：weights, biases = model.layers[0].get_weights()print("&lt;--- Weights And Biases Display Before training ---&gt;")print("weights: ", weights, "biases: ", biases)print("Begin to train ---&gt;")for step in range(5001): cost = model.train_on_batch(x_inputData, y_inputData) if step % 500 ==0: print("train cost: ", cost)# 打印一下训练之后的权重和偏置值：weights, biases = model.layers[0].get_weights()print("&lt;--- Weights And Biases Display After training ---&gt;")print("weights: ", weights, "biases: ", biases)## 查看非线性回归模型在训练数据集上的拟合情况：y_pred = model.predict(x_inputData)plt.scatter(x_inputData, y_inputData)plt.plot(x_inputData, y_pred, "r-", lw=3)plt.show() 可以看到，对于非线性模型数据，我们只是在线性模型的基础上加快了模型达到收敛的速度，网络结构并没有发生变化。下面给出上面程序输出结果： 12345678910111213141516&lt;--- Weights And Biases Display Before training ---&gt;weights: [[ 1.47613013]] biases: [ 0.]Begin to train ---&gt;train cost: 0.197396train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412train cost: 0.00601412&lt;--- Weights And Biases Display After training ---&gt;weights: [[-0.00278729]] biases: [ 0.08534285] 线性回归模型在非线性模型训练数据集上的拟合情况如下： 可以看到上面的拟合情况肯定不是我们想要的结果。这是由于之前针对线性模型输出数据集所采用的线性回归模型无法拟合非线性模型输出数据集（线性回归模型存在局限性：无法解决非线性模型问题）。 2）激活函数以及深层神经网络 这里我们可以通过引入激活函数去线性化，并且通过将单层网络变为多层网络的方式来提取更高维的数据特征来搭建非线性回归模型： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimport keras# 从 keras.models 模型模块中导入 Sequential 顺序模型：from keras.models import Sequential# 从 keras.layers 层模块中导入全连接层模块from keras.layers import Dense, Activationfrom keras.optimizers import SGD## 1. 创建数据集：# 随机产生 200 个样本输入数据：x_inputData = np.linspace(-0.5, 0.5, 200)# 添加样本数据噪音：data_nost = np.random.normal(0, 0.02, x_inputData.shape)y_inputData = np.square(x_inputData) + data_nost# 显示样本数据集分布：plt.scatter(x_inputData, y_inputData)plt.show()## 2. 开始构建建模：# 定义一个 Keras 模型：# Keras 有两种类型的模型： Sequential 顺序模型和 Funtional 函数式模型# 比较常用的是单输入输出的线性堆叠模型 Sequential：model = Sequential()# 通过 add() 方法向模型中添加一个全连接层：# Dense 为全连接层，第一层需要定义输入的维度;ba# 而第二层无需指定输入，一般第二层会自动将第一层的输出作为输入# 注意，网络结构已经变为：1-10-1，并且引入激活函数：model.add(Dense(units=10, input_dim=1, activation="tanh"))model.add(Dense(units=1, activation="tanh"))# 默认学习率(0.01)比较小，达到收敛时间太长，这里我们来修改其学习率：sgd = SGD(lr=0.3)# 训练之前需要编译模型：# 通过 compile() 来为模型配置损失函数和优化器# 这里我们选择使用随机梯度下降作为优化方法，均方误差作为损失函数model.compile(optimizer=sgd, loss="mse")## 3. 开始训练：# 打印一下训练之前的权重和偏置值：weights, biases = model.layers[0].get_weights()print("&lt;--- Weights And Biases Display Before training ---&gt;")print("weights: ", weights, "biases: ", biases)print("Begin to train ---&gt;")for step in range(5001): # 调用模型中的 train_on_batch() 函数每次取一个 batch 数据进行模型训练，注意 Keras 中提供了很多用于训练的函数： # 这里由于数据量较少，我们直接全部放入即可 cost = model.train_on_batch(x_inputData, y_inputData) if step % 500 ==0: print("train cost: ", cost)# 打印一下训练之后的权重和偏置值：weights, biases = model.layers[0].get_weights()print("&lt;--- Weights And Biases Display After training ---&gt;")print("weights: ", weights, "biases: ", biases)## 查看回归模型在训练数据集上的拟合情况：# 获取 x_inputData_train 训练样本数据对应的预测值y_pred = model.predict(x_inputData)# 显示模型预测结果plt.scatter(x_inputData, y_inputData)plt.plot(x_inputData, y_pred, "r-", lw=3)plt.show() 下面给出程序输出结果： 12345678910111213141516171819&lt;--- Weights And Biases Display Before training ---&gt;weights: [[-0.73369855 0.3708325 0.64431423 0.17141724 -0.06705815 -0.50297558 0.58436769 -0.47927734 0.45941085 -0.7240687 ]] biases: [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]Begin to train ---&gt;train cost: 0.014848train cost: 0.004966train cost: 0.00145104train cost: 0.000429263train cost: 0.000403715train cost: 0.000402976train cost: 0.000402589train cost: 0.000402275train cost: 0.000402006train cost: 0.000401767train cost: 0.000401548&lt;--- Weights And Biases Display After training ---&gt;weights: [[-1.59273016 0.37405086 0.64990026 0.18326743 -0.09531492 -0.75044197 0.58407837 -0.51858276 0.33061254 -1.05503953]] biases: [-0.61383563 0.00647766 -0.01462645 0.05978871 -0.17726044 0.17868811 0.00505544 0.00445918 -0.1227567 0.38105822] 可以发现，通过映入激活函数以及额外的隐藏层，我们已经很好的拟合了非线性模型输出数据集，上述神经网络模型其实就是一个最简单的非线性回归模型。 –&gt; 激活函数补充： 其实，除了上述在网络层中配置激活函数的方法（传递激活函数参数），Keras 中还提供一种激活函数的设置方法（添加激活层）： 1234567from keras.layers import Activationmodel.add(Dense(units=10, input_dim=1))# 通过添加激活函数网络层来设置激活函数：model.add(Activation("relu"))model.add(Dense(units=1))model.add(Activation("relu")) 关于 Keras 中为神经网络模型设置激活函数可参见：https://keras-cn.readthedocs.io/en/latest/other/activations/ 。 好了，就写到这吧，下面给出博文的参考连接： Reference Linkhttps://zhuanlan.zhihu.com/p/22129946 https://blog.csdn.net/zdy0_2004/article/details/74736656 https://www.cnblogs.com/Anita9002/p/8136357.html]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架 Keras 之 Keras 入门介绍和安装]]></title>
    <url>%2FKeras%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6-Keras-%E4%B9%8B-Keras-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 前言 使用过 TensorFlow 、Theano 等深度学习框架进行机器学习或者深度神经网络研究，你会发现它们都是非常偏基础的系统，上手就很困难，更不用说快速把你自己的想法变为结果，直接使用的话生产力过于低下。苦恼啊……而此时，你恰好发现了 Keras，快来开始学习使用 Keras 吧…… Keras 专为支持快速实验而生，能够迅速地帮助你把 idea 转换为结果。对于入门来说，可以说是所有深度学习框架中最简单的。 1. Keras 初识Keras 是一个基于 Python 的高层的深度学习框架，是由纯 Python 编写而成的高级神经网络 API（正如 Keras 描述，它是基于 Python 的深度学习库），并且能够以 TensorFlow, CNTK, MXNet 或者 Theano 作为后端运行。 其实，严格意义上来讲，Keras 并不能成为一个深度学习框架，它更像是一个构建于第三方框架（底层）之上的深度学习接口。 并且正如开篇中说的，Keras 专为支持快速实验而生，能够以最短的时间把你的想法转换为实验结果，Keras 应该是深度学习框架之中最容易上手的一个。 十全九美的是，高级 API 意味着过度封装，这使得 Keras 的程序过于缓慢。在绝大多数的场景下，Keras 是所有常用主流框架中最慢的一个。 下面开始为推荐阅读部分（非必须），你可以跳过以下部分直接开始 Keras 的安装。 1.1 为何选用 Keras ？如果你有疑惑，为什么选择 Keras 而不是 TensorFlow（Theano）等其它？以下给出与目前主流深度学习框架的一些比较： 1.1.1 允许简单而快速的原型设计Keras 具有用户友好、高度模块化、以及易扩展性等特点： 用户友好：Keras 是为人类而非机器设计的 API，优先考虑开发人员的经验，提供一致且简单的 API； 高度模块化：Keras 模型可以被理解为由独立的、完全可配置的模块构成的序列或图。模块可以以尽可能少的限制组装在一起，特别是神经网络层、损失函数、优化器、初始化方法、激活函数、正则化方法等等，它们都是可以结合起来构建新模型的模块。 易扩展性：可以很容易的添加新模块，现有的模块已经提供了充足的示例。 并且我们知道， Keras 还是 Pythonic 的，艾玛，太好了！ 正是由于上述的种种特性，使得 Keras 更加的易用。但易用不以降低灵活性为代价，正如我们前面说的 Keras 可以以 TensorFlow，CNTK, MXNet 或者 Theano 等深度学习框架作为后端，这也意味着 Keras 可以让你实现任何你可以用基础语言编写的东西。 例如，tf.keras 作为 Keras API 可以与 TensorFlow 工作流无缝集成，并且 TensorFlow 官方已经将 Keras API 内嵌为其官方前端。关于 TensorFlow 中 Keras API tf.keras 的使用参见 TensorFlow Team 给出的教程： https://tensorflow.google.cn/guide/keras 。 1.1.2 被工业界和学术界广泛采用这里首先来看由 Jeff Hale 给出的 2018 年中期主流深度学习框架影响力排名示意图： 可以看出，Keras 在行业和研究领域具有很高的热度。 1.1.3 Keras 模型适用多平台产品部署任何其他深度学习框架相比，你的 Keras 模型可以在更广泛的平台上轻松部署： 在 iOS 上，通过 Apple’s CoreML（苹果为 Keras 提供官方支持）。这里有一个教程。 在 Android 上，通过 TensorFlow Android runtime，例如：Not Hotdog app。 在浏览器中，通过 GPU 加速的 JavaScript 运行时，例如：Keras.js 和 WebDNN。 在 Google Cloud 上，通过 TensorFlow-Serving。 在 Python webapp 后端（比如 Flask app）中。 在 JVM 上，通过 SkyMind 提供的 DL4J 模型导入。 在 Raspberry Pi 树莓派上。 1.1.4 Keras 支持多个后端引擎，不会将你锁定到一个生态系统中你的 Keras 模型可以基于不同的 深度学习后端 开发。重要的是，任何仅利用 Keras 内置层构建的模型，都可以在所有这些后端中移植：你可以用一种后端训练模型，再将它载入另一种后端中（例如为了发布的需要）。支持的后端有： 谷歌的 TensorFlow 后端 微软的 CNTK 后端 Theano 后端 亚马逊的 MXNet 后端 1.1.5 Keras 支持强大的多 GPU 和分布式训练 Keras 内置对多 GPU 数据并行的支持。 Keras 模型可以被转换为 TensorFlow Estimators 并在 Google Cloud 的 GPU 集群上训练。 Keras 可以在 Spark（通过 CERN 的 Dist-Keras）和 Elephas 上运行。 1.1.6 深度学习生态系统大佬支持Keras 的开发主要由谷歌支持，Keras API 以 tf.keras 的形式包装在 TensorFlow 中。此外，微软维护着 Keras 的 CNTK 后端。亚马逊 AWS 正在开发 MXNet 支持。其他提供支持的公司包括 NVIDIA、优步、苹果（通过 CoreML）等。 格局决定成败。 2. Keras 安装赶紧完成安装开始使用…… 如果你已经有了 TensorFlow、Theano 等其它深度学习框架的安装以及使用经验，Keras 的安装可以说非常简单： 2.1 Python 开发环境我们说 Keras 是 Pythonic 的，所以首先你需要有一个 Python 开发环境，这是最基础的。 这里我们推荐基于 Anaconda 来搭建 Python 开发环境，关于 Anaconda 的介绍，安装以及配置可以参看我前面的博文：Anaconda 介绍、安装以及使用教程 。 先来展示一下我个人部署好的 Anaconda + Python 开发环境： 12345(deeplearning) ubuntu@ThinkCentre-M910s-N000:~$ pythonPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)[GCC 7.3.0] on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; 2.2 选择安装一款合适的后端引擎在安装 Keras 之前，我们还需要安装一款合适的后端引擎： TensorFlow 安装指引 ； Theano 安装指引； CNTK 安装指引 。 关于后端引擎的选择，根据个人需求选择就好。 这里，我们以 TensorFlow 作为 Keras 的后端（官方推荐）： 12345# GPU 版本的 TensorFLow 的安装：(deeplearning) ubuntu@ThinkCentre-M910s-N000:~$ pip install --upgrade tensorflow-gpu# CPU 版本的 TensorFlow 的安装：(deeplearning) ubuntu@ThinkCentre-M910s-N000:~$ pip install --upgrade tensorflow 关于更加详细的安装过程，可以参考我在前面给出的 TensorFlow 安装指引即可，博文中详细记录了适合 TensorFlow 的多种安装方式以及安装过程中的各种问题总结，帮你一站式解决 TensorFlow 安装问题。 2.3 Install Keras然后你就可以安装 Keras 本身了。Keras 官网提供了两种安装 Keras 的方法： 1 –&gt; 使用 PyPI 安装 Keras (推荐)： 1(deeplearning) ubuntu@ThinkCentre-M910s-N000:~$ pip install keras 2 –&gt; 使用 GitHub 中提供源码安装 Keras： 123456# 使用 git 克隆 Keras 源码：(deeplearning) ubuntu@ThinkCentre-M910s-N000:~$ git clone https://github.com/keras-team/keras.git# 然后，cd 到 Keras 目录并且运行安装命令即可：(deeplearning) ubuntu@ThinkCentre-M910s-N000:~$ cd keras(deeplearning) ubuntu@ThinkCentre-M910s-N000:~/keras$ sudo python setup.py install 至此，以 TensorFlow 为 backend（后端）的 Keras 环境的搭建就完成了。我们可以进行简单测试一下，Python 交互式解释器下 import keras 输出以下信息说明安装成功： 1234567(deeplearning) ubuntu@ThinkCentre-M910s-N000:~$ pythonPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)[GCC 7.3.0] on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import kerasUsing TensorFlow backend.&gt;&gt;&gt; 2.4 配置你的 Keras 后端哎，怎么还要配置 Keras 后端？上面我们基于 TensorFlow 安装完 Keras 后就结束了，没有进行其它配置呀。 事实上，默认情况下，Keras 将使用 TensorFlow 作为其后端。也就是说 Keras 的默认后端配置就是 TensorFlow，所以基于 TensorFlow 为后端安装 Keras 保持默认配置即可。 而对于其它 Keras 后端， 你需要戳 这里 来完成最终的后端引擎（也称为张量操作库）的配置。 啦啦啦，完成后你就可以开始 Keras 的学习了。 3. Useful Links 官方文档：https://keras.io/ 中文文档：https://keras.io/zh/]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 博客多平台协同管理问题【持续更新】]]></title>
    <url>%2FHexo%2FHexo-%E5%8D%9A%E5%AE%A2%E5%A4%9A%E5%B9%B3%E5%8F%B0%E5%8D%8F%E5%90%8C%E7%AE%A1%E7%90%86%E9%97%AE%E9%A2%98%E3%80%90%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E3%80%91%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 关于 Hexo + Github·Page + Coding·Page + Domain name 个人博客环境的搭建请参见：基于 Hexo 轻量级框架的博客搭建，博文中给出了 Hexo 新手小白如何快速搭建基于 Hexo 的个人博客进行产出。 但请注意，此时我们仅仅是将 Hexo 博文静态页面推送到了远程仓库（Github·Page 或 Coding·Page）上以实现公共访问，而 Hexo 个人博客框架相关配置信息都存放在本地终端。如果未进行其它备份或同步处理，一旦存放 Hexo 配置信息的本地终端出现问题（如系统崩溃），那么我们便无法再维护我们的的博客了（Game Over）。 所有当我们更换工作环境（公司、个人）或者原来用于搭建 Hexo 博客环境的设备需要重新更换系统，出于安全性以及灵活性考虑，我们不得不面对的一个问题就是如何将本地终端中的 Hexo 博客相关配置信息完美移植到新环境至关重要的。 1. 使用 Git 解决 Hexo 个人博客多平台同步管理关于 Hexo 博客多平台（多设备）协同管理以及更新教程网络上有很多，但对于刚刚接触 Hexo 、Git 的小白们不太友好，配置过程中容易出现各种问题，本文提到的所有配置方案均通过实际测试给出。 本文所作的主要目的：一方面作为学习记录回顾使用；另一方面适用于类似我这样的 Git 新手以供参考分享，可以快速对应以及解决问题。所以文中有些地方可能表达有误，欢迎各位大佬指正。 1.1 模拟场景公司和家里两台 PC： 公司 PC_A：最初用于搭建 Hexo 个人博客的终端； 个人 PC_B：移植 Hexo 个人博客项目的终端。 为了可以随时随地创作更新个人 Blog，两台 PC 中都必须搭建有相同的 Hexo 博客环境，并且要求我们的 Hexo 博客本地配置项目要保持同步（即实现版本控制）。 对于 同步 问题： 由于除了静态页面以外，其它全部文件都在本地生成，所以如果在公司终端上推送了 articleA 文章后回家又写了篇 articleB 文章，在家里推送后我们会发现只有 articleB 文章而 articleA 文章没了（因为家里的 PC 上没有 articleA 文章的 md 文件），故及时同步两台 PC 终端中的 Hexo 博客项目相当重要。也就是说，对我们的 Hexo 个人博客本地项目实现版本控制是必要的。 所以，这里我们首先给出解决 Hexo 个人博客多平台同步管理的通用方法： 搭建 Hexo 个人博客环境，包括： Node.js、git 以及 hexo 的安装，具体安装方式可见前面提到的搭建教程。 对 Hexo 个人博客本地配置信息项目实现同步，也就是版本控制。 1.2 实施方案分析介于模拟场景提到的解决方法，搭建 Hexo 个人博客环境本文不在做细致说明，参考前面教程即可。针对同步（备份），这里我们给出三种具体的版本控制（同步、备份）的实施方案分析： 1.2.1 存储设备备份使用存储设备备份，是指我们使用存储设备对 Hexo 个人博客项目本地文档进行备份。例如，我们可以直接对 PC_A 中最新的本地 Hexo 博客目录进行打包存储到硬盘、U盘或者云盘（大多数使用）中，然后将其移植 PC_B 中进行直接使用。 下面我们来分析其优略： 1）优点： 免费且操作简单快捷。 在某些场景下可以很快完成移植，而不需要进行额外的同步设置。例如，当我们的电脑需要重装系统时，我们可以直接将最新的 Hexo 博客项目进行打包，新系统中搭建好 Hexo 个人博客环境后，直接解压几乎就可以使用了。 2）缺点： 对于硬盘、U盘等设备，备份后的同步十分麻烦，每次其它终端都需要手动下载备份最新的 Hexo 博客文件夹，进行手动覆盖。 目前大多数云盘，可以开启云端自动备份功能，写完 Blog 后可以自动备份（同步）到云端。但是很容易产生一些云盘内部文件，影响 Hexo 解析产生一些不可预期的错误。 因此，使用存储设备备份使用的很少。 1.2.2 第三方代码仓库备份鉴于之前我们将 Hexo 个人博客产生的静态页面托管到了一些第三方 Git 服务平台，以实现远程访问。同样，类似于代码托管，我们可以将我们的 Hexo 个人博客项目本地配置信息文档托管到远程仓库进行版本控制，以实现多设备的同步管理。这是目前最合理，并且使用最多的解决方案。 使用第三方代码仓库进行备份是目前普遍使用的对 Hexo 个人博客进行备份同步的方法。 国内外现在知名的 Git 服务提供商主要有： Github、Coding 以及 Gitee（码云）等等，使用上没有比较大的差异，但国内站点访问较快。 下面我们来分析其优略： 1）优点： 部署完成后同步非常方便，Hexo 更新完后只需要再更新（push）全站到 Git 远程仓库即可。 2）缺点： 部署过程相对比较麻烦，对 Git 新手不友好，但这仅仅是 Git 使用上的问题，不是难点。 对于使用第三方代码仓库（以 Github 为例）进行备份的方法，目前主流的有两种方法： 分支备份法：我们知道，Hexo生成的静态博客文件都是上传到 GitHub 上的, 且默认放在 master 分支上。分支备份法是将本地的 Hexo 相关环境配置文件都推送到对应仓库新创建的分支上（如 hexo 分支），以实现备份。 将本地整个 Hexo Blog 项目进行备份：创建一个新的仓库用来对本地环境配置文件进行版本管理以及备份。 1.3 实施方案1.3.1 方案一：Hexo envs + yunpan–&gt; 步骤一：Hexo envsPC_A 中我们已经成功搭建和使用 Hexo 博客了，所以不需要重复安装。而关于 PC_B 中 Hexo 个人博客环境的搭建，参考：基于 Hexo 轻量级框架的博客搭建 。步骤如下： 安装 Node.js 安装 Git 安装 Hexo –&gt; 步骤二：yunpan1）将 PC_A 终端中 Hexo 个人博客项目目录进行打包（打包格式任选，在 PC_B 中可以快速解压即可），备份到云盘。 2）然后在 PC_B 中从云盘下载已经上传好的 Hexo 个人博客项目压缩文件，然后进行解压。 3）PC_B 中打开浏览器，通过访问 localhost:4000 进行 Hexo 博客本地测试，发现已经可以成功访问到我们的 Hexo 本地博客页面。 4）Hexo 个人博客本地测试通过后，由于更换设备，我们需要为Hexo Github·Page 仓库配置新设备的 SSH Key。此时只需要将新设备（PC_B ）的 SSH Key 添加到 Github 中即可。如果不进行设置，当使用 hexo d 进行推送时无法成功，原因在于 Hexo 无法连接到 Github·Page。 关于新设备创建 SSH Key 方法以及为 Github 配置 SSH Key 可参加：Git 使用指南之远程仓库。 除了上述添加方式外，还有一种更简单的方式，我们可以将 PC_A 生成的 .ssh 文件直接放到 PC_B 设备当前用户目录下即可。 自此，设置完成。这种方法缺点很明显，对于同步很不友好！！！ 1.3.2 方案二：Hexo envs + hexo 备份这一小节，我们来看如何将本地整个 Hexo Blog 项目进行备份。即创建一个新的仓库用来对本地环境配置文件进行版本管理以及备份。 –&gt; 步骤一：Hexo envsPC_A 中我们已经成功搭建和使用 Hexo 博客了，所以不需要重复安装。而关于 PC_B 中 Hexo 个人博客环境的搭建，参考：基于 Hexo 轻量级框架的博客搭建 。步骤如下： 安装 Node.js 安装 Git 安装 Hexo 步骤一和 1.3.1 中完全一样。 –&gt; 步骤二：hexo 项目备份注意，这一部分操作全部在 PC_A（保存了 Hexo Blog 最新进度）上进行，用于将本地整个 Hexo Blog 项目托管到 Github 上的一个全新仓库（以创建 HexoBackups 为例）。 首先，添加设备 SSH Key 到 Github 以提供访问权限（在搭建 Hexo 环境时应该已经添加过），并且在 Github 中创建 HexoBackups 仓库（操作方法可见：Git 使用指南之远程仓库）。 1）删除 Hexo 站点目录主题路径（Hexo站点目录\themes）下原有的 .git* 缓存文件夹，并编辑站点目录中的 .gitignore 文件。 有些插件或者主题是从 Github 上 clone 过来安装的，每个文件夹下都会有对应的 .git 文件夹，记得先删掉，否则会和 Blog 仓库冲突。（.git 默认是隐藏文件夹，需要先开启显示隐藏文件夹。.git 文件夹被删除后整个文件对应的 Git 仓库状态也会被清空，避免冲突） 编辑 .gitignore 文件的作用是声明不被 Git 记录的非必要文件（我们希望将必要的 Hexo 配置文件进行版本控制，而不是所有文件）。Blog 站点目录下的 .gitignore 是 Hexo 初始化时生成的，可以先删除或者直接编辑，对 Hexo 不会有影响。我的 Hexo 默认 .gitignore 文件内容如下： 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ .deploy_git 是 Hexo 默认的 .git 配置文件夹，不需要同步； public 内文件是根据 source 文件夹内容自动生成，不需要备份，不然版本仓库每次修改内容太多； node_modules 目录中存放了 Hexo 个人博客所需的所有插件，太大，使用时通过 npm install 安装即可。 .DS_Store、Thumbs.db、*.log 、db.json 等均属于非必要文件。 2）初始化本地仓库 Hexo Blog 站点目录下执行以下代码： 123# &lt;server&gt; 是指在线仓库的地址；origin 是本地分支；remote add 操作会将本地仓库映射到云端 git initgit remote add origin &lt;server&gt; 3）添加本地文件到本地仓库并同步到远程 Github 上 123456# 添加 Hexo Blog 站点目录下所有文件（.gitignore 声明过的文件不包含在内)git add .# 添加更新操作说明git commit -m "Hexo Blog Backups"# 推送更新到云端服务器git push -u origin master 在执行这步之前一定要注意检查下 .gitignore文件的内容，看看是否正确的把一些文件夹忽略掉了。如果加错了的话可以用： 1git rm -r --cached . 自此，我们已经成功将本地 Hexo 博客配置文件推送到了 Github 远程仓库 HexoBackups 中实现版本控制。 –&gt; 步骤三：其它终端克隆测试1）添加 SSH Key 到 GitHub 首先，我们需要将设备 PC_B 的 SSH Key 添加到 GitHub 以提供访问权限。 2）将 Hexo Github 仓库内容同步到本地 之前我们已经成功将 PC_A 电脑里的 Hexo 配置信息备份到 Github HexoBackups 仓库了。 现在在 PC_B 电脑准备通过为本地仓库配远程 Hexo Github 版本仓库以实现 Hexo 配置信息同步。 123456789# 创建本地博客目录mkdir HexoBlogProjectcd HexoBlogProjectgit init# 将本地文件和云端仓库映射起来。这步不可以跳过git remote add origin &lt;server_addr&gt;git fetch --allgit reset --hard origin/master 当然我们还可以直接 git clone 将 HexoBackups 仓库中的 Hexo 博客配置文件拉取下来： 1git clone &lt;server_addr&gt; 3）安装依赖插件 同步后需要安装相应的 Hexo 插件（这是由于我们之前备份时未备份 node_modules 插件目录），否则无法正常使用 Hexo： 1npm install 4）localhost 测试 PC_B 中打开浏览器，通过访问 localhost:4000 进行 Hexo 博客本地测试，发现已经可以成功访问到我们的 Hexo 本地博客页面。 1.3.3 方案三：Hexo envs + hexo 分支备份和 1.3.2 中备份整个 Hexo 本地配置信息文件到一个新仓库不同的是，分支备份是在原 Hexo 静态页面托管仓库（username.github.io）重新创建一个分支（以 hexo 分支为例）来备份 Hexo 本地配置信息文件。 最终，username.github.io 仓库的 master 分支和 hexo 分支各自保存着一个版本： master 分支用于保存 Hexo 博客静态资源，提供博客页面以供人访问；hexo分支用于备份博客部署文件，供自己维护更新，两者在一个 username.github.io 仓库内也不会有任何冲突。 –&gt; 步骤一：Hexo envsPC_A 中我们已经成功搭建和使用 Hexo 博客了，所以不需要重复安装。而关于 PC_B 中 Hexo 个人博客环境的搭建，参考：基于 Hexo 轻量级框架的博客搭建 。步骤如下： 安装 Node.js 安装 Git 安装 Hexo 步骤一和 1.3.1 中完全一样。 –&gt; 步骤二：hexo 分支备份注意，这一部分操作全部在 PC_A（保存了 Hexo Blog 最新进度）上进行。 和 1.3.2 中备份过程类似，这一部分给出 Hexo 分支备份通用流程，关于操作解读可对应 1.3.2 中的步骤二。 12345678910111213141516171819# 消除 Git 仓库冲突rm -rf Hexo站点目录\themes\主题目录、.git*# 即 Hexo 博客根目录cd Hexo站点目录# 初始化本地仓库git init# 将本地与 Github 远程仓库进行对接git remote add origin git@github.com:user/user.github.io.git# 添加 Hexo Blog 站点目录下所有文件（.gitignore 声明过的文件不包含在内)git add .# 添加更新操作说明git commit -m "Hexo Blog Backups"# 创建名为 hexo 的分支git checkout -b hexo# 推送分支到 GitHubgit push origin hexo –&gt; 步骤三：其它终端克隆测试1）添加 SSH Key 到 GitHub 首先，我们需要将设备 PC_B 的 SSH Key 添加到 GitHub 以提供访问权限。 2）克隆 Hexo 博客环境 123# 将 Github 中 hexo 分支 clone 到本地git clone -b hexo git@github.com:username/username.github.io.gitcd user.github.io 3）安装依赖插件 12# 安装 Hexo 博客中使用到的插件npm install 4）localhost 测试 PC_B 中打开浏览器，通过访问 localhost:4000 进行 Hexo 博客本地测试，发现已经可以成功访问到我们的 Hexo 本地博客页面。 1.3.4 Git + Hexo 管理博文这一部分我们来看加入版本控制后，如何进行 Hexo Blog 的多平台协同创作 （Git + Hexo）？ 假设在 PC_B 电脑上写完了文章，然后进行发布： 1hexo d -g 接下来，我们还需要将新文章的 .md 文件推送到备份仓库。（其实就是提交更新给 Hexo Github 备份仓库）： 1git add . 这时候可以用 git status 查看状态，一般会显示刚刚更改过的文件状态。如： 123456On branch masterChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) modified: db.json new file: source/_posts/test.md 上面的输出状态即说明 db.json 文件做了更改，source/_posts 目录下新增了 test.md 文件。 然后对更改添加说明并推送到远程仓库： 123456git commit -m '更新信息'# 推送git push# 或者（取决于整体备份还是分支备份）git push origin hexo 当显示类似如下提示的时候，即表示备份成功： 12To git@git.oschina.net:xxxx/HexoBackups.git + 2c77e1e...5616bc6 master -&gt; master (forced update) 再回到到 PC_A 电脑上的时候，我们需要拉取最新的 Hexo 配置信息到本地以实现本地仓库和远程仓库一致： 1234# 拉取最新版本git pull# 或者（取决于整体备份还是分支备份）git pull origin hexo 即可实现 Hexo 博客同步更新以及协同管理。 1.4 常见问题1.4.1 npm install 很慢很慢安装 Hexo 博客相关依赖插件时，我们需要使用 npm install 下载进行安装。但是执行指令后一直没反应，这是由于 npm 官方资源被墙，我们可以为其更换一个国内源： 1npm config set registry https://registry.npm.taobao.org 当然，当我们的 Hexo 个人博客使用较多的插件时，npm install 可能看起来很慢（好像卡在某一个安装语句不再执行），此时我们可以通过 Hexo 站点目录下的 node_modules 目录更新时间来判断。 如果 node_modules 目录下各种插件文件更新时间长时间不发生变化，就意外着安装已停止，Ctrl + c 即可。 1.4.2 Git 监测不到空文件夹Git 无法监测到空文件夹，也就意外着无法将空文件夹 git add、git commit -m &quot;&quot;、git push 推送到远程仓库，这会导致 Hexo 博客项目中丢失一些空的功能文件夹。这应该算是 Git 的设计失误。 解决方法： 需要在空文件夹中添加一个占位文件。主流做法是在空文件夹里放置一个 .gitkeep 文件。加个 .gitconfig 文件在里面比较实用，也不会觉得突兀，虽然绝大多数时候这个文件作用。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Backups</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 个人博客主题之个性优化]]></title>
    <url>%2FHexo%2FHexo-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E4%B9%8B%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 前言： 经过一番不懈的努力，在 基于 Hexo 轻量级框架的博客搭建 中我们终于成功搭建起了一个 Hexo 个人博客。并且在 Hexo 个人博客之主题实用优化 成功完成了 Hexo 个人博客 Next 主题实用性优化配置的一般步骤。本文我们继续来看 Next 主题的个性化优化配置。 如文中有表述不恰当的地方，欢迎各位在留言区进行指正，若有转载请注明出处!谢谢~ 1. NexT 主题优化以下主题优化配置保存后均可通过 hexo g &amp;&amp; hexo s 指令在本地 http://localhost:4000 进行预览修改。另外可通过 hexo d 发布到 GitHubPages 或 CodingPages上面在线查看效果。 优化开始之前，我们先给出两点必要的声明： 在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml:一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下（由主题作者提供），主要用于配置主题相关的选项。为了后续描述方便，故将前者称为 站点配置文件， 后者称为主题配置文件; 以下所有终端（Git hash）执行的命令都在 Hexo 站点目录下. 下面我们开始 Next 主题个性化显示配置的一般步骤： 1.1 修改文章内链接文本样式编辑 &lt;主题目录&gt;/source/css/_common/components/post/post.styl 配置文件，在末尾添加 CSS 样式： // 文章内链接文本样式 .post-body p a{ color: #0593d3; //原始链接颜色 border-bottom: none; border-bottom: 1px solid #0593d3; //底部分割线颜色 &amp;:hover { color: #fc6423; //鼠标经过颜色 border-bottom: none; border-bottom: 1px solid #fc6423; //底部分割线颜色 } } 1.2 修改底部标签样式编辑 Blog\themes\next\layout\_macro\post.swig 文件找到 rel=&quot;tag&quot;&gt;#,将 # 替换成 &lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;。Git bash 站点目录下运行如下命令即可： hexo clean hexo s 显示效果如下： 1.3 添加 “文章结束” 标记在路径 &lt;主题目录&gt;/layout/_macro 文件夹中新建 passage-end-tag.swig 文件，并添加如下内容： 编辑 Blog\themes\next\layout\_macro\post.swig，在目标位置添加相应代码。如下： 打开 主题配置文件 末尾添加如下配置代码： # 添加 “文章结束” 标记 passage_end_tag: enabled: true 1.4 开启博文更新时间显示打开 主题配置文件 找到 post_meta 字段下的 updated_at 关键字，将其设置为 true。如下： # Post meta display settings post_meta: item_text: true created_at: true updated_at: true categories: true 1.5 主页文章添加边框阴影效果编辑 &lt;主题目录&gt;/source/css/_custom/custom.styl 文件，加入以下配置代码： // 主页文章添加阴影效果 .post { margin-top: 0px; margin-bottom: 60px; padding: 15px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); } 1.6. 修改博文 URL 访问路径默认情况下博文 URL 访问路径格式为： #permalink: :year/:month/:day/:title/ 我们可以通过打开 站点配置文件 找到 permalink 字段配置其值为如下： permalink: :category/:title/ 此后，博文 URL访问路径便被修改为：&lt;站点访问 URL&gt;/&lt;category&gt;/&lt;title&gt; 格式。 1.7 开启文章加密访问编辑 themes/*/layout/_partials/head.swig 文件，在目标位置（下文给出）插入如下代码： &lt;script&gt; (function(){ if(&apos;{{ page.password }}&apos;){ if (prompt(&apos;请输入密码&apos;) !== &apos;{{ page.password }}&apos;){ alert(&apos;密码错误&apos;); history.back(); } } })(); &lt;/script&gt; 目标位置如下： 并且需要在编辑博文时添加 password 标记，如下： --- title: 2018 date: 2018-10-25 16:10:03 password: 123456 --- 1.8 修改网站标题栏背景颜色编辑 &lt;主题目录&gt;/source/css/_custom/custom.styl 文件,写入如下配置代码： .site-meta { // 修改为自己喜欢的颜色 background: $blue; } 1.9 添加背景图在 &lt;主题目录&gt;/source/css/_custom/custom.styl 中添加如下代码： body{ background:url(/uploads/header.jpg); background-size:cover; background-repeat:no-repeat; background-attachment:fixed; background-position:center; } 注意背景图片存放于 &lt;主题目录&gt;/source/uploads/ 目录下。 1.10 修改 Logo 字体在 &lt;主题目录&gt;/source/css/_custom/custom.styl 中添加如下代码： @font-face { font-family: Zitiming; src: url(&apos;/fonts/Zitiming.ttf&apos;); } .site-title { font-size: 40px !important; font-family: &apos;Zitiming&apos; !important; } 其中字体文件在 &lt;主题目录&gt;/source/fonts 目录下，里面有个 .gitkeep 的隐藏文件，打开写入你要保留的字体文件，比如我的是就是写入 Zitiming.ttf ，具体字库自己从网上下载即可。 1.11 设置首页不显示全文(只显示预览)打开 主题配置文件 找到 auto_excerpt 字段，内容如下： # Automatically Excerpt. Not recommend. # Please use &lt;!-- more --&gt; in the post to control excerpt accurately. auto_excerpt: enable: false length: 150 只需要将 enable 值设置为 true即可。length 就是预览显示的文字长度，你可以根据你的需要进行更改，然后重新部署，再进主页，你就发现你首页的文章多了一个阅读全文的按钮。 注意，也可以不在这里进行配置，只需在编辑博文时使用 &lt;!-- more --&gt; 也可实现。页面会自动显示 &lt;!-- more --&gt; 之前的内容。 1.12 取消标题自动编目打开 主题配置文件 找到 toc,作如下配置: # Table Of Contents in the Sidebar toc: enable: true # Automatically add list number to toc. number: false 1.13 开启 MathJax 渲染 LaTeX 数学公式打开 主题配置文件 找到 MathJax Support 模块下的 mathjax 字段，设置其 enable 关键字值为：true。如下： 12345# MathJax Supportmathjax: enable: true per_page: false cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 个人博客主题之实用性优化]]></title>
    <url>%2FHexo%2FHexo-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E4%B9%8B%E4%B8%BB%E9%A2%98%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 前言： 经过一番不懈的努力，在 基于 Hexo 轻量级框架的博客搭建 中我们终于成功搭建起了一个 Hexo 个人博客。但我们发现 Hexo 博客默认主题其实是比较简陋的，我们更希望的是拥有一个看起来就很 高逼格 的 个人 Blog，毕竟编程也是需要一点仪式感的…本文我们来看如何基于一个开源的博客主题继续优化我们的个人博客，心之所向啊。 如文中有表述不恰当的地方，欢迎各位在留言区进行指正，若有转载请注明出处!谢谢~ 1. Hexo 主题优化以下主题优化配置保存后均可通过 hexo g &amp;&amp; hexo s 指令在本地 http://localhost:4000 进行预览修改。另外可通过 hexo d 发布到 GitHubPages 或 CodingPages上面在线查看效果。 1.1 主题安装以及启用我们知道，Hexo 站点目录下的 themes 目录专门用来存放 Hexo 主题。通过查看 themes 目录我们发现，Hexo 安装后默认只有一个 landscape 主题。大多数情况下，landscape 主题的使用是无法满足我们的“精神追求”的… 1.1.1 选择主题这里我们提供一套比较简约的主题—NexT，它是目前使用最活跃的一个 Hexo 主题。简约就意味着其配置简单，而活跃意味着主题优化过程中更少的“麻烦”。当然你也可以选择其它的个性主题，它们一般都具有详细的说明文档帮助使用。 下面我们提供两个可供 clone 的 NexT Github 仓库地址: # Github NexT Repository Addr： https://github.com/theme-next/hexo-theme-next https://github.com/iissnan/hexo-theme-next.git 1.1.2 主题应用1）clone NexT： $ cd &lt;站点目录&gt;/themes $ git clone https://github.com/iissnan/hexo-theme-next.git 2）NexT 主题启用： 更改站点配置文件 _config.yml 的 theme 字段，为主题文件夹的名称。如下： # Extensions ## Plugins: https://hexo.io/plugins/ ## Themes: https://hexo.io/themes/ # theme: landscape # Hexo 默认主题 theme: hexo-theme-next 3）NexT 本地主题验证（预览）： 注意本地验证之前，需要清除 Hexo 缓存： $ hexo clean INFO Deleted database. INFO Deleted public folder. 然后，启动 Hexo 本地服务进行预览（验证）： $ hexo s INFO Start processing INFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. ok！浏览器访问: http://localhost:4000,此时发现我们的个人博客主题已经发生变更。 成功 clone 并启用了 NexT 主题之后，接下来我们就可以对该主题进行优化了： 1.2 NexT 主题优化优化开始之前，我们先给出两点必要的声明： 在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml:一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下（由主题作者提供），主要用于配置主题相关的选项。为了后续描述方便，故将前者称为 站点配置文件， 后者称为主题配置文件; 以下所有终端（Git hash）执行的命令都在 Hexo 站点目录下. 其实，NexT 主题优化很多，本文我们会看到常见的、主要的、实用性的主题优化配置一般步骤（为避免博文篇幅过长，其它个性化优化会在后续文章继续介绍）： 1.2.1 Next 主题样式设置 主题样式是指 Next 主题页面显示风格 打开 主题配置文件 找到 Scheme Settings 模块。如下： # --------------------------------------------------------------- # Scheme Settings # --------------------------------------------------------------- # Schemes #scheme: Muse #scheme: Mist #scheme: Pisces scheme: Gemini Next 主题为我们提供了 4 中不同的主题风格，默认配置的是：scheme: Muse。个人偏向于喜欢 Gemini，可以根据个人喜爱选择不同的 Next 主题风格。 注意配置修改保存后，可以重新启动 Hexo 本地服务，预览查看博客页面。 1.2.2 基本信息配置 基本信息包括：博客标题、作者、描述、语言等等。 打开 站点配置文件 ，找到 Site 模块。如下： # Site title: # 设置网站标题 subtitle: # 设置网站副标题 description: # 设置网站描述 keywords: author: # 设置作者（所有者） language: # 设置语言 timezone: # 设置网站时区：Hexo 默认使用当前电脑的时区（默认即可） 关于语言设置： 我们需要在 &lt;主题目录&gt;/languages/{language}.yml 查看当前下载的 Next 主题所支持的语言类型，你可以看到一系列的 {language}.yml 文件。我的如下： de.yml default.yml es.yml fr.yml ja.yml ko.yml nl.yml no.yml pt.yml ru.yml . . . zh-CN.yml zh-HK.yml zh-TW.yml 查看 default.yml 文件可以查看到主题当前默认使用的语言类型。 这里给出一个 NexT 支持的语言对照表，如下所示： 当然这里对照表和你实际下载的主题语言支持可能存在一定出入，视当前下载主题版本而定。例如，一般简体中文对应 zh-Hans.yml 文件，而我的对应 zh-CN.yml文件。 关于 站点配置文件 中的其他选项配置可参考 Hexo 官方站点配置文档。 1.2.3 菜单配置 菜单包括：首页、归档、分类、标签、关于等等 1）添加菜单项： 1.设置菜单项 默认的菜单项只有 home(首页) 和 archives(归档) 两个，我们可以通过个人需求添加其它菜单项（需要哪个菜单项就将其注释消掉）。 打开 主题配置文件 找到 Menu Settings 模块。如下： # --------------------------------------------------------------- # Menu Settings # --------------------------------------------------------------- # When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash from link value (/archives -&gt; archives). # Usage: `Key: /link/ || icon` # Key is the name of menu item. If translate for this menu will find in languages - this translate will be loaded; if not - Key name will be used. Key is case-senstive. # Value before `||` delimeter is the target link. # Value after `||` delimeter is the name of FontAwesome icon. If icon (with or without delimeter) is not specified, question icon will be loaded. # External url should start with http:// or https:// menu: home: / || home # 设置首页菜单项 #about: /about/ || user # 设置关于菜单项 #tags: /tags/ || tags # 设置标签菜单项 #categories: /categories/ || th # 设置分类菜单项 archives: /archives/ || archive # 设置归档菜单项 #schedule: /schedule/ || calendar # 设置日程表菜单项 #sitemap: /sitemap.xml || sitemap # 设置站点地图菜单项 #commonweal: /404/ || heartbeat # 设置公益404菜单项 菜单项设置格式说明，以 archives: /archives/ || archive 为例： archives 表示菜单项名称 “归档”，这个名称并不直接显示在页面上，它将用于匹配图标以及翻译（你可以将其看作一个变量标识）。 /archives/ 表示归档菜单项访问路径 archive 表示归档菜单项对应图标 2.设置菜单项的显示文本： 在第一步中设置的菜单的名称并不直接用于界面上的展示。Hexo 在生成的时候将使用这个名称查找对应的语言翻译，并提取显示文本。 这些翻译文本放置在 NexT 主题目录下的 languages/{language}.yml （{language} 为我们所使用的语言）。 以简体中文 zh-Hans（我的为：zh-CN.yml）为例： menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益 404 如果你发现博客页面菜单项文本显示错误，可以查看该翻译文本文件和选用语言类型是否正确。 3.设定菜单项的图标： 注意关于菜单项图标，可以去 Font Awesome 中选择或修改（ Next 主题所有的图标都来自 Font Awesome）。 2）初始化菜单项页面： 添加菜单项后，其实是无法正常显示菜单项页面的（只是增加了一个菜单项按钮模块）。此时点击菜单项按钮可能显示（以 categories 为例）： Cannot Get /categories/ 下面我们还是以 categories 菜单项为例来看如何初始化其对应的 categories 菜单页面： 1.初始化 categories页面，Git bash 站点目录下运行： $ hexo new page categories INFO Created: F:\HexoBlogProject\source\categories\index.md 2.初始化成功后，会在 &lt;站点目录&gt;/source 目录下出现一个 categories 目录（默认情况下只存在 _posts 目录）。 3.编辑 &lt;站点目录&gt;/source/categories 中 index.md 文件，添加页面类型 type: &quot;categories&quot;。如下： --- title: 分类 date: 2019-01-08 19:29:04 type: &quot;categories&quot; --- 注意其它菜单项页面可同理创建。 1.2.4 侧栏设置 侧栏设置一包括：侧栏位置、侧栏显示与否、文章间距、返回顶部按钮等等 打开 主题配置文件 找到 Sidebar Settings 模块下的 sidebar 字段。如下： sidebar: # 设置侧栏位置（only for Pisces | Gemini 主题风格） # Sidebar Position, available values: left | right (only for Pisces | Gemini). position: left # 设置靠左放置 #position: right # 设置靠右放置 # Manual define the sidebar width. # If commented, will be default for: # Muse | Mist: 320 # Pisces | Gemini: 240 #width: 300 # 设置侧栏显示时机(only for Muse | Mist 主题风格) # Sidebar Display, available values (only for Muse | Mist): # - post expand on posts automatically. Default. # - always expand for all pages automatically # - hide expand only when click on the sidebar toggle icon. # - remove Totally remove sidebar including sidebar toggle. display: post # 默认设置：在文章页面（拥有目录列表）时显示 #display: always # 设置在所有页面中都显示 #display: hide # 设置在所有页面中都隐藏（可以手动展开） #display: remove # 设置完全移除 # Sidebar offset from top menubar in pixels (only for Pisces | Gemini). offset: 12 # 设置文章间距(only for Pisces | Gemini) # Back to top in sidebar. #b2t: false b2t: true # 设置返回顶部按钮(only for Pisces | Gemini) # Scroll percent label in b2t button. # scrollpercent: false scrollpercent: true # 设置返回顶部按钮的百分比（显示文章浏览进度） # Enable sidebar on narrow view (only for Muse | Mist). onmobile: false 侧栏设置二包括：头像设置 打开 主题配置文件 找到 Sidebar Settings 模块下的 avatar 字段。如下： # Sidebar Avatar # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif avatar: /uploads/header.jpg 只需把头像图片命名为:header.jpg（随便命名）,放入 themes/next/source/uploads 中，将 avatar 字段的路径名改成 /uploads/header.jpg 就 ok 啦！uploads 目录不存在可以进行创建。 注意：还有一些主题版本头像配置项如下（存在 URL 字段）： # Sidebar Avatar avatar: # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /uploads/header.jpg 其余操作和 avatar 字段设置方法一样。 1.2.5 RSS 设置1）安装 hexo-generator-feed 插件： $ npm install hexo-generator-feed --save 2）打开 【站点配置文件】 找到 Extensions 模块，在下面添加 RSS 订阅相关内容： # Extensions ## Plugins: https://hexo.io/plugins/ ## Themes: https://hexo.io/themes/ # theme: landscape theme: hexo-theme-next # RSS 订阅 feed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: &apos; &apos; 3）打开 主题配置文件 找到 rss 字段，并作如下设置： # Set rss to false to disable feed link. # Leave rss as empty to use site&apos;s feed link, and install hexo-generator-feed: `npm install hexo-generator-feed --save`. # Set rss to specific value if you have burned your feed already. rss: /atom.xml 1.2.6 字体设置 注意此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后。 为了解决 Google Fonts API 不稳定的问题，NexT 在 5.0.1 中引入此特性。 通过此特性，你可以指定所使用的字体库外链地址；与此同时，NexT 开放了 5 个特定范围的字体设定，他们是： 全局字体：定义的字体将在全站范围使用 标题字体：文章内标题的字体（h1, h2, h3, h4, h5, h6） 文章字体：文章所使用的字体 Logo字体：Logo 所使用的字体 代码字体： 代码块所使用的字体 各项所指定的字体将作为首选字体，当他们不可用时会自动 Fallback 到 NexT 设定的基础字体组： 非代码类字体：Fallback 到 “PingFang SC”, “Microsoft YaHei”, sans-serif 代码类字体： Fallback 到 consolas, Menlo, “PingFang SC”, “Microsoft YaHei”, monospace 另外，每一项都有一个额外的 external 属性，此属性用来控制是否使用外链字体库。 开放此属性方便你设定那些 已经安装在系统中的字体，减少不必要的请求。主题配置模块如下： # --------------------------------------------------------------- # Font Settings # - Find fonts on Google Fonts (https://www.google.com/fonts) # - All fonts set here will have the following styles: # light, light italic, normal, normal italic, bold, bold italic # - Be aware that setting too much fonts will cause site running slowly # - Introduced in 5.0.1 # --------------------------------------------------------------- # CAUTION! Safari Version 10.1.2 bug: https://github.com/iissnan/hexo-theme-next/issues/1844 # To avoid space between header and sidebar in Pisces / Gemini themes recommended to use Web Safe fonts for `global` (and `logo`): # Arial | Tahoma | Helvetica | Times New Roman | Courier New | Verdana | Georgia | Palatino | Garamond | Comic Sans MS | Trebuchet MS # --------------------------------------------------------------- font: enable: false # Uri of fonts host. E.g. //fonts.googleapis.com (Default). host: # Font options: # `external: true` will load this font family from `host` above. # `family: Times New Roman`. Without any quotes. # `size: xx`. Use `px` as unit. # Global font settings used for all elements in &lt;body&gt;. global: external: true family: Lato size: # Font settings for Headlines (H1, H2, H3, H4, H5, H6). # Fallback to `global` font settings. headings: external: true family: size: # Font settings for posts. # Fallback to `global` font settings. posts: external: true family: # Font settings for Logo. # Fallback to `global` font settings. logo: external: true family: size: # Font settings for &lt;code&gt; and code blocks. codes: external: true family: size: 注意：如果引用国外字体镜像可以访问较慢，可以将其改用国内镜像。 即将：&lt;站点目录&gt;themes/&lt;next 主题目录&gt;/layout/_partials/head 目录下的 fonts.googleapis.com 改成 fonts.lug.ustc.edu.cn。 1.2.7 设置代码高亮主题NexT 使用 Tomorrow Theme 作为代码高亮，共有 5 款主题可供选择。 NexT 默认使用的是白色的 normal 主题，可选的值有 normal、night、night blue、night bright、night eighties。 打开 主题配置文件 找到 Code Highlight theme 模块下的 highlight_theme 字段，将其值设定成你所喜爱的高亮主题，例如选为黑亮主题： # Code Highlight theme # Available values: normal | night | night eighties | night blue | night bright # https://github.com/chriskempson/tomorrow-theme #highlight_theme: normal highlight_theme: night bright 1.2.8 设置动态背景1）开启动态背景： 打开 主题配置文件 中找到 Canvas-nest 模块，设置其 true。如下： canvas_nest: true 2）添加一个动态线条背景： 在 &lt;站点目录&gt;/themes/&lt;主题目录&gt;/layout 目录下打开 _layout.swig 配置文件，在 &lt;body&gt; 标签内添加一下代码： &lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt; 动态背景中线条太多了？如何更改当前动态背景线条显示数目？修改上述代码为： &lt;script type=&quot;text/javascript&quot; color=&quot;0,0,255&quot; opacity=&apos;0.8&apos; zIndex=&quot;-2&quot; count=&quot;99&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt; 我们还可以增加是否启用 canvas_nest 判断条件，具体代码块如下所示： 配置项说明： color ：设置线条颜色：默认(R,G,B)为 &apos;0,0,0&apos;； opacity: 设置线条透明度（0~1）：默认为 0.5； count: 设置线条的总数量：默认为 150； zIndex: 设置背景的 z-index 属性（css 属性用于控制所在层的位置）： 默认为 -1。 1.2.9 开启侧边栏社交链接侧栏社交链接的修改包含两个部分：第一是链接，第二是链接图标。 两者配置均在 主题配置文件 中。 1）设置社交链接： 链接放置在 social 字段下，一行一个链接。其键值格式是:显示文本: 链接地址 || 图标。 如下： social: GitHub: https://github.com/yourname || github #E-Mail: mailto:yourname@gmail.com || envelope 微博: https://weibo.com/yourname || weibo #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skype 2）设定社交链接的图标： 链接的图标对应的字段是 social_icons。其键值格式是:匹配键: Font Awesome 图标名称。 注意匹配键与 上一步所配置的链接的 显示文本 相同（大小写严格匹配），图标名称是 Font Awesome 图标的名字（不必带 fa- 前缀）。 enable 选项用于控制是否显示图标，可以设置成 false 来去掉图标。 具体设置实例如下： social_icons: enable: true GitHub: github 微博: weibo icons_only: false transition: false 1.2.10 开启打赏功能越来越多的平台（微信公众平台，新浪微博，简书，百度打赏等）支持打赏功能，付费阅读时代越来越近，特此增加了打赏功能，支持微信打赏和支付宝打赏。 只需要 主题配置文件 中填入 微信 和 支付宝 收款二维码图片地址 即可开启该功能。 打开 主题配置文件 找到 Reward 模块。设置 enable 字段为：true。comment（reward_comment） 字段用来设置显示一段感谢打赏文本。 打赏功能配置示例如下: # Reward # If true, reward would be displayed in every article by default. # And you can show or hide one article specially through add page variable `reward: true/false`. reward: enable: true comment: Donate comment here wechatpay: /uploads/wechatpay.png alipay: /uploads/alipay.png #bitcoin: /images/bitcoin.jpg 注意：uploads 是我个人自定义图像存储目录，将其存放于：&lt;站点目录&gt;/themes/&lt;主题目录&gt;/source 目录即可。wechatpay.png 和 alipay.png 是我们的 微信 和 支付宝 付款二维码图片（草料二维码 可以帮助我们对付款二维码图片二维码图片进行美化）。 取消打赏字体不闪动（鬼畜啊）： 打开 &lt;主题目录&gt;/source/css/_common/components/post/post-reward.styl 文件，然后注释其中的函数 wechat:hover 和 alipay:hover。如下： /* 注释掉鬼畜抖动： #wechat:hover p{ animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear; } #alipay:hover p{ animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear; } */ 1.2.11 开启友情链接有时候我们会在博客页面设置一些推荐性链接，这里我们来看如何在侧栏中开启友情链接模块。 打开 主题配置文件 找到 Blog rolls 模块下的 links_title 字段，作如下设置： links_title: Links links: #Title: http://example.com CloudStadio: https://dev.tencent.com/ 1.2.12 设置站点建立时间这个时间将在站点的底部显示，例如 © 2013 - 2015。 打开 主题配置文件，查找 since 字段。设置如下： # Specify the date when the site was setup. # If not defined, current year will be used. since: 2019 1.2.13 开启微信公众号订阅我们可以设置在每篇博文的末尾显示一个 微信公众号二维码，通过扫一扫，可以轻松订阅微信公众号推文。 注意： 此特性在版本 5.0.1 中引入，要使用此功能请确保所使用的 NexT 版本在此之后。 1）开启微信公众号订阅配置： 打开 主题配置文件 找到 Wechat Subscriber 模块，内容如下： # Wechat Subscriber #wechat_subscriber: #enabled: true #qcode: /path/to/your/wechatqcode e.g. /uploads/wechat-qcode.jpg #description: e.g. subscribe to my blog by scanning my public wechat account qcode 字段用来指定 微信公众号二维码 图片存放路径，例如：/uploads/wechat-qcode.jpg。 2）上传微信公众号二维码图片 我们需要在微信公众号平台下载平台二维码，并将它存放于 hexo 博客 &lt;站点目录&gt;/source/uploads/ 目录下即可（对应上述 /uploads/wechat-qcode.jpg）。示例配置如下： # Wechat Subscriber wechat_subscriber: enabled: true qcode: /uploads/wechat-qcode.jpg description: 欢迎您扫一扫上面的微信公众号，订阅我的博客！ 1.2.14 开启评论系统NexT 可以借助于第三方服务支持多款评论系统，如：网易云跟帖、DISQUS、搜狐畅言、友言、来必力等。这里推荐使用 来必力 评论系统，下面我们来看其配置过程： 登陆 来必力官网 完成账号注册。点击 “安装” 菜单项，选择现在安装： 选择安装之后，会跳入 “我的来必力安装代码” 界面，记录 UID 记录值（红色覆盖部分）： 然后，打开 主题配置文件 找到 livere_uid 字段，将上述 UID 记录值填入即可。如下： # Support for LiveRe comments system. # You can get your uid from https://livere.com/insight/myCode (General web site) livere_uid: xxxxxxxxxxxxxxxxxxxxxxx 如需取消某个 页面/文章 的评论，在 md 文件的 front-matter 中增加 comments: false。 1.2.15 开启访客量以及文章阅读量统计Next 中还可以借助于第三方服务实现 访客量以及文章阅读量 数据与分析功能。其中使用最多的是：不蒜子，很简单就可以实现统计功能。 打开 主题配置文件 找到 busuanzi_count 字段，其配置内容如下： # Show PV/UV of the website/page with busuanzi. # Get more information on http://ibruce.info/2015/04/04/busuanzi/ busuanzi_count: # count values only if the other configs are false enable: false # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; site_pv_footer: # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: |————————————————— busuanzi_count 字段格式说明： 当 enable: true 时，代表开启全局开关。若 site_uv、site_pv、page_pv 的值均为 false 时，不蒜子仅作记录而不会在页面上显示。 当 site_uv: true 时，代表在页面底部显示站点的 UV 值。当 site_pv: true 时，代表在页面底部显示站点的 PV 值。当 page_pv: true 时，代表在文章页面的标题下显示该页面的 PV 值（文章阅读数）。 site_uv_header 和 site_uv_footer 这几个为自定义样式配置：当相关的值留空时将不显示。可以使用 font-awesome。 —————————————————| 这里给出一个配置实例： busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; site_pv_footer: # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; page_pv_footer: 相比不蒜子的文章阅读量统计，LeanCloud 更加稳定靠谱，一般会推荐使用 LeanCloud。LeanCloud 文章阅读量统计配置如下： 登陆 LeanCloud官网注册 LeanCloud 账号，然后创建一个新应用（名称为：Hexo）。页面显示如下： leancloud 创建应用之后就可以获取到 APP ID 和 APP Key，之后就可以在配置文件中使用了。 然后点击 hexo 应用中的 存储 菜单项，进入 数据配置界面,新建一个 Counter Class,专门保存我们博客的文章访问量等数据: 添加安全域名 添加安全域名后，只有这些安全域名才有权访问后台的数据了，可以进一步提升安全性。 最后在 主题配置文件 找到 leancloud_visitors 模块下的 APP ID 和 APP Key 关键字，并将 hexo --&gt; 应用 Key 下的 APP ID 和 APP Key填入。如下： # Show number of visitors to each article. # You can visit https://leancloud.cn get AppID and AppKey. leancloud_visitors: enable: true app_id: xxxxxxxxxxxx app_key: xxxxxxxxxxxxxxxxx 1.2.16 开启字数与阅读时长统计安装 hexo-wordcount 插件，Git bash 下运行如下命令： $ npm i --save hexo-wordcount npm WARN babel-eslint@10.0.1 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;}) + hexo-wordcount@6.0.1 added 1 package from 1 contributor and audited 6957 packages in 7.823s found 0 vulnerabilities 打开 主题配置文件 找到 post_wordcount 模块下的 min2read 关键字，将其值设置为 true。如下： # Post wordcount display settings # Dependencies: https://github.com/willin/hexo-wordcount post_wordcount: item_text: true wordcount: true min2read: true totalcount: false separated_meta: true 1.2.17 开启版权声明打开 主题配置文件 找到 post_copyright 字段，设置其 enable 关键字改为 true。如下： # Declare license on posts post_copyright: enable: true license: CC BY-NC-SA 3.0 license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/ 1.2.18 隐藏 由 Hexo 强力驱动 | 主题 — NexT.Gemini v5.1.4 显示打开 &lt;主题目录&gt;/layout/_partials/footer.swig 文件，注释相关代码即可。如下： 1.2.19 添加 jiathis 分享打开 主题配置文件 找到 Share 模块下的 jiathis 字段，作如下设置: # Share # This plugin is more useful in China, make sure you known how to use it. # And you can find the use guide at official webiste: http://www.jiathis.com/. # Warning: JiaThis does not support https. jiathis: true ##uid: Get this uid from http://www.jiathis.com/ #add_this_id: 1.2.20 添加 Local Search（站内搜索）当站内文章数目基础较大时，分类功能已经无法满足我们的查找需求了，此时我们可以添加本地站点内容搜索功能: 1）安装 hexo-generator-searchdb 插件，Git bash 下运行如下命令： $ npm install hexo-generator-searchdb --save npm WARN deprecated ejs@1.0.0: Critical security bugs fixed in 2.5.5 npm WARN babel-eslint@10.0.1 requires a peer of eslint@&gt;= 4.12.1 but none is installed. You must install peer dependencies yourself. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;}) + hexo-generator-searchdb@1.0.8 added 3 packages from 3 contributors and audited 6961 packages in 12.406s found 0 vulnerabilities 2）编辑 站点配置文件，新增以下内容到任意位置（我增加到了 Extensions 模块）。如下： # Extensions search: path: search.xml field: post format: html limit: 10000 3）编辑 主题配置文件 找到 Local search 模块，启用本地搜索功能： # Local search local_search: enable: true 1.2.21 开启 DaoVoice 在线联系DaoVoice 提供博客在线联系的功能（可以在线留言，作者会收到邮件），我们可以借助于此在自己的博客站点上接入此功能。参见：配置教程 1.2.22 开启网页顶部进度加载条打开 主题配置文件 找到 pace 字段，设置其值为 true，选择一款你喜欢的样式即可。如下： # Progress bar in the top during page loading. pace: true # Themes list: #pace-theme-big-counter #pace-theme-bounce #pace-theme-barber-shop #pace-theme-center-atom #pace-theme-center-circle #pace-theme-center-radar #pace-theme-center-simple #pace-theme-corner-indicator #pace-theme-fill-left #pace-theme-flash #pace-theme-loading-bar #pace-theme-mac-osx #pace-theme-minimal # For example # pace_theme: pace-theme-center-simple pace_theme: pace-theme-minimal 1.2.24 设置首页文章显示篇数打开 站点配置文件 找到 Home page setting 模块 index_generator 字段下的 per_page 关键字，设置其值即可。假设这里我们设置首页显示 5（默认：10） 篇文章： 12345678# Home page setting# path: Root path for your blogs index page. (default = &apos;&apos;)# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: &apos;&apos; per_page: 5 order_by: -date]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 预安装环境部署之 Node.js]]></title>
    <url>%2FHexo%2FHexo-%E9%A2%84%E5%AE%89%E8%A3%85%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E4%B9%8B-NodeJs%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 我们知道，Hexo 是一个快速、简洁且高效的博客框架。Hexo 通过 Node.js 所带来的超快生成速度，可利用靓丽的主题在几秒内瞬间完成静态网页渲染。 故而，安装 Hexo 前，我们必须完成 Node.js 应用程序的下载和安装。 1. Node.js了解一定网页基础的同学肯定听说过 JavaScript。Node.js 是一个基于 Chrome JavaScript 运行时建立的一个平台。简单的说， Node.js 就是运行在服务端的 JavaScript。 当然不了解 JavaScript 的同学也不要慌张，我们只需要将 Node.js 理解为：是 Hexo 用来渲染我们博客页面的插件即可。毕竟这里我们更关注的 Node.js 下载和安装，并不关心其原理以及使用。 开始安装之前，这里先给出 Node.js 安装包及源码下载地址。截至目前， Node.js 最新可供下载安装版本为：v10.15.0 (includes npm 6.4.1)。官网下载界面如下，我们可以根据不同平台的下载要求获取相应的安装包： 当然，你也可以从 Node.js 历史版本 获取到较老的历史版本。 下面我们来看不同平台（Windows、Linux 以及 Mac）下 Node.js：v10.15.0 (includes npm 6.4.1) LST（长期支持版本） 的下载以及安装方法： 1.1 Windows 平台下安装 Node.js这里我们提供了两种 Windows 平台下来安装 Node.js 的方式： 1.1.1 使用 MSI Windows 安装包方式首先打开 Node.js 官网下载地址，下载最新版本 Node.js MSI 安装包：node-v10.15.0-x64.msi。 注意：要使用 MSI 安装应用程序 （例如：node-XYZ.msi）, Windows 系统必须支持 Microsoft Installer 2.0。需要看看你的机器是否支持 MSI，Windows XP 和更高版本已经有 MSI，很多老机器也可以安装 MSI。安装时，只要保存安装文件（.msi）到本地计算机，然后运行它即可完成安装（安装时，根据提示 Next 即可）。 下面我们正式开始 node-v10.15.0-x64.msi 的安装过程： 步骤 1 : 双击下载后的安装包 node-v10.15.0-x64.msi 开启安装界面，然后点击 Next 。如下所示： 步骤 2 : 勾选接受协议许可选项，然后点击 Next: 步骤 3 : Node.js 默认安装目录为 C:\Program Files\nodejs\ 。 你可以自定义修改存储目录（这里我存储于：E:\nodejs\），然后点击 Next： 步骤 4 : 点击树形图标来选择你需要的安装模式（默认为：Node.js runtime,这里我们选择：online documentation shortcuts）, 然后点击 Next 进入 Setup 界面。 步骤 5 : 点击 Install（安装） 开始安装 Node.js。 然后点击 Next，等待安装完成点击 Finish 即可： 步骤 6：DOS 安装检测 首先检测系统 PATH 环境变量中是否配置了 Node.js，日志信息输出如下： PATH=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem; C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\; E:\TortoiseSVN\bin;E:\Git\cmd;E:\nodejs\; C:\Users\guo_jie\AppData\Local\Microsoft\WindowsApps; E:\Anaconda3\Scripts;E:\Anaconda3;C:\Users\guo_jie\AppData\Roaming\npm 可以看到 E:\nodejs\ 路径已经被成功添加到系统环境变量 path 中。 此时我们来查看安装好的 Node.js 版本号： C:\Users&gt;node -v v10.15.0 C:\Users&gt; 至此，使用 “MSI Windows 安装包” 的方式安装 Node.js 的过程已经全部完成。 1.1.2 使用 EXE Windows 二进制文件方式首先打开 Node.js 官网下载地址，下载最新版本 Node.js Binary 安装包：node-v10.15.0-win-x64.zip`。 使用 Windows 二进制文件安装 Node.js 比较简单。直接解压安装包，然后双击安装目录下的：node.exe 等待即可。 注意，node.exe 安装过程中会出现如下安装窗口： 安装版本测试： 查看已安装 Node.js 版本号，信息如下表示安装成功： C:\Users&gt;node -v v10.15.0 C:\Users&gt; 1.2 Linux 平台下安装 Node.js 注意：Linux 上安装 Node.js 需要安装 Python 2.6 / 2.7 ，不建议安装 Python 3.0 以上版本。 1.2.1 使用已编译好的包Node.js 官网提供了已经编译好的 linux Node.js 安装包，解压即用： 123456789# 下载相应版本安装包（下载目录为：/usr/software）：$ wget -c https://nodejs.org/dist/v10.15.0/node-v10.15.0-linux-x64.tar.xz＃ 解压：$ tar -zxvf node-v10.15.0-linux-x64.tar.xz# Node.js 安装版本测试：＄ cd node-v10.15.0-linux-x64$ ./bin/node -vv10.15.0 注意，解压文件的 bin 目录底下包含了 node、npm 等命令，为了方便系统调用，我们可以使用 ln 设置命令软链接： 12ln -s /usr/software/node-v10.15.0-linux-x64/bin/npm /usr/local/bin/ ln -s /usr/software/node-v10.15.0-linux-x64/bin/node /usr/local/bin/ 1.2.2 使用源码安装1）Ubuntu 这一小节我们来看如何在 Ubuntu OS 环境下使用源码安装 Node.js： 步骤一：前往 Node.js 官网下载地址 获取其源码（source code）下载链接，然后进行下载： 1$ wget -c https://nodejs.org/dist/v10.15.0/node-v10.15.0.tar.gz 步骤二：开放目录权限： 1$ sudo chmod -R 755 node 步骤三：依次执行如下命令编译源码： 1234$ cd node-v10.15.0$ sudo ./configure$ sudo make$ sudo make install 步骤四：检测是否安装成功： 12$ node --versionv10.15.0 |——————————————————— 补充：Ubuntu 还可以通过 apt-get 命令 直接安装： 12sudo apt-get install nodejssudo apt-get install npm ———————————————————| 2）Centos 这一小节我们来看如何在 Centos OS 环境下使用源码安装 Node.js： 步骤一：前往 Node.js 官网下载地址 获取其源码（source code）下载链接，然后进行下载： 1$ wget -c https://nodejs.org/dist/v10.15.0/node-v10.15.0.tar.gz 步骤二：解压源码文件： 1$ tar -zxvf node-v10.15.0.tar.gz 步骤三：依次执行如下命令编译安装源码： 1234cd node-v10.15.0./configure --prefix=/usr/local/node/10.15.0makemake install 步骤四：设置 Node.js 环境变量，并且完成 source： 123456# 打开全局配置文件增加如下配置：$ vim /etc/profile# Set for node.jsexport NODE_HOME=/usr/local/node/10.15.0export PATH=$PATH:$NODE_HOME/bin 12# 使配置文件生效：$ source /etc/profile 步骤五：验证是否安装配置成功： 1$ node -v 1.3 Mac 平台下安装 Node.js这里我们提供两种方式来在 Mac OS 上安装 Node.js： 1）使用 pkg 安装包 前往 Node.js 官网下载地址 下载最新 Node 版本 .pkg 安装包，进行安装即可。 2）使用包管理器 1$ brew install node]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Node.js</tag>
        <tag>Setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Hexo 轻量级框架的博客搭建]]></title>
    <url>%2FHexo%2F%E5%9F%BA%E4%BA%8E-Hexo-%E8%BD%BB%E9%87%8F%E7%BA%A7%E6%A1%86%E6%9E%B6%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 读前需知： 在开始 Hexo 框架博客搭建之前，首先说明本文所作目的：一方面本文作为基于 Hexo Frame 搭建个人博客过程记录，方便日后回顾使用；另一方面由于在搭建过程中受益于博客上记录过 Hexo 环境搭建的前辈们，故作此以分享给更多的 Hexo 新手小白们。 如文中有表述不恰当的地方，欢迎各位在留言区进行指正，若有转载请注明出处!谢谢~ 1. Hexo 博客搭建Hexo 是一个快速、简洁且高效的博客框架。支持 GitHub Flavored Markdown 解析文章。并且通过 Node.js 所带来的超快生成速度，可利用靓丽的主题在几秒内瞬间完成静态网页渲染。 注意，在开始 Hexo 安装以及搭建之前，我们需要预先准备 Hexo 安装环境，这是必须的，否则会影响到后续的安装、搭建过程。所以下面我们需要先来部署 Hexo 预安装环境： 1.1 预安装环境部署1.1.1 安装 Node.js关于 Node.js 的详细下载以及安装过程说明请参见：Hexo 预安装环境搭建之 Node.js ，这里提供了不同平台下 Node.js 的安装过程。 1.1.2 安装 Git关于 Git 的详细下载以及安装过程说明请参见：Git 使用指南之初识 ，这里不仅提供了不同平台下 Git 的安装过程，还包含了 Git 快速入门指南。 1.1.3 维护一个 Github 远程仓库首先通过 Github 官网 ，注册一个 Github 账号并完成登陆。 在 Github 上新建一个 Repository（远程仓库），显示如下： 注意：Repository 名称最好是 Github 账号名(保证唯一，这里我的 Github 账户名是：TheNightIsYoung)，并且一定要加 .github.io。所以最终我所创建的 Github 仓库名称为：TheNightIsYoung.github.io。 接着，我们还需要为 Github 和本地 Git 配置 SSH Key，具体配置过程可以参见 Git 使用指南之远程仓库 中 “4.1.1 GitHub 配置 SSH Key” 章节内容。 1.1.4 维护一个 Coding 远程仓库首先通过 Coding 官网 ，注册一个 Coding 账号并完成登陆。当然使用过 腾讯云开发者平台 的用户无须使用 Coding 平台远程仓库（腾讯云开发者平台是由腾讯云与 CODING 共同开发的），可以直接使用腾讯云开发者平台。两者在使用上几乎没有差异。 当然你还可以选择其它远程仓库，这里配置另外一个远程仓库的目的主要是为了解决 Github 的访问速度较慢的问题（不要深究，后续会进行说明）。 下面我以 Coding 平台为例，新建一个项目（Repository），显示如下： 和 3）中 Github 一样，我们同样需要为 Coding（或 腾讯云开发者平台）配置 SSH Key，在项目 setting 中点击 SSH 公钥,新建一个 SSH，将之前公钥的内容添加进去。然后测试是否成功连接： $ ssh -T git@git.coding.net Coding 提示: Hello TheMusicIsLoud, You&apos;ve connected to Coding.net via SSH. This is a deploy key. TheMusicIsLoud，你好，你已经通过 SSH 协议认证 Coding.net 服务，这是一个部署公钥 此部署公钥可访问如下代码仓库： （只读）git@e.coding.net:/TheMusicIsLoud.git 预安装环境部署完成之后，下面我们正式开始 Hexo 博客的搭建： 1.2 Hexo 安装以及初始化1.2.1 NPM 安装 Hexo在预安装环境部署中我们已经成功安装 Git，打开 Git Bash（桌面点击鼠标右键）运行以下命令安装 Hexo： $ npm install -g hexo-cli 回车之后，日志信息输出中可能会出现一行 WARN 的警告语句，不要担心，等着即可…。过一段时间如果出现hexo 版本号信息输出的语句就代表安装成功了。如下： npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;}) +hexo-cli: 1.1.0 audited 4700 packages in 5.05s found 0 vulnerabilities 安装完后输入 hexo -v 验证是否安装成功。 $ hexo -v hexo: 3.8.0 hexo-cli: 1.1.0 os: Windows_NT 10.0.17134 win32 x64 http_parser: 2.8.0 node: 10.15.0 v8: 6.8.275.32-node.45 uv: 1.23.2 zlib: 1.2.11 ares: 1.15.0 modules: 64 nghttp2: 1.34.0 napi: 3 openssl: 1.1.0j icu: 62.1 unicode: 11.0 cldr: 33.1 tz: 2018e 1.2.2 初始化 Hexo 博客目录首先在任意一个位置新建一个文件夹用于存放 Hexo 初始化博客文件，这里我选择 F:/HexoBlogProject ： $ cd F: $ mkdir HexoBlogProject 然后在 Git Bash 中依次运行以下命令来初始化 Hexo 项目： $ hexo init &lt;floder&gt; $ cd &lt;floder&gt; $ npm install 实际环境测试如下： $ hexo init HexoBlogProject/ INFO Cloning hexo-starter to F:\HexoBlogProject Cloning into &apos;F:\HexoBlogProject&apos;... remote: Enumerating objects: 68, done. remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 68 Unpacking objects: 100% (68/68), done. Submodule &apos;themes/landscape&apos; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &apos;themes/landscape&apos; Cloning into &apos;F:/HexoBlogProject/themes/landscape&apos;... remote: Enumerating objects: 1, done. remote: Counting objects: 100% (1/1), done. remote: Total 867 (delta 0), reused 0 (delta 0), pack-reused 866 Receiving objects: 100% (867/867), 2.55 MiB | 1.64 MiB/s, done. Resolving deltas: 100% (459/459), done. Submodule path &apos;themes/landscape&apos;: checked out &apos;73a23c51f8487cfcd7c6deec96ccc7543960d350&apos; INFO Install dependencies npm WARN deprecated titlecase@1.1.2: no longer maintained npm WARN deprecated postinstall-build@5.0.3: postinstall-build&apos;s behavior is now built into npm! You should migrate off of postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater. &gt; nunjucks@3.1.6 postinstall F:\HexoBlogProject\node_modules\nunjucks &gt; node postinstall-build.js src npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;}) added 422 packages from 501 contributors and audited 4700 packages in 23.449s found 0 vulnerabilities INFO Start blogging with Hexo! $ cd HexoBlogProject/ $ npm install npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;}) audited 4700 packages in 5.05s found 0 vulnerabilities 看到 INFO Start blogging with Hexo! 是不很激动！！！此时我们来看一下 Hexo 初始化目录 HexoBlogProject 结构，如下： HexoBlogProject . ├── _config.yml ├── package.json ├── scaffolds ├── source | ├── _drafts | └── _posts └── themes 注意：hexo 相关命令均在 HexoBlogProject 目录下，用 Git Bash 运行。 通常我们会将 HexoBlogProject 目录称之为 Hexo 站点目录。 1.2.3 HexoBlogProject 站点目录说明这一小节我们简要介绍一下站点目录结构，这将有助于我们进一步了解 Hexo 博客框架的工作原理。 1）package.json：hexo 框架的参数和所依赖插件： { &quot;name&quot;: &quot;hexo-site&quot;, &quot;version&quot;: &quot;0.0.0&quot;, &quot;private&quot;: true, &quot;hexo&quot;: { &quot;version&quot;: &quot;3.8.0&quot; }, &quot;dependencies&quot;: { &quot;hexo&quot;: &quot;^3.7.0&quot;, &quot;hexo-generator-archive&quot;: &quot;^0.1.5&quot;, &quot;hexo-generator-category&quot;: &quot;^0.1.3&quot;, &quot;hexo-generator-index&quot;: &quot;^0.2.1&quot;, &quot;hexo-generator-tag&quot;: &quot;^0.2.0&quot;, &quot;hexo-renderer-ejs&quot;: &quot;^0.3.1&quot;, &quot;hexo-renderer-stylus&quot;: &quot;^0.3.3&quot;, &quot;hexo-renderer-marked&quot;: &quot;^0.3.2&quot;, &quot;hexo-server&quot;: &quot;^0.3.1&quot; } } 2.scaffolds—脚手架、骨架 当我们想要新建一篇文章的时候，hexo 是根据这个目录下的文件进行构建的，基本不用关心。 3.source—博文目录 source 目录下包含一个 _posts 目录 ：需要新建的博文都放在 _posts 目录下。 _posts 目录下是一个个 MarkDown 文件。默认情况下有一个 hello-world.md 的文件，博文就在这个文件中编辑。 _posts 目录下的 MarkDown 文件，会被编译成 html 文件，放到 public （此文件现在应该没有，因为还没有编译过）文件夹下。 4.themes—主题目录 themes 为博客网站主题目录，hexo 有非常好的主题拓展，支持的主题也很丰富。themes 目录下每一个子目录就是一个主题，我的子目录如下： themes . |-- landscape // 默认主题 当然我们可以下载自己满意的主题到该目录下,hexo主题传送门。 5._config.yml—站点配置文件： 网站的很多信息都在这里配置：诸如网站名称、副标题、描述、作者、语言、主题等等参数。这里我们给出_config.yml 文件中的内容，以及主要参数说明 # Hexo Configuration ## Docs: https://hexo.io/docs/configuration.html ## Source: https://github.com/hexojs/hexo/ # Site title: Hexo # 网站标题 subtitle: # 网站副标题 description: # 网站描述 keywords: author: John Doe # 作者 language: # 语言 timezone: # 网站时区：Hexo 默认使用当前电脑的时区。时区列表，比如说：America/New_York, Japan, 和 UTC 。 # URL ## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos; url: http://yoursite.com # 站点的 Url root: / # 站点的根目录 permalink: :year/:month/:day/:title/ # 文章的“永久链接”格式 permalink_defaults: # 永久链接中各部分的默认值 # Directory source_dir: source # 资源文件夹：用来存放 hexo 内容 public_dir: public # 公共文件夹：这个文件夹用于存放生成的站点文件。 tag_dir: tags # 标签文件夹 archive_dir: archives # 归档文件夹 category_dir: categories # 分类文件夹 code_dir: downloads/code # Include code 文件夹 i18n_dir: :lang # 国际化（i18n）文件夹 skip_render: # 跳过指定文件的渲染：可使用 glob 表达式来匹配路径 # Writing new_post_name: :title.md # File name of new posts # 新文章的文件名称 default_layout: post # 预设布局 titlecase: false # Transform title into titlecase # 把标题转换为 title case external_link: true # Open external links in new tab # 在新标签中打开链接 filename_case: 0 # 把文件名称转换为 ‘1’ 小写或 ‘2’ 大写 render_drafts: false # 是否显示草稿 post_asset_folder: false # 是否启动 Asset 文件夹 relative_link: false # 把链接改为与根目录的相对位址 future: true # 显示未来的文章 highlight: # 内容中代码块的设置 enable: true line_number: true auto_detect: false tab_replace: # Home page setting # path: Root path for your blogs index page. (default = &apos;&apos;) # per_page: Posts displayed per page. (0 = disable pagination) # order_by: Posts order. (Order by date descending by default) index_generator: path: &apos;&apos; per_page: 10 order_by: -date # Category &amp; Tag default_category: uncategorized category_map: # 分类别名 tag_map: # 标签别名 # Date / Time format ## Hexo uses Moment.js to parse and display date ## You can customize the date format as defined in ## http://momentjs.com/docs/#/displaying/format/ date_format: YYYY-MM-DD # 日期格式 time_format: HH:mm:ss # 时间格式 # Pagination ## Set per_page to 0 to disable pagination per_page: 10 # 分页数量 pagination_dir: page # Extensions ## Plugins: https://hexo.io/plugins/ ## Themes: https://hexo.io/themes/ theme: landscape # 主题名称 # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: # 部署部分的设置 type: # 类型：常用的是 Git 关于 HexoBlogProject 站点目录配置以及使用，后续我们遇到具体需求具体说明，这里不再赘述。 1.3 Hexo 本地博客上面我们已经完成了 Hexo 本地博客的安装以及搭建。现在我们来启动 Hexo 本地服务，进行博客预览测试。Git Bash 中运行以下命令： $ hexo server （或 hexo s） INFO Start processing INFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 然后通过浏览器访问 http://localhost:4000/ ，就可以看到 Hexo 的原始博客内容，页面如下所示： 至此，我们已经可以在本地使用 Hexo 博客了。但其实我们更加关注的是如何将 Hexo 博客发布到 Internet 上供其他人分享交流，下面我将会给出一个解决方案——将我们搭建好的本地 Hexo 博客项目推送到公共代码仓库以供访问。 1.4 Hexo 个人博客实施方案实施方案一：Github·Page Github·Page 方法是将本地 Hexo 博客推送至 GithubPages 以实现公共访问： 1）首先，安装 hexo-deployer-git 插件，Git bash 下运行下面命令： $ npm install hexo-deployer-git --save npm WARN deprecated swig@1.4.2: This package is no longer maintained npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted {&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;} (current: {&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;}) + hexo-deployer-git@0.3.1 added 31 packages from 36 contributors and audited 5870 packages in 11.225s found 1 low severity vulnerability run `npm audit fix` to fix them, or `npm audit` for details 2）然后，修改站点目录下 _config.yml（站点配置文件）。修改文件末尾为： # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repo: git@github.com:&lt;Your github account name&gt;/&lt;Your github account name&gt;.github.io.git branch: master 实际环境测试： # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: type: git repo: git@github.com:TheNightIsYoung/TheNightIsYoung.github.io.git branch: master 这里，我的 Github account name（Github 账户名称）为：TheNightIsYoung。 注意：仓库地址只支持 SSH 访问，不要填写 Http 地址。 3）将本地 Hexo 博客推送至 GithubPages，Git Bash 输入以下命令， 返回 INFO Deploy done: git 即表示成功推送： # 生成静态页面： $ hexo generate （或：hexo g） INFO Start processing INFO Files loaded in 272 ms INFO Generated: index.html INFO Generated: archives/index.html INFO Generated: fancybox/blank.gif INFO Generated: fancybox/jquery.fancybox.css INFO Generated: fancybox/fancybox_loading@2x.gif INFO Generated: fancybox/fancybox_loading.gif INFO Generated: archives/2019/index.html INFO Generated: fancybox/fancybox_sprite.png INFO Generated: fancybox/fancybox_sprite@2x.png INFO Generated: fancybox/fancybox_overlay.png INFO Generated: archives/2019/01/index.html INFO Generated: js/script.js INFO Generated: fancybox/helpers/jquery.fancybox-buttons.css INFO Generated: css/fonts/FontAwesome.otf INFO Generated: fancybox/jquery.fancybox.pack.js INFO Generated: fancybox/helpers/jquery.fancybox-buttons.js INFO Generated: css/style.css INFO Generated: css/fonts/fontawesome-webfont.woff INFO Generated: fancybox/helpers/jquery.fancybox-media.js INFO Generated: fancybox/helpers/jquery.fancybox-thumbs.js INFO Generated: fancybox/helpers/fancybox_buttons.png INFO Generated: css/fonts/fontawesome-webfont.eot INFO Generated: fancybox/helpers/jquery.fancybox-thumbs.css INFO Generated: css/fonts/fontawesome-webfont.svg INFO Generated: css/fonts/fontawesome-webfont.ttf INFO Generated: 2019/01/07/hello-world/index.html INFO Generated: fancybox/jquery.fancybox.js INFO Generated: css/images/banner.jpg INFO 28 files generated in 650 ms # 部署至 GithubPages： $ hexo deploy （或：hexo d） INFO Deploying: git INFO Setting up Git deployment... Initialized empty Git repository in F:/HexoBlogProject/.deploy_git/.git/ [master (root-commit) 6d469d7] First commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 placeholder INFO Clearing .deploy_git folder... INFO Copying files from public folder... INFO Copying files from extend dirs... [master c71b7a5] Site updated: 2019-01-08 12:03:08 29 files changed, 5777 insertions(+) create mode 100644 2019/01/07/hello-world/index.html create mode 100644 archives/2019/01/index.html create mode 100644 archives/2019/index.html create mode 100644 archives/index.html create mode 100644 css/fonts/FontAwesome.otf create mode 100644 css/fonts/fontawesome-webfont.eot create mode 100644 css/fonts/fontawesome-webfont.svg create mode 100644 css/fonts/fontawesome-webfont.ttf create mode 100644 css/fonts/fontawesome-webfont.woff create mode 100644 css/images/banner.jpg create mode 100644 css/style.css create mode 100644 fancybox/blank.gif create mode 100644 fancybox/fancybox_loading.gif create mode 100644 fancybox/fancybox_loading@2x.gif create mode 100644 fancybox/fancybox_overlay.png create mode 100644 fancybox/fancybox_sprite.png create mode 100644 fancybox/fancybox_sprite@2x.png create mode 100644 fancybox/helpers/fancybox_buttons.png create mode 100644 fancybox/helpers/jquery.fancybox-buttons.css create mode 100644 fancybox/helpers/jquery.fancybox-buttons.js create mode 100644 fancybox/helpers/jquery.fancybox-media.js create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.css create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.js create mode 100644 fancybox/jquery.fancybox.css create mode 100644 fancybox/jquery.fancybox.js create mode 100644 fancybox/jquery.fancybox.pack.js create mode 100644 index.html create mode 100644 js/script.js delete mode 100644 placeholder Branch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;git@github.com:TheNightIsYoung/TheNightIsYoung.github.io.git&apos;. To github.com:TheNightIsYoung/TheNightIsYoung.github.io.git * [new branch] HEAD -&gt; master INFO Deploy done: git 4) 最后，通过浏览器访问网址： https://&lt;Your github account name&gt;.github.io 就可以看到和本地 hexo 博客相同页面。 至此,Hexo 个人博客已经被成功部署到了 GithubPages, 域名为 https://&lt;Your github account name&gt;.github.io。 实施方案二：Github·Page + Coding·Page 使用过 Github 都知道，Github 有时候在国内访问较慢，而 Coding（或：腾讯云开发者平台）国外访问较快。故除了添加 Github 仓库外，我们可以在第一种实施方案的基础上配置 Coding·Page。 1）hexo-deployer-git 插件已经安装过，这里可以直接跳过； 2）修改站点目录下 _config.yml（站点配置文件）。修改文件末尾为： # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: - type: git repo: git@github.com:&lt;Your github account name&gt;/&lt;Your github account name&gt;.github.io.git branch: master - type: git repo: git@git.dev.tencent.com:&lt;Your coding account name&gt;/&lt;Your coding account name&gt;.git branch: master 实际环境测试： # Deployment ## Docs: https://hexo.io/docs/deployment.html deploy: - type: git repo: git@github.com:TheNightIsYoung/TheNightIsYoung.github.io.git branch: master - type: git repo: git@git.dev.tencent.com:TheMusicIsLoud/TheMusicIsLoud.git branch: master 这里，我的 coding account name（Coding 账户名称）为：TheMusicIsLoud。 3）将本地 Hexo 博客推送至 CodingPages，Git Bash 输入以下命令， 返回 INFO Deploy done: git 即表示成功推送： $ hexo g $ hexo d INFO Deploying: git INFO Setting up Git deployment... Initialized empty Git repository in F:/HexoBlogProject/.deploy_git/.git/ [master (root-commit) c9fcced] First commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 placeholder INFO Clearing .deploy_git folder... INFO Copying files from public folder... INFO Copying files from extend dirs... [master 498e47c] Site updated: 2019-01-08 13:55:57 29 files changed, 5777 insertions(+) create mode 100644 2019/01/07/hello-world/index.html create mode 100644 archives/2019/01/index.html create mode 100644 archives/2019/index.html create mode 100644 archives/index.html create mode 100644 css/fonts/FontAwesome.otf create mode 100644 css/fonts/fontawesome-webfont.eot create mode 100644 css/fonts/fontawesome-webfont.svg create mode 100644 css/fonts/fontawesome-webfont.ttf create mode 100644 css/fonts/fontawesome-webfont.woff create mode 100644 css/images/banner.jpg create mode 100644 css/style.css create mode 100644 fancybox/blank.gif create mode 100644 fancybox/fancybox_loading.gif create mode 100644 fancybox/fancybox_loading@2x.gif create mode 100644 fancybox/fancybox_overlay.png create mode 100644 fancybox/fancybox_sprite.png create mode 100644 fancybox/fancybox_sprite@2x.png create mode 100644 fancybox/helpers/fancybox_buttons.png create mode 100644 fancybox/helpers/jquery.fancybox-buttons.css create mode 100644 fancybox/helpers/jquery.fancybox-buttons.js create mode 100644 fancybox/helpers/jquery.fancybox-media.js create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.css create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.js create mode 100644 fancybox/jquery.fancybox.css create mode 100644 fancybox/jquery.fancybox.js create mode 100644 fancybox/jquery.fancybox.pack.js create mode 100644 index.html create mode 100644 js/script.js delete mode 100644 placeholder Branch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;git@github.com:TheNightIsYoung/TheNightIsYoung.github.io.git&apos;. To github.com:TheNightIsYoung/TheNightIsYoung.github.io.git + c425509...498e47c HEAD -&gt; master (forced update) INFO Deploy done: git INFO Deploying: git INFO Clearing .deploy_git folder... INFO Copying files from public folder... INFO Copying files from extend dirs... On branch master nothing to commit, working tree clean Branch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;git@git.dev.tencent.com:TheMusicIsLoud/TheMusicIsLoud.git&apos;. To git.dev.tencent.com:TheMusicIsLoud/TheMusicIsLoud.git * [new branch] HEAD -&gt; master INFO Deploy done: git ４）Coding 或 腾讯云开发者平台 还需要通过 Pages 服务 ，开启静态 Pages 应用。如下页面显示： 开启成功后界面如下： 5）最后，通过浏览器访问网址： https://&lt;Your coding account name&gt;.coding.me/ 同样也可以访问到 hexo 博客页面。 实施方案三：Github·Page + Coding·Page + domain name 上面我们已经将 Hexo 个人博客托管到 Gihub、Coding（或：腾讯云开发者平台）上了。 一方面每一次进行访问时，我们都需要通过 https://&lt;Your coding account name&gt;.coding.me 或者 https://&lt;Your github account name&gt;.github.io 一长串的域名来进行访问，显得非常繁琐；另一方面我们有一个闲置的域名，单纯想做域名映射，以达到通过域名即可访问我们的个人博客。 前提：有一个闲置的自定义域名 1）配置 Github·Page 的域名映射 1.域名解析（使用域名的前提）参数： 添加一条 CNAME 记录指向 &lt;Your github account name&gt;.github.io： # 记录类型：选择为 CNAME # 主机记录（即域名前缀）：选择为 www # 解析线路：默认选项即可 # 记录值：&lt;Your github account name&gt;.github.io # TTL：默认选项即可 注意：要确认域名解析状态为正常状态！ 2.博客 Github 仓库设置： 首先打开博客仓库设置：https://github.com/&lt;Your github account name&gt;/&lt;Your github account name&gt;.github.io/settings 找到 Custom domain，填写好自定义域名（例如：www.xxxxxx.com），点击 save。 3.站点目录 source 下，添加文件 CNAME.txt，写入自定义域名保存，并重命名为 CNAME 即可。 4.等待一段时间（默认 10 分钟）后，我们发现已经可以通过我们的域名访问到个人博客了。 2）配置 Coding·Page 的域名映射 在上述 配置 Github·Page 的域名映射 基础上进行如下设置即可将域名指向 &lt;Your coding account name&gt;.coding.me： 1.域名解析（使用域名的前提）参数： 添加一条 CNAME 记录指向 &lt;Your coding account name&gt;.coding.me： # 记录类型：选择为 CNAME # 主机记录（即域名前缀）：选择为 www # 解析线路：默认选项即可 # 记录值：&lt;Your github account name&gt;.github.io # TTL：默认选项即可 注意：要确认域名解析状态为正常状态！ 2.打开 Pages 服务 设置页:https://dev.tencent.com/u/&lt;Your coding account name&gt;/p/&lt;Your coding account name&gt;/git/pages/settings, 进行域名绑定即可。 至此，Hexo 个人博客已经解析到自定义域名。当然不要担心 https://&lt;Your coding account name&gt;.coding.me 或者 https://&lt;Your github account name&gt;.github.io 依然可用。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
        <tag>GithubPage&amp;&amp;CodingPage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之工厂模式]]></title>
    <url>%2FDesign-Pattern%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： 如果想成为一名更加优秀的软件设计师，了解优秀软件设计的演变过程比学习优秀软件设计本身更有价值，因为设计的演变过程中蕴藏着大智慧。—《重构与模式》 这里，我们首先给出工厂模式定义及其分类： 工厂模式：通过专门定义的工厂类来决定具体实例化哪个产品对象（对应多个产品类）。也就是说 产品对象的创建统一交由工厂去生产。简言之，工厂设计模式是用来实例化对象的。 工厂设计模式可以分为 3 类，分别是： 1 &gt;：简单工厂模式；2 &gt;：工厂方法模式；3 &gt;：抽象工厂模式。 下面我们分别来详细解读上述的各个模式： 1 &gt; 简单工厂模式1.1 初识简单工厂模式简单工厂模式是工厂模式家族中最简单常见，但最实用的模式。 简单工厂模式，又叫做静态工厂方法（Static Factory Method）模式，是通过专门定义的工厂类来负责创建产品类（其他类）实例的方法。也就是说，由一个工厂对象决定创建出哪一种产品类的实例。并且注意，被创建的实例通常都具有共同的父类。 1.2 场景模拟我们来模拟一个简单工厂模式的使用场景：使用 Java 面向对象语言实现一个计算机控制台程序，要求输入两个操作数以及运算符，得到计算结果。 可能你会觉得这有什么难的？So easy！于是你开始： import java.util.Scanner; public class Computer { public static void main(String[] args) { try { Scanner in = new Scanner(System.in); System.out.println(&quot;请输入操作数 A ：&quot;); float numberA = in.nextFloat(); System.out.println(&quot;请输入运算符号：&quot;); String strOperate = in.next(); System.out.println(&quot;请输入操作数 B ：&quot;); float numberB = in.nextFloat(); float operResult = 0; switch (strOperate) { case &quot;+&quot;: operResult = numberA + numberB; break; case &quot;-&quot;: operResult = numberA - numberB; break; case &quot;*&quot;: operResult = numberA * numberB; break; case &quot;/&quot;: if (numberB != 0) { operResult = numberA / numberB; }else{ System.out.println(&quot;Value Error : Divisor is zero ! Please check again !&quot;); } break; } System.out.println(&quot;The operation result is : &quot; + operResult); } catch (Exception e) { // TODO: handle exception System.out.println(&quot;Operation Error !!!&quot;); } } } 写完后，你可能还会觉得：Perfect！我已经实现了场景需求，并且在编程过程我还注意了编码规范问题。但是你真的瞒住了场景需求了吗？ 其实不然，上述编码其实犯了所有编程初学者都会犯的一个问题，仍然是以面向过程的思维来解决问题，本身代码没有错，但却没有表达出场景描述中面向对象的特性。导致，我们只满足了当前场景功能需求，后续代码不易维护、扩展，灵活性差，更不易复用。 实现“精彩”的代码，我们需要 面向对象的编程，以实现代码的易维护、易扩展、易复用，以及良好灵活性： 1）逻辑分离 基于之前你写的代码，假设如果我们项目内其它板块要使用计算功能时，你会怎么做？通过 Ctrl + C or Ctrl V？ 编程中有一个原则就是尽可能想办法避免代码重复。其实，我们很容易可以想到如果可以将计算模块 封装 出来（实现业务逻辑和界面逻辑分离），不就可以自由复用定义好的计算功能了么。 我们先来封装 Operation 运算类（业务逻辑）： public class Operation { public static float getOperResult(float numberA, float numberB, String strOperate) { float operResult = 0; switch (strOperate) { case &quot;+&quot;: operResult = numberA + numberB; break; case &quot;-&quot;: operResult = numberA - numberB; break; case &quot;*&quot;: operResult = numberA * numberB; break; case &quot;/&quot;: if (numberB != 0) { operResult = numberA / numberB; }else{ System.out.println(&quot;Value Error : Divisor is zero ! Please check again !&quot;); } break; } return operResult; } } 此时界面逻辑如下（客户端）： import java.util.Scanner; public class Computer { public static void main(String[] args) { try { Scanner in = new Scanner(System.in); System.out.println(&quot;请输入操作数 A ：&quot;); float numberA = in.nextFloat(); System.out.println(&quot;请输入运算符号：&quot;); String strOperate = in.next(); System.out.println(&quot;请输入操作数 B：&quot;); float numberB = in.nextFloat(); float operResult = 0; operResult = Operation.getOperResult(numberA, numberB, strOperate); System.out.println(&quot;The operation result is : &quot; + operResult); } catch (Exception e) { // TODO: handle exception System.out.println(&quot;Operation Error !!!&quot;); } } } 业务逻辑和界面逻辑分离后，不管在哪里使用，我们可以很容易复用之前我们写过的 Operation 运算类。 2）降低耦合 上面我们通过面向对象的 封装 特性，提高了代码的复用性。那么现在再来考虑一个问题：上述项目代码中 Operation 运算类是否可以灵活修改以及扩展？ 假设我们现在想要在其中 添加取整运算，或我们想要 修改除法运算为求平方根，我们需要拿到这一部分完整的代码（其它功能代码也暴露）才可以进行更改。并且开发完成后减法、乘法、加法等又参与一次编译，我们很容易在修改过程中在其它功能代码中引入错误，这是由于各运算功能代码耦合性太紧。所以如何降低各个功能模块耦合性在实际项目开发是非常必要且重要的！ 通过面向对象的继承和多态，我们很容易想到：将不同运算操作分别封装成一个个具有相同抽象父类（也可以是接口）的运算类，它们彼此不可见。同时，如果想要增加其它运算功能类时，只需要继承其抽象父类即可实现。如下： |————————————————- 首先，定义一个 Operation 运算基类（抽象父类），其它运算相关操作类继承至父类 Operation： public abstract class Operation { public abstract float getOperResult(float numberA, float numberB) ; } |— 加减乘除运算类定义 —| 加法类： public class AddOperation extends Operation { @Override public float getOperResult(float numberA, float numberB) { // TODO Auto-generated method stub return numberA + numberB; } } 减法类： public class SubOperation extends Operation{ @Override public float getOperResult(float numberA, float numberB) { // TODO Auto-generated method stub return numberA - numberB; } } 乘法类： public class MulOperation extends Operation { @Override public float getOperResult(float numberA, float numberB) { // TODO Auto-generated method stub return numberA * numberB; } } 除法类： public class DivOperation extends Operation { @Override public float getOperResult(float numberA, float numberB) { // TODO Auto-generated method stub if (numberB == 0) { System.out.println(&quot;Value Error : Divisor is zero ! Please check again !&quot;); } return numberA / numberB; } } ————————————————-| 通过封装、继承以及多态我们已经成功对 Operation 运算类完成解耦。此时，如果我们想要修改加法功能运算，只需要被提供加法类代码即可；如果想要增加一个运算功能只需要增加一个继承自 Operation 运算父类的功能类即可。 1.3 简单工厂模式应用上面我们已经完成了计算器控制台程序项目代码复用、维护以及扩展的思考。此时我们面临的问题是：客户端程序（界面逻辑）如何根据输入的运算符去实例化不同的运算类？简单工厂设计模式 可以帮助我们解决这个应用场景。 根据简单工厂设计模式说明，这里，需要实例化的运算类（加减乘除类）对应简单工厂模式中的产品类，简单工厂模式通过专门定义一个工厂类来创建运算类（产品类）目标实例。并且正如我们前面提过的简单工厂设计模式产品类（运算类）有一个公共的抽象父类—Operation 基类。 工厂类实现： public class OperationFactory { public static Operation createOperate(String strOperate) { Operation oper = null; switch (strOperate) { case &quot;+&quot;: oper = new AddOperation(); break; case &quot;-&quot;: oper = new SubOperation(); break; case &quot;*&quot;: oper = new MulOperation(); break; case &quot;/&quot;: oper = new DivOperation(); break; } return oper; } } 最后，客户端（界面逻辑）别可以根据输入的运算符去实例化不同的运算类： public class Computer { public static void main(String[] args) { try { Scanner in = new Scanner(System.in); System.out.println(&quot;请输入操作数 A ：&quot;); float numberA = in.nextFloat(); System.out.println(&quot;请输入运算符号：&quot;); String strOperate = in.next(); System.out.println(&quot;请输入操作数 B：&quot;); float numberB = in.nextFloat(); float operResult = 0; operResult = operate(numberA, numberB,strOperate); System.out.println(&quot;The operation result is : &quot; + operResult); } catch (Exception e) { // TODO: handle exception System.out.println(&quot;Operation Error !!!&quot;); } } private static float operate(float numberA, float numberB, String strOperate){ Operation oper = OperationFactory.createOperate(strOperate); return oper.getOperResult(numberA, numberB); } } 1.4 简单工厂模式 UML 类图以简单工厂设计模式在上述场景应用为例，我们给出其 UML 类图： 1.5 简单工厂模式优缺点1）优点： 对于客户端：简单工厂将对象的创建过程进行了封装，用户不需要知道具体的创建过程，只需要根据选择条件动态调用工厂类获取相应产品对象即可。 对于软件体系结构：简单工厂模式能够根据输入信息，决定究竟应该创建哪个具体（产品）类的对象（包含了必要的逻辑判断）。明确区分了各自的权力和义务，优化了软件体系结构。 降低代码耦合，遵循最少知识原则。 2）缺点： 简单工厂模式是通过 switch-case 来进行产品对象的创建，违背了开放-关闭原则（对维护和扩展同时开放）。因为如果需要增加产品类的话需要在工厂类中增加 case 语句。有些情况下我们可以通过 反射调用 来弥补这种不足。 2 &gt; 工厂方法模式2.1 初识工厂方法模式工厂方法模式可以看作是简单工厂模式的衍生，解决了许多简单工厂模式中存在的问题。 工厂方法模式（Factory Method），又称多态性工厂模式。是定义一个用于创建产品对象的工厂接口，让其子类（产品工厂类）决定具体实例化哪一个类。工厂方法模式使得一个产品类的实例化延迟到了子类。换句话说：在工厂方法模式中，核心的工厂类（对应工厂接口）不再负责所有的产品的创建，而是将具体创建的工作交给工厂子类去做。 工厂方法模式将核心工厂类定义为一个抽象工厂角色，仅负责提供具体工厂子类必须实现的接口方法，而不接触哪一个产品类应当被实例化这种细节，具体实例化哪个产品类的细节由工厂子类去做。 注意：一个工厂子类只用于生成它相对应的产品，也就是说每一个产品都对应一个工厂子类。 2.2 场景模拟以及工厂方法模式应用还是以简单工厂模式中计算机控制台程序实现计算功能为说明样例，这里我们会给出使用工厂方法模式来实例化不同的产品类（加减乘除运算类）的具体方法。 首先我们给出计算功能的工厂方法模式实现的 UML 类图： 产品类实例化实现过程如下： |————————————————- 将核心工厂类抽象为抽象工厂接口： public interface IFactory { public Operation generateOper(); } 加减乘除各对应一个工厂子类去实现核心工厂接口： 1）加法工厂子类： public class AddOperationFactory implements IFactory{ @Override public Operation generateOper() { // TODO Auto-generated method stub return new AddOperation(); } } 2）减法工厂子类： public class SubOperationFactory implements IFactory{ @Override public Operation generateOper() { // TODO Auto-generated method stub return new SubOperation(); } } 3）乘法工厂子类： public class MulOperationFactory implements IFactory{ @Override public Operation generateOper() { // TODO Auto-generated method stub return new MulOperation(); } } 4）除法工厂子类： public class DivOperationFactory implements IFactory{ @Override public Operation generateOper() { // TODO Auto-generated method stub return new DivOperation(); } } |————————————————- 最后我们来看客户端如何进行调用（以实例化加法类为例）： public class Computer { public static void main(String[] args) { try { Scanner in = new Scanner(System.in); System.out.println(&quot;请输入操作数 A ：&quot;); float numberA = in.nextFloat(); System.out.println(&quot;请输入操作数 B：&quot;); float numberB = in.nextFloat(); float operResult = 0; // 实例化加法类： IFactory operFactory = new AddOperationFactory(); Operation oper = operFactory.generateOper(); operResult = oper.getOperResult(numberA, numberB); System.out.println(&quot;The operation result is : &quot; + operResult); } catch (Exception e) { // TODO: handle exception System.out.println(&quot;Operation Error !!!&quot;); } } } 2.3 简单工厂 VS. 工厂方法我们知道，简单工厂模式在工厂类中包含了必要的逻辑判断，可以根据客户端的选择条件动态的选择实例化哪个产品类。对于客户端来说，这去除了与具体产品类的依赖。但带来的一个问题就是：工厂类和分支耦合。 而工厂方法模式将产品类的实例化从核心工厂类推迟到了其工厂子类，并且一个工厂子类只用于生成它相对应的产品对象，降低了核心工厂类与分支的耦合度。 但我们发现：使用工厂方法模式时，仍需要界面逻辑（客户端）决定实例化哪一个工厂子类，选择判断问题仍然是存在。相较于简单工厂模式，工厂方法将简单工厂的内部逻辑判断转移到了客户端代码来进行。 存在一个疑惑：当我们想增加新运算功能时，简单工厂模式需要增加运算类以及修改工厂类（修改原有的类，违反了“开放-关闭”原则），而工厂方法模式下我们除了增加运算类、产品工厂类，还需要去修改客户端。看起来这不但没有减化难度，反而增加了很多类和方法，这不是变得更加复杂了？ |————————————————- 工厂方法适用场景： 通过简单工厂模式和工厂方法模式的对比，我们发现工厂方法模式的使用好像使得实现更加复杂了。 其实，在 【某些场景下】 通过工厂方法，只需要修改一行实例化的代码就可以极其方便地实现软件系统元素的切换（例如：切换数据源等，后续我们会给出一个场景使用实例来进行说明）。 ————————————————-| 2.4 工厂方法模式优缺点1）优点 工厂方法解决了简单工厂违背“开放-封闭”原则的缺点（降低了简单工厂中工厂类和分支的耦合），又保持了产品对象创建的封装。 不需要修改原来的工厂类就可以增加新功能。 2）缺点 每新增一种产品，类会不断增加，而且逻辑基本一样，在一定程度上，代码冗余（感觉可以接受，毕竟我们不能要求十全十美）。 并且对于工厂方法模式中 “仍然存在着逻辑判断存在，无法避免修改客户端” 的问题，是可以通过 “反射” 来解决的（分支判断问题）。 注意：后续我们还好说明如何优化进工厂方法模式，这里明确抽象工厂模式的优缺点即可。 3 &gt; 抽象工厂模式3.1 初识抽象工厂模式相较于简单工厂以及工厂方法设计模式，抽象工厂模式是所有形态的工厂模式中最为抽象（最具一般性）的一种形态。 这一部分让我们通过模拟一个抽象工厂设计模式适用场景一起来认识抽象工厂设计模式： 3.1.1 场景模拟假设我们现在已经开发了一个线上平台，使用 MySQL 作为数据库，并且线上平台测试通过。正美滋滋的想着晚上（周五）整点什么活动，此时我们突然接到来自另一家公司一个类似的项目：紧急！！！唯一不同的是，需要将项目使用的数据库由 MySQL 变更为 Access。 MMP，尽管内心早就奔腾过无数…咳，本着可以提升个人能（xin）力（zi），干吧！然后就是痛苦的开始… Access 和 MySQL 使用上有不少的差异，替换数据库开始后，出现了各种问题（头皮发麻）…。终于改完了，一看时间已经晚上 10 点了,美好的周五泡汤了… 事后想想，之后线上平台的维护可就麻烦了，同时需要维护两个项目，工作量很大啊。如果哪一天需要用到其它数据库（例如：Oracle），改动的地方会更大，想想就头大，这个问题得解决啊！！！ 1）最基本的数据访问程序 这里先给出原先的（MySQL）数据访问方式，以访问用户表 User 为例： 先定义一个 User 用户类，包含两个字段 ID 和 Name： public class User { private String name; public int id; public void setName(String value) { name = value; } public String getName() { return name; } public void setId(int value) { id = value; } public int getId() { return id; } } 然后定义一个操作 User 表的 mysqlUser 类，只包含 “新增用户” 以及 “获取用户” 的方法，其它 mysql 具体相关语句屏蔽掉不作涉及： public class MysqlUser { public void insert(User user) { System.out.println(&quot; Insert into mysql db. &quot;); } public User getUser(int id) { System.out.println(&quot; Get user by id mysql db. &quot;); return null; } } 客户端逻辑调用： public class Client { public static void main(String args[]) { User user = new User(); mysqlUser clients = new mysqlUser(); clients.insert(user); clients.getUser(0); } } 2）使用工厂方法设计模式的数据访问程序 我们发现，上述实现之所以无法更换数据库在于数据库对象 clients 被高耦合到了 MySQL 上，我们自然而然地能够想到可以通过工厂方法模式来决定具体数据源： 我们先来看工厂方法模式实现上述数据访问的 UML 类图： 编程实现如下： 定义 IUser 抽象产品接口，解除和具体数据库访问的耦合： public interface IUser { void insert(User user); IUser getUser(int id); } 定义 MysqlUser 类，用户访问 MySQL 数据库的 User： public class MysqlUser implements IUser{ public void insert(User user) { System.out.println(&quot; Insert into MySql db. &quot;); } public User getUser(int id) { System.out.println(&quot; Get user by id MySql db. &quot;); return null; } } 定义 AccessUser 类，用户访问 Access 的 User： public class AccessUser implements IUser{ public void insert(User user) { System.out.println(&quot; Insert into Access db. &quot;); } public User getUser(int id) { System.out.println(&quot; Get user by id Access db. &quot;); return null; } } 接下来我们来看工厂类定义： 定义 IFactory 抽象工厂接口，用于创建 User 表访问对象： public interface IFactory { public IUser createUser(); } 定义 MysqlFactory 工厂子类，用于实例化产品类： public class MysqlFactory implements IFactory{ @Override public IUser createUser() { // TODO Auto-generated method stub return new MysqlUser(); } } 定义 AccessFactory 工厂子类，用于实例化产品类： public class AccessFactory implements IFactory{ @Override public IUser createUser() { // TODO Auto-generated method stub return new AccessUser(); } } 最后，我们来看客户端调用： public class Client { public static void main(String args[]) { User user = new User(); IFactory factory = new MysqlFactory(); IUser iUser = factory.createUser(); iUser.insert(user); iUser.getUser(0); } } 可以发现，数据库访问对象 iUser 事先是无法知道正在访问哪个数据库的，这是由工厂类决定的。 3）使用抽象工厂设计模式的数据访问程序 项目设计更改为工厂方法设计模式后仍存在问题：实际使用时，数据库不可能只有一个 User 表，很可能存在其它表，例如 Department 表（部门表）。如何做？ 先给出 Department 类定义： public class Department { private String deparName; public int id; public void setdeparName(String value) { deparName = value; } public String getdeparName() { return deparName; } public void setId(int value) { id = value; } public int getId() { return id; } } 在增加 Department 表数据访问之前，我们还是先给出其 UML 类图： 接下来，定义 IDepartment 抽象产品接口，解除和具体数据库访问的耦合： public interface IDepartment { void insert(Department department); Department getDepartment(int id); } 定义 MysqlDepartment 类，用户访问 MySQL 数据库的 Department： public class MysqlDepartment implements IDepartment{ public void insert(Department department) { System.out.println(&quot; Insert into MySql db. &quot;); } public Department getDepartment(int id) { System.out.println(&quot; Get department by id from MySql db. &quot;); return null; } } 定义 AccessDepartment 类，用户访问 Access 的 Department： public class AccessDepartment implements IDepartment{ public void insert(Department department) { System.out.println(&quot; Insert into Access db. &quot;); } public Department getDepartment(int id) { System.out.println(&quot; Get department by id from Access db. &quot;); return null; } } 接下来我们来看工厂类变化： IFactory 抽象工厂接口：用于创建 User 或 Department 表访问对象： public interface IFactory { public IUser createUser(); public IDepartment createDepartment(); } MysqlFactory 工厂子类：用于实例化产品类： public class MysqlFactory implements IFactory{ @Override public IUser createUser() { // TODO Auto-generated method stub return new MysqlUser(); } @Override public IDepartment createDepartment() { // TODO Auto-generated method stub return new MysqlDepartment(); } } AccessFactory 工厂子类：用于实例化产品类： public class AccessFactory implements IFactory{ @Override public IUser createUser() { // TODO Auto-generated method stub return new AccessUser(); } @Override public IDepartment createDepartment() { // TODO Auto-generated method stub return new AccessDepartment(); } } 此时我们来看客户端调用： public class Client { public static void main(String args[]) { User user = new User(); Department department = new Department(); // IFactory factory = new MysqlFactory(); IFactory factory = new AccessFactory(); // 解除了与具体数据访问的耦合： IUser iUser = factory.createUser(); iUser.insert(user); iUser.getUser(0); // 解除了与具体数据访问的耦合： IDepartment iDepartment = factory.createDepartment(); iDepartment.insert(department); iDepartment.getDepartment(0); } } 此时我们可以发现，通过 IFactory factory = new AccessFactory(); 我们已经成功完成数据源的切换。 其实，在上面不知不觉的过程中，我们已经完成了抽象工厂模式的使用，我们可以发现使用抽象工厂模式可以解决涉及多个产品系列（产品族）的问题。 下面我们来看抽象工厂模式严格说明。 3.1.2 抽象工厂设计模式抽象工厂模式（Abstract Factory）严格定义是：提供一个创建一系列相关或互相依赖对象的接口，而无需指定它们具体的类。可以为 不同产品族 的对象创建提供接口，系统可以在不同的产品族进行切换。 不好理解？不要紧。这里我们来看一个经典的例子：生产汽车。 由于生产汽车的品牌有很多（比如奔驰、宝马、大众等等），因此将会有很多个工厂（工厂子类）。此时我们需要将这些工厂抽象出来成一个工厂接口（核心工厂接口），这个工厂接口不用去关心这些工厂具体怎么生产产品以及这些产品在不同的工厂之间的差异，它只需要定义出来生产车的哪些部件就好了。比如 CreateWheel（车轮）、CreateFrame（车架）、CreateEngine（发动机）等。 不同的工厂（工厂子类）由于各自工艺不同，它在具体制造这些产品的时候必然要重写这些方法，且具体实现方式在具体工厂间的差异可能非常大。 假设现在有 M 家工厂，N 种部件。按照抽象工厂模式去设计的话，最终将会有 1 个抽象工厂接口（客户端用来初始化并决定具体用哪家工厂去生产产品），N 个抽象部件接口（抽象产品接口，取决于具体工厂子类），M 个具体工厂子类（每个工厂有 N 个方法，用来初始化 N 个具体部件生产类），M*N 个具体部件类（这些类是真正生产产品的类）。是不是很形象？ 还不好理解？我们来对应一下之前的数据访问样例。IFactory 是一个抽象工厂接口（决定具体由哪个工厂子类去生产具体产品）。而 MysqlFactory、AccessFactory 是由 IFactory 所管理的工厂子类，用于决定生产那种具体的产品。User or Department？ 是不是很牛X？ 3.2 抽象工厂模式优缺点1）优点： 抽象工厂模式最大的好处就是如果要切换工厂，只需要在最开始初始化工厂类的时候换另一家工厂就好了，而整个结构都无需改动。我们的设计不能防止需求的变更，最理想的就是让这种改动变得最小，现在如果我们想更改数据库访问，只需要更改到具体的工厂就可以做到了。 2）缺点： 抽象工厂模式下扩展困难：如果我们要增加功能，以数据访问为例，假设我们要增加一个项目表 Project，我们需要作哪些改动？ 我们需要至少增加 IProject、MysqlProject、AccessProject 类（必须的）。并且还需要修改 IFactory、MysqlFactory、AccessFactory 才可以实现，是不相当糟糕？ 抽象工厂模式下维护困难：项目对数据访问的客户端不仅一个，故需要在很多地方都使用 IUser、IDepartment，而每一次使用之前都需要类似 IFactory factory = new MysqlFactory 初始化一次。 如果要更换数据访问时，需要多次更改初始化操作，这是很繁琐的！ 注意：后续我们还好说明如何优化进抽象工厂模式，这里明确抽象工厂模式的优缺点即可。]]></content>
      <categories>
        <category>Design Pattern</category>
      </categories>
      <tags>
        <tag>Design Pattern</tag>
        <tag>Factory Pattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式之初识篇]]></title>
    <url>%2FDesign-Pattern%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%88%9D%E8%AF%86%E7%AF%87%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： 如果想成为一名更加优秀的软件设计师，了解优秀软件设计的演变过程比学习优秀软件设计本身更有价值，因为设计的演变过程中蕴藏着大智慧。—《重构与模式》 开篇：千里之行始于足下 作为一个刚刚入（tiao）行（keng）的编程菜鸟，我相信很多人都有过和我一样感受：拜读公司大佬们或者行业大牛写的项目代码，总是会“情”不自禁就来一句：“WC，写得太 TM 精彩了，相当 Artistic”。然后看着自己认（抓）认（耳）真（挠）真（腮）写的程序实现，哎，生活终于还是对我这个“刚学会说话的孩子”动手了（脸红）。 其实，对于程序开发人员而言，一个精彩的代码实现是如何想出来的，要远比看到精彩的代码来的更有期待。 而程序设计模式（Design Pattern）就是一套被多数程序员知晓的、反复使用的、并且经过分类整理的用于解决某些特定场景问题的代码设计经验总结。在不同的面向对象编程开发场景中，通过使用不同的设计模式，可以设计出 易维护、易复用、易扩展、灵活性好 的工程代码。 举个可能不恰当的例子，设计模式于面向对象编程，就像数学于算法，重要性不言而喻。 学前需知 设计模式学习开始之前，还需要说明本文所作目的。本文仅作为设计模式学习记录，方便日后回顾使用。所以要向博客上写过设计模式的前辈们致敬，学习过程中还拜读了程杰老师所著的《大话设计模式》（设计模式入门书籍），获益匪浅，故作此分享。 关于面向对象编程语言，我选择 “Java” 高级程序设计语言来进行样例说明。 常见设计模式]]></content>
      <categories>
        <category>Design Pattern</category>
      </categories>
      <tags>
        <tag>Design Pattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 使用指南之分布式开发]]></title>
    <url>%2FGit%2FPart-5-Of-Git-Tutorial%2F</url>
    <content type="text"><![CDATA[回顾： 前面章节我们已经学习了 Git 的两大杀手锏功能： Git 远程仓库、Git 分支管理。 版权说明：本文思路以及内容主要来自廖雪峰老师的 Git 教程 （强烈推荐膜拜原文）并结合个人学习使用所作，只作为学习记录使用，如内容有侵权请联系删除，禁止转载！]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Distributed Development</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 使用指南之分支管理]]></title>
    <url>%2FGit%2FPart-4-Of-Git-Tutorial%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 回顾： 上一章节我们介绍了 Git 的杀手锏功能 Git 远程仓库。这里我们来看 Git 的另一个重要功能：分支管理。使用过 SVN 等其它版本控制系统的童鞋可能会说：“SVN 也有分支管理啊，但由于创建和切换分支慢的一批，我几乎都不想去用。Git 分支管理功能有性能提升吗？”。 当然有！Git 的分支是与众不同的。无论创建、切换和删除分支，Git 都能在 1 秒钟之内就能完成！无论你的版本库是 1 个文件还是 1 万个文件（跟版本库大小无关，到底是怎么做到的？）。 读前须知 版权说明：本文思路以及内容主要来自廖雪峰老师的 Git 教程 （强烈推荐膜拜原文）并结合个人学习使用所作，只作为学习记录使用，如内容有侵权请联系删除，禁止转载！ 6. 认识分支管理我们先来看这样一个场景：参与一个项目开发任务，需要为在线平台增加一个新功能，半个月过去了，开发任务按照预期完成了 50%，就在此时在线平台突然出现 BUG。 如果立刻提交，由于代码还未完成，不完整的代码库会影响项目内其他人工作；如果等代码全部写完再一次提交，又存在丢失工作进度的巨大风险。 使用 Git 分支管理可以帮助你完美解决上述问题。借用廖雪峰老师的话就是：分支就像是科幻电影里面的平行宇宙，当你正在电脑前努力学习 Git 的时候，另一个你正在另一个平行宇宙里努力学习 SVN。如果两个平行宇宙互不干扰，那对现在的你也没啥影响。并且如果在某个时间点，两个平行宇宙合并了，结果你既学会了 Git 又学会了 SVN！ 如果 Git 可以帮助你同时进行功能任务开发提交以及平台原有功能维护，最后等我们功能任务开发完成后又可以很容易合并到平台上是不是就很 Perfect ！ 有了分支管理，你可以创建一个属于你自己的分支，项目内其他人是无法看到的，还继续在原来的分支（master）上正常工作。而你在自己的分支上干活，想提交就提交，直到开发完毕后，再一次性合并到原来的分支上，既安全，又不影响别人工作。 6.1 如何创建和合并分支在介绍版本回滚时，我们曾提到过：每次提交，Git 都把它们串成一条时间线，这条时间线就是一个分支。默认 Git 只有一条时间线，这个分支叫主分支，即 master 分支。 下面我们来分析 Git 实现分支管理的工作原理： HEAD 严格来说不是直接指向 commit，而是指向 master，master 才是指向提交的。故，HEAD 指向的就是当前分支。 也就是说：一开始的时候，master 分支是一条线，Git 用 master 指向最新的提交，再用 HEAD 指向 master，就能确定当前分支，以及当前分支的提交点。并且，每次提交，master 分支都会向前移动一步，随着你不断提交，master 分支的线也就越来越长： 创建分支时（以 dev 分支为例），Git 将新建一个指针 dev，并将 dev 指针指向 master 相同的提交，然后再将 HEAD 指向 dev。表示当前分支在 dev 上： 这也可以看出，为什么 Git 创建分支会如此之快，工作区的文件都没有任何变化，并且只是增加了一个 dev 指针以及改变了 HEAD 的指向，此后对工作区的修改和提交就是针对 dev 分支了。比如比如新提交一次后，dev 指针往前移动一步，而 master 指针不变： 一段时间后，我们成功在 dev 分支上完成新功能的开发，此时我们想把新功能整合到在线平台，也就是想把 dev 合并到 master 上,Git 如何实现呢？最简单的方法，就是直接把 master 指向 dev 的当前提交，就完成了合并： 可以看出，Git 合并分支也很快！就改改指针，工作区内容也不变！合并完分支后，甚至可以删除 dev 分支。删除 dev 分支就是把 dev 指针给删掉，删掉后，我们就剩下了一条 master 分支： 下面我们还是通过 GitTestProject 版本库来开始 Git 分支管理的学习： 学习之前，我们先根据 Git 分支管理工作原理，先设计给出一个分支管理实例流程图，我们基于此实例进行Git 分支管理学习： 1）GitTestProject 版本库创建分支 dev 分支，然后切换到 dev： 首先我们先来查看以下此时 Git 的版本历史记录： $ git log --pretty=oneline 4f15f792261ac642b2548ba892c6c9a013a93779 (HEAD -&gt; master, origin/master) rm git_rm_test.txt 6516e4897fd1366c4b3bb012f9bc422ff0faa023 Add git_rm_test.txt 9046f26e6d0e78732f5bebffd60589608c73c403 git tracks changes 847e6efec516aab03836442844f88c4c5cf7d442 understand how stage works dd32d2d9afae403ff3af84a03f434bfa1c627cf8 Add test code 8622ab2549f1c113739c6db5a066890de3057fc9 Add help info d9e0247cb21da0faa81fe05dccda8c65aedc0c15 Add Server API c31b82744a3a5d7ad48a6bd72de22fbbb0bf4420 wrote a readme file 开始创建，并切换 dev 分支： $ git checkout -b dev Switched to a new branch &apos;dev&apos; 再来查看一下此时 Git 的版本历史记录： $ git log --pretty=oneline 4f15f792261ac642b2548ba892c6c9a013a93779 (HEAD -&gt; dev, origin/master, master) rm git_rm_test.txt 6516e4897fd1366c4b3bb012f9bc422ff0faa023 Add git_rm_test.txt 9046f26e6d0e78732f5bebffd60589608c73c403 git tracks changes 847e6efec516aab03836442844f88c4c5cf7d442 understand how stage works dd32d2d9afae403ff3af84a03f434bfa1c627cf8 Add test code 8622ab2549f1c113739c6db5a066890de3057fc9 Add help info d9e0247cb21da0faa81fe05dccda8c65aedc0c15 Add Server API c31b82744a3a5d7ad48a6bd72de22fbbb0bf4420 wrote a readme file 发现有什么变化了么（我不会告诉你 HEAD 指向已经更改 ）？git checkout 命令加上 -b 参数表示创建并切换，相当于以下两条命令： $ git branch dev $ git checkout dev Switched to branch &apos;dev&apos; 2）git branch 查看当前分支： $ git branch * dev master 注意：git branch 命令会列出所有分支，当前分支前面会标一个 * 号。 3）分支功能测试： 以 认识分支管理 中给出的样例为场景结合 GitTestProject 版本库我们来测试分支功能。GitTestProject 项目目录如下： 我们在当前目录下创建一个新文件 Automated_op.py,内容随意。然后进行提交： $ git add Automated_op.py $ git commit -m &apos;branch test&apos; [dev 09e14d1] branch test 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 Automated_op.py $ ls Automated_op.py Client/ readme.txt Server/ 提交完成后，我们切换回 master 分支，查看刚才在 dev 分支下进行的修改是否对 master 分支的其他人来说是透明的： $ git checkout master Switched to branch &apos;master&apos; Your branch is up to date with &apos;origin/master&apos;. $ ls Client/ readme.txt Server/ 果然，我们发现刚才在dev 分支下进行的修改已经不可见了！！过程如下: 4）合并分支： 假设现在我们已经完成了在 dev 分支上的功能开发，此时我们想将最终的成果物合并到 master 分支上。可以通过 git merge 命令用于合并指定分支到当前分支（此时我们已经切换回 master 分支）： $ git merge dev Updating 4f15f79..09e14d1 Fast-forward Automated_op.py | 0 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 Automated_op.py $ ls Automated_op.py Client/ readme.txt Server/ 通过查看我们发现，master 分支最新提交已经和 dev 分支完全一样。 日志信息中 Fast-forward 表示当前合并是“快进模式”，也就是直接把 master 指向 dev 的当前提交，合并速度非常快。但请注意：不是每次合并都能 Fast-forward,后面我们还会看到其它合并方式。 5）删除分支： 合并分支（工作进度）后，dev 分支生命周期也就完成了，我们就可以放心删除了： $ git branch -d dev Deleted branch dev (was 09e14d1). 删除 dev 分支后，重新查看版本库分支，会发现只剩下 master 分支: $ git branch * master |———————————————————— 友情提示： 我们知道 Git 创建、合并和删除分支都非常快。所以，Git 鼓励用户使用分支完成某个任务，合并后再删掉分支，这和直接在 master 分支上工作效果是一样的，但过程更安全。 ————————————————————| 6.2 解决冲突上面我们提到分支合并，其实，合并分支过程中大多都会产生版本库冲突，你需要解决冲突才可以继续进行合并。下面我们来看一个真实场景冲突是如何产生的，以及如何解决冲突： 首先准备一个新的分支 feature1 ,继续我们的新分支开发： $ git checkout -b feature1 Switched to a new branch &apos;feature1&apos; 向 Automated_op.py 脚本末尾追加内容如下： Creating a new branch is quick AND simple. 将当前修改在当前分支上进行提交： $ git add Automated_op.py $ git commit -m &apos;conflicts test&apos; [dev1 c4a3a95] conflicts test 1 file changed, 1 insertion(+) 切换回 master 分支： $ git checkout master Switched to branch &apos;master&apos; Your branch is ahead of &apos;origin/master&apos; by 1 commit. (use &quot;git push&quot; to publish your local commits) Git 提示当前 master 分支比远程的 feature1 分支要超前 1 个提交。 在 master 分支上把 Automated_op.py 文件的末尾追加如下内容,然后进行提交： Creating a new branch is quick &amp; simple. $ git add Automated_op.py $ git commit -m &apos;conflicts test for master&apos; [master 47a2526] conflicts test for master 此时，master 分支和 feature1 分支各自都分别有新的提交，变成了这样： 这种情况下，Git 如果执行“快速合并”，就会产生合并冲突： $ git merge feature1 Auto-merging Automated_op.py CONFLICT (content): Merge conflict in Automated_op.py Automatic merge failed; fix conflicts and then commit the result. 此时，如果我们查看 Automated_op.py 脚本文件，你会看到： $ cat Automated_op.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Creating a new branch is quick &amp; simple. ======= Creating a new branch is quick AND simple. &gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1 Git 用 &lt;&lt;&lt;&lt;&lt;&lt;&lt;、=======、&gt;&gt;&gt;&gt;&gt;&gt;&gt; 标记出不同分支的内容，此时，HEAD 表示当前 master 分支。 Git 告诉我们此处语句存在冲突，我们需要明确语句到底是什么？我们选择：Creating a new branch is quick AND simple. 为最新语句，将 Automated_op.py 文本末尾内容只保留最新语句，在进行提交： $ git add Automated_op.py $ git commit -m &apos;deal with conflicts&apos; [master d527599] deal with conflicts 现在，master 分支和 feature1 分支变成了下图所示时间线结构： 并且，使用带参数的 git log 也可以看到分支的合并情况，如下： $ git log --graph --pretty=oneline --abbrev-commit * d527599 (HEAD -&gt; master) deal with conflicts |\ | * c4a3a95 (dev1) conflicts test * | 47a2526 conflicts test for master |/ * 09e14d1 branch test * 4f15f79 (origin/master) rm git_rm_test.txt * 6516e48 Add git_rm_test.txt * 9046f26 git tracks changes * 847e6ef understand how stage works * dd32d2d Add test code * 8622ab2 Add help info * d9e0247 Add Server API * c31b827 wrote a readme file 最后，删除 feature1 分支即可： $ git branch -d feature1 Deleted branch feature1 (was c4a3a95). 6.3 No Fast forward前面我们提到过 Git 合并分支默认采用 Fast forward 模式，但 Fast forward 模式有一个缺点就是：删除分支后，会丢掉分支信息。 Git 可以强制禁用 Fast forward 模式，这时如果使用 git merge 合并分支时会生成一个新的 commit ，此时分支历史上就可以看出分支信息。 合并分支时添加 --no-ff 参数，可以禁用 Fast forward 模式,如下： $ git merge --no-ff 下面我们来看一个实例： ## 1. 创建并切换dev分支： $ git checkout -b dev Switched to a new branch &apos;dev&apos; ## 2. 修改 Automated_op.py 脚本，并进行提交： $ git add Automated_op.py $ vim Automated_op.py $ git commit -m &apos;Automated_op add&apos; [dev e079427] Automated_op add 1 file changed, 1 insertion(+) ## 3. 切换回 master: $ git checkout master Switched to branch &apos;master&apos; ## 4. 禁用 Fast forward 合并分支： $ git merge --no-ff -m &apos;merge with no-ff&apos; dev Merge made by the &apos;recursive&apos; strategy. Automated_op.py | 1 + 1 file changed, 1 insertion(+) ## 5. git log 看看分支历史： $ git log --graph --pretty=oneline --abbrev-commit * 90f1293 (HEAD -&gt; master) merge with no-ff |\ | * e079427 (dev) Automated_op add |/ * d527599 deal with conflicts |\ | * c4a3a95 conflicts test * | 47a2526 conflicts test for master |/ * 09e14d1 branch test * 4f15f79 (origin/master) rm git_rm_test.txt * 6516e48 Add git_rm_test.txt * 9046f26 git tracks changes * 847e6ef understand how stage works * dd32d2d Add test code * 8622ab2 Add help info * d9e0247 Add Server API * c31b827 wrote a readme file |———————————————————— 实际开发分支管理策略： master 分支应该是非常稳定的，也就是仅用来发布新版本，不在其上进行开发； 创建一个 dev 分支，项目内开发都在 dev 分支，也就是说，dev 分支是不稳定的。某一项目版本开发完成后，比如 1.0 版本发布时，再把 dev 分支合并到 master 上，在 master 分支发布 1.0 版本； 项目中所有成员都在 dev 分支上进行开发，另外每个人都有自己的分支，完成相应功能开发后往 dev 分支上合并就可以了。 ————————————————————| 下面我们给出一个项目协作开发分支管理的结构图： 6.4 Bug 分支项目开发过程中一个不可避免的问题就是：修改 BUG。自然而然的，我们能够想到每个 BUG 都可以通过一个新的临时分支来修复，修复后合并分支，然后将临时分支删除即可。 假设场景：我们正在 dev 分支开发一个功能（未完成），此时我们接到一个修复代号 007 的 BUG 的任务时，很自然地可以想到创建一个分支 issue-007 来修复它，但是注意此时 dev 分支上进行的工作还没有提交。 $ git status On branch dev Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: Client/request.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 为了不影响 dev 分支上其它人的工作，一种方法是在当前分支上创建一个新的分支用于当前开发功能的提交，然后就可以创建 issue-007 分支来修复项目了。Git 还提供了另一种方法来帮助项目 BUG 修复—stash 功能。 1）git stash： git stash 它可以将当前工作现场“储藏”起来，等以后再将“储藏”的现场恢复后继续工作： $ git stash Saved working directory and index state WIP on dev: 90f1293 merge with no-ff 此时，再次查看版本库状态，发现工作区是干净的，之后就可以来修复 BUG 了： $ git status On branch dev nothing to commit, working tree clean 2）master 分支上修复 BUG： 首先切换 master 分支，然后创建修复 BUG 的临时分支 issue-007： $ git checkout master Switched to branch &apos;master&apos; Your branch is ahead of &apos;origin/master&apos; by 7 commits. (use &quot;git push&quot; to publish your local commits) guo_jie (master) GitTestProject $ git checkout -b issue-007 Switched to a new branch &apos;issue-007&apos; 现在修复bug，需要把 “# print (‘This is a stash function test’)” 中注释消掉 “print (‘This is a stash function test’)”，然后提交： $ git add Client/request.py $ git commit -m &apos;debug 007&apos; [issue-007 37f5f49] debug 007 1 file changed, 1 insertion(+), 1 deletion(-) BUG 修复完成后，切换到 master 分支，并完成合并，最后删除 issue-101 分支： $ git checkout master Switched to branch &apos;master&apos; Your branch is ahead of &apos;origin/master&apos; by 7 commits. (use &quot;git push&quot; to publish your local commits) $ git merge --no-ff -m &apos;merged debug 007&apos; issue-007 Merge made by the &apos;recursive&apos; strategy. Client/request.py | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) $ git branch -d issue-007 Deleted branch issue-007 (was 37f5f49). BUG 修复完成后，在线项目又可以正常运行了。此时需要返回 dev 分支继续工作了： $ git checkout dev Switched to branch &apos;dev&apos; $ git status On branch dev nothing to commit, working tree clean 3）git stash list 工作区是干净的，我们得需要知道 git stash 将之前的工作现场存储到哪了？ $ git stash list stash@{0}: WIP on dev: 90f1293 merge with no-ff 我们发现，之前的工作现场确实还在，Git 将其存储在某个地方了。下面我们来看如何将先前的工作现场恢复出来继续工作： 4）git stash apply/pop Git 提供了两种方法用于恢复先前 git stash 存储的工作现场： 使用 git stash apply 进行恢复的话，stash 存储内容并不删除，可以使用 git stash drop 进行删除； 使用 git stash pop 进行恢复的话，恢复的同时 stash 存储内容也会被删除掉。 现在我们使用 git stash pop 来恢复先前的工作现场： $ git stash pop On branch dev Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: Client/request.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) Dropped refs/stash@{0} (3e80a8f339cc31ea981838b28d4b86612d1bacf7) 此时再次使用 git stash list 查看，就看不到 stash 存储的内容了： $ git stash list |———————————————————— 补充说明： 可以多次 stash，恢复的时候，先用 git stash list 查看，然后恢复指定的 stash，用命令： $ git stash apply stash@{0} ————————————————————| 6.5 Feature 分支软件开发过程中，下一秒的“惊喜”就是客户的新需求，我们通常会使用一个临时的分支用于新功能的开发。 假设场景：我们接到一个新需求，用于开发代号为 Vulcan 的新功能。 1）准备临时分支 $ git checkout -b feature-vulcan Switched to a new branch &apos;feature-vulcan&apos; 2）新功能开发 新功能实现文档为 vulcan.py，向其中添加任意内容即可。然后进行提交： $ git add vulcan.py $ git status On branch feature-vulcan Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: vulcan.py $ git commit -m &apos;add feature vulcan&apos; [feature-vulcan 0c3a359] add feature vulcan 1 file changed, 1 insertion(+) create mode 100644 vulcan.py 3）git branch -D 提交之后，一切顺利的话，切换回 dev 分支，合并 feature-vulcan 分支，然后删除即可。 但就在此时，领导突然说，客户因经费不足，新功能必须取消！相关资料必须销毁: $ git branch -d feature-vulcan error: The branch &apos;feature-vulcan&apos; is not fully merged. If you are sure you want to delete it, run &apos;git branch -D feature-vulcan&apos;. 我们发现，分支删除失败（个人决定是一种防护措施）。feature-vulcan 分支还没有被合并，如果删除，将丢失掉修改，如果要强行删除，需要使用大写的 -D 参数。。 $ git branch -D feature-vulcan Deleted branch feature-valcan (was 0c3a359).]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>GitBranch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 使用指南之远程仓库]]></title>
    <url>%2FGit%2FPart-3-Of-Git-Tutorial%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 回顾： 上一章节我们已经掌握了 Git 版本库管理日常操作指令，已经初步达到了简单管理项目开发的目标了，再也不用担心文件备份或者丢失的问题了。有用过集中式版本控制系统 SVN 的童鞋会说，这些功能在 SVN 里早就有了，没看出 Git 有什么特别的地方。 确实没错，如果只是作为仓库管理文件历史，Git 和 SVN 还真没啥区别，但项目开发不仅仅是管理文件历史。为了体现 Git 作为分布式版本控制系统较于 SVN 的优势（不做“杠精”哈~），下面我们将介绍 Git 的杀手锏功能之一：Git 远程仓库，这也是 Git 迅速流行的主要原因。 读前须知 版权说明：本文思路以及内容主要来自廖雪峰老师的 Git 教程 （强烈推荐膜拜原文）并结合个人使用所作，只作为学习记录使用，如内容有侵权请联系删除，禁止转载！ 5. 远程仓库Git 是分布式版本控制系统，同一个 Git 仓库，可以分布到不同的机器上。怎么分布呢？起初肯定只有一台机器有一个原始版本库，此后别的机器可以 “克隆” 这个原始版本库，而且每台机器的版本库其实都是一样的，并没有主次之分。 那么如何保证所有分布式节点版本库数据同步呢？考虑以下场景： 某个项目由多人负责开发，要想实现所有人项目版本库数据同步 多地点不同机器进行项目开发（公司，家等），难道每天需要将项目拷贝过来拷贝过去？ 我们在介绍版本控制系统的集中式和分布式区别时，我们提到过：分布式不像集中式一样有中央服务器，但它会选用一台服务器充当“中央服务器”。 实际使用情况确实如此，分布式系统中其他每个人都从这个 “中央服务器” 仓库克隆一份到自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交。在公司局域网内进行项目开发（存在信息安全）时，完全可以搭建一台运行 Git 的 “中央服务器” 进行项目开发（后续章节会讲解如何快速搭建 Git “中央服务器”，公司内部开发必备）。 不过为了学习 Git 先搭个服务器绝对是小题大作。不知道你还记不记得之前我们提过的 GitHub—提供了 Git 仓库托管服务的，只要注册一个 GitHub 账号，就可以免费获得 Git 远程仓库。P.S. 类似 GitHub 的产品还有许多，如：GitLab、Bitbucket、码云等。 后续我们将以 GitHub 作为远程仓库进行继续学习，阅读后续内容前，请自行注册 GitHub 账号。 5.1 GitHub首先我们给出 GitHub 登陆界面： 5.1.1 GitHub 配置 SSH Key我们需要知道：本地 Git 仓库和远程 GitHub 仓库之间的传输是通过 SSH 加密的，所以首先我们需要设置 Git 和 GitHub 之间的 SSH，只有配置了 SSH，Git 才可以和 GitHub 进行同步。下面来看如如何为 GiHub 配置本地 Git 仓库机器节点的 SSH Key： 1）本地 Git 创建 SSH Key： 检查用户主目录（~）下是否有 .ssh 目录,以及 .ssh 目录下是否有 id_rsa 和 id_rsa.pub 这两个文件。如果都存在，可以跳过当前步骤，否则通过 Shell（ Windows 下打开 Git Bash），使用邮箱创建 SSH Key： $ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 将 “youremail@example.com“ 换成可用的邮件地址，然后一路回车，使用默认值即可。 如果创建成功的话，可以在用户目录下找到 .ssh 目录，并且 .ssh 目录中还有 id_rsa 和 id_rsa.pub 两个文件（SSH Key 的秘钥对）。其中 id_rsa 是私钥，id_rsa.pub 是公钥。 2）GitHub Settings 登陆 GitHub，打开 “Settings” –&gt; “SSH and GPG keys” 页面： 然后点击 “Add SSH Key”，填任意 Title，在 Key 文本框里粘贴本地 Git 节点 id_rsa.pub 文件的内容点击下方 “Add SSH Key” 即可查看到添加好的 SSH Key 了： |—————————————————————– 内容补充： 为什么 GitHub 需要 SSH Key 呢？因为 GitHub 需要识别出推送的提交确实是你推送的，而不是别人冒充的，而 Git 支持 SSH 协议，所以 GitHub 只要知道了你的公钥，就可以确认只有你自己才能推送。 当然，GitHub 允许用户添加多个 SSH Key。假定你有若干台设备时，只要把每台设备的 Key 都添加到 GitHub，就可以在每台设备上往 GitHub 进行推送了（是不实现了版本库的数据同步了）。 注意：GitHub 上免费托管的 Git 仓库，任何人都可以看到（但只有你自己才能改）。所以，不要把敏感信息放进去。当然你可以上交费用让 GitHub 把公开仓库变成私有的也可以。 —————————————————————–| SSH Key 添加成功之后，我们可以在 Git bash 中进行测试： 123$ ssh -T git@github.comWarning: Permanently added the RSA host key for IP address 'X.X.X.X' to the list of known hosts.Hi TheNightIsYoung! You've successfully authenticated, but GitHub does not provide shell access. 返回 “Hi username ！You’ve successfully ……” 说明你已经成功啦！ 5.1.2 Git 添加远程库当前的场景是：我们的本地机器已经拥有了一个 Git 仓库，想将 GitHub 作为本地 Git 某个仓库（GitTestProject）的远程仓库。这样 GitHub 上的仓库既可以作为备份，又可以让其他人通过该仓库来协作，真是一举多得。 1）GitHub 上创建一个新仓库，作为中央仓库（备份、同步仓库）： 这里我们还是以 GitTestProject 作为项目名称。创建好后，GitHub 上的这个 GitTestProject 仓库还是空的。 GitHub 告诉我们，可以从这个仓库克隆出新的仓库;也可以把一个已有的本地仓库与之关联，然后把本地仓库的内容推送到 GitHub 仓库;还可以从其它仓库导入代码。 这里我们只关注本地 Git 仓库与其进行关联。 2）本地 Git 仓库关联 GitHub 远程仓库 根据你自己的 …or push an existing repository from the command line 中关联命令在 GitTestProject 目录下进行关联操作： git remote add origin git@github.com:TheNightIsYoung/GitTestProject.git 注意：TheNightIsYoung 应该是你 GitHub 的用户名。如果使用成我的，关联没问题，但你无法推送（未添加你设备的 SSH Key）。 添加后，远程库的名字就是 origin，这是 Git 默认的叫法（比较形象），当然也可以改成别的。 3）推送本地 Git 所有内容到 GitHub： 把本地库的内容推送到远程，用 git push 命令，实际上是把当前分支 master 推送到远程。 $ git push -u origin master The authenticity of host &apos;github.com (127.8.0.1)&apos; can&apos;t be established. RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;github.com&apos; (RSA) to the list of known hosts. Enumerating objects: 29, done. Counting objects: 100% (29/29), done. Delta compression using up to 8 threads Compressing objects: 100% (19/19), done. Writing objects: 100% (29/29), 2.32 KiB | 395.00 KiB/s, done. Total 29 (delta 5), reused 0 (delta 0) remote: Resolving deltas: 100% (5/5), done. To github.com:TheNightIsYoung/GitTestProject.git * [new branch] master -&gt; master Branch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;origin&apos;. |—————————————————————– SSH 连接警告 当你第一次使用 Git 的 clone 或者 push 命令连接 GitHub 时，会得到一个警告： The authenticity of host &apos;github.com (xx.xx.xx.xx)&apos; can&apos;t be established. RSA key fingerprint is xx.xx.xx.xx.xx. Are you sure you want to continue connecting (yes/no)? 这是因为 Git 首次使用 SSH 连接，而 SSH 连接在第一次验证 GitHub 服务器的 Key 时，需要你确认 GitHub 的 Key 的指纹信息是否真的来自 GitHub 的服务器，输入 yes 回车即可。 后续，Git 会输出一个警告，告诉你已经把 GitHub 的 Key 添加到本机的一个信任列表里了： Warning: Permanently added &apos;github.com&apos; (RSA) to the list of known hosts. 这个警告只会出现一次，后面的操作就不会有任何警告了。 如果你实在担心有人冒充 GitHub 服务器，输入 yes 前可以对照 GitHub 的 RSA Key 的指纹信息是否与 SSH 连接给出的一致。 参数说明 由于远程库是空的，我们第一次推送 master 分支时，加上了 -u 参数，Git 不但会把本地的 master 分支内容推送的远程新的 master 分支，还会把本地的 master 分支和远程的 master 分支关联起来，在以后的推送或者拉取时就可以简化命令。 —————————————————————–| 4）查看 GitHub GitTestProject 仓库： 推送成功后，可以立刻在 GitHub 页面中看到远程库的内容已经和本地一模一样： |—————————————————————– 从现在起，只要本地作了提交，就可以通过命令： $ git push origin master 把本地 master 分支的最新修改推送至 GitHub。到这里你就拥有了真正的分布式版本库！ —————————————————————–| 5.1.3 GitHub 远程库克隆之前我已经创建好了一个 GitTestProject 远程库，并且关联到了我自己的本地测试项目仓库。多个人协作开发时，其他人要想获取我的版本库的话，可以直接使用 git clone 从我的 GitHub 克隆一个 GitTestProject 本地库： $ git clone git@github.com:TheNightIsYoung/GitTestProject.git Cloning into &apos;gitskills&apos;... remote: Counting objects: 3, done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3 Receiving objects: 100% (3/3), done. GitHub 给出的地址不止一个，还可以用 https://github.com/TheNightIsYoung/GitTestProject.git 这样的地址。实际上 Git 支持多种协议，默认使用 SSH 协议，即 git:// ，但也可以使用 https 等其他协议。 使用 https 除了速度慢以外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放 http 端口的公司内部就无法使用 ssh 协议而只能用 https。 1）Git Clone 仓库中的单个目录 很多时候，由于项目较大，我们只想 git clone 仓库的某单个或多个文件夹，而不是全部的仓库内容。 Git1.7.0 以前，这无法实现。但是幸运的是，在 Git1.7.0 以后加入了 Sparse Checkout 模式，这使得 CheckOut 指定文件或者文件夹成为可能。 假设我们有一个 Github 仓库：https://github.com/yourgithub/Demo，我们想要 git clone Demo 里面的 childDir子目录： 1234567891011121314151617181920212223# 'childDir' 需要替换为自己想要下载的目录名称：# 初始化本地仓库：$ git init Demo &amp;&amp; cd Demo# 设置允许进行子目录克隆：$ git config core.sparsecheckout true$ 设置目标子目录名称（假设我的 childDir 位于 https://github.com/yourgithub/Demo/code 下）：echo 'code/childDir*' &gt;&gt; .git/info/sparse-checkout$ 映射：git remote add origin https://github.com/yourgithub/Demo.git# 拉取：$ git pull origin masterremote: Enumerating objects: 2750, done.remote: Total 2750 (delta 0), reused 0 (delta 0)Receiving objects: 100% (2750/2750), 36.41 MiB | 10.38 MiB/s, done.Resolving deltas: 100% (542/542), done.From e.coding.net:xdzsoft/mashangwuyou * branch master -&gt; FETCH_HEAD * [new branch] master -&gt; origin/master OK!!!]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 使用指南之时光穿梭机]]></title>
    <url>%2FGit%2FPart-2-Of-Git-Tutorial%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 回顾： 上一章节我们已经了解了 Git 版本控制系统的基本概念，Windows、Linux 平台下 Git 的安装以及相关配置，以及 Git 版本库。并且在上一章节我们已经成功将一个项目 GitTestProject 添加到 Git 版本管理中，下面我们将基于这个版本库为实例继续来看 Git 如何进行版本控制。 读前须知 版权说明：本文思路以及内容主要来自廖雪峰老师的 Git 教程 （强烈推荐膜拜原文）并结合个人使用所作，只作为学习记录使用，如内容有侵权请联系删除，禁止转载！ 在学习之前我先给出 GitTestProject 版本库内容,如下： 4. Git 基础版本库操作这一部分我们来看 Git 版本库管理日常操作指令，通过这一小节的学习我们就可以达到初步管理项目的目标了。下面让我们一起来看看 Git 到底有多神奇： 4.1 版本库文件修改前面我们已经成功地添加并提交了 Server、Client 目录以及其下所有文件，Server 目录下有一个 service.py 服务脚本，其内容如下： # This is a Test! 到了工作时间，我们想要接着继续写 service.py 脚本了，我做如下变更： # Git is a distributed version control system. # Git is free software. # This is a git Test! 1）git status 我已经对文档做了变更并且 wq，在提交变更之前，我想要知道 Git 版本库（GitTestProject）是否已经实时地跟踪我的修改（很慌），这是很重要的！！！当然这是可以的，毕竟 Git 就是做这个的。 命令：git status 可以让我们时刻监控到版本仓库当前的状态，我们在 Git Bash 中运行查看一下 Git监控信息： $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: Server/service.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 可以看到，Git 告诉我们 modified: Server/service.py（service.py 已经被修改），并且提醒我们 Changes not staged for commit（变更未提交）。 2）git diff 然而，我们往往更加关注的是对 service.py 做了哪些修改。这时需要使用 git diff 来进行查看（diff：difference）： $ git diff Server/service.py diff --git a/Server/service.py b/Server/service.py index 68f5fe0..433af3b 100644 --- a/Server/service.py +++ b/Server/service.py @@ -1 +1,3 @@ -This is a Test! +# Git is a distributed version control system. +# Git is free software. +# This is a Test! 可以看到 Git 已经跟踪了我们对 service.py 文件的修改，这下舒服了，将其提交到版本库也就放心了。 3）git add &amp; git commit 提交提交… $ git add service.py 执行后同样没有任何输出（成功）。在执行 git commit 之前，我们再次运行 git status 查看一下当前仓库的状态（不放心再确认一下）： $ git status On branch master Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: Server/service.py Git 告诉我们将要被提交的修改是：Server/service.py（心里默念一句 NB），确定之后我们直接提交给版本库： $ git commit -m &apos;Add help info&apos; [master 8622ab2] Add help info 1 file changed, 3 insertions(+), 1 deletion(-) 提交完成后,我们再次查看版本库状态： $ git status On branch master nothing to commit, working tree clean Git 监控信息显示：当前没有需要提交的变更，并且工作目录是干净（working tree clean）的，完美。 4.2 版本回滚上面我们已经了解如何修改版本库中的文件，再练习一次，修改 service.py 文件如下： # Git is a distributed version control system. # Git is free software. # This is a Test! print (&apos;Git is very useful!!!&apos;) 然后尝试提交： $ git add Server/service.py $ git commit -m &apos;Add test code&apos; [master dd32d2d] Add test code 1 file changed, 2 insertions(+) 像这样，不断对文件进行修改，然后不断提交变更到版本库里，就好像游戏存档一样：一旦失败我们可以回滚到最新的存档。 Git 也是一样，每当你觉得文件修改到一定程度的时候，就可以 “保存一个快照”，这个快照在 Git 中被称为 commit。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个 commit 恢复，然后继续工作，而不是把几个月的工作成果全部丢失。 下面我们来看一下 Git 的回滚实现。开始之前我们先来回顾一下 service.py 一共有多少个版本被提交到 Git 仓库里： 版本 1：Add Server API # service.py 被提交到 GitTestProject，内容如下： # This is a Test! 版本 2：Add help info # Git is a distributed version control system. # Git is free software. # This is a Test! 版本 3：Add test code # Git is a distributed version control system. # Git is free software. # This is a Test! print (&apos;Git is very useful!!!&apos;) 1）git log 当然，实际工作中，我们怎么可能记得一个几千行的文件每次都改了什么内容（不然要版本控制系统干什么，和原始版本控制有何区别）。我们需要版本控制系统可以告诉我们项目的变更历史记录，git log 就是做这个事的，它会显示我们从最新到最远的提交日志。 $ git log commit dd32d2d9afae403ff3af84a03f434bfa1c627cf8 (HEAD -&gt; master) Author: staff_ming &lt;staff_ming@xxxx.com&gt; Date: Fri Dec 21 11:20:04 2018 +0800 Add test code commit 8622ab2549f1c113739c6db5a066890de3057fc9 Author: staff_ming &lt;staff_ming@xxxx.com&gt; Date: Fri Dec 21 11:02:34 2018 +0800 Add help info commit d9e0247cb21da0faa81fe05dccda8c65aedc0c15 Author: staff_ming &lt;staff_ming@xxxx.com&gt; Date: Thu Dec 20 19:53:06 2018 +0800 Add Server API commit c31b82744a3a5d7ad48a6bd72de22fbbb0bf4420 Author: staff_ming &lt;staff_ming@xxxx.com&gt; Date: Thu Dec 20 19:39:16 2018 +0800 wrote a readme file 如果嫌输出信息太多，看得眼花缭乱的，可以试试加上 --pretty=oneline 参数（清晰了很多）： $ git log --pretty=oneline dd32d2d9afae403ff3af84a03f434bfa1c627cf8 (HEAD -&gt; master) Add test code 8622ab2549f1c113739c6db5a066890de3057fc9 Add help info d9e0247cb21da0faa81fe05dccda8c65aedc0c15 Add Server API c31b82744a3a5d7ad48a6bd72de22fbbb0bf4420 wrote a readme file 当然我们也可以查看某个文件的提交日志： $ git log Server/service.py |—————————————————————– 内容解析： 你看到的一大串类似 dd32d2d... 的是 commit id（版本号），和 SVN 不一样，Git 的 commit id 不是 1，2，3…… 递增的数字，而是一个非常大的数字（十六进制）。 为什么采用这种机制？因为 Git 是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用 1，2，3…… 作为版本号，很容易就冲突了。 每提交一个新版本，实际上 Git 就会把它们自动串成一条时间线。如果使用可视化工具（Git GUI）查看 Git 历史，就可以更清楚地看到提交历史的时间线： —————————————————————–| 有了文件版本时间线，接下来启动时光穿梭机进行版本回滚吧…假设我们想要将 service.py 回滚到上一个版本（版本 2：Add help info），怎么办？ 2）HEAD 要想实现版本回滚，Git 必须被指定要回滚到哪个版本。 在 Git 中，用 HEAD 表示当前版本，也就是最新的提交 dd32d2...（commit id）。上一个版本表示为：HEAD^，上上一个版本可以表示为:HEAD^^，很多人就要问了那如果要表示往上 90 个版本呢，难道要写 90 个 ^,当然不可能。版本数目较多时可以表示为：HEAD~90。 3）git reset 回到过去 现在我们就可以从当前版本【 Add test code 】回滚到上一版本【 Add help info 】了，Git 提供了 git reset（时光机）来进行 “时空穿梭”。 $ git reset --hard HEAD^ HEAD is now at 8622ab2 Add help info 这里我们侧重看 git reset 实现功能，参数 --hard 会在后续补充说明，这里你只需知道它是版本指定相关参数即可。回滚后，我们来看 service.py 当前版本内容： $ cat Server/service.py # Git is a distributed version control system. # Git is free software. # This is a Test! 我们发现，果然 service.py 时空倒流到上一个版本了。 穿越到未来 此时，我们查看一下当前版本库的提交日志信息： $ git log --pretty=oneline 8622ab2549f1c113739c6db5a066890de3057fc9 (HEAD -&gt; master) Add help info d9e0247cb21da0faa81fe05dccda8c65aedc0c15 Add Server API c31b82744a3a5d7ad48a6bd72de22fbbb0bf4420 wrote a readme file 可以发现，之前最新的那个版本 【 Add test code 】 已经看不到了！就好比你从现在回到了过去，但你又想回到现在（未来），你想通过 git log 察看现在时间节点在时间线上的版本标识，结果发现你已经找不到了。难道只能活在过去了么？ 在给出办法之前你先祈祷吧…之前我们一直使用的 Git Bash 窗口你还没手贱关掉。我们可以找到先前 【 Add test code 】 的 commit id 是 dd32d2d9....，这样又找到了回去的时间节点版本标识。（确实关闭也不要紧，是不是有点慌，哈哈。4）中我们回给出解决方法） $ git reset --hard dd32d2d9 HEAD is now at dd32d2d Add test code 注意：版本号没必要写全，Git 回去自动检索的，当然也不能太少，否则可能无法和其它版本区别。 此时参看 service.py，发现我们已经回到了“现在”： $ cat Server/service.py # Git is a distributed version control system. # Git is free software. # This is a Test! print (&apos;Git is very useful!!!&apos;) 4）HEAD &amp;&amp; commit id 在进行版本回滚的时候我们发现 Git 的版本回退速度非常快，这是因为 Git 内部有个指向当前版本的指针，就是我们前面说的 Head，当你回滚版本的时候，Git 仅仅是把 HEAD 从指向 【 Add test code 】 版本： 移动指向 【 Add help info 】： 然后顺便把工作区的文件更新了。所以你让 HEAD 指向哪个版本号，你就把当前版本定位在哪（讲到这里很多人会想到程序设计语言中的指针，而 commit id 就是版本库实际的地址）。 前面我们在 穿越到未来 中提到 Git Bash 关掉后，表面看起来找不到最新版本的 commit id。实际上，Git 提供了指令 git reflog 用来记录你的每一次命令： $ git reflog dd32d2d (HEAD -&gt; master) HEAD@{0}: reset: moving to dd32d2d9 8622ab2 HEAD@{1}: reset: moving to HEAD 8622ab2 HEAD@{2}: reset: moving to HEAD 8622ab2 HEAD@{3}: reset: moving to HEAD^ dd32d2d (HEAD -&gt; master) HEAD@{4}: commit: Add test code 8622ab2 HEAD@{5}: commit: Add help info d9e0247 HEAD@{6}: commit: Add Server API c31b827 HEAD@{7}: commit (initial): wrote a readme file 从日志输出可以看出，commit: Add test code 的 id 是：dd32d2d。 4.3 工作区和暂存区Git 和其他版本控制系统（如：SVN）的一个不同之处就是有暂存区的概念。 4.3.1 名词解释工作区（Working Directory） 我们将当前程序开发所在目录称为工作区，也就是我们的项目开发目录（如：GitTestProject）。该区域的文件会有状态的变化且状态由 Git 自动检测，如果程序中文件做任何操作（增、删、改），文件状态均会被检测到（类似于 SVN）。 版本库（Repository） 工作区有一个隐藏目录 .git ，这个不算工作区，而是 Git 的版本库。Git 的版本库里存了很多东西，其中最重要的就是称为 stage（或者叫index）的暂存区，还有 Git 为我们自动创建第一个分支 master，以及指向 master 的一个指针叫 HEAD（分支后续再补充讲解）。 4.3.2 Git 版本控制原理当工作区检测到有文件发生变化时，那么意味着我们在上一个版本之后再次对项目进行了变更。变更完成之后，我们可以将当前变更当做下一版本进行提交，那么就是执行 git add . 将所有文件提交到暂存区（stage），然后再执行 git commit -m &#39;another version&#39; 提交到版本库的当前分支。 简单理解为：首先将需要提交的文件修改通通放到暂存区，然后一次性提交暂存区的所有修改即可。 下面我们完成一个实例来深入了解 Git 版本控制原理，详细过程如下所示： 1）在工作区修改文件 Server/service.py 内容为： # Git is a distributed version control system. # Git is free software. # This is a Test! print (&apos;Git is very useful!!!&apos;) # Git has a mutable index called stage. 2）在工作区添加文件 Client/request.py（内容随意），然后查看版本库当前状态： $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: Server/service.py Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) Client/ no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) Git 非常清楚地告诉我们，Server/service.py 脚本被修改了，而 Client/ 还从来没有被添加过，所以它的状态是 Untracked。 3）git add . 将所有修改文件提交到暂存区（stage），然后查看此时版本库状态： $ git status On branch master Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: Client/request.py modified: Server/service.py 此时暂存区的状态会成为： 4）git commit 一次性把暂存区的所有修改提交到分支。 $ git commit -m &apos;understand how stage works&apos; [master 847e6ef] understand how stage works 2 files changed, 3 insertions(+) create mode 100644 Client/request.py 5）提交后，如果工作区没有做任何修改，那么工作区就是 “干净” 的： $ git status On branch master nothing to commit, working tree clean 此时版本库的暂存区（stage）内容会被提交的 master 分支，stage 内容清空。 4.4 Git 管理的是修改上一小节我们已经掌握了暂存区的概念。这一小节我们讨论的是：Git 跟踪并管理的是修改，而非文件。 什么是修改？比如：新增了一行，这就是一个修改；删除了一行，也是一个修改；更改了某些字符，也是一个修改；删了一些又加了一些，也是一个修改；甚至创建一个新文件，也算一个修改。 为什么说 Git 管理的是修改，而不是文件呢？我们还是通过实例给出解读： 1）对 Server/service.py 进行第一次修改： # Git is a distributed version control system. # Git is free software. # This is a Test! print (&apos;Git is very useful!!!&apos;) # Git has a mutable index called stage. # Git tracks changes. 2）将其添加到暂存区： $ git add Server/service.py $ git status On branch master Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: Server/service.py 3）对 Server/service.py 进行第二次修改： # Git is a distributed version control system. # Git is free software. # This is a Test! print (&apos;Git is very useful!!!&apos;) # Git has a mutable index called stage. # Git tracks changes of files. 4）提交： $ git commit -m &apos;git tracks changes&apos; [master 9046f26] git tracks changes 1 file changed, 1 insertion(+) $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: Server/service.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 我们发现还有一次修改未被提交，我们先回顾一下操作过程： 第一次修改 -&gt; git add -&gt; 第二次修改 -&gt; git commit 所以，就像我们前面说的，Git 管理的是修改，当你用 git add 命令后，在工作区的第一次修改被放入暂存区，准备提交，但是，在工作区的第二次修改并没有放入暂存区，所以，git commit 只负责把暂存区的修改提交了，也就是第一次的修改被提交了，第二次的修改不会被提交。 我们可以使用：git diff HEAD -- Server/service.py 命令可以查看工作区和版本库里面最新版本的区别： $ git diff HEAD -- Server/service.py diff --git a/Server/service.py b/Server/service.py index 6bef824..a17dbf0 100644 --- a/Server/service.py +++ b/Server/service.py @@ -5,4 +5,4 @@ print (&apos;Git is very useful!!!&apos;) # Git has a mutable index called stage. -# Git tracks changes. +# Git tracks changes of files. 可见，第二次修改确实没有被提交。所以：如果不用 git add 到暂存区，那就不会加入到 commit 快照中。 4.5 撤销修改还记得上一小节我们做了两次修改，现在假设第一次修改为正确修改已被提交。第二次修改未提交，提交前我们突然发现修改为错误修改，我们能想到的直接的纠正方法是手动把文件恢复到上一个版本的状态。但我们查看此时版本库状态： $ git status On branch master Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: Server/service.py no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 我们发现，Git 提示可以通过 git checkout -- file 可以丢弃工作区的修改： $ git checkout -- Server/service.py |—————————————————————– 命令解析： 命令 git checkout -- Server/service.py 意思就是，把 service.py 文件在工作区的修改全部撤销，这里有两种情况： 一种是 service.py 自修改后还没有被放到暂存区，撤销修改就回到和版本库一模一样的状态； 一种是 service.py 已经添加到暂存区后又作了修改，撤销修改就回到添加到暂存区后的状态。 简单地说：git checkout -- file 会让文件 file 回到最近一次 git commit 或 git add 时的状态。 —————————————————————–| 可以发现，丢弃工作区的修改之后 service.py 文件又恢复到之前的内容了： $ cat Server/service.py # Git is a distributed version control system. # Git is free software. # This is a Test! print (&apos;Git is very useful!!!&apos;) # Git has a mutable index called stage. # Git tracks changes. 注意：git checkout -- file 命令中的参数 -- 很重要，没有 --，就变成了 “切换到另一个分支” 的命令，后面的分支管理中会再次遇到 git checkout 命令。 4.6 删除文件前面我们接触的修改文件都是新建、修改等。Git 中，删除也是一个修改操作。 下面我们通过一个删除实例来看 Git 如何管理删除，先添加一个新文件 git_rm_test.txt 到 Git 并且提交： $ touch git_rm_test.txt $ git add git_rm_test.txt $ git commit -m &apos;Add git_rm_test.txt&apos; [master 6516e48] Add git_rm_test.txt 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 git_rm_test.txt 然后我们删除 git_rm_test.txt 文件。这时 Git 会跟踪删除文件操作，工作区和版本库就不一致了。此时我们来看版本库状态，会发现 Git 已经知道了我们的删除操作： $ git status On branch master Changes not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) deleted: git_rm_test.txt no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 此时我们面临两个选择：一是确实要从版本库中删除该文件；那就用命令 git rm file 删掉，并且 git commit 提交： $ git rm git_rm_test.txt rm &apos;git_rm_test.txt&apos; $ git commit -m &apos;rm git_rm_test.txt&apos; [master 4f15f79] rm git_rm_test.txt 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 git_rm_test.txt 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： $ git checkout -- git_rm_test.txt]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>VersionControl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 使用指南之初识]]></title>
    <url>%2FGit%2FPart-1-Of-Git-Tutorial%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇 在日常项目开发中，我们肯定会或多或少地听说或者使用过版本管理工具。之前我刚好有幸参与过公司项目版本管理—–SVN 版本控制系统的搭建与管理，再加上项目组日常项目开发使用 SVN，也就没有花费太多精力去学习其它版本管理工具。 作为一名入行（深度学习方向）刚满一年的新手，初次接触 Git 还是因为常常会从 GitHub clone 一些大佬们开源的深度学习项目以供学习和借鉴，但对 Git 的使用仅限于：git clone XXX（脸红）。而随着身边越来越多的人开始使用 Git，以及学习和工作需要，才发现：这年头不会点 Git 是真不行啊… 读取须知 版权说明：本文思路以及内容主要来自廖雪峰老师的 Git 教程 （强烈推荐膜拜原文）并结合个人使用所作，只作为学习记录使用。如内容有侵权请联系删除，禁止转载！ 以便日后再次回顾使用，故将 Git 学习过程进行记录留存。下面我们正式开始 Git 的学习： 1. Git 简介和 SVN 类似，Git 也是用于版本控制。Git 是时下最流行的分布式版本控制系统，没有之一。 1.1 版本控制系统我们一直在说版本控制，那么究竟什么是版本控制？ 其实说到版本控制，我总会想到大学毕业写论文时的场景，你电脑上的毕业论文一定也出现过和我一样的场景： 以上就是使用最原始的方式进行版本控制，可以发现存在着显著缺点： 当保留所有版本文件时，需要为每个版本保存一个文件… 当文档需要多人协同操作时，需要将文件打包发来发去，拿到后还需要修改整合… 容易丢失，被删除意味着永远失去… 通过以上样例的解读，下面我们给出关于版本控制更加严格的解读： 版本控制是指对软件开发过程中各种程序代码、配置文件及说明文档等文件变更的管理。 版本控制最主要的功能就是追踪文件的变更。它将什么时候、什么人更改了文件的什么内容等信息忠实地了记录下来。每一次文件的改变，文件的版本号都将增加。 除了记录版本变更外，版本控制的另一个重要功能是并行开发。软件开发往往是多人协同作业，版本控制可以有效地解决版本的同步以及不同开发者之间的开发通信问题，提高协同开发的效率。 为了解决以上版本控制存在问题，应运而生了一批版本控制工具：VSS、CVS、SVN、Git 等，其中 Git 已经成为当前最流行的分布式版本控制系统。通过使用这些版本控制工具，你就结束了手动管理多个 “版本” 的史前时代，进入到版本控制的新世纪。 例如使用 Git 版本控制工具我们可以清晰、便捷的管理文档的不同版本，如下图所示： 1.2 Git 的诞生我们知道，Linux 是开源的代名词，Linux 的系统日益壮大是靠全世界热心的志愿者共同参与的，这么多人在世界各地为 Linux 编写代码，那 Linus（发起 Linux 项目） 的代码是如何管理整合的呢？ 事实是，起初世界各地的志愿者把源代码文件发给 Linus，然后由 Linus 本人通过手工方式合并代码！ 你也许会想，为什么 Linus 不把 Linux 代码放到版本控制系统里呢？不是有 CVS、SVN 这些免费的版本控制系统吗？因为 Linus 坚定地反对 CVS 和 SVN，这些集中式的版本控制系统不但速度慢，而且必须联网才能使用。有一些商用的版本控制系统，虽然比 CVS、SVN 好用，但那是付费的，这和 Linux 的开源精神不符。 不过，随着 Linux 系统的不断发展，代码库之大让 Linus 很难继续通过手工方式管理了，社区的弟兄们也对这种方式表达了强烈不满，于是 Linus 选择了一个商业的版本控制系统 BitKeeper，BitKeeper 的东家 BitMover 公司出于人道主义精神，授权 Linux 社区免费使用这个版本控制系统。 没几年，这种安定团结的大好局面就被打破了。原因是 Linux 社区牛人聚集，不免沾染了一些梁山好汉的江湖习气。开发 Samba 的 Andrew 试图破解 BitKeeper 的协议（这么干的其实也不只他一个），被 BitMover 公司发现了（监控工作做得不错！），于是 BitMover 公司怒了，要收回 Linux 社区的免费使用权。 Linus 可以向 BitMover 公司道个歉，保证以后严格管教弟兄们，嗯，这是不可能的。实际情况是这样的：Linus 花了两周时间自己用 C 写了一个分布式版本控制系统，这就是 Git！一个月之内，Linux 系统的源码已经由 Git 管理了！牛是怎么定义的呢？大家可以体会一下。 Git 迅速成为最流行的分布式版本控制系统 。尤其是 GitHub 的上线，它为开源项目免费提供 Git 存储，无数开源项目开始迁移至 GitHub。 GitHub 是一个基于 Git 的远程文件托管平台。Git 本身完全可以做到版本控制，但其所有内容以及版本记录只能保存在本机。如果想要将文件内容以及版本记录同时保存在远程服务器，则需要结合 GitHub 来使用。 1.3 集中式 vs 分布式Linus 一直痛恨的 CVS、SVN 都是集中式的版本控制系统，而 Git 是分布式版本控制系统，集中式和分布式版本控制系统有什么区别呢？ 1.3.1 集中式版本控制系统先说集中式版本控制系统，都有一个单一的集中管理的中央服务器，保存所有的项目版本库。而协同工作的人们首先都需要通过客户端连到这台中央服务器取出最新的文档版本进行修改，修改完成后需要提交更新给中央服务器完成服务器上的修改。 多年以来，这已成为版本控制系统的标准做法。 集中式版本控制系统缺点在于：中央服务器的单点故障。 如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。 如果中心数据库所在的磁盘发生损坏，又没有做恰当备份，毫无疑问你将丢失所有数据——包括项目的整个变更历史，只剩下人们在各自机器上保留的单独快照。只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 1.3.2 分布式版本控制系统那分布式版本控制系统与集中式版本控制系统有何不同呢？ 首先，分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库。这也导致和集中式版本控制系统相比，分布式版本控制系统的安全性要高很多，因为每个人电脑里都有完整的版本库，某一个人的电脑坏掉了不要紧，随便从其他人那里复制一个就可以了。 既然每个人电脑上都有一个完整的版本库，那多个人如何协作呢？当然可以相互之间进行推送，但在实际使用分布式版本控制系统时通常也有一台充当“中央服务器”的电脑，但这个服务器的作用仅仅是用来方便 “交换” 大家的修改，没有它大家也一样干活，只是交换修改不方便而已。 当然，Git 的优势不单是分布式这么简单，后面我们还会看到 Git 极其强大的分支管理，把 SVN 等远远抛在了后面。 1.4 Git 安装最早 Git 是在 Linux 上开发的，很长一段时间内，Git 也只能在 Linux 和 Unix 系统上跑。目前，Git 已经为 Linux、Unix、Mac 和 Windows 等多平台提供了支持。这一部分我们来看如何在不同平台完成 Git 的安装： 1.4.1 Linux 平台上安装 Git1.4.1-1 Centos 安装方法YUM 方法安装 1）检测系统是否安装有 Git $ git The program &apos;git&apos; is currently not installed. You can install it by typing: sudo yum install git 如果系统已安装 Git，你可以使用 git --version 查看相应的安装版本，确定是否需要重新安装 Git（一般 Centos 系统 Git 版本较低，以 Centos7 为例为：git version 1.8.3.1）。这里我们给出 GitHub 上的 Git 版本发布界面，你可以查看最新的 Git 版本。 如果未安装有 Git，则执行 Git 安装： 2）安装 Git 登陆待安装服务器，输入如下命令进行下载安装： $ yum install git ## 日志信息如下： 已加载插件：fastestmirror, langpacks gitlab_gitlab-ee/x86_64/signature | 836 B 00:00:00 gitlab_gitlab-ee/x86_64/signature | 1.0 kB 00:00:00 !!! gitlab_gitlab-ee-source/signature | 836 B 00:00:00 gitlab_gitlab-ee-source/signature | 951 B 00:00:00 !!! Loading mirror speeds from cached hostfile * base: centos.ustc.edu.cn * extras: centos.ustc.edu.cn * updates: centos-mirror.rbc.ru 正在解决依赖关系 --&gt; 正在检查事务 ---&gt; 软件包 git.x86_64.0.1.8.3.1-14.el7_5 将被 升级 --&gt; 正在处理依赖关系 git = 1.8.3.1-14.el7_5，它被软件包 perl-Git-1.8.3.1-14.el7_5.noarch 需要 ---&gt; 软件包 git.x86_64.0.1.8.3.1-20.el7 将被 更新 --&gt; 正在检查事务 ---&gt; 软件包 perl-Git.noarch.0.1.8.3.1-14.el7_5 将被 升级 ---&gt; 软件包 perl-Git.noarch.0.1.8.3.1-20.el7 将被 更新 --&gt; 解决依赖关系完成 依赖关系解决 ========================================================================================================================================================================================================== Package 架构 版本 源 大小 ========================================================================================================================================================================================================== 正在更新: git x86_64 1.8.3.1-20.el7 updates 4.4 M 为依赖而更新: perl-Git noarch 1.8.3.1-20.el7 updates 55 k 事务概要 ========================================================================================================================================================================================================== 升级 1 软件包 (+1 依赖软件包) 总下载量：4.4 M 接着询问是否进行安装，输入 y，然后等待安装完成即可（接上）： ## 日志信息如下： 总下载量：4.4 M Is this ok [y/d/N]: y Downloading packages: updates/7/x86_64/prestodelta | 182 kB 00:00:04 Delta RPMs reduced 4.4 M of updates to 2.6 M (41% saved) (1/2): perl-Git-1.8.3.1-14.el7_5_1.8.3.1-20.el7.noarch.drpm | 28 kB 00:00:02 (2/2): git-1.8.3.1-14.el7_5_1.8.3.1-20.el7.x86_64.drpm | 2.6 MB 00:00:18 Finishing delta rebuilds of 1 package(s) (4.4 M) ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 总计 126 kB/s | 2.6 MB 00:00:21 Running transaction check Running transaction test Transaction test succeeded Running transaction 正在更新 : git-1.8.3.1-20.el7.x86_64 1/4 正在更新 : perl-Git-1.8.3.1-20.el7.noarch 2/4 清理 : git-1.8.3.1-14.el7_5.x86_64 3/4 清理 : perl-Git-1.8.3.1-14.el7_5.noarch 4/4 验证中 : perl-Git-1.8.3.1-20.el7.noarch 1/4 验证中 : git-1.8.3.1-20.el7.x86_64 2/4 验证中 : git-1.8.3.1-14.el7_5.x86_64 3/4 验证中 : perl-Git-1.8.3.1-14.el7_5.noarch 4/4 更新完毕: git.x86_64 0:1.8.3.1-20.el7 作为依赖被升级: perl-Git.noarch 0:1.8.3.1-20.el7 完毕！ 3）验证安装是否成功 输入命令： git --version，查看安装好的 Git 版本，验证是否安装成功： [root@node3 ~]# git --version git version 1.8.3.1 源码安装 Git 我们会发现，使用 yum 安装之后 Git 版本和当前最新版本之间仍然差很多版本号（Git 版本号不太好控制，取决于 yum 源最新 Git 版本），那么如何解决这个问题？除了使用 yum 安装，还可以使用 Git 源码进行编译安装，我们可以根据需要下载相应版本的 Git 源码包进行安装（源码下载请前往：Git 版本发布界面）。下面我将给出 Centos7 下如何通过源码安装 Git v2.20.0： 安装前我们首先移除系统原有 Git： [root@node3 Download]# yum remove git 1）在目录 Download 中下载相应 Git 版本源码包： [root@node3 Download]# wget https://github.com/git/git/archive/v2.20.0.tar.gz 2）解压 v2.20.0.tar.gz 安装包： [root@node3 Download]# tar -zxvf v2.20.0.tar.gz # 查看当前目录，可看到解压文件： [root@node3 Download]# ls git-2.20.0 v2.20.0.tar.gz 3）准备编译环境，否则后续安装可能发生 Error： [root@node3 Download]# yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel gcc perl-ExtUtils-MakeMaker # 等待安装或更新完成即可... 4）进入解压文件目录 git-2.20.0，然后进行 Git 源码的配置、编译以及安装命令，耐心等待即可： [root@node3 Download]# cd git-2.20.0 # 配置 Git 的安装路径： [root@node3 Download]# ./configure prefix=/usr/local/git/ |--------------------------------------------------------------| ### 注意此处可能报错：&apos;-bash: ./configure: 没有那个文件或目录&apos;。 # 按照如下步骤解决： # 生成 configure： $ make configure $ ./configure prefix=/usr/local/git/ # 如果 make configure 的时候报错：/bin/sh: autoconf: command not found $ yum install install autoconf automake libtool |--------------------------------------------------------------| # 编译以及安装 Git 源码： [root@node3 Download]# make &amp;&amp; make install 5）配置环境变量： # 打开系统配置文件：/etc/profile [root@node3 Download]# vim /etc/profile # 添加如下环境变量： export PATH=$PATH:/usr/local/git/bin # 生效环境变量： $ source /etc/profile 6）验证安装是否成功 [root@node3 Download]# git --version git version 2.20.0 1.4.1-2 Ubuntu 安装方法较新版本的 Debian 或 Ubuntu Linux 系统一般都自带较高版本的 Git。如果没有，直接可通过下列命令就可以完成 Git 的安装: $ sudo apt-get install git 老一点的 Debian 或 Ubuntu Linux，要把命令改为: $ sudo apt-get install git-core 这是由于：有个软件也叫 GIT（GNU Interactive Tools），结果 Git 就只能叫 git-core 了。由于 Git 名气实在太大，后来就把 GNU Interactive Tools 改成 gnuit，git-core 正式改为 git。 1.4.1-3 其它版本 Linux 安装方法其它版本 Linux 可以统一采用源码安装。 1.4.2 Windows 平台上安装 Git这一部分我们来看 Win 下如何安装和配置 Git（未提到安装页面选择默认即可）： Git 安装 1）从 Git 官网下载一个 Git For Windows 安装包（官网下载地址）： 2）双击安装程序，进入欢迎界面阅读协议，然后点击 【 Next 】： 3）选择 Git 安装位置，点击 【 Next 】： 4）选择安装组件：推荐使用默认选项，然后点击 【 Next 】： 图标组件（Additional icons）：选择是否创建快速启动图标和桌面快捷方式 桌面浏览（Windows Explorer integration）使用 Git Bash 方式、Shell 方式 是否关联 Git 配置文件：该配置文件主要显示文本编辑器样式 是否关联 Shell 脚本文件：是否关联 Bash 命令执行脚本文件 使用 TrueType 编码：在命令行中是否使用 TrueType 编码，该编码是微软和苹果公司制定的通用编码 5）在开始菜单创建快捷方式，然后点击 【 Next 】： 6）选择默认的 Git 编辑器（默认 VIM），然后点击 【 Next 】： 7）设置环境，设置命令行工具（默认配置即可），然后点击 【 Next 】： Git自带：使用 Git 自带的 Git Bash 命令行工具； 系统自带以及第三方软件：使用 windows 系统以及第三方软件命令行工具； Git 自带和 Unix Tools：注意，这样会将 windows 中的 find.exe 和 sort.exe 工具覆盖，如果不懂这些尽量不要选择。 8）选择换行格式，然后点击 【 Next 】： 检查出 Windows 格式转换为 Unix 格式：将 Windows 格式的换行转为 Unix 格式的换行再进行提交; 检查出原来格式转换为 Unix 格式：不管什么格式的，一律转换为 Unix 格式的换行再进行提交； 不进行格式转换：不进行转换，检查出什么格式就提交什么格式。 9）配置 Git bash 终端仿真器（默认即可），然后点击 【 Next 】： 使用 MinTTY 终端 使用 Windows 默认的命令行 10）性能配置，是否启用文件系统缓存（默认即可），然后点击 【 install 】 等待安装完成即可： Git 环境变量配置 有时安装完成以后可能在 Windows CMD 中无法正常使用 Git，可以将 GIT_HOME/bin 添加到系统环境变量 path 中，就可以在 CMD 中正常使用 Git了。 1.4.3 Git Bash Here自此我们已经完成了 Git 的安装以及配置，这里我们可以尝试在 Windows 下打开 Git Bash 来看一下： 是不是不同于我们熟悉的 Linux shell 或者 Windows CMD？下面给出一种 Git 控制台美化方法： Git 控制台格式以及字体美化 1）下载必要的配置文件： ### Git 控制台中 clone 美化配置仓库： $ git clone https://github.com/shaonianruntu/Git-setting.git # 查看项目可以发现有 3 个文件： $ ls Git-setting bash_profile_course git-completion.bash git-prompt.sh ### Git 控制台中 clone 字体文件仓库： $ git clone https://github.com/shaonianruntu/YaHei-Consolas-Hybrid-1.12.git 2）配置格式以及字体 # 将 git-completion.bash 配置文件 copy 当前用户主目录； # 将 git-prompt.sh 配置文件 copy 当前用户主目录； # 将 bash_profile_course 配置文件 copy 当前用户主目录，如果当前用户主目录中已存在名为 .bash_profile 的文件，则将 bash_profile_course 中的内容复制并粘贴到 .bash_profile 的尾部。如果不存在，通过 mv 命令将 bash_profile_course 重命名为 .bash_profile。 自此，关闭 Git Bash 再重新打开，你会发现其格式已经变为我们熟悉的 Linux Shell 样式： 有一个 Yahei 和 Consolas 的混合字体，相当漂亮，很适合在 Windows 平台下编程使用，我们之前已经从 GitHub 上 clone 了下来。下面我们通过 注册表配置 来将其应用于控制台： # Win+R 输入 regedit 命令打开注册表。 # 在 [HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersion\Console\TrueTypeFont] 设置： “936” = ”YaHei Consolas Hybrid” 2. 版本仓库用户以及邮箱设置我们知道，Git 是分布式版本控制系统，所以每个机器都必须自报家门：你的名字和 Email 地址。也就是说在使用之前我们还需要为 Git 中的版本仓库设置用户以及邮箱。 通常我们会面临一种场景：公司工作时，我们一般会参与多个项目开发，而向不同的项目提交变更时，一般请求情况下提供的用户都是同一个，而我们为了方便可能会使用全局进行 Git 用户名的配置。但当我们处于学习变更自己的学习项目时，我们可能更多地不希望使用公司公用的用户以及邮箱配置。 模拟场景： 公司参与多个项目：A、B、C、D，用于提交的公司用户名是：”staff_ming”，会使用 Git 全局配置； 个人学习项目：E，用于提交的个人学习用户名是：”Pilot_ming”,会为项目 E 进行单独配置。 全局配置步骤 1）Git Bash 控制台执行： $ git config --global user.name &quot;staff_ming&quot; $ git config --global user.email &quot;staff_ming@xxxx.com&quot; # 使用 git config --list 可查看到 Git 全局配置 2）可在当前用户目录下生成一个 .gitconfig 配置文件，可以查看文件内容中的配置信息： [user] name = staff_ming email = staff_ming@163.com 注意 git config 命令的 --global 参数表示：当前机器上所有的 Git 版本仓库都会使用这个配置。 单独配置项目步骤 1）Git Bash 中进入项目所在目录（后续学习后你会知道 Git 管理下的项目目录会产生一个 .git 目录），执行： $ git config user.name &quot;Pilot_ming&quot; $ git config user.email &quot;Pilot_ming@163.com&quot; # 使用 git config --list 可同时查看到 Git 全局以及当前项目配置 2）可在当前项目目录下的 .git 目录中生成一个 config 配置文件，可以查看其配置信息 3. 初识 Git 版本库前面我们一直在提 Git 分布式系统中的每一个节点（用户设备）都是一个完整的版本库。那什么是版本库呢？版本库又名版本仓库（repository）。 repository 可以简单理解成一个目录，这个目录里面的所有文件都可以被 Git 管理起来。什么时候、什么人更改了目录下文件的什么内容等信息都能被 Git 忠实地跟踪以及记录下来。以便 Git 任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。 以下实操部分测试均在 Windows10 下完成： 3.1 如何创建 Git 版本库其实创建一个全新的 Git 版本库非常简单: 1）创建版本库目录（项目目录） # Windows 系统下，为了避免遇到各种莫名其妙的问题，请确保目录名（包括父目录）不包含中文。 $ mkdir GitTestProject $ cd GitTestProject 2）创建（初始化）版本库： # git init 命令可以将当前目录变成 Git 可以管理的版本库： $ git init Initialized empty Git repository in F:/GitTestProject/.git/ # 查看版本库（产生一个 .git 目录）： $ ll -al total 8 drwxr-xr-x 1 staff_ming 197121 0 12月 20 18:55 ./ drwxr-xr-x 1 staff_ming 197121 0 12月 20 18:46 ../ drwxr-xr-x 1 staff_ming 197121 0 12月 20 18:55 .git/ 注意：初始化后，会在当前目录自动创建 .git 目录（是 Git 来跟踪管理版本库的），该文件是 Git 中最重要的文件夹，因为 Git 相关文件以及版本都将保存 .git 中，.git 受损 Git 版本仓库就会被破坏掉。 事实上，我们更多的时候是需要在非空目录下创建 Git 版本库（Git 管理既有项目），这也是可以的（但不意味着当前目录下的文件或目录已被 Git 所管理）。 3.2 如何添加文件到版本库进行管理在介绍如何使文件能够被 Git 版本库管理之前，我们先来补充一部分很重要内容： |—————————————————————– 首先我们必须明确：所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git 也不例外。 版本控制系统可以告诉你文件每次的变更，比如在第 5 行加了一个单词 “Linux”，在第 8 行删了一个单词 “Windows”。 而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是只知道图片从 100KB 改成了 120KB，但到底改了啥，版本控制系统不知道，也没法知道。最初我使用过一个毕设论文示例其实是不恰当的，Microsoft Word 格式是二进制格式，因此 Git 是没法跟踪 Word 文件的改动的。 还有一个很重要的问题就是文本的编码问题。强烈建议使用标准的 UTF-8 编码，所有语言使用同一种编码，既没有冲突，又被所有平台所支持。 —————————————————————–| 接下来我们简单来看一下文件如何能够被 Git 版本库管理： # 1. 在版本库 GitTestProject 中创建一个用于 Git 管理的新文档：readme.txt，内容如下 Git is a version control system. Git is free software. # 2. 创建好 readme.txt 之后并不意味着文档已被版本库所管理，需要告知 Git 把文件添加到仓库： $ git add readme.txt # 执行上面的命令，没有任何显示。Unix 的哲学是“没有消息就是好消息”，说明添加成功。 # 3. git add 之后，我们还需要提交操作到仓库（后续我们会做解释）： $ git commit -m &quot;wrote a readme file&quot; # 日志信息如下表示成功： # 1 file changed：1 个文件被改动；2 insertions：插入了两行内容 [master (root-commit) c31b827] wrote a readme file 1 file changed, 2 insertions(+) create mode 100644 readme.txt 注意：git commit 命令之后的 -m 参数表示的是：本次向 Git 版本库提交的说明（变更注释）。强烈建议添加变更说明！！！不加 -m 参数 Git 会跳转进入 VIM。 我们之前提到过需要在非空目录下创建 Git 版本库，git init 之后版本库目录下的文件其实并没有被 Git 管理，想要将其填加到版本库的话可以（以 GitTestProject 版本库下 git init 有一个 Server 目录，Server 目录下有一个文件 service.sh 为例）： # git add + dir(目录)会将 dir 以及其下所有文件添加到 Git 版本库： $ git add Server/ Client/ # 提交： $ git commit -m &apos;Add Server&amp;Client API to Project&apos; [master d9e0247] Add Server API 1 file changed, 1 insertion(+) create mode 100644 Server/service.py 补充：git add . 可以将当前目录下所有文件添加到版本库。 ​ ​]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Setup</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jupyter Notebook（JupyterLab） 远程访问服务器]]></title>
    <url>%2FAnaconda%2FJupyter-Notebook%EF%BC%88JupyterLab%EF%BC%89-%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 熟悉 Python 的同学应该都知道 JupyterLab（Jupyter Notebook） 这一数据分析神器，它能帮助我们有效地组织输入输出，将我们探索数据的过程记录下来，后续稍加整理便可以生成一篇分析报告或学习记录。Jupyter Notebook 支持 Markdown，也支持 Python、R 甚至 Julia 等语言，完全可以满足一个数据工作者的大多数分析需求。 然而考虑以下场景： 通常我们更加专注于本地机器开发，实际运行环境是远程服务器。我们会苦恼于本地机器[Local_Server]和远程服务器[Remote_Server]上都要配置相同的机器学习环境（Anaconda），这既消耗时间又占用硬盘资源。 想要在某一台本地机器上访问远程服务器上的 Jupyter Notebook 文档。一方面我们需要在本地机器重新搭建 Jupyter Notebook 环境；另一方面我们需要将目标 Jupyter Notebook 文档 Download 到本地机器？ 当你也面临这些场景时，你就有必要考虑搭建一个可以远程访问的 Jupyter Notebook 或者 JupyterLab 服务了。所以今天花了点时间研究了下 Jupter notebook（JupyterLab） 远程访问服务器（在Remote_Server 安装 Jupter ，然后在本地 Local_Server 可以访问 Remote_Server ，使用其内部搭建好的机器学习环境），所以记录一下。 1. Jupyter notebook（JupyterLab）User Tutorial我们知道：Jupyter notebook（JupyterLab） 是一个基于浏览器的 python 数据分析工具，使用起来非常方便，具有极强的交互方式和富文本的展示效果，安装非常方便。 注意：JupyterLab &amp;&amp; Jupyter Notebook 可以作为单独的数据分析工具使用，故可以单独安装使用。为什么强调单独安装使用呢？事实上，Anaconda 中已经将其作为内部的插件，可以极其方便的使用 Anaconda 的虚拟隔离环境。所有，更常见的是配合 Anaconda 一起使用。 故，一般我们习惯在 Anaconda 环境下使用 Jupyter notebook（JupyterLab），并且 Anaconda 安装包中自带了 Jupyter 。安装 Anaconda 时会默认将 Jupyter notebook（JupyterLab） 安装到 Anaconda 的 base 环境（免安装）。 当然，你也可以在其它虚拟环境中再次安装。这里我们给出在深度学习服务器 Anaconda 虚拟环境安装配置 Jupter 服务的简明步骤： 通常，首先需要根据我们已创建的虚拟开发环境，选择【Jupyter】服务安装的虚拟环境（例如： DeepLearning(python2.7) 或者 DeepLearning(python3.5）），然后激活相应虚拟环境，在其中安装 Jupyter ，完成后续配置后，我们就可以在相应的虚拟环境中启动 Jupyter 进行使用了。 JupyterLab 与 Jupyter Notebook 师出同源，配置过程完全一样（除了安装）。下面我将会以 Jupter notebook 安装以及配置为主（JupyterLab 与之有差异的地方会另外标注出）介绍如何搭建远程访问。 在安装以及配置 Jupter notebook（JupyterLab）之前，我们先给出 Jupter 服务的两种访问方式： 本地访问：部署 Jupter 服务的服务器本地浏览器中访问； 远程访问：远程浏览器中访问部署在服务器上的 Jupter 服务（常用访问模式）。 下面我们来看如何安装和部署 Server Jupter 服务的本地访问和远程访问： 1.1 本地访问这里的本地访问是指在远程服务器（Remote_Server）本地实现访问，首先需要登录远程服务器，然后： 【1】明确所处虚拟环境 首先我们需要明确我们是使用 Anaconda base 环境中自带的 Jupyter notebook（JupyterLab）还是在相应的虚拟环境中安装新的 Jupyter notebook（JupyterLab）？？？ –&gt; Anaconda base envs 如果我们使用 Anaconda base 环境中自带的 Jupyter notebook（JupyterLab）可以直接跳过【2】 安装步骤 开始【3】 本地使用。 注意，如果安装的是 Anaconda 2 的话，base 环境是不支持 JupyterLab 的（无法安装）。但此时可以在其创建的 conda envs 中安装 JupyterLab 。 –&gt; Other conda envs (create) 如果在其它新创建的 conda envs 中安装配置的话，我们需要先切换到相应环境中： 1source activate envsname 【2】 安装步骤： 检查 Remote_Server 是否有安装 jupyter notebook（JupyterLab）服务，如果没有则安装。 1234567891011### --&gt; jupyter Notebook 检测与安装：# 终端输入 jupyter notebook,如果报错就是没有安装，那么就要用下面命令安装。(安装完成后可重新检查：$ jupyter notebook)$ pip install jupyter notebook### --&gt; jupyter jupyterlab 检测与安装：# 检测：$ jupyter lab# 安装（注意，Python2.7 中不支持 jupyterlab）$ pip install jupyter jupyterlab 【3】 本地使用 12345# 1. Terminal 下先使用下面的命令开启 notebook service：（之后便可以在浏览器中使用，默认下只能在远程服务器本地进行访问）$ jupyter notebook# 开启 jupyterlab service：$ jupyter lab 12345# 2. 在 [`Remote_Server`] 浏览其中输入 url：（就可以实现本地访问了）$ http://localhost:8888# 访问：jupyter jupyterlab$ http://localhost:8888/lab 补充如何查看当前已启动的 Jupyter 服务： 12345$ jupyter notebook list# 或：$ jupyter notebook list 1.2 远程访问Jupyter notebook（JupyterLab）默认只能在本地访问，如果想把它安装在服务器上，然后在本地远程访问 Jupyter 服务器，则需要进行如下配置： 【1】 相关配置： 123456789101112131415161718192021222324# 1. 生成配置文件$ jupyter notebook --generate-config# Log：Write default config to : ~/.jupyter/jupyter_notebook_config.py# 2. 打开ipython，创建一个密文的密码：$ ipythonIn [1]: from notebook.auth import passwdIn [2]: passwd()Enter password: Verify password: Out[2]: 'sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274'# 3. 复制密文，并 `exit` ipython# 4. 修改默认配置文件$ vim ~/.jupyter/jupyter_notebook_config.py# 添加如下内容：c.NotebookApp.ip='*'c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口c.IPKernelApp.pylab = 'inline'# 5. 至此，配置完成。接下来，看如何使用 【2】 远程访问 12345# 1. 在 [`Remote_Server`] 上启动 Jupter notebook(root 用户下安装需要加参数： --allow-root)，从 Log 可以获取到开放的端口号，该远程连接切勿关闭！！！$ jupyter notebook# 开启 jupyter jupyterlab service：$ jupyter lab 12345# 2. 远程访问：在本地[`Local_Server`]中浏览器直接访问[`Remote_Server`]$ https://address_of_remote:8888# 远程访问：jupyter jupyterlab$ https://address_of_remote:8888/lab 补充启动方式，可以指定启动时的端口： 12345# --ip：Remote_Server_ip , --port:开放端口$ jupyter notbook --ip 0.0.0.0 --port 9999# 远程访问：$ https://address_of_remote:9999 1.3 配置问题1）配置远程访问后，在启动时报错：KeyError：&#39;allow_remote_access&#39;。解决方法如下： 1234$ vim ~/.jupyter/jupyter_notebook_config.py# 补充添加如下语句：c.NotebookApp.allow_remote_access = True 2. Switching Conda Envs（Kernels） In Jupyter在 Jupyter 中进行深度学习实验时，肯定会涉及到不同的环境配置，看如下场景： Python3 和 Python2 的不同虚拟环境。 不同版本的 tensorflow 的虚拟环境。 这就要求我们需要随时切换虚拟环境。 -&gt; 给一种解决方案: 从上面 Jupyter notebook（JupyterLab）安装配置中知道，我们可以在不同的虚拟环境中分别安装 Jupyter notebook（JupyterLab），然后通过在不同的 conda 环境中启动 Jupyter notebook（JupyterLab）实现切换不同的 Jupyter 环境。 –&gt; 引发一个问题：这不是有病么？既浪费系统资源又浪费时间！ 那么，有么有一种办法我只在一个虚拟环境下启动 Jupyter notebook（JupyterLab）却可以在 Jupyter 中切换不同的虚拟运行环境？？？答案肯定是有的。 下面我们给出一种合理的解决思路： 2.1 Install ipykernel需要用到一个 ipykernel 插件，首先，我们选择在 Anaconda base 环境安装： 1$ conda install ipykernel 此时，我们可以在 Anaconda base 中启动 Jupyter notebook（JupyterLab），发现启动后只有一个原生的 base 环境。 2.2 添加虚拟环境到 Jupyter事实上，这一部分就是将各种创建好的 conda 虚拟环境（也称为不同的 Kernel）添加 Jupyter Kernel 中。 【1】激活虚拟环境 1$ source activate myenv 【2】将环境写入 notebook 的 kernel 中 1python -m ipykernel install --user --name myenv --display-name &quot;Python (myenv)&quot; 推荐将 --name 和 --display-name 设置为当前虚拟环境的名称。 如果显示 “No module named ipykernel”，说明当前虚拟环境没有安装 ipykernel 插件： 1$ conda install ipykernel 其它环境添加也一样： 12source activate other-envpython -m ipykernel install --user --name other-env --display-name "Python (other-env)" 【3】测试 重新在 Anaconda base 环境中启动 Jupyter notebook（JupyterLab），可以看到 Jupyter 中已经可以切换其它虚拟环境了。 注意：切换 Jupyter notebook（JupyterLab）虚拟环境其实相当于切换其 Kernel（内核）。 2.3 Kernel 移除有时，由于添加了错误的内核或者不想要该内核，我们需要移除 Jupyter notebook（JupyterLab）虚拟环境内核（Kernel）。 首先，我们来看如何查看当前系统中安装的内核以及其位置： 1$ jupyter kernelspec list –&gt; 如何移除？ 1$ jupyter kernelspec remove myenv myenv 表示 Kernel 所对应的虚拟环境的名称。 移除后，重新启动 Jupyter notebook（JupyterLab）即可发现内核已经被移除。]]></content>
      <categories>
        <category>Anaconda</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
        <tag>Jupyter Notebook</tag>
        <tag>JupyterLab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda 生态环境初体验]]></title>
    <url>%2FAnaconda%2FAnaconda-%E7%94%9F%E6%80%81%E7%8E%AF%E5%A2%83%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 前面我们已经介绍了 Anaconda 基本安装和使用教程。这一部分我们来看 Anaconda 生态环境中的常用支持。 1.1 与 JetBrains PyCharm 连接在工作环境中我们会选择在集成开发环境去编码,正如我们前面推荐的 PyCharm, 而 PyCharm 也能很方便的配置和 anaconda 的虚拟环境结合使用。 在 File =&gt; Settings =&gt; Project:XXX =&gt; Project Interpreter 里面修改 Project Interpreter,点击齿轮标志再点击 Add Local 为你某个环境的 python.exe 解释器就行了： 比如你要在 DeepLearning 环境中编写程序, 那么就修改为 E:\Anaconda3\envs\DeepLearning\python.ext。同时可以看到这时候下面的依赖包也变成了 DeepLearning 环境中的包了.接下来我们就可以在 Pycharm 中愉快的编码了。 2.2 Windows 平台 Anaconda 初体验Windows 环境下，按下 Windows 徽标键，调出 Windows 开始菜单。我们可以看到 “最近添加” 的：Anaconda3(64-bit)。 Anaconda3（64-bit）中包含：Anaconda Navigator、Anaconda Prompt、Jupyter Notebook 以及 Spyder 等菜单项。下面我们来看如何使用这些功能项： 2.2.1 Anaconda Prompt打开 Anaconda Prompt，这个窗口和 DOS 窗口一样的，输入命令就可以控制和管理 Python 环境。最常用的是 conda 命令，前面我们介绍的所有 conda 命令此软件都集成了，你可以直接用（可以将其看作一个 Anaconda 内嵌的用于执行 conda 命令行指令的工具）。点开的话如下图： 进入后你会发现我们处于 Anaconda 的 base 环境中。 同时，安装完 anaconda，就相当于默认安装了 Python、IPython、集成开发环境 Spyder、部分包等等。你可以在 Windows 下的 cmd 下启动相应组件。以启动 IPython 为例： 2.2.2 Anaconda Navigator从其名称（Anaconda 导航员）即可看出，它应该是一个用于帮助用户快速使用 Anaconda 的工具。确实，Anaconda Navigator 是用于管理工具包和环境的图形用户界面，同时我们可以快速启动 Anaconda 内嵌的一些工具：JupyterLab、Jupyter Notebook、IPython、Spyder 等。 2.2.3 Jupyter NotebookJupyter Notebook 是基于 web 的交互式计算环境。支持富文本，可以编辑易于人们阅读的文档。所以常用于展示数据分析的过程（强烈推荐使用）。 我们还可以为远程服务器配置 Jupyter Notebook 服务后，实现远程调用（一方面可以在图形化界面进行开发编程；另一方面免除了进程需要在本地搭建搭建一套和服务器相同开发环境的问题），体验度极高。 2.2.4 SpyderSpyder 一个使用 Python 语言、跨平台的、科学运算集成开发环境（Anaconda 内嵌的一款 IDE）。点击 Anaconda Navigator ，第一次启用，会初始化，耐心等待一段时间，加载完成，界面如图: Spyder 编辑器，没有安装 IDE 的同学可以直接选用这款编辑器来编写代码，它最大的优点就是模仿 MATLAB 的“工作空间”。spyder.exe 放在安装目录下的 Scripts 里面，可以将其发送到桌面快捷方式进行使用。 2.2.5 JupyterLab打开 JupyterLab 会在默认浏览器打开 http://localhost:8888/lab 这样一个东东，这里就可以打开 Jupyter Notebook、Python 解释器以及 终端（Terminal）。 我们可以打开 Anaconda Navigator -&gt; Launch jupyterlab ，也可以直接在浏览器输入 http://localhost:8888/lab （可以保存为书签）。如果是布置在云端，可以输入服务器域名（IP），是不是很爽？ 2.2.6 Orange3交互式数据可视化，通过巧妙的数据可视化执行简单的数据分析。探索统计分布，箱形图和散点图，或深入了解决策树，层次聚类，热图，MDS和线性投影。即使您的多维数据也可以在 2D 中变得合理，特别是在智能属性排名和选择方面。 据说：老师和学生都喜欢它。在教授数据挖掘时，我们喜欢说明而不是仅仅解释。而橙色很棒。Orange在世界各地的学校，大学和专业培训课程中使用，支持数据科学概念的实践培训和视觉插图。甚至还有专门为教学设计的小部件。 附加组件扩展功能。使用 Orange 中可用的各种附加组件从外部数据源挖掘数据，执行自然语言处理和文本挖掘，进行网络分析，推断频繁项目集并执行关联规则挖掘。此外，生物信息学家和分子生物学家可以使用Orange通过差异表达对基因进行排序并进行富集分析。]]></content>
      <categories>
        <category>Anaconda</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda 介绍、安装以及使用教程]]></title>
    <url>%2FAnaconda%2FAnaconda-%E4%BB%8B%E7%BB%8D%E3%80%81%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇 我们知道：Python是一种面向对象的解释型计算机程序设计语言，其使用具有跨平台的特点，可以在 Linux、macOS 以及 Windows 系统中安装环境并使用。也就是说，其编写的代码在不同平台上运行时，几乎不需要做较大的改动，使用者无不受益于它的便捷性。 此外，Python的强大之处在于它的应用领域范围之广，遍及人工智能、科学计算、Web开发、系统运维、大数据及云计算、金融、游戏开发等等。而实现其强大功能的前提，就是 Python 具有数量庞大且功能相对完善的标准库和第三方库。通过对库的引用，能够实现对不同领域业务的开发。 然而正是由于库的数量庞大，对于这些库的管理维护成为既重要但复杂度又高的事情，这对于 Python 开发人员来说是极不友好的。同时 Python 多版本控制也是 Python 开发过程中极其常见的并且难以管理的。此时，Anaconda 粉墨登场… Anaconda 提供一种使用虚拟隔离环境来解决库管理以及维护问题的策略，它通过 conda 工具解决了 Python 开发者的两大痛点： 提供包管理，功能类似于 pip，Windows 平台安装第三方包经常失败的场景得以解决。 提供虚拟环境管理，功能类似于 virtualenv，解决了多版本 Python 并存问题。 话不多说，下面我们开始 Anaconda 学习： 1. What is Anaconda？Anaconda 官方地址：https://www.anaconda.com/ 1.1 简介Anaconda 就是可以便捷获取包且对包进行管理，同时对环境可以统一管理的开源的 Python 发行版本。其包含了conda、Python 等 180 多个科学包及其依赖项。 由于包含了大量的科学计算包，Anaconda 的下载文件比较大（约 531 MB），如果只需要某些包，或者需要节省带宽或存储空间，也可以使用 Miniconda 这个较小的发行版（仅包含 conda 和 Python）。 1.2 特点（优点）鉴于 Anaconda 拥有的 conda 环境管理器、conda 包、1,000+ 开源库 等，Anaconda 具有如下特点： 开源 安装过程简单 高性能使用 Python 和 R 语言 免费的社区支持 1.3 Anaconda、conda、pip、virtualenv 的区别进行Python开发过程中，你肯定使用或者听说过 Pip、Virtualenv，再加上这里我们介绍的 Anaconda、conda 工具。下面我们来横向比较一下这些工具的特点: 1）AnacondaAnaconda 是一个包含 180+ 的科学包及其依赖项的 Python 发行版本。其包含的科学包包括：conda, numpy, scipy, ipython notebook 等。 2）conda conda 是包及其依赖项和环境的管理以及维护工具 conda 为 Python 项目而创造，但同样也适用于：Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN 等语言。 适用平台：Windows, macOS, Linux conda 用途： 12# 通过 conda 快速安装、运行和升级包及其依赖项。# 使用 conda 可以在计算机中便捷地创建、保存、加载和切换环境。 前面我们提到过多版本 Pyhton 环境的问题。如果使用 conda ，仅需要几条命令，你就可以创建一个个完全独立的环境来运行不同的 Python 版本，同时你可以继续在常规的环境中使用你常用的 Python 版本。 3）pip pip 是常用的用于安装和管理软件包的包管理器。 pip 编写语言：Python。 Python 中默认安装的版本： 12# Python 2.7.9 及后续版本：默认安装，命令为 pip# Python 3.4 及后续版本：默认安装，命令为 pip3 pip 名称的由来：pip 采用的是递归缩写进行命名的。其名字被普遍认为来源于两处： 12# “Pip installs Packages”（“ pip 安装包”）# “Pip installs Python”（“pip 安装 Python”） 4）Virtualenv Virtualenv：用于创建一个独立的 Python 环境的工具。 解决问题： 12341. 当一个程序需要使用 Python 2.7 版本，而另一个程序需要使用 Python 3.6 版本，如何同时使用这两个程序？2. 如果将所有程序都安装在系统下的默认路径（如：/usr/lib/python2.7/site-packages），当不小心升级了本不该升级的程序时，将会对其他的程序造成影响。3. 如果想要安装程序并在程序运行时对其库或库的版本进行修改，都会导致程序的中断。4. 在共享主机时，无法在全局 site-packages 目录中安装包。 Virtualenv 将会为它自己的安装目录创建一个环境，这并不与其他 virtualenv 环境共享库；同时也可以选择性地不连接已安装的全局库。 我们已经知道，pip 和 conda 都可以用于安装和管理 Python 相关软件包以及其依赖项。那么它们有什么区别？ 5）pip 与 conda 比较1 –&gt; 依赖项检查 pip： 12# 不一定会展示所需其他依赖包。# 安装包时或许会直接忽略依赖项而安装，仅在结果中提示错误。 conda： 罗列出所需其他依赖包。 安装包时自动安装其依赖项。 可以便捷地在包的不同版本中自由切换。 注意：同样可以在 conda 独立环境中使用环境内安装的 pip 进行软件包的安装，所以在 conda 封装的独立环境中我们可以同时使用 conda、pip 进行包管理。 2 –&gt; 环境管理 pip：维护多个环境难度较大。 conda： 比较方便地在不同环境之间进行切换，环境管理较为简单。 3 –&gt; 对系统自带 Python 的影响 pip：在系统自带 Python 中包的 更新/回退版本/卸载 将影响其他程序。 conda：多版本环境实现隔离，不会影响系统自带 Python。 4 –&gt; 适用语言 pip：仅适用于 Python。 conda：适用于 Python, R, Ruby, Lua, Scala, Java, JavaScript, C/C++, FORTRAN。 5 –&gt; conda 与 pip、Virtualenv 的关系 你可以认为：conda 结合了 pip 和 virtualenv 的功能。 2. How to setup Anaconda？这一部分我们来看，Anaconda 安装环境要求以及如何快速完成 Anaconda 的安装。 2.1 安装环境需求 Anaconda 平台支持：Windows、macOS、Linux（x86 / Power8） 系统位数要求：32 位或 64 位系统均可 下载文件大小：约 500 MB（建议预留一定空间（至少 3 G），用于存放各种科学包以及 Python 库） 2.2 Anaconda 安装Anaconda 官方下载地址：https://www.anaconda.com/download/ 检测好安装环境后，下面我们正式开始 Anaconda 的安装过程（Windows、Linux）： 2.2.1 在 Windows 平台安装 Anaconda1) 安装包下载 前往官方下载页面选择 Windows 下载项。你会发现，有两个版本可供选择：Python 3.7（最新 Python3.X） 和 Python 2.7。选择版之后根据自己操作系统的情况点击 “64-Bit Graphical Installer” 或 “32-Bit Graphical Installer” 进行安装包下载。 例如：Anaconda3-5.2.0-Windows-x86_64.exe 2）完成下载之后，双击下载文件，启动安装程序开始安装,Next： 3）阅读许可证协议条款，然后勾选 “I Agree” 进行下一步。 4）Install for: Just me 还是 All Users。假如你的电脑有多个 Users ，可以选择考虑 All Users，一般选择 Just me 即可。Next： 5）设置 Anaconda 安装位置，默认是安装到 C:\Users\XXX\Anaconda3。Next: 6） 配置 Advanced Options（高级选项）： 在 “Advanced Options” 设置中，不要勾选 “Add Anaconda to my PATH environment variable.” （“添加 Anaconda 至我的环境变量。”）。因为如果勾选，则将会影响其他程序的使用。如果想要使用 Anaconda，则通过打开 Anaconda Navigator 或者在开始菜单中的 “Anaconda Prompt”（类似 DOS）中进行使用。 我们选择 “Register Anaconda as my default Python 3.6” 。Insatll： Anaconda 安装过程还是很漫长的，请耐心等待… 经过漫长的等待，终于安装完成 Installation Complete （安装完成）了，点击最后一个 Next&gt;。之后会出现推荐安装 IDE：Microsoft Visual Studio Code 界面： 如果需要安装 Microsoft VSCode，点击 Install Microsoft VSCode；否则我们选择 Skip&gt;: 7）“Thanks for installing Anaconda!” 进入 “Thanks for installing Anaconda!” 界面则意味着安装成功，点击 “Finish” 完成安装即可。 注意：如果你不想了解 “Anaconda云” 和 “Anaconda支持” ，则可以选择不勾选 “Learn more about Anaconda Cloud” 和 “Learn more about Anaconda Support”。 8）安装验证。可选用以下任意方法 “开始（Win） → Anaconda3（64-bit）→ Anaconda Navigator”。若可以成功启动 Anaconda Navigator，进入如下界面，则说明安装成功。 “开始（Win） → Anaconda3（64-bit）→ 右键点击 Anaconda Prompt → 以管理员身份运行”，在 Anaconda Prompt 中输入 conda list / conda –version，可以查看已经安装的包名和版本号。若结果可以正常显示，则说明安装成功。 9）配置环境变量 可能你不太信任 Anaconda Prompt 或者你更加习惯在 windows DOS 下使用命令行，这时时需要配置环境变量的。【控制面板\系统和安全\系统\高级系统设置\环境变量\用户变量\PATH】 中添加 anaconda 的安装目录的 Scripts 文件夹, 比如我的路径是 【E:\Anaconda3\Scripts 】，看个人安装路径不同需要自行调整。 之后就可以打开命令行提示符界面(最好用管理员模式打开) 输入 conda –version 完成验证了。 2.2.2 在 Linux 平台安装 Anaconda1）获取安装包前往官方下载页面选择 Linux 下载项。你会发现，有两个版本可供选择：Python 3.7（最新 Python3.X） 和 Python 2.7。这里我们提供两种方式用于下载适用于 Linux 的 Anaconda 版本： 如果是从官网直接下载安装包，我们需要将下载好的安装包上传到服务器。局域网还好，上传速度一般都比较快。如果服务器是一台阿里云服务器（外网服务器），上传速度是很慢的，而直接采用 wget 方式直接下载的速度取决于服务器网络带宽。 获取下载链接，然后通过 wget 直接下载到服务器。如下： 12345# Python 2.7：$ wget -c https://repo.anaconda.com/archive/Anaconda2-5.3.1-Linux-x86_64.sh# Python 3.7：$ wget -c https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh 下载完成后，Centos/ubuntu系统 ~/Downloads 下会有一个文件：Anaconda3-5.3.1-Linux-x86_64.sh（或：Anaconda2-5.3.1-Linux-x86_64.sh）；它就是我们要安装的 anaconda3（anaconda2） 的安装包。 下面我以 “ Anaconda3 in Linux 的安装以及配置” 为样例介绍 Anaconda 安装过程（Anaconda2 安装过程同理）： 2）Anaconda3 in Linux 的安装以及配置过程注意：安装前请确认 anaconda3 的安装用户（user）！！！假设我们的 anaconda3 安装包存放路径为： ~/Downloads。 [1] 在 ~/Downloads 路径下执行： $ bash ~/Downloads/Anaconda3-5.0.1-Linux-x86_64.sh 注意：除非被要求使用 root 权限，否则均选择 “Install Anaconda as a user”。 [2] “In order to continue the installation process, please review the license agreement.” 安装过程中，你会看到提示 “In order to continue the installation process, please review the license agreement.”（“请浏览许可证协议以便继续安装。”），点击 “Enter” 查看 “许可证协议”。 在 “许可证协议” 界面将屏幕滚动至底，输入 “yes” 表示同意许可证协议内容。然后进行下一步。。 [3] 设置安装路径 安装过程中，提示 “Press Enter to accept the default install location, CTRL-C to cancel the installation or specify an alternate installation directory.”（“ 按回车键确认安装路径，按 ‘CTRL-C’ 取消安装或者指定安装目录。 ”） 如果接受默认安装路径，则会显示 “PREFIX=/home/{~username}/anaconda” 并且继续安装。安装过程大约需要几分钟的时间。 建议：直接接受默认安装路径。 [4] 设置环境变量 安装过程中最后安装器会询问 “Do you wish the installer to prepend the Anaconda install location to PATH in your /home/{~username}/.bashrc ?”（“ 你希望安装器添加 Anaconda 安装路径在 【/home/{~username}/.bashrc 】文件中吗？ ”）。建议输入 “yes” 进行自动配置。 注意： 路径 /home/{~username}/.bash_rc 中 “{~username}” 即用户目录，取决于安装用户。 如果输入 “no”，则需要手动添加路径，否则 conda 将无法正常运行。 –&gt; 如何手动添加环境变量？： 安装完成后，可以察看 ~/.bashrc 文件末尾是否有追加 anaconda相关的 export PATH,如果没有，则执行以下命令： $ echo &apos;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrc [5] Thank you for installing Anaconda! 当看到 “Thank you for installing Anaconda!” 则说明已经成功完成安装。安装完成后，会产生一个 ~/anaconda3 目录，即安装目录。 [6] 更新 .bashrc 使其生效 关闭终端，然后再打开终端以使安装后的 Anaconda 启动。或者直接在终端中输入以下命令： $ source ~/.bashrc [7] 安装验证。可选用以下任意方法: 在终端中输入命令 condal list / conda --version，如果 Anaconda 被成功安装，则会显示 conda 已经安装的包名和版本号。 在终端中输入 python，这条命令将会启动 Python 交互解释器界面。如果 Anaconda 被成功安装并且可以运行，则将会在 Python 版本号的右边显示 “Anaconda custom (64-bit)”。退出 Python 交互界面则输入 exit() 或 quit() 即可。 在终端中输入 anaconda-navigator。如果 Anaconda 被成功安装，则 Anaconda Navigator 将会被启动。 3. Anaconda 包以及虚拟环境管理工具：conda安装好 Anaconda 后，我们就可以用 Anaconda 来创建我们一个个独立的 Python 环境了。接下来均是以命令行模式进行操作的，Windows 用户请打开 “Anaconda Prompt”（配置 DOS 环境后可以之间在 DOS 下操作）；macOS 和 Linux 用户请打开 “Terminal”（“终端”）进行操作。 3.1 conda前面我们提过，Anaconda 下的包管理和环境管理是鉴于 conda 命令行工具得以实现的。我们先来看一下 anaconda 安装之后，如何来快速管理 conda： 1）验证 conda 已被安装 $ conda --version / conda -V # 或 $ which conda（Linxu） 终端上将会以 conda 版本号的形式显示当前安装 conda 的版本号。如：conda 3.11.0 注意：如果出现错误信息，则需核实是否出现以下情况： 使用的用户是否是安装 Anaconda 时的账户。 是否在安装 Anaconda 之后重启了终端。 2）更新 conda 至最新版本 # 升级 conda： $ conda update conda 执行命令后，conda 将会对版本进行比较并列出可以升级的版本。同时，也会告知用户其他相关包也会升级到相应版本。当较新的版本可以用于升级时，终端会显示 Proceed ([y]/n)?，此时输入 y 即可进行升级。 3）如何查看 conda 帮助信息？ $ conda --help #或 $ conda -h 4）卸载 conda [1] Linux 或 macOS： $ rm -rf ~/anaconda2(anaconda3) 即删除 Anaconda 的安装目录。根据安装的 Anaconda 版本选择相应的卸载命令。 [2] Windows 控制面板 → 添加或删除程序–&gt; 选择 “Python X.X (Anaconda)” –&gt; 点击“删除程序” 即可。 3.2 conda 多环境管理base 环境 Window 下 activate 命令（Linxu/Mac 下是 source activate）能将我们引入 anaconda 设定的虚拟环境中, 如果你后面什么参数都不加那么会进入 anaconda 自带的 base 环境（anaconda 基础运行环境，anaconda 运行也是需要 Pyhton 支持的）。命令行前面也会多一个(base) 说明当前我们处于的是 base 环境下。 安装配置好 anaconda 之后，通过在终端输入 python，我们可以进入一个 Python 交互式解释器（带有 Anaconda custom (64-bit)，已经不是系统原生的 Pyhton 解释器了），这个解释器实质就是 base 环境中的 python 解释器。 我们当然不满足一个 base 环境, 我们更加期望的是更加广阔的天地—可以自由定制的使用多版本虚拟环境。 3.2.1 如何创建新环境？conda 创建环境指令： $ conda create --name &lt;env_name&gt; &lt;package_names&gt; #或 $ conda create -n &lt;env_name&gt; &lt;package_names&gt; 命令说明： &lt;env_name&gt; 即创建的环境名。建议以英文命名，且不加空格（名称两边不需要加尖括号 “&lt;&gt;”）。 &lt;package_names&gt; 即安装在环境中的包名（名称两边不加尖括号 “&lt;&gt;”），如：python。 如果要安装指定的版本号，则只需要在包名后面以 “=” 和版本号的形式执行。如：conda create –name python2 python=2.7，即创建一个名为 “python2” 的环境，环境中安装版本为 2.7 的 python。 如果要在新创建的环境中创建多个包，则直接在 &lt;package_names&gt; 后以空格隔开，添加多个包名即可。如：conda create -n python3 python=3.5 numpy pandas，即创建一个名为 “python3” 的环境，环境中安装版本为 3.5 的 python，同时也安装了 numpy 和 pandas。 3.2.2 如何切换环境？1）激活环境 [1] Linux 或 macOS $ source activate &lt;env_name&gt; 例如：source activate python3 [2] Windows $ activate &lt;env_name&gt; 例如：activate python3 提示信息： 如果创建环境后安装 Python 时没有指定 Python 的版本，那么将会安装与 Anaconda 版本相同的 Python 版本，即如果安装 Anaconda 第2版，则会自动安装 Python2.x；如果安装 Anaconda 第3版，则会自动安装 Python 3.x。 当成功切换环境之后，在该行行首将以 “(env_name)” 或 “[env_name]” 开头。其中，“env_name” 为切换到的环境名。 2) 退出环境 [1] Linux 或 macOS $ source deactivate [2] Windows $ deactivate 提示信息： 当执行退出当前环境命令后，原本行首以 “(env_name)” 或 “[env_name]” 开头的字符将不再显示。 3.2.3 如何查看已创建环境？当我们同时管理多个版本环境时，很容易忘记环境名称，那么如何查看 anaconda 中已经创建好的环境呢？ $ conda info --envs 或 $ conda info -e 或 $ conda env list 如下显示，星号 “*” 所在行即为当前所在环境： 3.2.4 如何复制已创建环境？$ conda create --name &lt;new_env_name&gt; --clone &lt;copied_env_name&gt; 命令说明： &lt;copied_env_name&gt;：为被 复制/克隆 的环境名（环境名两边不加尖括号 “&lt;&gt;”）。 &lt;new_env_name&gt;：为复制之后新环境的名称（环境名两边不加尖括号 “&lt;&gt;”）。 如：conda create –name py2 –clone python2 。即为克隆名为 “python2” 的环境，克隆后的新环境名为 “py2”。此时，环境中将同时存在 “python2” 和 “py2” 环境，且两个环境的配置相同。 3.2.5 如何删除已创建环境？$ conda remove --name &lt;env_name&gt; --all 命令说明： &lt;env_name&gt;:为被删除环境的名称（环境名两边不加尖括号 “&lt;&gt;”）。 3.2.6 如何导入导出环境？如果想要导出当前环境的所有包的信息，即将包信息存入 yaml 文件中： $ conda env export &gt; environment.yaml 此时，如果需要重新创建一个相同的虚拟环境时可以用（用配置文件创建新的虚拟环境）： $ conda env create -f environment.yaml 3.3 conda 包管理conda 除了多环节管理功能之外，还具有包管理功能（类似于 pip）。 3.3.1 查找可供安装的包版本1）精确查找 $ conda search --full-name &lt;package_full_name&gt; 命令说明： –full-name：为精确查找的参数。 &lt;package_full_name&gt;：是被查找包的全名（包名两边不加尖括号 “&lt;&gt;”）。 例如：conda search –full-name python 即查找全名为 “python” 的包有哪些版本可供安装。 2）模糊查找 $ conda search &lt;text&gt; 命令说明： 是查找含有此字段的包名（此字段两边不加尖括号 “&lt;&gt;”）。 例如：conda search py 即查找含有 “py” 字段的包，有哪些版本可供安装。 3.3.2 获取当前环境中已安装的包信息 执行以下命令后将在终端显示当前环境已安装包的包名及其版本号: $ conda list 3.3.3 如何安装第三方包1）在指定环境中安装包 $ conda install --name &lt;env_name&gt; &lt;package_name&gt; 命令说明： &lt;env_name&gt;：即将包安装的指定环境名（环境名两边不加尖括号 “&lt;&gt;”）。 &lt;package_name&gt;：即要安装的包名（包名两边不加尖括号“&lt;&gt;”）。 例如：conda install –name python2 pandas 即在名为 “python2” 的环境中安装 pandas 包。 2）在当前环境中安装包 $ conda install &lt;package_name&gt; 命令说明： &lt;package_name&gt;:即要安装的包名(包名两边不加尖括号 “&lt;&gt;”)。 执行命令后在当前环境中安装包。 例如：conda install pandas 即在当前环境中安装 pandas 包。 3) 使用 pip 安装包 –&gt; 使用场景 当使用 conda install 无法进行安装时，可以使用 pip 进行安装。例如：see 包。 –&gt; 命令 $ pip install &lt;package_name&gt; –&gt; 注意 pip 只是包管理器，无法对环境进行管理。因此如果想在指定环境中使用 pip 进行安装包，则需要先切换到指定环境中，再使用 pip 命令安装包。 pip 无法更新 python，因为 pip 并不将 python 视为包。 pip 可以安装一些 conda 无法安装的包；conda 也可以安装一些 pip 无法安装的包。因此当使用一种命令无法安装包时，可以尝试用另一种命令。 4）从 Anaconda.org 安装包 Anaconda.org 官方网址：http://anaconda.org –&gt; 使用场景 当使用 conda install 无法进行安装时，可以考虑从 Anaconda.org 中获取安装包的命令，并进行安装。 –&gt; 注意 从 Anaconda.org 安装包时，无需注册。 在当前环境中安装来自于 Anaconda.org 的包时，需要通过输入要安装的包在 Anaconda.org 中的路径作为获取途径（channel）。查询路径的方式如下： 进入 Anaconda Cloud 官方网址：http://anaconda.org 在页面 “Anaconda Cloud” 的上方搜索框中输入要安装的包名，search&gt;。 搜索结果中有数以千计的包可供选择，此时点击 “Downloads” 可根据下载量进行排序，最上面的为下载最多的包,（图中以搜索 pymysql 包为例）。选择满足需求的包或下载量最多的包，点击包名。 3 复制 “To install this package with conda run:” 下方的命令，并粘贴在终端中执行即可完成安装。 3.3.4 如何卸载已安装第三方包1）卸载指定环境中的包 $ conda remove --name &lt;env_name&gt; &lt;package_name&gt; 命令说明： &lt;env_name&gt;：即卸载包所在指定环境的名称（环境名两边不加尖括号 “&lt;&gt;”）。 &lt;package_name&gt;即要卸载包的名称（包名两边不加尖括号 “&lt;&gt;”）。 例如：conda remove –name python2 pandas 即卸载名为 “python2” 中的 pandas 包。 2）卸载当前环境中的包 $ conda remove &lt;package_name&gt; 命令说明： &lt;package_name&gt;：即要卸载包的名称（包名两边不加尖括号“&lt;&gt;”）。 执行命令后即在当前环境中卸载指定包。 例如：conda remove pandas 即在当前环境中卸载 pandas 包。 3.3.5 如何更新已安装第三方包1）更新所有包 $ conda update --all #或 $ conda upgrade --all 建议：在安装 Anaconda 之后执行上述命令更新 Anaconda 中的所有包至最新版本，便于使用。 2) 更新指定包 $ conda update &lt;package_name&gt; #或 $ conda upgrade &lt;package_name&gt; 注意： &lt;package_name&gt;：为指定更新的包名（包名两边不加尖括号 “&lt;&gt;”）。 更新多个指定包，则包名以空格隔开，向后排列。如：conda update pandas numpy matplotlib 即更新 pandas、numpy、matplotlib 等包。 3.4 深入了解 Anaconda了解了以上内容，或许你会觉得奇怪为啥 anaconda 能做这些事, 他的原理到底是什么？我们来看看 anaconda 的安装目录： 这里只截取了一部分, 其实这里就是我们前面介绍的 base 环境. 里面有着一个基本的 python 解释器, Lib 里面也有 base 环境下的各种包文件。 那我们自己创建的环境去哪了呢, 我们可以看见一个 envs, 这里就是我们自己创建的各种虚拟环境的入口, 点进去看看： 可以发现我们之前创建的所有虚拟环境目录就在下面, 再点进去： 这不就是一个标准的 Python 环境目录吗？这么一看, anaconda 所谓的创建虚拟环境其实就是安装了一个真实的 Python 环境, 只不过我们可以通过 activate,conda 等命令去随意的切换我们当前的 Python 环境, 用不同版本的解释器和不同的包环境去运行 Python 脚本。 至此，你已经掌握了 Anaconda 的日常使用方法。下面我会补充一部分内容用于提升使用体验（推荐阅读），参见：Anaconda 生态环境初体验。]]></content>
      <categories>
        <category>Anaconda</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
        <tag>Python</tag>
        <tag>Setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda JupyterLab（Jupyter Notebook） Home]]></title>
    <url>%2FAnaconda%2FAnaconda-JupyterLab%EF%BC%88Jupyter-Notebook%EF%BC%89-Home%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 熟悉 Python 的同学应该都知道 JupyterLab（Jupyter Notebook） 这一数据分析神器，它能帮助我们有效地组织输入输出，将我们探索数据的过程记录下来，后续稍加整理便可以生成一篇数据分析报告。 但是在使用 Jupyter （Windows、Linxu）过程中，你会发现，通过 Anaconda Navigator launch 后其默认目录是用户目录，而通过命令行终端启动的 Jupyter 默认目录是当前执行启动 Jupyter 命令所在目录。 通常对于我个人来说，我一般习惯将项目存放于固定目录（盘符），我希望启动 Jupyter 后可以固定到我的项目所在目录，而不是人为先去 Location 项目目录。于是产生一个小需求：如何自定义 Jupyter 启动之后的默认目录到一个固定目录？ 下面我们开始分享这一方法： 1. 自定义 Jupyter Home注意：只要以下 JupyterLab、Jupyter Notebook 配置一个，另外一个不用配置也可生效。 1.1 JupyterLab1）确认 Anaconda 安装路径，我的是安装 Anaconda 的用户目录：~ 2）配置文件：jupyter_notebook_config.json 查看 ~/Anaconda/etc/jupyter 路径下是否存在文件：jupyter_notebook_config.json 。 默认 jupyter_notebook_config.json 文件内容格式： { &quot;NotebookApp&quot;: { &quot;nbserver_extensions&quot;: { &quot;jupyterlab&quot;: true } } } 如果不存在，则根据上述内容手动创建。 3）添加 notebook_dir 在其中添加：”notebook_dir”:”自定义目录” 项，用于指定自定义 Home 目录。 例如我们自定义 Home 目录为：”E:/“ 修改 jupyter_notebook_config.json 文件后变为： { &quot;NotebookApp&quot;: { &quot;nbserver_extensions&quot;: { &quot;jupyterlab&quot;: true }, &quot;notebook_dir&quot;:&quot;E:/&quot; } } 4）重新启动 JupyterLab 后，你会发现更改已经成功！ 1.2 Jupyter Notebook1）查找配置文件 首先查找 Jupyter Notebook 配置文件，我们在命令行终端输入： jupyter notebook --generate-config Windows 显示如下： Linux 显示如下： Jupyter Notebook 配置文件路径：”.jupyter/jupyter_notebook_config.py” 2）修改配置文件：jupyter_notebook_config.py 找到如下行： “ c.NotebookApp.notebook_dir = ‘’ “ 用于自定义 Jupyter Notebook Home 目录。 我们将其修改为：c.NotebookApp.notebook_dir = ‘E:/‘,并将其注释消掉。 3）重新启动 Jupyter Notebook 后，你会发现更改已经成功！]]></content>
      <categories>
        <category>Anaconda</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
        <tag>Python</tag>
        <tag>JupyterLab(Notebook)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pycharm Community In Windows Tutorial]]></title>
    <url>%2FPycharm%2FPycharm-Community-In-Windows-Tutorial%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… PyCharm 是一款功能强大的 Python 集成开发环境，具有跨平台性（支持：macOS、 Windows、 Linux 等），鉴于方便日后参考，我们来介绍一下 PyCharm 在 Windows 下是如何安装的。 这里，我们首先给出 Pycharm 官方下载地址：PyCharm For Windows 。 1. Download PyCharm Community进入该网站后，我们会看到如下界面: Professional 表示专业版，是收费版本；Community 是社区版，推荐安装免费的社区版。 2. PyCharm Community Setup1）当下载好以后，点击进行安装，Next–&gt;进入如下窗口。可以修改安装路径，假设我们存放于：E:\Program Files 目录下。修改好以后，Next： 2）接下来是选择 PyCharm Community 安装位数（根据自己的电脑选择安装 32-bit 还是 64-bit，目前主流应该都是 64-bit）等，Next： 3）进入如下界面，点击 Install，静静的等待安装结束即可： 3. 检测本地 Python 环境要想使用安装好的 PyCharm，我们必须保证本地下载安装有 Python 解释器（本地已经成功安装配置 Python），不然 Pycharm 只是一副没有灵魂的驱壳（其实相当于将 Python 解释器内嵌到 Pycharm 中，由 Pycharm 来进行管理解释执行 Python 脚本）。 关于本地安装配置 Python 语言环境（解释器）可以参见前面章节教程：Python 简介与开发环境搭建 一文中关于 Windows 平台安装 Python 部分，这里由于篇幅原因不做赘述。 4. PyCharm Community 使用4.1 首次启动 PyCharm1、Pycharm 安装好后，我们启动 PyCharm，–&gt; 同意软件许可，并且–&gt;采用默认主题设置即可。完成后，显示如下界面： 2、首次启动后一般会创建我们的第一个 Pycharm 工程，点击 Create New Project（创建一个新工程）： Location 是我们默认存放工程的目录（文件目录用于存放：工程项目代码以及 Virtualenv）。 Virtualenv 是当前创建工程所选用的虚拟编译环境（前面我们提到过要将 Python 解释器内嵌到 Pycharm 中，故这里的虚拟编译环境是指 Python 运行环境（解释器）。比如，Python 解释器环境可以来源于：原生 Python、virtualenv、anaconda、docker 等），可以将其看成一个独立的，当前项目的 Python 运行环境容器，封装了当前项目所有用到的库。 untitled 指定当前工程的名字。 3、接下来是重点，然后点击 Project Interpreter……（用于查看项目选用的：Virtualenv 的详细信息，即 Python 解释程序或运行环境信息）： 可以看到：默认的 New environment using 【Virtualenv】 方式（包括：Virtualenv，Pipenv，以及 Conda）。并且我们发现 Pycharm 已经自动获取了系统中的 Python 3.7（多版本共存时，可选择不同 Python 版本的解释器）。如果已安装 Anaconda 的话，还可以使用 Conda封装好的虚拟环境。 Location（第二个）表示：Virtualenv 存放路径（用于存放 Python 解释器、相关开发 Python 库等 Python 脚本运行相关）。 4、了解默认选项后，我们来自定义工程存放目录：通过修改最上面的 Location（第一个）路径，比如 F:\PycharmProjects\FirstProjects ,来自定义工程存放目录。 注意，我们选择的路径需要为空，不然无法创建。Create 之后出现如下界面，这是 Pycharm 在配置环境，静静等待。最后点击 close 关掉提示就好了。 5、成功创建项目运行环境： 6、修改 Pycharm 主题风格 可以通过 Files –&gt; Settings –&gt; Editor –&gt; Color Scheme 打开如下界面,选择喜欢的主题风格： 我个人比较喜欢 Darcula 风格（上面你看到的黑色背景界面就是我配置了 Darcula 主题之后的显示）。 4.2 工程开发前面我们已经成功创建了一个新的工程，下面我们来看如何在创建好的工程中编写以及运行 Python 脚本： 1、 选中 python_pycharm 项目右键，点击 New,选择 Python File（创建 Python 脚本）： 给 file（脚本）命名，点击 OK: 2、 系统会默认生成空白 HelloWorld.py 脚本文件： 向文件 HelloWorld.py 中写入： print (&quot;Hello Python World&quot;) 3、 如何运行写好的 HelloWorld.py 脚本？ 快捷键：ctrl + shift + F10 ，表示运行 HelloWorld.py。运行结果如下： 至此，你已经掌握了 Pycharm 的基本使用方法。下面我们来补充一些 Pycharm 使用过程中经常遇到的一些问题以及其解决办法。 4.3 如何修改 Pycharm 项目编译环境？我们在首次启动 Pycharm 时提到过项目的默认 Virtualenv（编译/运行环境）配置，创建项目后我们还可以重新修改 Pycharm 项目编译环境，即使用非项目创建 Virtualenv。 点击 File 选择 settings,选择目标项目（如：PycharmProjects），可选择查看以及修改项目的 Virtualenv 相关配置项： 4.4 如何在项目编译环境添加项目依赖的第三方库或模块？当项目依赖第三方库或模块，而我们的 Virtualenv 中不包含该第三方库或模块时，Pycharm 会有红线做警告标识，表示未导入的库或模块。 下面给出两种解决方法（这里我们只讨论 Virtualenv 中不存在第三方库或模块的情况，有时会存在检测不到的情况）： 1）Virtualenv Add 点击 File 选择 settings，选择目标项目（如：PycharmProjects），我们可以查看到项目 Virtualenv 中 Python 解释器信息以及已安装 Pyhton 扩展包。如果需要添加新的模块（Pyhton 扩展包，例如：pymysql），可点击 “+” 号： 然后在搜索框直接搜索：pymysql，然后点安装 Install Package。 上述方法实质上是 pip install pymysql 到 Virtualenv 中。 2）快捷键：alt + enter 让鼠标光标到红线的这个位置 –&gt; alt + enter –&gt; 选择 Install Package …。如下： 提示安装包成功，此时发现红线警告的地方已经不警告了。同时发现 【lib library root/Lib/site-package】 目录下增加了 requests 模块，这就是刚刚引入的这个模块的文件。 4.5 如何打开已存在的项目文件进行二次开发？1、 进行 Pycharm 的界面当中之后，进行点击菜单中的 file 的选项菜单，点击 open 。 2、 这样就会弹出了一个open file or project 的窗口的界面当中，进行选中列表选中项目的文件。进行点击 ok 即可。 3、 这样就会弹出了 open project 的窗口的，进行默认选中即可（可以选择打开的窗口），进行点击 ok 即可导入成功。]]></content>
      <categories>
        <category>Pycharm</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pycharm</tag>
        <tag>setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 实战之循环神经网络（Recurrent Neural Network）]]></title>
    <url>%2FTensorFlow%2F%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 在前面的系列章节中，我们已经讲解了全连接神经网络（Full Connection Neural Network）和卷积神经网络（Convolutional Neural Network），以及如何训练和使用 FCNN、CNN 模型。它们都只能单独的处理一个个的输入，而前一个输入和后一个输入是完全没有关系的。 但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有一定关系的。比如：当我们在理解一句话意思时，孤立的理解这句话中的每个词的意思是不够的，我们需要分析这些词连接起来的整个序列的意思；当我们处理视频的时候，我们也不能只单独的去分析每一帧的信息，而要分析这些帧连接起来的整个序列信息。也就是说待处理信息前后是存在一定序列的。 基于上述，我们需要介绍深度学习领域中另一种非常重要且常用的神经网络结构：循环神经网络（Recurrent Neural Network，RNN） 以及循环神经网络中的一个重要结构——长短时记忆网络（long short-term memory，LSTM）。 为了进一步说明循环神经网络的使用，这一章节也将介绍循环神经网络在 自然语言处理（Natural Language Processing，NLP） 问题以及 时序分析 问题中的应用，并给出具体的 TensorFlow 程序来解决一些经典的问题。 这一章节内容我们主要分为四个部分： 第一部分，将介绍循环神经网络的基本知识并以机器翻译样例来说明循环神经网络工作原理。这一小节将给出一个具体的样例来说明一个最简单的循环神经网络的前向传播过程； 第二部分，介绍基于上述循环神经网络结构的常用变种（BRNN、DRNN、Dropout）； 第三部分，这一小节将介绍循环神经网络中的长短时记忆网络（LSTM）的网络结构； 第四部分，最后一小节将结合 TensorFlow 对上述神经网络结构的支持，通过两个经典的循环神经网络模型的应用案例，介绍如何针对语言模型和时序预测两个问题，设计和使用循环神经网络。 第一部分：循环神经网络工作原理循环神经网络（recurrent neural network，RNN）源自于 1982 年由 Saratha Sathasivam 提出的霍普菲尔德网络（玩音乐的应该听过德国霍普菲尔德钢琴…）。霍普菲尔德网络由于实现困难，在其提出时并没有被合适的应用，该网络结构也于 1986 年后被全连接神经网络以及一些传统的机器学习算法所取代。然而由于传统机器学习算法非常依赖于人工提取的特征，使得基于传统机器学习的图像识别、语音识别以及自然语言处理等问题存在特征提取的瓶颈。 而基于全连接神经网络的方法也存在参数太多、无法利用数据中时间序列信息等问题，随着更加有效的循环神经网络结构被不断提出，循环神经网络挖掘数据中的 时序信息 以及 语义信息 等的深度表达能力被充方利用，并在语言识别、语言模型、机器翻译以及时序分析等方面实现了突破。 1.1 循环神经网络简介循环神经网络的主要用途是用来 处理和预测序列数据。我们之前介绍的全连接神经网络或卷积神经网络模型中，网络结构都是从输入层到隐藏层再到输出层，层与层之间是全连接或者部分连接的，但每一层的节点之间是无连接的。 我们先考虑这样一个 NLP 问题：如果要预测句子的下一个单词是什么，一般需要用到当前单词以及前面的单词，这是由于句子中前后单词并不是独立的。比如，当前的单词是“很”，前一个单词是“天空”，那么下一个单词很大概率是“蓝”。循环神经网络的来源就是为了刻画一个序列当前的输出和之前信息的关系，从网络结构上来看，循环神经网络会 记忆 之前节点的信息，并利用之前的信息影响后面节点的输出。 也就是说，循环神经网络的隐藏层之间的节点是有连接的，隐藏层的输入不仅包括输入层的输入，还包括上一时刻隐藏层的输出。 循环神经网络结构： 循环神经网络种类繁多，下面我们给出一个典型的循环神经网络： 图 8-1 循环神经网络经典结构示意图 相信第一次看到这个玩意的读者内心和我一样是崩溃的（说好的神经网络，网络呢？！）。这是因为循环神经网络实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的： 如果把上面有 W 的那个带箭头的圈去掉，它就变成了一个最简单的 全连接神经网络：$x$ 表示输入向量值；$s$ 表示隐藏层值；$o$ 表示输出向量值（$U$ 是输入层到隐藏层的权重矩阵；$V$ 是隐藏层到输出层的权重矩阵）。 那么接下来我们看看 “带 W 的带箭头的环” 又是指什么：它表示网络结构为循环结构，隐藏层权重为 $W$（参数共享）。循环神经网络理论上可以被看作是同一神经网络结构被无限复制的结果（其实出于优化考虑，循环神经网络无法做到真正的无限循环。所以，一般可以将循环神经网络的循环体展开。如下图所示）。循环神经网络中当前隐藏层的值 $h$（$s$） 不仅仅取决于当前这次的输入 $x$，还取决于上一次隐藏层的值 $h$（$s$）。权重矩阵 $W$ 就是隐藏层上一次的值作为这一次的输入的权重矩阵（参数 $W$ 共享）。 时刻： 对于循环神经网络，一个非常重要的概念就是 时刻。我们知道 RNN 主要用于解决序列数据问题，对于一个数据序列的不同数据，可以看作这个序列不同时刻的数据。于是，RNN 数据输入可以看作是将不同时刻的数据依次传入循环神经网络的输入层，循环神经网络会对于每一时刻的输入结合当前模型的状态（上一次隐藏层的值）给出一个输出。通俗的讲，循环神经网络主体结构 $s$（$h$） 的当前时刻的输入除了来自于输入层 $x$，还有一个循环的边来提供当前时刻的状态（上一时刻隐藏层的值）。在每一时刻，循环神经网络的模块 $s$ 会读取当前时刻的输入 $x$，并输出一个值 $o$。同时 $s$ 的状态会从当前时刻传递到下一时刻。 结合上述 RNN 网络结构示意图以及时刻概念，我们来看其 工作原理详解： 图 8-2 循环神经网络按时刻展开后的结构 从图中可以看到循环神经网络在每一时刻都会有一个输入 $x_t$，然后根据循环神经网络当前的状态 $s_t$ 提供一个输出 $o_t$。以时刻 $t$ 为例，网络在 $t$ 时刻接收到输入 $x_t$ 之后，隐藏层的值是 $s_t$，输出值是 $o_t$。关键一点是，$s_t$ 的值不仅仅取决于 $x_t$，还取决于 $s_{t-1}$。可以用下面的公式来表示循环神经网络的计算过程： $$o_t = g(V * s_t)$$ $$s_t = f(U * x_t + W * s_{t-1})$$ 如果反复把 式2 带入到 式1，我们将得到： 可以看出，循环神经网络的输出值 $o_t$，是受前面历次输入值 $x_t$、$x_{t-1}$、$x_{t-2}$、$x_{t-3}$、……影响的，这就是为什么循环神经网络可以往前探索任意多个输入值的原因（看到这里我们应该存在一个疑惑：对于序列问题，只依靠看前面的词就足够了么？）。 对于输出 $o_t$，可以是对序列中下一个时刻的预测，也可以是对当前时刻信息的处理（如语言识别结果）。循环神经网络要求每一个时刻都要有一个输入，但是不一定每个时刻都需要有输出。 下面我们给出一个以机器翻译为样例的演示来说明循环神经网络如何解决实际问题： 对于机器翻译，循环神经网络每一时刻的输入为需要翻译的句子中的单词。如图 8-3 所示，需要翻译的句子为：ABCD，那么循环神经网络第一段每一时刻的输入就分别是：A、B、C、D，然后用 “_” 作为待翻译句子的结束符。在第一段中，循环神经网络没有输出。从结束符 “_” 开始，循环神经网络进入翻译阶段，该阶段中每一时刻的输入就是上一时刻的输出，而最终得到的输出就是句子 ABCD 翻译的结果。从图 8-3 中可以看到句子 ABCD 对应的翻译结果就是 XYZ，而 Q 是代表翻译结束的字符。 RNN 总结 循环神经网络可以被看作是同一神经网络结构在时间序列上被复制多次的结果，这个被复制多次的结构被称之为循环体； 如何设计循环体的网络结构是循环神经网络解决实际问题的关键； 和卷积神经网络过滤器中参数共享类似，RNN 中的参数在不同时刻也是共享的。 1.2 循环神经网络前向传播这一小节我们解读循环神经网络的前向传播过程。首先，图 8-4 展示了一个使用最简单的循环体结构（A）的循环神经网络，在这个循环体中只使用一个类似全连接层的神经网络结构。下面我们通过 图 8-4 展示的神经网络来介绍循环神经网络的前向传播流程。 循环神经网络中的状态（A）可以通过一个向量来表示（上一时刻隐藏层的输出，假设其维度为：$h$）。从 图 8-4 中可以看出，循环体中的神经网络输入有两部分：一部分为上一时刻的状态；另外一部分为当前时刻的输入样本向量。对于时间序列输入数据来说（比如不同时刻商品的销量），每一时刻的输入样例可以是当前时刻的数值（比如销售量）；对于语言模型来说，输入样例可以是当前单词对应的单词向量（word embedding）。 假设输入向量的维度为 x，那么图 8-4 中循环体的全连接层神经网络的输入大小为 $h+x$。也就是将上一时刻的状态与当前时刻的输入拼接成一个大的向量作为循环体中神经网络的输入（样例为了方便显示，采用了向量拼接的方式）。因为该神经网络的输出为当前时刻的状态，于是输出层的节点个数也为 $h$，循环体中的参数个数为 $(h+x) * h + h$ 个。 从 图 8-4 中看出，循环体中的神经网络输出不但提供给了下一时刻作为状态，同时也会提供给当前时刻的输出。为了将当前时刻的状态转化为最终的输出，RNN 还需要另外一个全连接神经网络来完成这个过程，这和 CNN 中最后的全连接层的意义是一样的。类似的，不同时刻用于输出的全连接神经网络中的参数也是一致的。 下面我们来看一个循环神经网络前向传播的具体计算过程： 下面代码给出了上述循环神经网络前向传播实现过程： import numpy as np X = [1, 2] state = [0.0, 0.0] # 分开定义不同输入部分权重以便操作： w_cell_state = np.asarray([[0.1, 0.2], [0.3, 0.4]]) w_cell_input = np.asarray([0.5, 0.6]) b_cell = np.asarray([0.1, -0.1]) # 定义用于输出的全连接层参数： w_output = np.asarray([[1.0], [2.0]]) b_output = 0.1 # 按时间顺序执行循环神经网络的前向传播过程： for i in range(len(X)): # 计算循环体中的全连接神经网络： before_activation = np.dot(state, w_cell_state) + X[i] * w_cell_input + b_cell state = np.tanh(before_activation) # 根据当前时刻状态计算最终输出： final_output = np.dot(state, w_output) + b_output # 输出每个时刻的信息： print (&quot;before activation: &quot;, before_activation) print (&quot;state: &quot;, state) print (&quot;output: &quot;, final_output) 运行输出以下结果，和上图中前向传播结果数值一致： before activation: [0.6 0.5] state: [0.53704957 0.46211716] output: [1.56128388] before activation: [1.2923401 1.39225678] state: [0.85973818 0.88366641] output: [2.72707101] Loss Function 在得到循环神经网络的前向传播结果之后，可以和其它神经网络类似地定义损失函数。循环神经网络唯一的区别在于每一时刻都有一个输出，所以循环神经网络的总损失为所有时刻（或部分时刻）上的损失函数的总和。 和其它神经网络类似，在定义完损失函数之后，就可以使用 TensorFlow 完成模型训练了。 注意: 需要特别指出的是，理论上循环神经网络可以支持任意长度的序列，然而实际中，如果序列过长会导致优化时出现梯度消散的问题（下一章节会进行说明）。所以实际中一般会规定一个最大长度，当序列长度超过规定长度之后会对序列进行截断。 第二部分：循环神经网络扩展在前一章节我们已经掌握了循环神经网络的工作原理以及前向传播过程，这一节我们来看 RNN 的几个常用变种（BRNN、DRNN、Dropout 等）以及它们所解决的问题，同时也会给出如何使用 TensorFlow 来实现这些变种。 2.1 双向循环神经网络（bi-directional RNN）在前面介绍的经典循环神经网络架构中，我们知道状态的传输时从前往后 单向的。然而有些问题中，当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。例如对于语言模型来说，很多时候光依赖前文信息来预测语句单词时不够的，也需要根据后面的内容。比如下面这句话： 我的手机坏了，我打算____一部新手机。 可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是 【一部新手机】，那么横线上的词填 【买】的概率就大得多了。 基于上节介绍的基本循环神经网络是无法对此进行建模的，这是就需要使用双向循环神经网络（bi-directional RNN，BiRNN）来解决这类问题。BiRNN 是由两个循环神经网络上下叠加在一起组成的，输出由这两个循环神经网络的状态共同决定。下面给出一个双向循环神经网络结构示意图： 从图中可以看出，BiRNN 的主体结构就是两个单向循环神经网络的结合。在每一时刻 $t$，输入会同时提供给这两个方向相反的循环神经网络，而输出是由这两个单向循环神经网络共同决定的。 BiRNN 前向传播过程： 沿着时刻 $T_0$ 到时刻 $T_n$ 正向计算一遍，得到并保存每个时刻向前隐含层的输出。 沿着时刻 $T_n$ 到时刻 $T_0$ 反向计算一遍，得到并保存每个时刻向后隐含层的输出。 正向和反向都计算完所有输入时刻后，每个时刻根据向前向后隐含层得到最终输出。 为了帮助理解，我们来分析一个特殊场景，然后再总结一般规律。我们先考虑上图中 $y_2$ 的计算： 从上图可以看出，双向卷积神经网络的隐藏层要保存两个值，一个 $A$ 参与正向计算，另一个值 $A’$ 参与反向计算。最终的输出值 $y_2$ 取决于 $A_2$ 和 $A_2’$ 。其计算方法为： $$ y_2 = g(VA_2 + V’A_2’ ) $$ $A_2$ 和 $A_2’$ 则分别计算： 现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值与有关 $S_t$ 和 $S_{t-1}$；反向计算时，隐藏层的值 $S_t’$ 与 $S_{t+1}’$ 有关；最终的输出取决于同一时刻的正向和反向计算。下面给出一个双向循环神经网络的计算方法： 同时可以看出，正向计算和反向计算不共享权重，也就是说 $U$ 和 $U’$、$W$ 和 $W’$、$V$ 和 $V’$ 都是不同的权重矩阵。 2.2 深层循环神经网络（deepRNN）为了只关注循环神经网络工作原理，前面我们介绍的循环神经网络结构都只有一个隐藏层。为了增强模型的表达能力，可以将每一时刻上的循环体重复多次（堆叠多个隐藏层），这样就得到了深度循环神经网络。下图（图 8-8）给出了深层神经网络结构示意图： 从图 8-8 中可以看出，相比前面所介绍的循环神经网络结构，深层循环神经网络在每个时刻上将循环体结构复制了多次。和卷积神经网络类似，每一层的循环体中参数是一致的（共享），而不同层中的参数可以不同。为了更好地支持深层循环神经网络，TensorFlow 中提供了 MultiRNNCell 类来实现深层循环神经网络的前向传播过程。以下我们将给出代码展示如何使用这个类： # 定义一个基本的 LSTM 结构（后续会介绍 LSTM）作为循环体的基础结构。深层循环神经网络也支持使用其它的循环体 lstm = rnn_cell.BasicLSTMCell(lstm_size) # 通过 MultiRNNCell 类实现深层循环神经网络中每一时刻的前向传播过程。其中， # number_of_layers 表示循环体设置隐藏层层数，这里表示设置的 LSTM 结构数目 stached_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers) # 和经典循环神经网络一样，可以通过 zero_state 函数来获取初始状态 state = stacked_lstm.zero_state(batch_size, tf.float32) # 计算前向传播结果： for i in range(len(num_steps)): if i&gt; 0: tf.get_variable_scope().reuse_variables() stacked_lstm_output, state = stacked_lstm(current_input, state) final_output = fully_connected(stacked_lstm_output) loss += calc_loss(final_output, expected_output) 从上述 TensorFlow 实现中看出，TensorFlow 中只需要在 BasicLSTMCell 的基础上再封装一层 MultiRNNCell 就可以非常容易地实现深层 LSTM 循环神经网络了（是不感觉很简单）。 2.3 循环神经网络的 dropout前面章节中介绍过再卷积神经网络上使用 dropout 的方法。通过 dropout 可以防止过拟合，可以让卷积神经网络更加健壮（robust）。类似的，在循环神经网络中使用 dropout 也有同样的功能。 注意：类似卷积神经网络只有在最后的全连接层中使用 dropout，循环神经网络一般只在不同层循环体结构之间使用 dropout ，而不在同一层的循环体结构之间使用。也就是说，从时刻 $t-1$ 传递到时刻 $t$ 时，循环神经网络不会进行状态的 dropout；而在同一时刻 $t$ 中，不同层循环体之间会使用 dropout。 下图（图 8-9）展示了循环神经网络使用 dropout 的示意图。假设要从$t-2$ 时刻的输入 $x_{t-2}$ 传递到 $t+1$ 时刻的输出 $y_{t+1}$，那么 $x_{t-2}$ 将首先传入第一次循环体结构，这个过程会使用 dropout。但是从 $t-2$ 时刻的第一次循环体结构传递到第一层的 $t-1$、$t$、$t+1$ 时刻不会使用 dropout。在 $t+1$ 时刻的第一层循环体结构传递到同一时刻内更高层的循环体结构时，会再次使用 dropout。 在 TensorFlow 中，使用 tf.nn.rnn_cell.DropoutWrapper 类可以很容易实现 dropout 功能。以下代码展示了如何在 TensorFlow 中实现带 dropout 的循环神经网络。 # 定义 LSTM 结构 lstm = rnn_cell.BasicLSTMCell(lstm_size) # 使用 DropoutWrapper 类来实现 dropout 功能。该类通过两个参数来控制 dropout 的概率：一个参数为 input_keep_prob(用来控制输入的 dropout 的概率)；另一个参数为 output_keep_prob(用于控制输出的 dropout 的概率) dropout_lstm = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=0.5) # 在使用 dropout 的基础上定义： stacked_lstm = rnn_cell.MultiRNNCell([dropout_lstm] * number_of_layers) # 计算前向传播结果： for i in range(len(num_steps)): if i&gt; 0: tf.get_variable_scope().reuse_variables() stacked_lstm_output, state = stacked_lstm(current_input, state) final_output = fully_connected(stacked_lstm_output) loss += calc_loss(final_output, expected_output) 2.4 RNN 的梯度爆炸和梯度消失问题阅读这一部分之前，感兴趣或者想深入了解 RNN 实现的读者可以练习推导一下 RNN 的训练过程（损失函数、梯度下降等算法实现）。 前面我们提到：理论上循环神经网络可以支持任意长度的序列，然而实际中，前面介绍的几种 RNNs 并不能很好的处理较长的序列。一个主要的原因是：RNN 在训练中很容易发生梯度爆炸和梯度消失，这导致训练时梯度不能在较长序列中一直传递下去，从而使 RNN 无法捕捉到长距离的影响。 为什么 RNN 会产生梯度爆炸和消失问题呢？我们接下来将详细分析一下原因。我们根据训练过程中任意时刻 $k$ 的误差项 $\delta_k$： 上式的 $\beta$ 定义为矩阵的模的上界。因为上式是一个指数函数，如果 $t-k$ 很大的话（也就是向前看很远的时候），会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失问题（取决于大于 1 还是小于 1）。 通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到 $NaN$ 错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。 梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题： 合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。 使用 relu 代替 sigmoid 和 tanh 作为激活函数。 使用其他结构的 RNNs，比如长短时记忆网络（LTSM）和 Gated Recurrent Unit（GRU），这是最流行的做法。我们会在随后的章节中介绍这两种网络。 第三部分：长短时记忆网络（LSTM）结构我们都知道，循环神经网络工作的关键点就是使用前文（未来）或后文（历史）的信息来帮助当前的决策。例如语言模型中使用之前出现的单词来加强对当前文字的理解。循环神经网络可以更好地利用传统神经网络结构所不能建模的信息，但同时带来了更大的挑战——长期依赖（long-term dependencies）。 何为长期依赖？在有些问题中，模型仅仅需要短期（短距离）内的信息来执行当前的任务。比如预测短语 “大海的颜色是蓝色” 中的最后一个单词 “蓝色” 时，模型并不需要记忆这个短语之前更长的上下文信息—-因为这一句话已经包含了足够的信息来预测最后一个词。在这样的场景中，相关的信息和待预测的词的位置之间的间隔很小，循环神经网络比较容易地利用先去的信息。 但同样也会有一些上下文场景更加复杂的情况。比如当前模型试着去预测段落 “某地开设了大量工厂，空气污染是否严重······这里的天空都是灰色的” 的最后一个单词时，仅仅根据短期依赖就无法很好的解决这种问题。因为只根据最后一小段，最后一个词可以是 “蓝色的” 或 “灰色的”。如果模型需要预测清楚具体是什么颜色，就需要考虑先前提到的比较远的上下文信息。因此，当前预测位置和相关信息之间的文本间隔就有可能变得很大。当这个间隔不断增大时，类似图 8-4 中给出的简单循环神经网络有可能丧失学习到距离如此之远的信息的能力（上下文距离越远学习能力越低）。或者在更加复杂的语言场景中，有用的信息的间隔有大有小，长短不一，循环神经网络的性能会受到限制。 前面介绍的循环神经网络结构在实际应用中，很难处理长期依赖（长距离的依赖）。下面我们将介绍一种改进之后的循环神经网络：长短时记忆网络(Long Short Term Memory Network, LSTM)，它成功的解决了原始循环神经网络的缺陷，成为 RNN 在语音识别、图片描述、自然语言处理等许多领域中成功应用的关键。。但不幸的一面是，LSTM 的结构很复杂，因此，我们需要花上一些力气，才能把 LSTM 以及它的训练算法弄明白。在搞清楚 LSTM 之后，我们再介绍一种 LSTM 的变体：GRU (Gated Recurrent Unit)。 它的结构比 LSTM 简单，而效果却和 LSTM 一样好，因此，它正在逐渐流行起来。 3.1 LSTM]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Recurrent Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条形码和二维码检测与识别原理]]></title>
    <url>%2FBarcode%2F%E6%9D%A1%E5%BD%A2%E7%A0%81%E5%92%8C%E4%BA%8C%E7%BB%B4%E7%A0%81%E6%A3%80%E6%B5%8B%E4%B8%8E%E8%AF%86%E5%88%AB%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[开篇： 谈起条码、二维码，你可能并不陌生… 从商场实体店到大街小巷的小商小贩都开始布满了腾讯爸爸和阿里爸爸的二维码之后，越来越多的人感受到了二维码的使用带来的共享支付的便捷性与优越性。我还记得当初微信推出二维码扫码功能时，觉得这相当的 Imagine！我们可以通过一张简单的二维码图片就可以扫码添加好友以及实现收付款功能。 随着二维码技术的不断普及，目前越来越多不同领域的应用都开始支持扫码功能。例如：扫码登陆、扫码分享、扫码支付、扫码验真等等。那么，二维码是如何作为凭证使用的呢？How can ？！ 其实，条码的使用不仅仅局限于上面我提到的二维条码，还有传统的一维条码，我们也称为条形码。如果注意观察，你会发现我们生活的方方面面都和条码相关：商品条形码、物流管理码、图书条形码、医疗码…… 本文我将以常见条码（一维条形码、二维码）使用为样例由浅入深、由易到难逐渐解析条码的检测与识别原理。下面开始记录我在条码检测与识别学习过程中的学习路线，以供日后复习使用。 1. 条形码条形码或称条码（barcode）是将宽度不等的多个黑条和空白，按照一定的编码规则排列，用以表达一组信息的图形标识符。常见的条形码是由反射率相差很大的黑条（简称：条）和白条（简称：空）排成的平行线图案及其对应编码字符组成的信息标识（如图所示：ISBN 国际标准书号）。 条形码可以标出商品信息（物品的生产国、制造厂家、商品名称、生产日期等）、图书信息（图书出版国家、语言、出版社等）、邮件信息（邮件起止地点、类别、日期）等多样信息。因而条形码技术被广泛应用于商品流通、图书管理、邮政管理、仓储、交通运输、包装、配送等许多领域。 为何使用黑条、白条标识条码？ 我们知道，物体的颜色是由其反射光的类型决定的。众所周知，黑色吸收光中的所有颜色，白色反射光中的所有颜色，此时条形码的光学特性最好。当然，也可以用其他两种颜色来表示条形码，只要两种颜色有不同的反射率，足够的对比度。 1.1 条形码组成一个完整的条码，不论是采取何种规则印制的条形码，都由静区、起始字符、数据字符与终止字符组成。有些条码在数据字符与终止字符之间还有中间分隔符、校验字符（如下图所示）。 （1）静区： 静区也叫空白区，分为左空白区和右空白区，左空白区是让扫描设备做好扫描准备，右空白区是保证扫描设备正确识别条码的结束标记。 为了防止左右空白区（静区）在印刷排版时被无意中占用，可在空白区加印一个符号（左侧没有数字时印’&lt;’, 右侧没有数字时加印’&gt;’）,这个符号就叫静区标记。主要作用就是防止静区宽度不足。只要静区宽度能保证，有没有这个符号都不影响条码的识别。 （2）起始/终止符： 指位于条码开始和结束的若干条与空，标志条码的开始和结束，同时提供了码制识别信息和阅读方向的信息。 （3）数据符： 位于条码中间的条、空结构，它包含条码所表达的特定信息。 （4）校验字符： 检验读取到的数据是否正确。不同编码规则可能会有不同的校验规则，根据不同的校验规则可以得到不同的校验字符。 基本单位：模块 构成条码的基本单位是模块，模块是指条码中最窄的条或空，模块的宽度通常以 mm 或 mil（千分之一英寸）为单位。构成条码的一个条或空称为一个单元，一个单元包含的模块数是由编码方式决定的。 1.2 条码运作原理(1) 识别原理 要将按照一定规则编译出来的条形码转换成有意义的信息，需要经历扫描和译码两个过程。 首先来看扫描 &gt;&gt;&gt; 条形码的扫描需要扫描器，扫描器利用自身光源照射条形码，再利用光电转换器接受反射的光线，将反射光线的明暗转换成数字信号（0，1）。并且白条、黑条的宽度不同，会导致相应的电信号持续时间长短也不同。 条码扫描器有光笔、CCD 、激光、影像四种。这里简单介绍常见的两种：激光、影像。 激光：以激光作为发光源的扫描器。又可分为线型、全角度等几种。 影像：以光源拍照利用自带硬解码板解码，通常影像扫描可以同时扫描一维及二维条码。 再来看译码 &gt;&gt;&gt; 译码器可以通过测量脉冲数字电信号 0，1 的数目来判别条和空的数目。通过测量 0，1 信号持续的时间来判别条和空的宽度。此时所得到的数据（条形码符号）仍然是杂乱无章的，要想知道条形码所包含的信息，则需根据对应的编码规则，将条形符号换成相应的数字、字符信息。 1.3 条形码码制码制即指：条码条和空的排列规则。目前，国际广泛使用的一维条码种类有： （1）EAN/UPC 码：商品条码，用于在世界范围内唯一标识一种商品。我们在超市中最常见的就是这种条码。 （2）Code39/93/128 码：为目前国内企业内部自定义码制，可以根据需要确定条码的长度和信息。它编码的信息可以是数字，也可以包含字母，主要应用于工业生产线领域等。 （3）ITF25 码（交叉 25 码）：主要应用于包装、运输（物流管理）以及国际航空系统的机票顺序编号等。 （4）Codabar（库德巴码）：多用于血库、图书馆、包裹等的跟踪管理等。 1.4 条码重要参数（1）密度（Density）： 条码的密度指单位长度的条码所表示的字符个数。对于一种码制而言，密度主要由模块的尺寸决定，模块尺寸越小，密度越大，所以密度值通常以模块尺寸的值来表示（如5mil）。 通常 7.5 mil 以下的条码称为高密度条码，15 mil 以上的条码称为低密度条码。条码密度越高，要求条码识读设备的性能（如分辨率）也越高。 （2）对比度（PCS）： 条码符号的光学指标，PSC 值越大则条码的光学特性越好。PCS =（RL-RD）/RL×100%（RL：条的反射率 RD：空的反射率） 上面我们已经基本掌握了条码的相关概念。下面我们以我国广泛流传的“ EAN-13 条形码”的识别为样例来印证上述所学。 1.5 EAN13 条形码EAN 码是国际物品编码协会制定的一种商品条码，通用于全世界。EAN 码符号有标准版（EAN-13）和缩短版（EAN-8）两种。标准版表示 13 位数字，又称为 EAN13 码;缩短版表示 8 位数字，又称 EAN8。两种条码的最后一位为校验位，由前面的 12 位或 7 位数字计算得出。 EAN13 码是我国主要采取的商品码编码标准。EAN-13 码包含了商品的名称、生存厂商、产品型号、所属国家地区等信息。 （1）EAN13 条形码编码字符格式说明 EAN13 条形码一共编码有 13 位数字； 前 2 位或者前 3 位称为前缀，表示国家、地区或者某种特定的商品类型。（例如：1 中国区条形码开头：690~699；2 图书类条形码开头：978~979）； 前缀后的 4 位或者 5 位称为厂商代码，表示产品制造商； 厂商代码后 5 位称为商品代码，表示具体的商品项目； 最后1位是校验码，根据前 12 位计算而出，可以用来防伪以及识别校验。 （2）EAN13 条形码编码说明 EAN13 码是模块组合型条码。我们知道，模块是指条码中最窄的条或空，而构成条码的一个条或空都称为一个单元，并且一个单元可以由多个模块构成。一个模块宽的条（条形码黑色部分：bar）单元表示二进制 “1”，两个模块宽的条（黑）单元表示二进制 “11”，一个模块宽的空（条形码白色部分：space）单元表示二进制 “0”。 这样，便可以用二进制的 0、1 表示信息。 我们已经知道了EAN13 条形码编码字符格式，它是 13 个数字字符。这里我们来看 EAN13 码中的每一个字符的编码细节： 在 EAN 码中，每一个编码字符（例如：数字 1），都是由两个条（黑）和两个空（白）组成。条和空又分别由 1~4 个同宽、同颜色的模块组成。每个字符总共有 7 个模块（宽/长度），并规定每个字符从外观上包含两个条、两个空。所以 EAN 码又称（7,2）编码（7 位二进制 bit）。 条形码一共有8个区域：左侧空白区-&gt;起始符-&gt;左侧数据符-&gt;中间分隔符-&gt;右侧数据符-&gt;校验符-&gt;终止符-&gt;右侧空白区 编码字符为：0~9 起始符/终止符编码为：101；分隔符编码为：01010 0~9 每种字符有 3 种编码方式，A、B为左侧数据奇偶编码，C为右侧数据偶编码（不理解后面会讲解） 起始符到终止符一共有 95 个长度（95=3+7*6+5+7*6+3） 起始符到终止符一共有 59 个 bar 和 space（59=3+6*4+5+6*4+3） （3）EAN13 条形码编码细则 前置符 在 EAN-13 码以图形标识符表示时，基于上述理论，我们会发现：第一位字符没有其对应的条、空表示（难道第一位字符不用表示么？！）。其实，第一位字符只是没有显示表示（如图中第一位的：6），而是采用隐式表示，既不用条和空（表示），而用第2位~第7位（总六位）的奇偶性来隐式表示（后面会说）。 现在，第一位用隐式表示，那么只需要表示 13-1=12 个字符，将 12 个字符，分成两半，左侧 6 个字符，右侧 6 个字符。 左/右侧数据符 左侧字符有奇偶性，右侧字符全是偶的。左侧的奇偶性取决于隐式表示的第一位字符（前置符/码）。具体奇偶性如图：A 代表偶数位，B 代表奇数位。如前置符 0 表示：左侧六个字符都是奇数位。 那么奇、偶数位有什么用呢？ 其实，相同字符在偶数位、和奇数位的二进制表示是不一样的（正如我们上面提到的 0~9 每个字符都有 3 中编码方式）。如图： 校验码 EAN-13 码最后一个字符是校验码，我们知道，校验码用来保证条形码识别的正确性。在前面 【1.1 条形码组成】 中我们介绍校验字符时提到：不同编码规则（码制）可能会有不同的校验规则，根据不同的校验规则可以得到不同的校验字符。 这里我们给出条形码校验规则 &gt;&gt;&gt; 首先，把条形码从右往左依次编序号为“1,2,3,4……”从序号二开始把所有偶数序号位上的数相加求和，用求出的和乘3，再从序号三开始把所有奇数序号上的数相加求和，用求出的和加上刚才偶数序号上的数，然后得出和。再用10减去这个和的个位数，就得出校验码。 样例条形码为：977167121601X（X为校验码） 1．1+6+2+7+1+7=24 2．24×3=72 3．0+1+1+6+7+9=24 4．72+24=96 5．10-6=4 所以最后校验码 X=4。此条形码为：9771671216014。 （4）EAN13 条形码识别原理 这里，我们还是以 “识别条形码中一个字符” 为例，给出一个最基本的识别方法： C1，C2，C3，C4 表示该字符中四个相邻的条（黑）或空（白）的宽度，T 是一个字符的宽度。 我们知道，C1+C2+C3+C4=7（模块），用 n 表示一个模块的宽度，n=T/7。 用 $$ m_i=C_i/n，i=1,2,3,4 $$，便可以得到编码。若 m1=1，m2=3，m3=1，m4=2,且条码排列位【条——空——条——空】,则当前字符二进制编码为: 【1 000 1 00】。我们发现当前字符为右侧偶字符 “7”。 （5）条形码扫描方向的判别（为什么左侧字符要有奇偶性？） 为了能够正确地解译条形码，在解译条形码符号所表示的数据之前，需要先进行条形码扫描方向的判别。我们知道EAN-13的起始字符和终止字符的编码结构都是 “101”，所以是不能通过起始字符和终止字符来判别它的扫描方向。 通过【（3）EAN13 条形码编码细则】我们知道，EAN-13 码的编码结构右侧字符为全偶，而左侧字符的奇偶顺序由前置符决定，没有全偶的。从而可以利用此原理来确定 EAN-13 码的扫描方向。如果扫描到的前 6 个字符没有奇数位，即为反向扫描，否则为正向扫描。 这里给出另外一个假设：假如固定左侧奇编码，右侧偶编码那么也能判断扫描方向是否相反；假如把前置码放到数据区也用 bar&amp;space 表示也更精确。 1.6 条形码检测与识别流程图 1.7 总结 条形码有明确的编码标准，条形码检测就是根据编码标准逆向读出其所代表的信息； 条形码检测的难点：剪裁出条形码区域并校正至理想状态； 译码的依据：字符及其二进制表示的表； 前置码并没有用 bar &amp; space 表示。 2. 二维码上面我们已经了解了一维条形码的检测与识别原理。其实，一维条形码只是在一个方向（一般是水平方向）表达信息，而在垂直方向则不表达任何信息，其一定的高度通常是为了便于阅读器的对准。一维条形码的应用可以提高信息录入的速度，减少差错率，但是一维条形码也存在一些不足之处： 数据容量较小： 30个字符左右； 只能包含字母和数字； 条形码尺寸相对较大（空间利用率较低）； 条形码遭到损坏后便不能阅读。 一维条码所携带的信息量有限，如商品上的条码仅能容纳 13 位（EAN-13 码）阿拉伯数字，更多的信息只能依赖商品数据库的支持，离开了预先建立的数据库，这种条码就没有意义了，因此在一定程度上也限制了条码的应用范围。基于这个原因，在 90 年代发明了二维条码。二维条码除了具有一维条码的优点外，同时还有信息量大、可靠性高，保密、防伪性强等优点。 二维条码依靠其庞大的信息携带量，能够把过去使用一维条码时存储于后台数据库中的信息包含在条码中，可以直接通过阅读条码得到相应的信息，并且二维条码还有错误修正技术及防伪功能，增加了数据的安全性。 在水平和垂直方向的二维空间存储信息的条形码， 称为二维条形码（2-dimensional bar code），也称为二维码。它是用某种特定的几何图形按一定规律在平面（二维方向上）分布的黑白相间的图形记录数据符号信息的。 在代码编制上巧妙的利用构成计算机内部逻辑基础的 0，1 比特流的概念，使用若干个与二进制相对应的几何形体来表示文字数值信息，通过图像输入设备或光电扫描设备自动识读以实现信息自动处理. 2.1 二维码分类与一维条形码一样，二维条形码也有许多不同的编码方法（码制）。根据码制的编码原理而言，主要可分为以下二种类型： （1）堆叠式/行排式二维码 编码原理是建立在一维条码基础之上，将多个一维码在纵向堆叠而产生的。典型的码制如：Code16K、Code 49、PDF417等。 （2）矩阵式二维码 它是在一个矩形空间通过黑、白像素在矩阵中的不同分布进行编码。在矩阵相应元素位置上，用点（方点、圆点或其他形状）的出现表示二进制 “1”，点的不出现表示二进制的 “0”，点的排列组合确定了矩阵式二维条码所代表的意义。典型的码制如：Aztec、Maxi Code、QR Code、Data Matrix等。 在许多种类的二维条形码中，常用的码制有：Data Matrix, Maxi Code, Aztec, QRCode, Vericode, PDF417, Ultracode, Code 49, Code 16K 等,其中： Data Matrix 主要用于电子行业小零件的标识，如 Intel 的奔腾处理器的背面就印制了这种码。 Maxi Code 是由美国联合包裹服务（UPS）公司研制的，用于包裹的分拣和跟踪。 Aztec 是由美国韦林（Welch Allyn）公司推出的，最多可容纳3832个数字或3067个字母字符或1914个字节的数据。 这里我们提一下 PDF417 码，尽管 PDF417 码目前颇受欢迎，但我们不谈其流通性。PDF 是取英文：Portable Data File 三个单词的首字母的缩写，意为“便携数据文件”。我们可以认为条码就寓意便携数据文件，这对我们理解条码很重要。 最为常见的二维码为：QR Code，QR 全称:Quick Response，是一个近几年来移动设备上超流行的一种编码方式，它比传统的 Bar Code 条形码能存更多的信息，也能表示更多的数据类型：比如：字符，数字，日文，中文等等。因此很多人习惯将二维码称为 QR 码。 下面我们便以“ QR Code 码制二维码”为样例来解析二维码运作以及识别原理： 2.2 QR Code 运作原理2.2.1 二维码基础知识二维码存在 40 种尺寸。在官方文档中，尺寸又被命名为 Version。尺寸与 Version 存在线性关系：Version 1 是 21×21 的矩阵，Version 2 是 25×25 的矩阵，每增加一个 Version，尺寸都会增加 4，故尺寸 Size 与 Version 的线性关系为：Size=(Version−1)×4 。 例如：Version 的最大值是 40，故尺寸最大值是：(40-1)*4+21 = 177，即 177 x 177 的矩阵。 下面给出二维码（QR Code）结构示意图： 二维码的各部分构成都有自己的作用，基本上可被分为定位、功能数据、数据内容三部分： 定位图案： Position Detection Pattern（定位图案）：用于对二维码矩形的定位，对每个QR码来说，位置都是固定存在的，只是大小规格会有所差异。用三个定位图案即可标识并确定一个二维码矩形的位置和方向了； Separators for Position Detection Patterns（定位图案分割器）：用白边框将定位图案与其他区域区分； Timing Patterns（时序图案）：用于指示标识密度和确定坐标系，二维码如果尺寸过大，扫描时容易畸变，时序图案的作用可以防止扫描时畸变的产生； Alignment Patterns（校准图案）：只有在 Version 2 及其以上才会需要，校正标识用于进一步校正坐标系。校正标识的数量取决于版本； 功能数据： Format Information（格式信息）：存在于所有尺寸中，存放格式化数据，表示该二维码的纠错级别，分为L、M、Q、H： 级别 L、M、Q、H 分别表示: level L : 最大 7% 的错误能够被纠正； level M : 最大 15% 的错误能够被纠正； level Q : 最大 25% 的错误能够被纠正； level H : 最大 30% 的错误能够被纠正； Version Information（版本信息）：即二维码的规格，QR码符号共有40种规格的矩阵（一般为黑白色），从21x21（版本1），到177x177（版本40），每一版本符号比前一版本 每边增加 4 个模块。 需要预留两块 3×6 的区域存放部分版本信息； 数据内容：剩余部分存储数据内容 Data Code(数据码)：使用黑白的二进制网格编码内容。8个格子可以编码一个字节； Error Correction Code(纠错码)：用于修正二维码损坏带来的错误； 2.2.2 二维码数据编码说明数据编码就是把目标字符等转换成 QR 码的方法。这一部分我们来看 QR 码支持的编码内容（或者称为数据编码模式），包括纯数字、数字和字符混合编码、8 位字节码和包含汉字在内的多字节字符。 介绍具体的编码之前，先说一下 QR 码的最大容量问题： QR码的最大容量取决于选择的版本、纠错级别和编码模式（Mode:数字、字符、多字节字符等）。以“Version 1、纠错级别为Level Q 的 QR 码”为例，可以存储 27 个纯数字，或 17 个字母数字混合字符或 11 个 8bit 字节数据。如果要存储同样多的内容同时提高纠错级别，则需要采用更高的版本。版本 1~3 数据容量、纠错码容量对照如下表： （1）数据编码信息这里我们会介绍二维码数据编码涉及到的两个编码信息： [1.1] 模式标识符(Mode Indicator) QR 码的模式（Mode)就是前文提到的数字、字符、8bit 字节码、多字节码等。对于不同的模式，都有对应的模式标识符（Mode Indicator)来帮助解码程序进行匹配。模式标识符用 4 bit 的二进制数表示，如下图所示： 图 2.1 二维码支持的数据编码模式（注：中文编码模式为 1101；） [1.2] 文本串计数标识符（Character count indicator) 表 2.2 中显示了不同版本（即不同尺寸）的二维码，不同编码模式下用于编码字符串的长度的二进制位数（bits）。如要编码的文本串的长度为 8 个字符（Version 1；采用 Alphanumeric 编码模式），混合字符的长度为 9bit，因此将字符个数8编码为9位二进制表示：000001000。 图 2.2 字符计数指示器中的位数 （2）数据内容编码过程解读[1.1] 数字模式下的编码 在数字模式下，数据被限制为 3 个数字一段，分成若干段。如：”123456” 将分成”123” 和 “456”，分别被编码成 10bit（Version 1 to 9）的二进制数。“123” 的 10bit 二进制表示法为：0001111011，实际上就是二进制的 123。 当数据的长度不足 3 个数字时，如果只有 1 个数字则用 4bit，如果有 2 个数字就用 7 个 bit 来表示。如：”9876” 被分成 “987” 和 “6” 两段，因此被表示为”1111011011 0110”。 [1.2] 混合字符模式下的编码 字符编码的范围有：数字 0~9；大写 A~Z（无小写）；几个符号【$ % * + - . / 】和【空格】。将上述字符映射为一个索引表，如下图所示（图中 Char 表示字符，Value 表示字符对应的索引值）： 字符编码的过程，就是将每两个字符分为一组，然后转成上图的 45 进制，再转为 11bits 的二进制结果。如下所示，每段的第一个字符乘上 45，再用第二个数字相加。因此每段变成了 11bit 的二进制码，如果字符个数只有 1 个，则用 6bit 表示。 [1.3] 8-bit Byte模式下的编码 8bit 字节数据不经编码转换直接保存。 [1.4] 其它模式下的编码 其他类型的编码本文中不详细说明（如中文和日文都是双字节编码）。其中包括： 特殊字符集(Extended Channel Interpretation Mode)：主要用于特殊的字符集，并不是所有的扫描器都支持这种编码； 混合编码(Structured Append Mode)：说明该二维码中包含了多种编码格式（模式）； 综上所述，一个字符串数据的QR 编码格式现在我们可以看为：【模式标识符 + 文本串计数标识符 + 数据内容编码】。 基于上述各数据编码模式过程说明，下面分别用一个数字编码与字符编码的示例，说明数据编码的过程： （1）例程 1：数字编码 问题：对于 Version 1 尺寸的二维码，纠错级别为 H，编码为：01234567 解析步骤： 1. 将上述数字分为三组：012, 345, 67 2. 查询图 2.2 表格内容，Version 1 二维码的数字编码应转换为 10bits 的二进制数字，故将上面三组数字转为二进制分别为：012→0000001100, 345→0101011001, 67→1000011； 3. 将三个二进制串连接起来：0000001100 0101011001 1000011； 4. 将数字的个数转成二进制：对于数字编码，数字长度依旧用图 2.2 表格中查到的 10bits 二进制数字来表示，数字共有 8 个，故数字个数的二进制形式为：8→0000001000； 5. 查询图 2.1 表格内容，数字编码模式的标志符为： 0001，将编码标志与步骤 4 编码结果加到步骤 3 结果之前，故最终结果为：0001 0000001000 0000001100 0101011001 1000011 （2）例程2：字符编码 问题：对于 Version 1 尺寸的二维码，纠错级别为 H，编码为：AE-86 解析步骤： 1. 在图 2.3 的字符索引表中分别找到 AE-86 五个字符的索引分别为：(10, 14, 41, 8, 6)； 2. 将五个字符两两分组：(10, 14) (41, 8) (6)； 3. 字符编码应将字符组转换为 11bits 的二进制，故上述三组字符首先转为 45 进制后再转为二进制： * (10, 14)：转为 45 进制：10×45+14=464；再转为 11bits 的二进制：00111010000； * (41, 8)：转为 45 进制：41×45+8=1853；再转为 11bits 的二进制：11100111101； * (6)：转为 45 进制：6；再转为 6bits 的二进制：000110； 4. 将步骤 3 中得到的三个二进制结果连接起来：00111010000 11100111101 000110； 5. 查询图 2.2 表格内容，Version 1 二维码的字符个数应转换为 9bits 的二进制数字，对于 5 个字符，二维码字符个数转为 9bits 二进制为：000000101； 6. 查询图 2.1 表格内容，字符编码的标志为 0010，将编码标志与步骤 5 编码结果加到步骤 4 结果之前，故最终编码结果为：0010 000000101 00111010000 11100111101 000110； （3）结束符与补齐符对于结束符和补齐符，我们直接举例进行说明： 问题：对于 Version 1 尺寸的二维码，纠错级别为 H，以一串英文串作为编码：CHANDLERGENG 按照 2.3.2 字符编码例程进行分析，得到编码如下： 注意：上述图中字符数编码应该为：000001100，而非 00001101（13），下面也一样。 [1.1] 结束符 对于上述字符的编码完成后，需要在最后加上结束符。结束符为连续 4 个 0 值。加上结束符后，得到的编码如下： 如果所有的编码加起来不是 8 的倍数，则还需要在后面加上足够的 0。如上面一共有 83bits，所以与 8 的倍数还相差两位，故在最后加上 5 个 0，上表最终的数据变为： 00100000 01100010 00101101 00111011 00101001 01111001 01001000 10101101 11101000 00110110 00000000 [1.2] 补齐符 如果编码后的数据不足版本及纠错级别的最大容量 Bits 数限制，则需要在编码最后加上补齐符(Padding Bytes)。补齐符内容是不停重复两个字节：11101100 和 00010001。这两个二进制转成十进制，分别为 236 与17，具体不知道为什么选这两个值……关于每一个Version的每一种纠错级别的最大Bits限制，可以参看 QR Code Spec 的第35页到44页的 Table-7 一表（笔者参考的是《 ISO/IEC 18004 》2000 版），大致如下图 3.1 所示： 图3.1 二维码纠错级别的最大Bits限制（部分） 上图 3.1 中提到的 codewords，可译为码字，一个码字是一个字节。对于 Version 1 的 H 纠错级别，共需要 26 个码字，即 208bits。现在加上用 0 补全的结束符，已经有了 88bits（11码字），故还需要补上 15 码字。补齐后的编码数据即为数据码(Data Codewords)。 2.2.3 二维码纠错码前文提到了不同的纠错级别(Error Correction Code Level)。有了纠错机制，才可以使得有些二维码有了残缺也可以扫码解析出来，才可以使得二维码中心位置可以供某些商家加上对解析不必要的图标。 如上文种介绍的，二维码一共有四种纠错级别： （1）纠错码二维码对数据码加上纠错码的过程，首先要对数据码进行分组，即分成不同的块(Block)。参看如上图 3.1 所示 QR Code Spec 的第35页到44页的 Table-7 中的最下方说明了分组的定义表： 图4.1 二维码纠错级别说明（部分） 对于表中的最后两列的内容： 纠错块个数(Number of error correction blocks)：需要划分纠错块的个数； 纠错块码字数(Error Correction Code Per Blocks)：每个块中的码字个数，即有多少个字节 Bytes； 表中最下面关于 (c,k,r) 的解释： c：码字总个数； k：数据码个数； r：纠错码容量； c,k,r的关系公式：c = k + 2 × r （2）纠错码添加以上图中的 Version 5 + H 纠错机为例：图中红色方框说明共需要 4 个块（上下行各一组，每组 2 个块）。 第一组的属性： 纠错块个数 = 2：该组中有两个块； (c, k, r) = (33, 11, 11)：该组中每个块共有 33 个码字，其中 11 个数据码， 11×2=22 个纠错码； 第二组的属性： 纠错块个数 = 2：该组中有两个块； (c, k, r) = (34, 12, 11)：该组中每个块共有 34 个码字，其中 12 个数据码， 11×2=22 个纠错码； 具体示例如下表所示，且由于使用二进制会使得表格过大，故转为范围在 0~255 的十进制。其中组 1 的每个块，都有 11 个数据码， 22 个纠错码；组 2 的每个块，都有 12 个数据码，22 个纠错码。 二维码的纠错码主要是通过里德-所罗门纠错算法(Reed-Solomon Error Correction)实现的。 2.2.4 最终编码此时得到了数据，但还不能开始画图，因为二维码还需要将数据码与纠错码的各个字节交替放置。 （1）穿插放置继续以上一小节中给出的示例为例，给出其穿插放置的过程。 上一小节示例中的数据码如下表所示： 提取每一列数据： 第一列：67, 66, 247, 194； 第二列：85, 7, 119, 6； …… 第十一列：6, 199, 134, 17； 第十二列：151, 236； 将上述十二列的数据拼在一起：67, 66, 247, 194, 85, 7, 119, 6,…, 6, 199, 134, 17, 151, 236。 纠错码如下表所示： 同样的方法，将 22 列数据放在一起：199, 177, 96, 173, 11, 212, 60, 24, …, 148, 117, 118, 76, 235, 129, 134, 40。 上述部分即为二维码的数据区。 2.2.5 剩余位 (Remainder Bits)对于某些 Version 的二维码，得到上面的数据区结果长度依旧不足，需要加上最后的剩余位。比如对于 Version 5 + H 纠错等级的二维码，剩余位需要加 7bits，即加 7 个 0。参看 QR Code Spec 的 Table-1 一表即可查询不同 Version 的剩余位信息，如下图 5.1 所示： 图5.1 不同 Version 的剩余位 2.3 二维码的绘制终于讲到二维码绘制过程了，绘制的过程按照顺序对照二维码结构示意图中各个重要部分依次讲解: 2.3.1 定位图案 (Position Detection Pattern)首先在二维码的三个角上绘制定位图案。定位图案与尺寸（Version）大小无关，一定是一个 7×7 的矩阵。如下图 6.1 所示： 图6.1 定位图案 (Position Detection Pattern) 2.3.2 对齐图案 (Alignment Pattern)然后绘制对齐图案。对齐图案与尺寸大小无关，一定是一个 5×5 的矩阵。如下图 6.2 所示： 图6.2 对齐图案 (Alignment Pattern) 对齐图案绘制的位置，可参看 QR Code Spec 的 Table-E.1 一表查询，部分内容如下图 6.3 所示： 图 6.3 对齐图案位置索引表（部分） 下图 6.4 是上述表格中 Version 8 的一个例子，对于 Version 8 的二维码，行列值在 6, 24, 42 的几个点都会有对齐图案。 图6.4 对齐图案例程 1 下面我会给出一个可以扫描后可打开百度首页链接的二维码图片样例。该二维码中只有一个对齐图案，故 Version 应该在 V2—V6 之间。 图6.5 对齐图案例程 2 2.3.3 时序图案 (Timing Pattern)时序图案是两条连接三个定位图案的坐标线，如下图 6.6 所示： 图6.6 时序图案例程 1 依旧以上面百度链接二维码为例，其时序图案如图 6.7 所示： 图6.7 时序图案例程 2 2.3.4 格式信息格式信息如下图 6.8 所示： 图6.8 格式信息 格式信息在定位图案周围分布，由于定位图案个数固定为 3 个，且大小固定，故格式信息也是一个固定 15bits 的信息。每个 bit 的位置如下图 6.9 所示：（注：图中的 Dark Module 是固定永远出现的） 图6.9 格式信息位置 15bits 中数据，按照 5bits 的数据位 + 10bits 纠错位的顺序排列： 数据位占 5bits：其中 2bits 用于表示使用的纠错等级 (Error Correction Level)，3bits 用于表示使用的蒙版 (Mask) 类别； 纠错位占 10bits：主要通过 BCH Code 计算； 为了减少扫描后图像识别的困难，最后还需要将 15bits 与 101010000010010 做异或 XOR 操作。因为我们在原格式信息中可能存在太多的 0 值（如纠错级别为 00，蒙版 Mask 为 000），使得格式信息全部为白色，这将增加分析图像的困难。 纠错等级的编码如下图 6.10 的表格所示： 图6.10 纠错等级编码 关于蒙版图案的生成，在后文 2.3.7 中具体说明。格式信息的示例如下： 假设存在纠错等级为 M（对应 00），蒙版图案对应 000，5bits 的数据位为 00101，10bits 的纠错位为 0011011100：则生成了在异或操作之前的 bits 序列为：001010011011100与 101010000010010 做异或 XOR 操作，即得到最终格式信息：100000011001110 2.3.5 版本信息 (Version Information)对于 Version 7 及其以上的二维码，需要加入版本信息。如下图 6.11 蓝色部分所示： 图6.11 版本信息 版本信息依附在定位图案周围，故大小固定为 18bits。水平竖直方向的填充方式如下图 6.12 所示： 图6.12 版本信息填充方式 18bits 的版本信息中，前 6bits 为版本号 (Version Number)，后 12bits 为纠错码 (BCH Bits)。示例如下： 假设存在一个 Version 为 7 的二维码（对应 6bits 版本号为 000111），其纠错码为 110010010100；则版本信息图案中的应填充的数据为：000111110010010100 2.3.6 数据码与纠错码此后即可填充前面章节得到的数据内容了。填充的思想如下图 6.13 的 Version 3 二维码所示，从二维码的右下角开始，沿着红线进行填充，遇到非数据区域，则绕开或跳过。 图6.13 二维码数据填充（原始版） 然而这样难以理解，我们可以将其分为许多小模块，然后将许多小模块串连在一起，如下图 6.14 所示（截取自 QR Code Spec 的图 15）： 图6.14 二维码数据填充 小模块可以分为常规模块和非常规模块，每个模块的容量都为 8。常规情况下，小模块都为宽度为 2 的竖直小矩阵，按照方向将 8bits 的码字填充在内。非常规情况下，模块会产生变形。 填充方式上图 6.14，图中深色区域（如 D1 区域）填充数据码，白色区域（如 E15 区域）填充纠错码。遍历顺序依旧从最右下角的 D1 区域开始，按照蛇形方向（D1→D2→…→D28→E1→E2→…→E16→剩余码）进行小模块的填充，并从右向左交替着上下移动。下面给出若干填充原则： 原则 1：无论数据的填充方向是向上还是向下，常规模块（即 8bits 数据全在两列内）的排列顺序应是从右向左，如下图 6.15所示； 图6.15 常规模块内的填充方向 原则 2：每个码字的最高有效位（即第7个bit）应置于第一个可用位。对于向上填充的方向，最高有效位应该占据模块的右下角；向下填充的方向，最高有效位占据模块的右上方。 注：对于某些模块（以下图 6.17 为例），如果前一个模块在右边模块的列内部结束，则该模块成为不规则模块，且与常规模块相比，原本填充方向向上时，最高位应该在右上角，此时则变为左下角； 原则 3：当一个模块的两列同时遇到对齐图案或时序图案的水平边界时，它将继续在图案的上方或下方延续； 原则 4：当模块到达区域的上下边界（包括二维码的上下边界、格式信息、版本信息或分隔符）时，码字中任何剩余 bits 将填充在左边的下一列中，且填充方向反转；如下图 6.16 中的两个模块遇到了二维码的上边界，则方向发生变化； 图6.16 非常规模块填充方向的改变（举例于 QR Code Spec 图 13） 原则 5：当模块的右一列遇到对齐图案，或遇到被版本信息占据的区域时，数据位会沿着对齐图案或版本信息旁边的一列继续填充，并形成一个不规则模块。如果当前模块填充结束之前，下一个的两列都可用，则下一个码字的最高有效位应该放在单列中，如下图 6.17 所示： 图6.17 模块单列填充 2.3.7 蒙版图案按照上述思路即可将二维码填充完毕。但是那些点并不均衡，如果出现了大面积的空白或黑块，扫描识别会十分困难，所以按照在前文 6.4 中格式信息的处理思路，对整个图像与蒙版进行蒙版操作(Masking)，蒙版操作即为异或 XOR 操作。二维码又 8 种蒙版可以使用，如下图 6.18 所示，公式也在图中说明。蒙版只会和数据区进行异或操作，不会影响与格式信息相关的功能区。 注：选择一个合适的蒙版也是有一定算法的。 蒙版图案如下图 6.18 所示，对应的产生公式与蒙版 ID 如下图 6.19 的表格所示： 图6.18 蒙版图案 图6.19 蒙版图案产生公式 蒙版操作的过程与对比图如下图 6.20 所示，图中最上层是没有经过蒙版操作的原始二维码，其中存在大量黑色区域，难以后续的分析识别。经过两种不同蒙版的处理，可以看到最后生成的二维码变的更加混乱，容易识别。 图6.20 蒙版操作示例 蒙版操作之后，得到的二维码即为最终我们平常看到的结果。 3. 参考内容二维码生成原理及解析代码 QR 码生成原理 QR Code 编码方式]]></content>
      <categories>
        <category>Barcode</category>
      </categories>
      <tags>
        <tag>Barcode</tag>
        <tag>Detection</tag>
        <tag>recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN 配置以及使用过程问题记录]]></title>
    <url>%2FSVN%2FSVN-%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇 这一章节我们来记录一下 SVN 配置以及使用过程中出现的问题以及其解决办法。 1. SVN 用户登陆缓存信息清理问题描述： SVN 每次操作之后需要我们进行用户信息验证以加载用户策略，一般情况下每个使用者只会分配有一个用户登陆信息。为了方便使用（避免每次进行登陆操作），大多时候我们登陆时选择保存 SVN 用户登陆信息（用户名和密码），账号信息被缓存到本地。 有时，我们需要在同一台 Client 上切换 SVN 账户测试或者清除我们已经保存的 SVN 用户名和密码时，可以通过以下方法来解决： 解决方法： 类似于网页 cookie，只需清理掉本地缓存的账号信息即可。 1）TortoiseSVN（Windows） 安装好SVN客户端后，右键 -&gt; TortoiseSVN -&gt; Settings 在弹出的 “Settings” 窗口右侧，点击 “Saved Data”。然后点击左侧的 “Authentication” 后方的 “clear” 按钮即可： 2）Linux 下清除方法： # 删除 ~/.subversion/auth 即可： [svn@node3 ~]$ rm -rf ~/.subversion/auth # 查看 auth 目录： [svn@node3 ~]$ ls .subversion/auth/ svn.simple svn.ssl.client-passphrase svn.ssl.server svn.username # 查看 svn.simple 目录下文件： [svn@node3 ~]$ ls .subversion/auth/svn.simple/ 1536ce017edf3a98fc512b19d66cdf84 2db12f8c4e4f4bbc3a8094149b66886a e32130e64b530d920e4e52659e55cf38 ebfa26e64ae4c3498248b0d27827f62a # 1536ce017edf3a98fc512b19d66cdf84 存储有登陆信息： [svn@node3 ~]$ vim .subversion/auth/svn.simple/1536ce017edf3a98fc512b19d66cdf84 K 15 svn:realmstring V 60 &lt;svn://10.1.0.128:3690&gt; b422724a-6242-456b-a449-37b6c0181153 K 8 username V 5 usage END 2. import 目录作为工作副本问题描述： 在 SVN 服务器配置与使用指南中我们介绍过：初始化 SVN 版本仓库目录，我相信很多人可能会想到这样一个问题：初始化（导入）版本库之后的临时目录能作为 SVN 工作副本使用么，我们就不必去删除掉重新 checkout？ SVN 本来就没有考虑过这个问题，貌似就只能删掉原始工程目录，而重新从库中检出一个带版本控制信息的一模一样的工程（import 仅仅是把文件导入到 SVN 服务器中，而这个文件本身还只是一个普通的文件，与 SVN 版本库没有关系）。 此时，我们面临一个极其恶劣的问题：将本地的工程目录全部提交上去之后，但是这个原始工程目录却不是工作拷贝，于是我还得把这个原始工作目录删掉，重新从版本库中 checkout 出来一个拷贝。这太不爽了，暂且不论这个工程占多大磁盘空间，明明有这个工程，只是缺少版本控制信息，有必要就删掉而重新检出整个工程么。 解决方法： 下面我们介绍一个小技巧来解决这种问题（以一个简单项目：Kmeans/testCode）： 1）导入 Kmeans 工程到版本库： [root@node3 svn]# svn import Kmeans file:///home/svn/SVNProject/SVNRepos/Kmeans -m &quot;Just a Kmeans Test&quot; 2）执行最关键的命令(参数 –depth=empty 是精髓，它实际上只检出版本控制信息，除此之外不检出任何文件)： [root@node3 svn]# svn co --depth=empty file:///home/svn/SVNProject/SVNRepos Kmeans 3）进入 Kmeans 工程目录，查看工程文件状态： [root@node3 svn]# cd Kmeans/ # 显示所有文件前全带？号，因为此时这些文件还不在版本控制管辖范围内。 [root@node3 Kmeans]# svn st ? test 4）加入待变更列表： [root@node3 Kmeans]# svn add * A test 5）为了与版本库一致，执行 update 命令(其实此时这些文件与现在版本库中是一模一样的，因为我们才刚刚提交完，没有作任何更改，此举是为了 “骗过” svn): [root@node3 Kmeans]# svn up * # 或： [root@node3 Kmeans]# svn commit -m &quot;first&quot; 此时更新必然会有冲突，而且还是 100％，程序提示“在 “xxx” 中发现冲突。选择: (p) 推迟，(mf) 全用我的，(tf) 全用他人的,(h) 使用帮助以得到更多选项:”,这时我们一定要选“(mf) 全用我的“，即输入 mf，否则我们前面的工作就没有意义了。 6）此时，使用下面的命令均无显示，因为无状态变化，现在这个原始工程已经“转变“成一个work copy： [root@node3 Kmeans]# svn st [root@node3 Kmeans]# svn ci]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TortoiseSVN 快速使用指南]]></title>
    <url>%2FSVN%2FTortoiseSVN-%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 在 Centos 6/7 SVN + Apache（HTTP）服务器配置与使用指南中，我们已经详细讲解了 SVN 服务器配置以及使用指南，为了快速掌握（快速入门）通过远程客户端管理和使用 SVN 版本仓库，这里我们来介绍日常项目版本库常见的 SVN 版本库管理和使用操作。 关于 SVN Linux 客户端环境下的 SVN 版本库管理和使用操作可以参见我们之前在 “ [Centos 6/7 SVN + Apache（HTTP）服务器配置与使用指南” 中 【附录：I】 的内容，附录中我们讲解了很多常见操作。如果你熟知上一小节的内容，可以与这一节进行一一映照。 事实上更多数情况下，我们是以 Linux Server 为生产环境，而以 Windows 环境进行项目开发，故我们以 Windows TortoiseSVN（SVN 客户端应用程序）的使用来看常见的 SVN 版本库管理和使用操作。 1. TortoiseSVN 安装说明首先我们来看基于 Windows 安装 TortoiseSVN： 开发人员强烈建议使用 IDE 中的 SVN 插件更加智能与便利化。 准备安装包：Windows 一般选择 TortoiseSVN（乌龟）应用程序作为 SVN 客户端。官网地址：https://tortoisesvn.net/downloads.html 通过官网下载链接，我们可以根据个人 PC Windows 系统位数选择相应版本客户端进行下载安装。 安装注意事项： 如果你喜欢（想掌握）在命令行下操作，请务必记得勾选 command line client tool 为 will be install on local hard driver（如下图所示）。如果你不想配置命令行操作直接跳过这一步即可： Install Success 标识: 其它安装步骤很简单，一路 next 即可完成安装。安装成功标志：安装完毕后，在任意地方点击右键查看快捷菜单，可以发现 TortoiseSVN 功能项即表示安装成功（如下图所示）: 如果在安装过程中勾选了安装命令行工具选项，那么在 DOS 窗口（cmd）输入命令： svn help，有如下提示也表示安装成功（如下图所示）: 语言包配置（汉化）: 默认 TortoiseSVN 是全英文界面的，TortoiseSVN 支持多国语言包。可以通过下载对应系统位数语言安装包进行汉化（官方支持语言包如下图所示）： 安装完语言包之后，可以通过点击：右键 -&gt; TortoiseSVN -&gt; Settings 设置 TortoiseSVN 语言环境: 安装以及设置完成后，你会发现 TortoiseSVN 界面已经被汉化！！！ 安装教程这里已经结束，下面我们开始正式介绍如何 TortoiseSVN 快速使用指南。 2. TortoiseSVN 使用说明2.1 检出项目（SVN checkout）假设目标项目存储于 SVN 服务器的版本仓库中，想要对其进行开发。我们首先需要将它从版本仓库中 checkout 到本地: 步骤如下： 首先创建一个空文件夹（例如：SVNTest）。进入到空文件夹内：右键 -&gt; SVN 检出（checkout）（如下图所示：）。 上述操作之后自动跳转到下面界面（如下图所示）：（填入待检出）版本库 URL（URL of repository）-&gt; 确定。 此时会弹出一个对话框（如下图所示）让你输入账号密码（验证用户信息），输入你的账号密码即可。记得勾选保存认证（清除缓存方法件 【 SVN 配置以及使用过程问题记录】），不然每次操作都会让你输入。 然后，等几分钟（取决于目标项目大小）就可以检出完毕（如下图）: 检出完成后，此时在你创建的新目录 SVNTest 下就能看到你 checkout 到的项目: 2.2 导入项目很多时候我们已经在本地建立好了项目（例如：SVNProject），需要我们把项目推到 SVN 上，此时应怎么做呢？（如下图所示）：右键 -&gt; TortoiseSVN -&gt; Repo-browser(版本库浏览器)。 明确我们要将项目导入到哪个版本库（例如：[/] 路径下）中，然后在相应目录（[/]）下:右键 -&gt; 加入文件/加入文件夹，选择中相应目录（SVNProject）即可。 样例 假设我们现在有个项目叫 SVNProject，我们想把它上传到 SVN 上进行版本控制。由上可知，我们只需选择【加入文件夹】 即可。 务必要输入提交信息。这样别人才能知道你干了什么（如下图）： 注意 不要以为导入成功就可以对导入项目进行版本控制了（SVNProject 只是一个本地项目）。你还得将其重新检出，重新检出的项目才是受 SVN 控制的，务必记得重新检出。（SVN 导入机制问题：当导入项目比较大时，想想都头皮发麻….难受….如果想解决请参考 【 SVN 配置以及使用过程问题记录】） 在 SVNProject 上右键检出到本地，然后在里面进行编辑就可以愉快的工作了。检出过后的右键菜单变成了这样。 2.3 提交以 checkout 出来的 SVNProject 为例： 1) 修改文档提交测试 项目中绿色对勾状态表示当前文件没有被修改过（如果看不见颜色状态，重启计算机）。 假如我们在 ReadMe.txt 文档中加了一行字（修改文档操作），然后保存。你会发现当前文件状态变成了红色叹号：表示当前文件已被修改。 那么，怎么提交这种修改操作至 SVN 服务器呢？在根目录下，右键 -&gt; 提交（SVN Commit）。 务必记得输入提交信息（虽然不输入也能提交）。提交信息可以方便日后查看：文档做过哪些修改。 提交完毕后，可以发现文件状态又恢复到了绿色。 2) 添加文档 假如我们现在准备在项目中加入一个新文件。可以看出文件状态是：蓝色问号。蓝色问号表示不属于版本库的未知文件，未知文件是无法提交的。 首先，记住选择增加 【svn add】 将当前新增加的文件添加到待变更列表，先让其纳入版本库管理范畴。 svn add 完毕后，项目文件状态变成了蓝色加号，表示新增加的版本库文件。 接下来，只需写代码，然后提交即可。删除文件也需要进行提交，如下图所示： 注意 记得随时检查你的文件状态，如果没有添加到版本控制里要及时添加进去，不然你的文件提交不上去。 2.4 更新假如你和 B 同学在协作。B 同学写完代码提交到了 SVN 上，如果你想获取 SVN 服务器上的最新修改，就需要选择更新（如果服务器上已经有别人提交过的新的，你是提交不上去的，必须先更新再提交）。 怎么知道服务器有没有更新？你可以直接选择更新，有没有更新一下就知道。或者:右键 -&gt; check for modifications，然后检查版本库(check Repository)，就能看到服务器上改了哪些文件。 右键 -&gt; Compare with base 左边的表示你的代码，右边的表示服务器上的代码。 如果有修改记得及时更新到本地然后再继续工作。但是有时候更新会冲突，比如你和服务器上的改了同一个地方。这时候你需要更新下来解决冲突。 它会提示你哪个文件冲突，你只需打开那个文件，按照需求解决冲突即可。 &lt;&lt;&lt;&lt;&lt;&lt;.mine 到 ==== 表示你的代码，其他表示服务器的代码。你只需改成你想要的。 然后选择解决，告诉 SVN 我已经解决冲突了就行了。 剩下的就是团队协作间的更新提交操作，这里不做赘述。 2.5 查看日志查看项目日志信息：Tortoise -&gt; show log（显示日志），可以看出团队里面的人干了什么（项目版本库有哪些变动）。 可以看出谁谁谁，什么时间，干了什么事。最后那一列信息是自己提交的时候写的。建议大家提交时务必要填写提交信息，这样别人一看就知道你干了什么。提交信息对于自己也是有好处的，时间长了也能看到当初做了什么。 2.6 版本回滚如果你修改了某个文档，但是还没有提交，可以使用还原功能。 但是如果我们写错了东西并且提交了上去怎么办？通过版本回滚可以将文件恢复到以前的版本。右键 -&gt; update to reversion（更新至版本），通过查看日志来选择版本，然后回滚即可。 有时候我们需要查看以前版本的代码。此时我们可以新建个文件夹检出到指定版本。 2.7 版本控制版本控制有好几种方法: 1）在提交发布版本时添加版本信息，这是最简单的一种方法。 2）打标签 每次发布版本时应该打标签。右键选择分支/标记。在至路径以版本号打上标签即可 这样你就有了一个 v1.0 版本的标签。 以后如果你想查看某个版本的代码，只需切换过去就行（如下图所示）: 3. TortoiseSVN 使用总结我们在日常使用中，最常用的是更新和提交操作，这两个步骤务必要非常熟练，其他的可以在遇到问题是查看文档。此外，需要注意的是，所有版本控制工具只能跟踪文本文件（能用记事本打开查看的文件），不要妄想SVN能记录你 word 改了哪一行。一旦遇到 word 冲突，记住仔细对比两个版本，然后解决冲突。]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
        <tag>TortoiseSVN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 6/7 SVN + Apache（HTTP）服务器配置与使用指南]]></title>
    <url>%2FSVN%2F%E5%9F%BA%E4%BA%8E-Centos-6-7-SVN-%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇 在日常项目开发中，我们肯定会或多或少地听说或者使用过版本管理工具。刚好最近由于新项目需求：需要专门为当前项目搭建一套 SVN 版本控制系统，再加上项目组日常项目开发使用 SVN，学习 SVN 势在必行… 再说明一下本文所作目的：一方面本文作为 “SVN 版本控制系统的搭建与管理” 过程记录，方便日后回顾使用；另一方面由于在搭建过程中受益于博客上记录过 SVN 环境搭建的前辈们，故作此以分享给更多的 SVN 新手们。 1. SVN 服务简介1.1 何为版本控制？版本控制是指对软件开发过程中各种程序代码、配置文件及说明文档等文件变更的管理。 版本控制最主要的功能就是追踪文件的变更。它将什么时候、什么人更改了文件的什么内容等信息忠实地了已录下来。每一次文件的改变，文件的版本号都将增加。 除了记录版本变更外，版本控制的另一个重要功能是并行开发。软件开发往往是多人协同作业，版本控制可以有效地解决版本的同步以及不同开发者之间的开发通信问题，提高协同开发的效率。 1.2 什么是 SVN（Subversion）以及 SVN 版本库？Subversion 是一个 自由/开源 的版本控制系统，它管理文件和目录可以超越时间。 一组文件存放在中心版本库 （Subversion 的核心就是 Repository ，中文翻译成“版本库”。位于服务器端，统一管理和储存数据的地方。），这个版本库很像一个普通的文件服务器，只是它可以记录每一次文件和目录的修改，这便使你可以取得数据以前的版本，从而可以检查所作的更改。从这个方面看，许多人把版本控制系统当作一种“时间机器”。 SVN（Subversion）数据存储方式（两种）：一种是在 Berkeley DB 数据库中存储数据（BDB）； 另一种是使用普通的文件 FSFS 存储数据。由于 Berkeley DB 方式在使用中有可能锁住数据，一般建议使用 FSFS 方式更安全。 SVN（Subversion）的运行模式（两种）：一种是基于 SVN Server 的独立服务器；另一种是借助 Apache 的 http、https 网页访问模式。即 【svn://】 或 【http://；https：//】。 目前很多的开源软件仍然在使用 SVN 作为代码版本管理软件。Subversion 支持：linux、windows、Mac 等平台安装，但较多安装在 linux 下。 1.3 SVN 基本工作原理首先我们给出 SVN 工作原理示意图： SVN 的基本工作原理： 在一台服务器（SVN Server）上建立一个源代码库（版本库：Repository），源码库里可以存放许多不同项目的源程序，由源代码库管理员统一管理。 （Client）每个用户在使用源代码库之前，首先要把源代码库里的项目文件下载（svn checkout）到本地，然后开发人员可以在本地修改，修改完成后用 SVN 命令（svn commit）进行提交，然后还可以通过 SVN 命令将本地仓库的代码更新到最新。由源代码统一库管理、修改、更新。 1.4 Subversion 解决的问题（1）代码管理混乱； （2）解决代码冲突困难； （3）在代码整合期间引发 BUG； （4）无法对代码的拥有者进行权限控制； （5）项目不同版本的发布困难； 1.5 SVN（Subversion）基本操作（1） 版本库创建（create）：版本库一般创建在一个服务器上，其他用户将自己的文件提交到这个服务器进行保存。版本库包括提交的文件，还有修改历史。 （2）检出（checkout）：将创建一个版本库的工作副本。工作副本是开发者私人空间，可以在工作副本中进行代码的修改，添加文件等操作。当然，不用担心会对服务器的版本库造成破坏，因为未提交到服务器上。 （3）执行变更（add、del）：当检出一个工作副本后，可以对这个副本中的文件进行修改或删除，操作完毕后，通过 add 或 del 将其加到待变更列表中。直到执行了 commit 之后才真正删除。 （4）提交更改（commit）：可以将在自己工作副本中修改的内容提交到服务器上，修改服务器版本库的内容。其他用户再次 checkout 的时候，将会是你 commit 的内容。当然，commit 之前，先必须将其修改的内容加到待变更列表中。 （5）更新（update）：用来更新版本库，将工作副本与服务器上的版本库进行同步。 （6）复查变化，修复错误，解决冲突等操作。这些可以从网上找到相关资料，此处不深入研究（这里只解释基本操作）。 通过上文内容的说明，我们已经了解了 SVN（Subversion）的主要功能：版本控制，基本工作原理（C/S）以及其基本相关操作。我们知道 SVN 有两种运行模式：1）基于 SVN Serve 的独立服务器；2）基于 Apache 的 HTTP(S) 网页访问模式。 本文我们将以“基于 SVN Serve 的独立服务器，并配置 Apache 支持 HTTP(S) 方式访问”进行 SVN 服务环境的搭建。 Base Environment： 操作系统：CentOS 6.x / 7.x； 服务器 IP:10.1.0.123； SVN 数据存储：FSFS 数据存储方式； SVN 运行模式：SVN（Server） + Apache （HTTPS）； 2. SVN + Apache（HTTP）服务器配置在开始搭建 SVN 服务器之前，我个人建议先进行预安装环境部署，这里是指：我们创建一个 svn 用户并进行授权，使用 svn 用户来管理所有 SVN 安装以及版本库数据。 当然这是可选择的，如果你不希望添加用户的话，你可以直接创建一个 svn 目录用于 SVN 后续安装操作的基础目录（表示 svn 用户目录：~）： $ mkdir /home/svn 注意，如果你不希望添加用户的话，可以直接跳过 预安装环境 部分，从 2.1 直接开始阅读即可。 预安装环境（推荐）1）添加 svn 用户 首先以 root 用户登陆服务器，然后添加新用户的信息 —&gt; 用户名:【 svn 】，密码:【 N@us0ft 】 # 创建一个用户名为 svn 的普通用户： [root@node3 ~]# adduser svn # 初始化 svn 用户密码（linux 会判断密码复杂度，可以强行忽略）： [root@node3 ~]# passwd svn 2） svn 用户授权 个人用户的权限只可以在本 home （/home/svn）下拥有完整权限，操作其他用户目录需要被授权。而我们经常需要 root 用户的权限。 sudo 命令可以使得普通用户临时获取 root 权限。 新创建的用户并不能直接使用 sudo 命令，需要为其进行授权： 【1】授权管理：sudo 命令的授权管理是在 sudoers 文件里进行控制的。 ### 1. 查找 sudoers 文件位置信息： [root@node3 ~]# sudoers bash: sudoers: 未找到命令... [root@node3 ~]# whereis sudoers sudoers: /etc/sudoers /etc/sudoers.d /usr/libexec/sudoers.so /usr/share/man/man5/sudoers.5.gz ### 2. 找到 sudoers 文件位置之后，再查看其操作权限： [root@node3 ~]$ ls -l /etc/sudoers -r--r-----. 1 root root 3934 8月 22 14:21 /etc/sudoers ### 3. 我们发现：对 sudoers 文件只有读权限，需要添加 w 权限： [root@node3 svn]# chmod -v u+w /etc/sudoers mode of &quot;/etc/sudoers&quot; changed from 0440 (r--r-----) to 0640 (rw-r-----) # 再次查看，已经添加了 w 权限： [root@node3 svn]# ls -l /etc/sudoers -rw-r-----. 1 root root 3934 8月 22 14:21 /etc/sudoers ### 4. 在 sudoers 文件为用户进行 sudo 命令的授权： [root@node3 svn]# vim /etc/sudoers # 在目标位置添加以下内容，wq 保存退出： ## Allow root to run any commands anywher root ALL=(ALL) ALL svn ALL=(ALL) ALL # 新添加用户 sudo 授权 ### 5. 完成后，记得回收 sudoers 操作权限： [root@node3 svn]# chmod -v u-w /etc/sudoers mode of &quot;/etc/sudoers&quot; changed from 0640 (rw-r-----) to 0440 (r--r-----) 【2】通过 sudo 临时获取 root 权限 ### 1. svn 用户下使用 yum install： # 切换到 svn 用户： [root@node3 svn]# su svn [svn@node3 ~]$ yum install 已加载插件：fastestmirror, langpacks Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast 您需要 root 权限执行此命令。 [svn@node3 ~]$ sudo yum install We trust you have received the usual lecture from the local System Administrator. It usually boils down to these three things: #1) Respect the privacy of others. #2) Think before you type. #3) With great power comes great responsibility. [sudo] password for svn: # 输入 svn 用户密码即可执行命令 第一次使用 sudo cmd 会提示你，你已经化身超人，身负拯救世界的重则。而且需要输入密码才可以下一步。如果不想需要输入密码怎么办，将最后一个ALL修改成 NOPASSWD: ALL。 下面让我们正式开始 SVN 服务器搭建： 2.1 基于 SVN Serve 的 SVN 服务器搭建之 Server 端2.1.1 SVN 的安装1）安装 SVN（Subversion） 12345678### 1. 检测系统是否安装了低版本 SVN：$ rpm -qa subversion# 如果存在旧版本 SVN，则卸载：[svn@node3 ~]$ yum remove subversion### 2. 开始安装 SVN：[svn@node3 ~]$ yum -y install subversion Complete！完成之后，开始后续操作： 2）查看版本信息 ### 1. 安装是否成功：svn --version 或 svnserve --version [svn@node3 ~]$ svnserve --version svnserve，版本 1.7.14 (r1542130) 编译于 Apr 11 2018，02:40:28 版权所有 (C) 2013 Apache 软件基金会。 此软件包含了许多人的贡献，请查看文件 NOTICE 以获得更多信息。 Subversion 是开放源代码软件，请参阅 http://subversion.apache.org/ 站点。 下列版本库后端(FS) 模块可用: * fs_base : 模块只能操作BDB版本库。 * fs_fs : 模块与文本文件(FSFS)版本库一起工作。 Cyrus SASL 认证可用。 可使用以下的版本库访问模块: * ra_neon : 通过 WebDAV 协议使用 neon 访问版本库的模块。 - 处理“http”方案 - 处理“https”方案 * ra_svn : 使用 svn 网络协议访问版本库的模块。 - 使用 Cyrus SASL 认证 - 处理“svn”方案 * ra_local : 访问本地磁盘的版本库模块。 - 处理“file”方案 2.1.2 SVN 的配置1）新创建目录（包）存储 SVN 数据 # Path： /home/svn/SVNProject # svn 用户目录下创建 SVNProject 目录，用于存放所有 SVN 数据； # 如果没有创建 svn 用户，我们直接在 /home/svn 下创建 SVNProject 目录： [svn@node3 ~]$ mkdir -p /home/svn/SVNProject 示例：/home/svn/SVNProject 存储了所有的版本库数据。 2）创建版本仓库（Repository） 我们可以在 SVNProject 目录下创建多个版本仓库。下面我们来看一下如何创建我们的第一个版本仓库？ 步骤一：创建版本仓库目录 [svn@node3 ~]$ mkdir -p SVNProject/SVNRepos 步骤二：构建 SVN 版本库 ### 1. 使用 FSFS 存储方式创建版本库 # 如果需要使用 BDB 方式存储，则将 --fs-type fsfs ---&gt; --fs-type bdb [svn@node3 ~]$ svnadmin create --fs-type fsfs SVNProject/SVNRepos/ # 执行上面的命令后，自动在 SVNRepos 下自动生成多个文件， 分别是：conf、db、format、hooks、locks、README.txt。 [svn@node3 ~]$ ls SVNProject/SVNRepos/ conf db format hooks locks README.txt |—————————————————————— 下面我们给出 Subversion 版本库目录结构说明： （1）dav 目录：提供 Apache 与 mod_dav_svn 使用的目录，用于其存储内部数据；（某一版本？） （2）db 目录：所有版本控制的数据存放文件； （3）hooks 目录：放置 hook 脚本文件的目录； （4）locks 目录：用来放置 subversion 锁定数据的目录，用来追踪存取文件库的客户端； （5）format 文件：是一个文本文件，里面只放了一个整数。表示当前文件库配置的版本号； （6）conf 目录：当前仓库的配置文件。里面有 3 个文件【 passwd # 添加使用 svnserve 的用户和密码；authz # 记录不同用户的访问策略；svnserve.conf # svn 服务配置文件，例如： 生效 authz 和 passwd 配置。】 ——————————————————————| 2.1.3 版本仓库的配置我们知道创建一个版本仓库之后，会自动生成一个 conf 目录，目录中存放了 passwd、 authz、 svnserve.conf 三个文件，它们共同构成了svnserve 的配置文件。下面我们来看如何配置这些文件，更加安全便利地使用 SVN 服务。 1）passwd 用户配置 passwd 文件用于给 SVN 版本仓库添加使用用户，文件配置非常简单。我们先来看文件内容： ### This file is an example password file for svnserve. ### Its format is similar to that of svnserve.conf. As shown in the ### example below it contains one section labelled [users]. ### The name and password for each user follow, one account per line. [users] # harry = harryssecret # sally = sallyssecret 可见：该文件由一个 [users] 配置段组成，格式：&lt;用户名&gt;=&lt;密码口令&gt; 注：口令为未经过任何处理的明文。添加使用用户时，我们只需要按格式在末尾处增加即可。样例如下： [svn@node3 conf]$ vim passwd [users] # harry = harryssecret # sally = sallyssecret admin = admin # 注意：居左顶齐，不要留空格； 2）authz 用户策略配置 authz 文件用来记录 SVN 用户的访问策略。它由 [groups] 配置段和若干版本库路径权限段组成，文件主要内容如下： [groups] # harry_and_sally = harry,sally # harry_sally_and_joe = harry,sally,&amp;joe # [/foo/bar] # harry = rw # &amp;joe = r # * = # [repository:/baz/fuz] # @harry_and_sally = rw # * = r 详细说明如下： First：[groups] 配置段 [groups] 配置段格式：&lt;用户组&gt; = &lt;用户列表&gt; 用户列表由若干个用户组或用户名构成，用户组或用户名之间用逗号”,”分隔；引用用户组时要使用前缀”@”。 Second：版本库路径权限段 【1】版本库路径权限段格式： [&lt;版本库名&gt;:&lt;路径&gt;] 样例：版本库 test 路径下 /tmp 的版本库路径权限段的段名为”[test:/tmp]”。 再如：[/tmp] 。表示：还可省略段名中的版本库名。若省略版本库名，则该版本库路径权限段对所有版本库中相同路径的访问控制都有效。 [/]：表示可以对版本库下的所有文件都可以进行访问控制（[/] 是相对 svn 版本库 src（源）目录下的访问路径）。 【2】版本库路径权限段中配置行格式有如下三种（每行配置只能配置单个用户或用户组）： &lt;用户名&gt; = &lt;权限&gt; &lt;用户组&gt; = &lt;权限&gt; \* = &lt;权限&gt; 其中，”*”：表示任何用户。 权限的取值范围为’’、’r’和’rw’。’’：表示对该版本库路径无任何权限，’r’表示具有只读权限，’rw’表示有读写权限。 下面我们来看一下配置实例： # 只有 project_p 用户组有根目录的读写权。 [svn@node3 conf]$ vim authz [groups] project_p = pm project_s = server1,server2,server3 project_c = client1,client2,client3 project_t = test1,test1,test1 [project:/] @project_p = rw * = [project:/server] @project_p = rw @project_s = rw * = [project:/client] @project_p = rw @project_c = rw * = [project:/doc] @project_p = rw @project_s = r @project_c = r @project_t = r * = # &apos;* =&apos; 表示：除了上面设置了权限的用户组之外，其他任何人都被禁止访问本目录。 3）svnserve.conf SVN 服务配置 svnserve.conf 文件用来设置 SVN 服务配置。例如：使得 passwd 以及 authz 配置生效。由两个配置段构成： [general] 配置段和 [sasl] 配置段。这里我们主要关注 [general] 配置段，其格式如下（详细配置段讲解看附录 II：svnserve 配置详解）： &lt;配置项&gt;=&lt;值&gt; 配置项分为以下 5 项： 【1】anon-access：匿名（非鉴权）用户访问版本库的权限。取值范围为”write”、”read”和”none”。即：”write”为可读可写；”read”为只读；”none”表示无访问权限。缺省值：read。 【2】auth-access：一般（鉴权）用户访问版本库的权限。取值范围为”write”、”read”和”none”。即：”write”为可读可写；”read”为只读；”none”表示无访问权限。缺省值：write。 【3】password-db：指定用户名口令（用户）配置文件路径。除非指定绝对路径，否则文件位置为相对 conf 目录的相对路径。缺省值：passwd。 【4】authz-db：指定用户访问策略配置文件路径。通过该文件可以实现以路径为基础的访问控制。除非指定绝对路径，否则文件位置为相对 conf 目录的相对路径。缺省值：authz。 【5】realm 指定版本库的认证域。即在登录时提示的认证域名称（可以理解为版本库目录）。若两个版本库的认证域相同，建议使用相同的用户名口令数据文件。缺省值：一个UUID(Universal Unique IDentifier，全局唯一标示)。这里我们不用设置。 如下，我们给出一个设置样例： [svn@node3 conf]$ vim svnserve.conf [general] anon-access = none auth-access = write password-db = passwd authz-db = authz SVN 管理员可以通过这 3 个 配置文件设置 svnserve 服务的用户名口令，以及对版本库路径的访问权限。这些配置文件保存后就立即生效，不需要重启 svnserve 服务。 2.1.4 启动 SVN 服务上文我们以及学会了创建 SVN 版本库以及如何对其进行配置，这里我们来看如何使用（启动）创建好的 SVN 版本库（即：SVN 的使用）。 1）SVN 服务启动版本库 和大多数 C/S 软件一样，只有启动 SVN 服务【默认端口：3390】后才可以正常使用。 SVN 启动版本库的两种方式：单库启动和多库启动。下面我们开始分别测试单库启动和多库启动。假设我们构建了两个版本库：SVNRepos1 和 SVNRepos2： 单库方式启动: #### 【1】 只启动 SVNRepos 版本库 [svn@node3 SVNProject]$ svnserve -d -r /home/svn/SVNProject/SVNRepos #### 【2】 启动后，简单检出测试是否可以直接访问到 SVNRepos 版本库副本： # 1. SVN Server：服务器端测试 [svn@node3 ~]$ svn checkout svn://127.0.0.1 --username=admin 或： [svn@node3 ~]$ svn checkout svn://10.1.0.123 --username=admin # 2. Client： 客户端检测 TortoiseSVN： URL：svn://10.1.0.123 #### 【3】 Completed At revision:0 ！！！ 成功了 多库方式启动 #### 【1】 启动 SVNProject 版本仓库下所有版本库（多启动） [svn@node3 SVNProject]$ svnserve -d -r /home/svn/SVNProject/ #### 【2】 启动后，简单检出测试是否可以直接访问到 SVNRepos 版本库副本： # 1. SVN Server：服务器端测试 [svn@node3 ~]$ svn checkout svn://127.0.0.1/SVNRepos --username=admin 或： [svn@node3 ~]$ svn checkout svn://10.1.0.123/SVNRepos --username=admin # 2. Client： 客户端检测 TortoiseSVN： URL：svn://10.1.0.123/SVNRepos #### 【3】 Completed At revision:0 ！！！ 成功了 SVN 版本库查看 #### 【1】 查看版本库中文件：svn list --verbose [svn@node3 ~]$ svn list --verbose file:///home/svn/SVNProject/SVNRepos 0 ? 10月 26 14:35 ./ #### 【2】 查看 SVN 版本库信息：svn info [svn@node3 ~]$ svn info file:///home/svn/SVNProject/SVNRepos 路径: SVNRepos URL: file:///home/svn/SVNProject/SVNRepos 版本库根: file:///home/svn/SVNProject/SVNRepos 版本库 UUID: ab05587b-7a0e-4ee5-b505-0a4163b0f8a5 版本: 0 节点种类: 目录 最后修改的版本: 0 最后修改的时间: 2018-10-26 14:35:04 +0800 (五, 2018-10-26) 注意：上面我们提到，SVN 服务默认端口：3690。而 SVN 服务器支持多 svnserver 同时启动的。如果你发现当前已经有 svnserve 在运行，可以通过更换端口来运行： [svn@node3 SVNProject]$ svnserve -d -r /home/svn/SVNProject/ --listen-port 3391 2）重启 SVN 服务 当我们修改了 SVN 配置文件后，需要重启 SVN 服务。这里我们给出相关涉及命令供选择使用： #### 【1】 启动：svnserve -d -r [svn@node3 SVNProject]$ svnserve -d -r /home/svn/SVNProject/ #### 【2】 查看 SVN 进程：ps -aux|grep svnserve [svn@node3 ~]$ ps -aux|grep svnserve svn 5724 0.0 0.0 197188 1284 ? Ss 16:48 0:00 svnserve -d -r /home/svn/SVNProject svn 9322 0.0 0.0 112720 976 pts/1 S+ 17:13 0:00 grep --color=auto svnserve #### 【3】 强制 KILL SVN 进程： [svn@node3 ~]$ kill -9 Process_ID #### 【4】 KILL 所有 SVN 服务： [svn@node3 ~]$ killall svnserve #### 【5】 查看 3690 端口状态： [svn@node3 ~]$ netstat -apn |grep 3690 tcp 0 0 0.0.0.0:3690 0.0.0.0:* LISTEN 5724/svnserve 2.1.5 初始化 SVN 版本仓库目录为了方便管理版本仓库中的项目，一般建立版本仓库之后，我们需要组织版本库目录结构。下面我们给出一个通用的标注目录结构： # 一般情况下，SVN 版本库中需要创建下面三种文件夹： trunk：主分支，日常进行开发的地方。 tags：一般是只读目录，一般是阶段性的发布版本。作为一个里程碑式的文档。 branchs：分支目录，一些阶段性的 release 版本 下面我们来看一个初始化版本仓库目录的实例： 创建初始化临时目录 临时目录结构可见：附录 III：Test_Library 目录结构说明。 初始化版本库目录 #### 【1】 使用临时目录结构初始化版本库目录： [svn@node3 Test_Library]$ svn import Test_Library file:///home/svn/SVNProject/SVNRepos/Test_Library -m &quot;Just Test this Trick.&quot; # 或 [svn@node3 Test_Library]$ svn import Test_Library svn://localhost/Test_Library -m &quot;Just Test this Trick.&quot; # 输出如下： 正在增加 Test_Library/2.SourceCodeLibrary 正在增加 Test_Library/2.SourceCodeLibrary/DevelopLibrary 正在增加 Test_Library/2.SourceCodeLibrary/DevelopLibrary/04.Vendor 正在增加 Test_Library/2.SourceCodeLibrary/DevelopLibrary/03.Trunk 正在增加 Test_Library/2.SourceCodeLibrary/DevelopLibrary/02.Branches 正在增加 Test_Library/2.SourceCodeLibrary/DevelopLibrary/01.Tags 正在增加 Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary 正在增加 Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary/01.InternalRelease 正在增加 Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary/02.ExternalRelease 正在增加 Test_Library/2.SourceCodeLibrary/BaseLineLibrary 正在增加 Test_Library/2.SourceCodeLibrary/BaseLineLibrary/01.XX_COD(源代码) 提交后的版本为 9。 # 上述两种方法中，唯一不同的是版本库访问方式不同：【file:///】 和 【svn://】 #### 2. 查看初始化 SVN 版本库目录（已经初始化）： [svn@node3 ~]$ svn list --verbose file:///home/svn/SVNProject/SVNRepos 9 svn 10月 26 18:40 ./ 9 svn 10月 26 18:40 Test_Library/ 删除初始化临时目录 [svn@node3 ~]$ sudo rm -rf Test_Library/ 2.1.6 SVN Server 测试（本地测试）上面我们已经完成了 SVN Server 的搭建，并且基于 /home/svn/SVNProject 目录构建了一个版本仓库。这一部分我们通过一个简单的 Python Hello 脚本来在 SVN 服务器本地进行测试 SVN 版本控制功能。 关于 SVN 基础操作命令见【附录：I】。 检出目标版本库 # 从服务器导出目标版本库： [svn@node3 ~]$ svn co file:///home/svn/SVNProject/SVNRepos/Test_Library A Test_Library/2.SourceCodeLibrary A Test_Library/2.SourceCodeLibrary/DevelopLibrary A Test_Library/2.SourceCodeLibrary/DevelopLibrary/01.Tags A Test_Library/2.SourceCodeLibrary/DevelopLibrary/04.Vendor A Test_Library/2.SourceCodeLibrary/DevelopLibrary/03.Trunk A Test_Library/2.SourceCodeLibrary/DevelopLibrary/02.Branches A Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary A Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary/02.ExternalRelease A Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary/01.InternalRelease A Test_Library/2.SourceCodeLibrary/BaseLineLibrary A Test_Library/2.SourceCodeLibrary/BaseLineLibrary/01.XX_COD(源代码) 取出版本 9。 # 查看到处的版本库： [svn@node3 ~]$ ls Test_Library 项目开发 cd Test_Library/2.SourceCodeLibrary/BaseLineLibrary/01.XX_COD(源代码) #### 【1】 将 &quot;print (&apos;Hello SVN Server!&apos;)&quot; 写入脚本： vim test.py # svn st 查看文件状态（是否在版本控制范畴） [svn@node3 01.XX_COD(源代码)]$ svn st ? test.py #### 【2】 将 test.py 文件加入待变更列表： [svn@node3 01.XX_COD(源代码)]$ svn add test.py A test.py # svn st 查看文件状态 [svn@node3 01.XX_COD(源代码)]$ svn st A test.py #### 【3】 开发完成后，提交到 SVN 服务器： [svn@node3 01.XX_COD(源代码)]$ svn commit test.py -m &quot;This is a Commit Test&quot; Adding main.c Transmitting file data . Committed revision 2. ( 测试提交成功 ) 2.2 基于 SVN Serve 的 SVN 服务器搭建之 Client 端上面我们提到：SVN 版本控制系统是一个 C/S 模式应用，客户端通过服务端某一开放端口访问服务端服务实现通信，SVN 服务维护一个默认的端口：3690（注意：SVN 是支持同时启动多个服务的）。在使用远程客户端测试（访问）SVN 服务之前我们需要保证其 3690 端口对局域网络开放，防止由于防火墙原因导致客户端无法连接至服务端。 2.2.1 防火墙配置我们先来看看防火墙端口可能引发的 SVN 客户端连接问题（对症下药）： SVN 服务成功启动，并且我们可以通过上文 【6】 SVN Server 测试（本地测试），但是在客户端(其它电脑)上无法实现检出，一直提示如下报错： （1）Can’t connect to host ‘...‘:由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接失败。 （2）Can’t connect to host ‘...‘:由于目标计算机积极拒绝，无法连接。 解决办法：开启防火墙端口 #### 【1】 开启防火墙端口 # 1. 编辑防火墙配置文件 [root@node3 svn]# vim /etc/sysconfig/iptables # 添加如下内容： -A INPUT -m state --state NEW -m tcp -p tcp --dport 3690 -j ACCEPT # 修改后文件如下： # Firewall configuration written by system-config-firewall # Manual customization of this file is not recommended. *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 3690 -j ACCEPT # 为了好区分，上下留空行 -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -j REJECT --reject-with icmp-host-prohibited COMMIT # 或者（另一种方法）：直接开启端口后保存修改 [root@node3 svn]# iptables -I INPUT -i eth0 -p tcp --dport 3690 -j ACCEPT [root@node3 svn]# iptables -I OUTPUT -o eth0 -p tcp --sport 3690 -j ACCEPT # 保存修改 [root@node3 svn]# /etc/init.d/iptables save #### 【2】 重启防火墙，生效配置 [root@node3 svn]# /etc/init.d/iptables restart # 或者： [root@node3 svn]# service iptables restart # 防火墙已重启： iptables: Setting chains to policy ACCEPT: filter [ OK ] iptables: Flushing firewall rules: [ OK ] iptables: Unloading modules: [ OK ] iptables: Applying firewall rules: [ OK ] 查看防火墙端口情况 # 查看端口 tcp:port 占用情况 root@node3 svn]# lsof -i tcp:3690 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME svnserve 19512 svn 3u IPv4 42605144 0t0 TCP *:svn (LISTEN) # 或： # 列出所有端口： root@node3 svn]# netstat -ntlp [root@dev6 cdh]# netstat -ntlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:3690 0.0.0.0:* LISTEN 7997/svnserve tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 2228/rpcbind tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 2598/sshd tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN 2331/cupsd tcp 0 0 0.0.0.0:34839 0.0.0.0:* LISTEN 2285/rpc.statd tcp 0 0 :::3306 :::* LISTEN 3079/mysqld tcp 0 0 :::111 :::* LISTEN 2228/rpcbind tcp 0 0 :::22 :::* LISTEN 2598/sshd tcp 0 0 ::1:631 :::* LISTEN 2331/cupsd tcp 0 0 :::59323 :::* LISTEN 2285/rpc.statd 2.2.2 SVN 客户端连接 开发人员强烈建议使用 IDE 中集成 SVN 插件更加智能与人性化。 这一部分来看 Linux/Windows SVN 客户端环境连接与简单测试： （1）Linux 环境 （1_1）安装 SVN（Subversion）客户端 ### 1. 检测系统是否安装了低版本 SVN： $ rpm -qa subversion # 如果存在旧版本 SVN，则卸载： [svn@node3 ~]$ yum remove subversion ### 2. 开始安装 SVN： [svn@node3 ~]$ yum -y install subversion ​Complete！完成之后，开始后续操作： （1_2）查看版本信息 ### 1. 安装是否成功：svn --version 或 svnserve --version [svn@node3 ~]$ svnserve --version svnserve，版本 1.7.14 (r1542130) 编译于 Apr 11 2018，02:40:28 版权所有 (C) 2013 Apache 软件基金会。 此软件包含了许多人的贡献，请查看文件 NOTICE 以获得更多信息。 Subversion 是开放源代码软件，请参阅 http://subversion.apache.org/ 站点。 下列版本库后端(FS) 模块可用: * fs_base : 模块只能操作BDB版本库。 * fs_fs : 模块与文本文件(FSFS)版本库一起工作。 Cyrus SASL 认证可用。 可使用以下的版本库访问模块: * ra_neon : 通过 WebDAV 协议使用 neon 访问版本库的模块。 - 处理“http”方案 - 处理“https”方案 * ra_svn : 使用 svn 网络协议访问版本库的模块。 - 使用 Cyrus SASL 认证 - 处理“svn”方案 * ra_local : 访问本地磁盘的版本库模块。 - 处理“file”方案 Client 远程客户端安装好 SVN 服务之后，接下来我们尝试 checkout 一个上面 Server 端已经创建好的版本库来模拟实际环境下的使用： （1_3）Linux 下的测试： 检出目标版本库 # 从服务器导出目标版本库： [svn@node3 ~]$ svn co file:///home/svn/SVNProject/SVNRepos/Test_Library A Test_Library/2.SourceCodeLibrary A Test_Library/2.SourceCodeLibrary/DevelopLibrary A Test_Library/2.SourceCodeLibrary/DevelopLibrary/01.Tags A Test_Library/2.SourceCodeLibrary/DevelopLibrary/04.Vendor A Test_Library/2.SourceCodeLibrary/DevelopLibrary/03.Trunk A Test_Library/2.SourceCodeLibrary/DevelopLibrary/02.Branches A Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary A Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary/02.ExternalRelease A Test_Library/2.SourceCodeLibrary/ProductReleaseLibrary/01.InternalRelease A Test_Library/2.SourceCodeLibrary/BaseLineLibrary A Test_Library/2.SourceCodeLibrary/BaseLineLibrary/01.XX_COD(源代码) 取出版本 9。 # 查看到处的版本库： [svn@node3 ~]$ ls Test_Library 项目开发 cd Test_Library/2.SourceCodeLibrary/BaseLineLibrary/01.XX_COD(源代码) #### 【1】 将 &quot;print (&apos;Hello SVN Server!&apos;)&quot; 写入脚本： vim test.py # svn st 查看文件状态（是否在版本控制范畴） [svn@node3 01.XX_COD(源代码)]$ svn st ? test.py #### 【2】 将 test.py 文件加入待变更列表： [svn@node3 01.XX_COD(源代码)]$ svn add test.py A test.py # svn st 查看文件状态 [svn@node3 01.XX_COD(源代码)]$ svn st A test.py #### 【3】 开发完成后，提交到 SVN 服务器： [svn@node3 01.XX_COD(源代码)]$ svn commit test.py -m &quot;This is a Commit Test&quot; Adding main.c Transmitting file data . Committed revision 2. ( 测试提交成功 ) （2）Windows 环境 （1_1）安装 SVN（Subversion）客户端 TortoiseSVN TortoiseSVN 官网下载地址：https://tortoisesvn.net/downloads.html 详细安装教程见网络。点击鼠标右键出现下图所示，说明安装成功： （1_2）Windows 下的测试： 下面我们来进行一个简单的检出试验，用来检测客户端连接： 首先选择：SVN 检出（checkout） 然后，填入版本库信息： 此时会弹出一个对话框让你输入账号密码，输入你的账号密码即可。记得勾选保存认证，不然每次操作都会让你输入。 等待检出完成： 测试通过！！！！ 2.3 配置 Apache 支持 HTTP(S) 方式访问先搁置，有需要再来补充。 附录：I1.svn help：可以通过该命令查看 svn 的所有子命令（包括命令的缩写）。 2.svn checkout：表示从 svn 版本库中 checkout（检出、导出）对应的工作项目副本，样例如下： ##（1）svn 项目路径为：svn://192.168.1.1/myproject/doc，需要将项目路径下的文件 checkout 到 /home/test 目录下（若没有 /home/test 路径会自动创建）。 svn checkout svn://192.168.1.1/myproject/doc /home/test/ --username myname --password password ##（2）以下命令会在当前目录建 doc 目录，然后将项目 checkout 到该目录下 svn checkout svn://192.168.1.1/myproject/doc ##（3）以下命令会建一个空的 doc 目录，该目录会与 svn 关联，该方法主要是为了只 checkout 对应项目下的部分文件。 # 此时进到 doc 目录，然后使用svn up abc 命令则会将该项目下的 doc/abc 目录 checkout 到 doc，而不会 checkout 其他文件。 svn checkout --depth=empty svn://192.168.1.1/myproject/doc ##（4）上面的命令可以将 checkout 替换为其缩写 co ##（5）svn export：从仓库中导出代码到本地，导出的内容不包括 .svn 版本控制文件（和 co 相反），相当于导出的是一个干净的本地目录。 3.添加文件，通过 2 将项目 checkout 到本地之后，就可以对项目进行操作，添加变更列表样例如下： ##（1）在 svn 目录下，添加了新文件（例如：aa.xml），然后提交，需要下面两步： svn add aa.xml svn commit -m &quot;add file aa.xml&quot; aa.xml ##（2）添加多个文件，全部提交 svn add * svn commit -m &quot;add all update file&quot; * ##（3）其中上面的 commit 命令可缩写为 ci。另外 -m 参数后面引号为此次修改的注释，最后面为提交的文件。 4.修改文件 ##（1）当对存在的文件进行修改之后直接使用 commit 即可提交到 svn。例如：修改了：aa.txt，之后执行下面命令提交 svn commit -m &quot;update file aa.txt&quot; aa.txt 5.更新 ##（1）更新当前目录下的全部文件 svn update ##（2）更新abc目录中的文件 svn update abc ##（3）将abc目录文件更新恢复到某个版本 svn update -r 版本号 abc ##（4）update可用 up 代替 6.删除文件 ##（1）删除文件 aa.txt，需要如下两步操作，删除文件夹也一样 svn delete aa.txt svn commit -m &quot;delete file aa.txt&quot; 7.查看文件的状态： # 正常状态则执行命令之后无信息，文件前面带M表示对应文件有修改，C 表示对应文件冲突，A 表示新加文件，？ 表示文件还不在在 svn 管理下。 # 如添加文件，没有进行 add 命令操作，则状态为？，进行了 add 操作但是没 commit，则为 A 状态 ##（1）查看当前目录所有文件状态 svn status ##（2）查看 abc 目录下所有文件状态 svn status abc ##（3）查看文件进行过的修改，第一列显示当前版本，第二列显示修改时的版本，第三列显示修改人，第四列显示修改文件 svn status -v abc ##（4）status可用 st 代替 ##（5）所有文件状态： 1.&apos; &apos;没有修改 2.&apos;A&apos;被添加到本地代码仓库 3.&apos;C&apos; 冲突 4.&apos;D&apos; 被删除 5.&apos;I&apos; 被忽略 6.&apos;M&apos; 被修改 7.&apos;R&apos; 被替换 8,&apos;?&apos; 文件没有被添加到本地版本库内,不在 SVN 的管理之下 9.&apos;!&apos; 文件丢失或者不完整(不识别该文件) 10.&apos;~&apos; 受控文件被其他文件阻隔 11.&apos;U&apos; 本地有文件的情况下更新最新的代码到本地 12.&apos;G&apos; 产生冲突后,更新操作去解决冲突,相当于进行合并 8.查看 svn 的提交日志记录 ##（1）查看所有记录，一般会很多，不这么做 svn log ##（2）查看最近的四条提交记录 svn log -l4 ##（3）查看 aa.txt 文件的记录 svn log aa.txt 9.查看项目信息 ##（1）查看项目的svn信息 svn info ##（2）查看aa.txt文件的svn信息 svn info aa.txt 10.查看当前 svn 目录下有什么文件，方便根据需要 checkout 对应需要的文件 svn list svn://192.168.1.1/doc 11.拿出项目，但是拿出来的项目没有与 svn 进行关联，不在其控制范围，命令格式类似 checkout ##（1）拿项目的最新版本 svn export svn://192.168.1.1/doc /home/mytest ##（2）拿项目指定版本的文件 svn export -r 需要的版本号 svn://192.168.1.1/doc /home/mytest 12.文件修改之后回退 ##（1）撤销对ab.txt 文件的修改 svn revert ab.txt ##（2）撤销当前目录下的所有修改 svn revert --recursive 13.文件对比 ##（1）查看文件与svn中的不同 svn diff ab.txt ##（2）查看 ab.txt 在版本为 12与14 之间所做的修改 svn diff -r 12:14 ab.txt 附录 II：svnserve 配置详解### This file controls the configuration of the svnserve daemon, if you ### use it to allow access to this repository. (If you only allow ### access through http: and/or file: URLs, then this file is ### irrelevant.) ### Visit http://subversion.tigris.org/ for more information. [general] ### These options control access to the repository for unauthenticated ### and authenticated users. Valid values are &quot;write&quot;, &quot;read&quot;, ### and &quot;none&quot;. The sample settings below are the defaults. ### 对于授权用户与未被授权（匿名）用户的访问级别控制：read，write，write anon-access = read auth-access = write ### The password-db option controls the location of the password ### database file. Unless you specify a path starting with a /, ### the file&apos;s location is relative to the directory containing ### this configuration file. ### If SASL is enabled (see below), this file will NOT be used. ### Uncomment the line below to use the default password file. ### 密码数据文件的保存位置，默认为相对路径，如果以 / 开头则为绝对路径 ### 如果 SASL 开启的话，那么就不会验证该文件 password-db = passwd ### The authz-db option controls the location of the authorization ### rules for path-based access control. Unless you specify a path ### starting with a /, the file&apos;s location is relative to the the ### directory containing this file. If you don&apos;t specify an ### authz-db, no path-based access control is done. ### Uncomment the line below to use the default authorization file. ### 用户数据文件的保存位置，默认为相对路径，如果以 / 开头则为绝对路径 ### 如果未指定路径，则无访问控制 authz-db = authz ### This option specifies the authentication realm of the repository. ### If two repositories have the same authentication realm, they should ### have the same password database, and vice versa. The default realm ### is repository&apos;s uuid. ### 指定验证的范围，如果两个 repo 的 realm 属性一样，那么它们就应该使用同一个 password 数据库，反之亦然。 ### 默认的 realm 就是 repo 的唯一标示符 realm = 0.1 [sasl] ### This option specifies whether you want to use the Cyrus SASL ### library for authentication. Default is false. ### This section will be ignored if svnserve is not built with Cyrus ### SASL support; to check, run &apos;svnserve --version&apos; and look for a line ### reading &apos;Cyrus SASL authentication is available.&apos; ### 是否开启 SASL 验证，默认是 false 的 ### 此选项会默认 svn 服务器支持 Cyrus，检查的方法是，运行&apos;svnserve --version&apos;命令，查看输出是否有&apos;Cyrus SASL authentication is available.&apos; # use-sasl = false ### These options specify the desired strength of the security layer ### that you want SASL to provide. 0 means no encryption, 1 means ### integrity-checking only, values larger than 1 are correlated ### to the effective key length for encryption (e.g. 128 means 128-bit ### encryption). The values below are the defaults. ### 下面两个选项用来指定加密强度的 # min-encryption = 0 # max-encryption = 256 附录 III：Test_Library 目录结构说明1234567891011121314151617181920212223Test_Library ||- SourceCodeLibrary |​ |-- BaseLineLibrary | |​ | |--- 01.XX_COD(源代码) |​ |-- DevelopLibrary | |​ | |--- 01.Tags | |​ | |--- 02.Branches | | ​ | |--- 03.Trunk | |​ | |--- 04.Vendor |​ |-- ProductReleaseLibrary |​ |--- 01.InternalRelease |​ |--- 02.ExternalRelease]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>Setup</tag>
        <tag>SVN</tag>
        <tag>Subversion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 系列之经典卷积神经网络模型（LeNet-5 && Inception-v3）]]></title>
    <url>%2FTensorFlow%2FTensorFlow-%E7%B3%BB%E5%88%97%E4%B9%8B%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%EF%BC%88LeNet-5-Inception-v3%EF%BC%89%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： Part1 &amp;&amp; Part2 中我们已经详细介绍了卷积神经网络架构以及卷积层、池化层网络结构等。然而，我们可以通过这些网络结构任意组合得到的神经网络有无限多种，怎样的卷积神经网络结构更有可能解决真实的图像处理问题呢？ Part 3 经典卷积神经网络模型这一部分，我们将通过一些经典的卷积神经网络结构来总结卷积神经网络结构设计的一些通用模式，为我们搭建卷积神经网络结构解决图像领域问题提供思路。 这一部分我们内容将分为两个小节： 第一小节：具体介绍 LeNet-5 模型，并给出一个完整的 TensorFlow 程序来实现 LeNet-5 模型。最后通过这个模型给出一个卷积神经网络结构设计的通用模式； 第二小节：将介绍卷积神经网络结构设计的另外一种思路—Inception结构，并且介绍 TensorFlow-Slim 工具实现谷歌提出的 Inception-v3 模型中一个模块； 3.1 LeNet-5 模型LeNet-5 模型是 Yann LeCun 教授于 1998 年在论文 Gradient-based learning applied to document recognition 中提出的，它是第一个成功应用于数字识别问题的卷积神经网络。在 MNIST 数据集上，LeNet-5 模型可以达到大约 99.2% 的正确率。 下面我们具体来看 LeNet-5 模型架构以及其在 TensorFlow 中的实现： 3.1.1 LeNet-5 模型架构LeNet-5 模型总共有 7 层，下面我们首先来看 LeNet-5 模型架构示意图： 下面篇幅将详细介绍 LeNet-5 模型每一层的结构（和真实的 LeNet-5 模型有细微差别）： 1 –&gt; 第一层：卷积层 这一层的输入就是原始的图像像素矩阵，LeNet-5 模型接受的输入层大小为 $ 32 × 32 × 1 ​$。 第一个卷积层过滤器的尺寸为 $ 5 × 5 ​$，深度为 6，不使用全 0 填充，步长为 1。由于没有使用全 0 填充，所以由输出矩阵尺寸公式计算可得出这一层的输出尺寸为：$ (32 - 5 + 1)/1 = 28 ​$，深度为 6。 这一层卷积层总共有：$ 5 × 5 × 1 × 6 + 6 = 156 $ 个参数，其中 +6 为偏置项参数。因为下一层的节点矩阵有 $ 28 × 28 × 6 = 4704 $ 个节点，每个节点和 $ 5 × 5 = 25 $ 个当前层节点相连，所以本层卷积层总共有 $ 4704 × (25 + 1) = 122304 $ 个连接。 注意，每个连接对应一次计算（FLOPS），参与运算共 122304 次。 2 –&gt; 第二层：池化层 这一层的输入为第一层的输出，是一个 $ 28 × 28 × 6 ​$ 的节点矩阵。 本层采用的过滤器大小为 $ 2 × 2 $，长和宽的步长均为 2，所以本层的输出矩阵大小为：$ 14 × 14 × 6 $（切记：池化层矩阵不改变输出节点矩阵的深度）。 注意，原始的 LeNet-5 模型中使用的过滤器和上面小节中池化层过滤器的介绍有细微差别，这里不做具体介绍。 3 –&gt; 第三层：卷积层 本层的输入矩阵大小为 $ 14 × 14 × 6 $，是上一层的输出。 本层使用的过滤器尺寸大小为：$ 5 × 5 $，深度为：16。本层不使用全 0 填充，步长为 1。故输出的矩阵大小为：$ 10 × 10 × 16 $。 按照标准的卷积层，本层应该有 $ 5 × 5 × 6 × 16 + 16 = 2416 $ 个参数，$ 10 × 10 × 16 × (25 +1) = 41600 $ 个连接。 4 –&gt; 第四层：池化层 本层的输入矩阵大小为：$ 10 × 10 × 16 $。采用的过滤器大小为 $ 2 × 2 $,步长为 2。本层的输出矩阵大小为：$ 5 × 5 × 16 $。 5 –&gt; 第五层：全连接层 本层的输入矩阵大小为：$ 5 × 5 × 16 $，在 LeNet-5 模型的论文中将这一层称之为卷积层（或可以称为全卷积层），这是因为这一层过滤器的大小就是 $ 5 × 5 $，所以和全连接层没有区别，在之后的 TensorFlow 程序实现中也会将这一层看成全连接层。 故我们将 $ 5 × 5 × 16 ​$ 的矩阵中的节点拉成一个向量，那么这一层和在前面中介绍的全连接层输入就一样了。本层的输出节点个数为 $ 120 ​$，总共有 $ 5 × 5 × 16 × 120 + 120 = 48120 ​$ 个参数。 6 –&gt; 第六层：全连接层 本层的输入节点个数为 $ 120 $ ，输出节点个数为 $ 84 $ 个，总共参数为：$ 120 × 84 + 84 = 10164 $ 个。 7 –&gt; 第七层：全连接层 本层的输入节点个数为：$ 84 $ 个，输出节点个数为：$ 10 $，总参数为：$ 84 × 10 + 10 = 850 $ 个。 3.1.2 类 LeNet-5 的 TensorFlow 实现上面我们也就介绍了 LeNet-5 模型每一层结构和设置，下面将给出一个 TensorFlow 程序来实现一个 类似 LeNet-5 模型 的卷积神经网络来解决 MNIST 数字识别问题。 前面我们说过，通过 TensorFlow 训练卷积神经网络的过程和前面章节介绍的训练全连接神经网络模型完全一样。损失函数、反向传播算法过程的实现都可以复用前面博文中的 MNIST_FCNN_Train_And_Evaluate.py 程序。唯一区别在于因为卷积神经网络的输入层为一个三维矩阵，所以需要调整一下输入数据的格式，修改后重新命名为：MNIST_ConvNN_Train_And_Evaluate.py ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341''' ### 神经网络模型优化以及模型评估 TensorFlow_Hello : MNIST 手写数字图片识别示例 Model : Convolution Neural Network like LeNet5 Code For : Neural network training and testing'''import osimport shutilimport sysimport timefrom datetime import timedeltaimport numpy as npimport tensorflow as tf# 导入用于下载和读取 MNIST 数据集的 python 源文件：见 input_data.pyfrom tensorflow.examples.tutorials.mnist import input_dataimport MNIST_ConvNN_Model###################### FCNN Model Training and Test ########################### 1. 定义神经网络结构的相关参数 ###### 一个训练 batch 块中的训练数据个数。数值越小，训练过程越接近随机梯度下降；数值越大，训练越接近梯度下降。BATCH_SIZE = 100# 设置基础学习率LEARNING_RATE_BASE = 0.01# 设置学习率的衰减率LEARNING_RATE_DECAY = 0.99# 设置描述模型复杂度（结构风险）的正则化项在损失函数中的系数REGULARIZATION_RATE = 0.0001# 设置训练轮数TRAINING_STEPS = 30000# 设置滑动平均衰减率MOVING_AVERAGE_DECAY = 0.99# 设置模型的保存路径MODEL_SAVE_PATH = "Save_Model/"MODEL_NAME = "model.ckpt"# 指定 MNIST 数据集的下载和读取的路径：MNIST_DATA_PATH = "../MNIST_data"# 每 10 秒加载一次最新模型，并在测试数据集上测试最新模型正确率EVAL_INTERVAL_SECS = 5##### 2. Referenced Function Define ####### [1]. get_time_dif：获取已使用时间def get_time_dif(start_time): end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=int(round(time_dif)))##### 3. Model Training #####def train(mnist): # 定义神经网络输入输出 x = tf.placeholder(tf.float32, [ BATCH_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.NUM_CHANNELS], name='input_x') y_ = tf.placeholder(tf.float32, [BATCH_SIZE, MNIST_ConvNN_Model.OUTPUT_NODE], name='input_y') # 定义 L2 正则化损失函数（结构风险）： regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE) ### 1. avg_class = None ### # 使用 MNIST_ConvNN_Model.py 中定义的前向传播函数计算 FP 结果(不会使用参数的滑动平均值): y = MNIST_ConvNN_Model.inference(x, False, regularizer) ### 2. avg_class = variable_averages ### # 定义存储训练轮数的变量，无需计算滑动平均值。所以一般需要指定训练轮数变量为不可训练变量(trainable=False) global_step = tf.Variable(0, trainable=False) # 通过滑动平均衰减率和训练轮数变量，初始化滑动平均类。给定训练轮数可以加快训练早期变量的更新速度 variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) # 在所有（可训练，Except global_step）神经网络变量上使用滑动平均 # tf.trainable_variables 可返回 GraphKeys.TRAINABLE_VARIABLES 集合中元素，即未指定 trainable=False 的变量 variables_averages_op = variable_averages.apply(tf.trainable_variables()) ''' # 计算使用了滑动平均之后的 FP 结果。滑动平均之后不会改变变量本身取值，会维护一个 shadow_variable 来记录其滑动平均值 movingAvg_y = MNIST_ConvNN_Model.inference(x, None, variable_averages) ''' ### 3. Loss Function ### # 定义交叉熵损失函数（经验风险）:该函数第二个参数需要提供的是正确答案的数字，tf.argmax(y_, 1)可以获取 y 对应的类别编号 cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1)) # 计算当前 batch 中所有样例的交叉熵平均值 cross_entropy_mean = tf.reduce_mean(cross_entropy) # 计算 Loss Function loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses')) ### 4. ANN BP ### # 设置指数衰减的学习率:随着迭代进行，更新变量的学习率在这个基础上递减 learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, # 过完所有训练数据需要的迭代次数 LEARNING_RATE_DECAY, staircase=True) # 使用优化器来优化损失函数 train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # 在 ANN 中，每迭代一次数据就需要通过 BP 来更新 ANN 中参数，又要更新每一个参数的滑动平均值。Tensorflow提供了两种机制：tf.control_dependencie和tf.group： # train_op = tf.group(train_step, variables_averages_op) 等价于 == with tf.control_dependencies([train_step, variables_averages_op]): train_op = tf.no_op(name='train') # 检验不使用滑动平均模型的 ANN FP 结果是否正确 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # tf.cast(x, dtype) 会将x数据格式转化成 dtype 数据格式。这里，将 bool 型数值转为 float 后，x会变为0/1序列。 # 计算正确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) ### 5. TensorFlow ANN Model 持久化 ### print("Configuring Saver...") # 初始化 TensorFlow 持久化类 # saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2) saver = tf.train.Saver() if not os.path.exists(MODEL_SAVE_PATH): os.makedirs(MODEL_SAVE_PATH) else: shutil.rmtree(MODEL_SAVE_PATH) os.makedirs(MODEL_SAVE_PATH) ### 6. Begin To Training ### with tf.Session() as sess: # 初始化所有变量 tf.global_variables_initializer().run() print('Begin to train and Validate : ') # 迭代训练神经网络 for epoch in range(TRAINING_STEPS): start_time = time.time() # 准备验证数据集，一般用于在训练过程中大致判断停止 val_x, val_ys = mnist.validation.next_batch(BATCH_SIZE) val_reshaped_xs = np.reshape(val_x, ( BATCH_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.NUM_CHANNELS)) validate_feed = &#123;x: val_reshaped_xs, y_: val_ys&#125; # 产生这一轮使用的一个 batch 的训练数据，并进行训练 xs, ys = mnist.train.next_batch(BATCH_SIZE) train_reshaped_xs = np.reshape(xs, ( BATCH_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.NUM_CHANNELS)) sess.run(train_op, feed_dict=&#123;x: train_reshaped_xs, y_: ys&#125;) # 每 1000 轮输出一次模型训练结果以及验证集上的结果 if epoch % 1000 == 0: ''' # 计算滑动平均模型在验证集上的结果。这里，由于 MNIST 数据集较小，故一次处理所有验证数据，没有将验证数据划分为更小的batch。 # 当神经网络模型比较复杂或者验证数据集比较大时，太大的 batch 会导致运行时间过长甚至发生内存溢出的 Error ''' loss_train, acc_train = sess.run([loss, accuracy], feed_dict=&#123;x: train_reshaped_xs, y_: ys&#125;) loss_val, acc_val = sess.run([loss, accuracy], feed_dict=validate_feed) # todo time_dif = get_time_dif(start_time) msg = 'Iter: &#123;0:&gt;6&#125;, Train Loss: &#123;1:&gt;6.2&#125;, Train Acc: &#123;2:&gt;7.2%&#125;,' \ + ' Val Loss: &#123;3:&gt;6.2&#125;, Val Acc: &#123;4:&gt;7.2%&#125;, Time: &#123;5&#125;' print(msg.format(epoch, loss_train, acc_train, loss_val, acc_val, time_dif)) # 保存当前模型，注意这里给出 global_step 参数，可以使得每个保存模型的文件末尾加上训练轮数：model.ckpt-1000 saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step) # 每 2000 次迭代持久化一次模型，防止程序断点 if epoch % 2000 == 0: # 保存当前模型，注意这里给出 global_step 参数，可以使得每个保存模型的文件末尾加上训练轮数：model.ckpt-1000 saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step, write_meta_graph=False) print("----- %s has been writen local dics -----" % tf.train.latest_checkpoint(MODEL_SAVE_PATH))##### 3. Model evaluating #####def evaluate(mnist): with tf.Graph().as_default() as g: # 定义神经网络输入输出 x = tf.placeholder(tf.float32, [ BATCH_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.NUM_CHANNELS], name='input_x') y_ = tf.placeholder(tf.float32, [None, MNIST_ConvNN_Model.OUTPUT_NODE], name='input_y') # 使用 inference() 函数计算 FP 结果。由于测试时不关注正则化损失的值，所以设置 regularizer_class：None y = MNIST_ConvNN_Model.inference(x, None, None) # 检验不使用了滑动平均模型的 ANN FP结果是否正确 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # tf.cast(x, dtype) 会将x数据格式转化成 dtype 数据格式。这里，将 bool 型数值转为 float 后，x会变为0/1序列。 # 计算正确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 通过变量重命名的方式加载模型，故在 FP 过程中不需要调用滑动平均函数来获取平均值，可以完全共用 MNIST_ConvNN_Model 中定义的 FP 过程 variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY) variables_to_restore = variable_averages.variables_to_restore() saver = tf.train.Saver(variables_to_restore) # 每隔 EVAL_INTERVAL_SECS 秒调用计算一次正确率的过程以检测训练过程中正确率的变化 while True: with tf.Session() as sess: # 准备测试数据集，一般用于评价模型优劣的标准 xs, ys = mnist.test.next_batch(BATCH_SIZE) test_reshaped_xs = np.reshape(xs, ( BATCH_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.IMAGE_SIZE, MNIST_ConvNN_Model.NUM_CHANNELS)) test_feed = &#123;x: test_reshaped_xs, y_: ys&#125; # tf.train.get_checkpoint_state() 会通过 checkpoint 文件自动找到目录中最新模型的文件名 ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH) if ckpt and ckpt.model_checkpoint_path: # Load Model saver.restore(sess, ckpt.model_checkpoint_path) # 通过文件名得到模型保存时的迭代轮数 global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1] accuracy_score = sess.run(accuracy, feed_dict=test_feed) # 输出模型当前训练情况： print("After %s training step(s), test " "accuracy = %g." % (global_step, accuracy_score)) else: print('No checkpoint file found') return time.sleep(EVAL_INTERVAL_SECS)################ Main Function as the begin of program #################def main(argv=None): ###################### Functions for downloading and reading MNIST data. ###################### ''' ## 初始化：下载或读取用于训练、测试以及验证的 MNIST 手写数字图片（28px * 28px）数据集 ## MNIST 数据集分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。每一个 MNIST 数据单 元有两部分组成：一张包含手写数字的图片和一个对应的标签。比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。 mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。 mnist.train.labels 是一个形状为 [60000, 10] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的分类标签（one-hot vectors）。在此张量里的每一个元素，都表示某张图片对应分类的 one-hot vectors 向量，数值只有0和1 。 实际，read_data_sets 类会将数据从原始数据包格式解析成训练和测试神经网络时的数据格式 。read_data_sets会自动将 MNIST 数据集 划分为 train（55000）、test（10000） 以及 validation（5000）三个数据集。 ''' print("Loading training and validation data...") start_time = time.time() mnist = input_data.read_data_sets(MNIST_DATA_PATH, one_hot=True) # print mnist.train dataSet size : print("Training data size : ", mnist.train.num_examples) # print mnist.validation dataSet size : print("Validating data size : ", mnist.validation.num_examples) # print mnist.test dataSet size : print("Testing data size : ", mnist.test.num_examples) time_dif = get_time_dif(start_time) print("Time usage:", time_dif) if len(sys.argv) != 2 or sys.argv[1] not in ['train', 'eval']: raise ValueError("""usage: python MNIST_ConvNN_Train_And_Test.py [train / test]""") if sys.argv[1] == 'train': train(mnist) else: evaluate(mnist)# TensorFlow 提供的一个主程序入口，tf.app.run 会调用上面定义的 main 函数：if __name__ == '__main__': tf.app.run() 调整完输入格式之后，只需要在程序 MNIST_FCNN_Model.py 中实现类似 LeNet-5 模型结构的前向传播结果即可。修改后重新命名为：MNIST_ConvNN_Model.py。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168''' TensorFlow_Hello : MNIST 手写数字图片识别示例 Model : Convolution Neural Network like LeNet5 Code For : Definition of forward Propagation process in Neural Networks'''import tensorflow as tf###################### Build Forward Propagation Prcess ########################## 1. 定义神经网络结构的相关参数 ##### 图像像素（28px * 28px像素矩阵转化），输入层的节点数；INPUT_NODE = 784# 输出节点数，等于类别数目。分别对应 0~9 10个数字类别；OUTPUT_NODE = 10IMAGE_SIZE = 28NUM_CHANNELS = 1NUM_LABELS = 10# 定义第一层卷积层的尺寸和深度：CONV1_SIZE = 5CONV1_DEEP = 32# 定义第二层卷积层的尺寸和深度：CONV2_SIZE = 5CONV2_DEEP = 64# 定义全连接层的节点数：FC_SIZE = 512#### 2. 构建卷积神经网络的前向传播过程 ####'''## inference(): ##Function ：1] Build Forward Propagation Prcess 2] Build Forward Propagation Prcess Using ExponentialMovingAverage 区别： 1] 添加一个新的参数 train，用于区分训练过程和测试过程 2] 使用 dropout 方法，进一步提升模型可靠性并可以防止过拟合，只在训练过程使用。'''def inference(input_tensor, train, regularizer_class): ### Layer1 : convolutionial layer ### # 声明第一层卷积层的变量并实现前向传播过程： # 通过使用不同的命名空间来隔离不同层的变量，使得变量命名只需考虑在当前层的作用，不用担心变量重名的问题。 # 和标准LeNet5模型不一样，这里定义卷积层的输入为 [28*28*1] 的原始MNIST图片像素，并且卷积层使用全 0 （SAME）填充，所以输出为 [28*28*32]的矩阵。 with tf.variable_scope('layer1-conv1'): conv1_weights = tf.get_variable( "weight", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP], initializer=tf.truncated_normal_initializer(stddev=0.1)) conv1_biases = tf.get_variable( "bias", [CONV1_DEEP], initializer=tf.constant_initializer(0.0)) # 使用尺寸为[5*5] ,深度为[32]的过滤器，过滤器移动步长为[1],且使用全[0]填充。 conv1 = tf.nn.conv2d( input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding="SAME") relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases)) ### Layer2 : pooling layer ### # 实现第二层池化层的前向传播过程： # 选用最大池化层：池化层过滤器尺寸为[2*2],使用全[0]填充且移动步长为[2]. # 当前层输入为上一层输出，即[28, 28, 32]的节点矩阵。输出为[14, 14, 32]的节点矩阵。 with tf.name_scope("layer2-pool1"): pool1 = tf.nn.max_pool( relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") ### Layer3 : convolutionial layer ### # 声明第三层卷积层的变量并实现前向传播过程： # 当前层输入为上一层池化层的输出，即[14, 14, 32]的节点矩阵。输出为[14, 14, 64]的节点矩阵。 with tf.variable_scope("layer3-conv2"): conv2_weights = tf.get_variable( "weight", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP], initializer=tf.truncated_normal_initializer(stddev=0.1)) conv2_biases = tf.get_variable( "bias", [CONV2_DEEP], initializer=tf.constant_initializer(0.0)) # 使用尺寸为[5*5] ,深度为[64]的过滤器，过滤器移动步长为[1],且使用全[0]填充。 conv2 = tf.nn.conv2d( pool1, conv2_weights, strides=[1, 1, 1, 1], padding="SAME") relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases)) ### Layer4 : pooling layer ### # 实现第四层池化层的前向传播过程： # 选用最大池化层：池化层过滤器尺寸为[2*2],使用全[0]填充且移动步长为[2]. # 当前层输入为上一层输出，即[14, 14, 64]的节点矩阵。输出为[7, 7, 64]的节点矩阵。 with tf.name_scope("layer4-pool2"): pool2 = tf.nn.max_pool( relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") ### Layer5 : full connection layer ### ## [1] 将第四层池化层的输出矩阵转化为第五层全连接层的输入格式： # 全连接层的输入格式为向量，这里需要将上一层输出的[7, 7, 64]节点矩阵张成一个向量： # pool2.get_shape()可以获取到池化层输入矩阵的维度。注意：该维度中包含一个 batch 中数据的个数。 pool_shape = pool2.get_shape().as_list() # 计算将矩阵张成向量的长度，即全连接层输入节点个数。这个长度就是矩阵长宽以及深度的乘积： # 注意：pool_shape[0]为一个 batch 中数据的个数，即 batch_size。 nodes = pool_shape[1] * pool_shape[2] * pool_shape[3] # 通过 tf.reshape() 函数将第四层池化层输出的节点矩阵张成一个 batch 的向量： reshaped = tf.reshape(pool2, [pool_shape[0], nodes]) ## [2] 声明第五层全连接层的变量并实现前向传播过程： # 当前层的输入是 reshaped 以后的一个 batch 的向量，长度为 [7, 7, 64]=3136。输出也是一个batch 的向量，长度为 512。 # 引入 dropout: 训练时，dropout 会随机将部分节点输出改为 0，可以避免过拟合，使得学得模型在测试数据上的效果更好。 with tf.variable_scope("layer5-fc1"): fc1_weights = tf.get_variable( "weight", [nodes, FC_SIZE], initializer=tf.truncated_normal_initializer(stddev=0.1)) # 只有全连接层的权重需要加入正则化： if regularizer_class != None: tf.add_to_collection("losses", regularizer_class(fc1_weights)) fc1_biases = tf.get_variable( "bias", [FC_SIZE], initializer=tf.constant_initializer(0.1)) fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases) if train: fc1 = tf.nn.dropout(fc1, 0.5) ### Layer6 : full connection layer ### # 当前层的输入是长度为 512 的向量，输出向量为一个 batch 的长度为 10 的向量。 # 当前层的输出通过 Softmax 之后就得到最终的分类结果 with tf.variable_scope("layer6-fc2"): fc2_weights = tf.get_variable( "weight", [FC_SIZE, NUM_LABELS], initializer=tf.truncated_normal_initializer(stddev=0.1)) # 只有全连接层的权重需要加入正则化： if regularizer_class != None: tf.add_to_collection("losses", regularizer_class(fc2_weights)) fc2_biases = tf.get_variable( "bias", [NUM_LABELS], initializer=tf.constant_initializer(0.1)) logit = tf.matmul(fc1, fc2_weights) + fc2_biases # 返回卷积神经网络的前向传播结果： return logit 至此，你可以通过运行上述程序测试这个卷积神经网络在 MNIST 数据集上的正确率了。在 MNIST 测试数据上，上面给出的卷积神经网络可以达到大约 99.4% 的正确率。相比前面介绍的全连接神经网络最高的 98.4% 的正确率，卷积神经网络可以大幅度提升神经网络在 MNIST 数据集上的表现。 3.1.3 基于 LeNet-5 模型的通用设计模式上面我们详细讲解了 LeNet-5 模型的原理以及实现，然而仅仅一种卷积神经网络架构不能解决所有的问题。比如 LeNet-5 模型就无法很好的处理类似 ImageNet 这样比较大的图像数据集。那么如何设计不同的可以满足需要的卷积神经网络架构呢？ 1 –&gt; 卷积神经网络通用架构： 下面给出一个经典的用于图像分类的卷积神经网络通用架构公式： $$ 输入层 –&gt; (卷积层 + –&gt; 池化层？) + –&gt; 全连接层 + … ​$$ |↓↓↓↓↓ 公式说明 ↓↓↓↓↓| “卷积层 + ” ：表示一层或者多层卷积层。大部分卷积神经网络中一般最多连续使用三层卷积层。 “池化层 ？” ：表示没有或者一层池化层。池化层虽然可以起到减少参数防止过拟合的问题，但在部分论文中发现可以直接通过调整卷积层步长来完成，所以有些卷积神经网络中没有池化层。 “全连接层 + ” ：在多轮卷积层和池化层之后，卷积神经网络在输出之前一般会经过 1 到 2 个全连接层。 2 –&gt; 卷积层和池化层通用配置 经验总结： 一般卷积层的 过滤器边长 不会超过 $5$ (一般为：$1$、$3$、$5$ )，但有些卷积神经网络结构中，处理输入的卷积层中使用了边长为 $7$ 甚至是 $11$ 的过滤器。而在过滤器深度上，大部分卷积神经网络都是采用逐层递增（翻倍）的方式。卷积层的步长一般为 $1$ ,但是在有些模型中也会使用 $2$ 或者 $3$ 作为步长。 池化层的配置相对简单，用的最多的是最大池化层。池化层的过滤器边长一般为 $2$ 或者 $3$ ，步长一般也为 $2$ 或者 $3$ 。 3 –&gt; 经典样例 使用样例：LeNet-5 模型架构 输入层−−&gt;卷积层−−&gt;池化层−−&gt;卷积层−−&gt;池化层−−&gt;全连接层−−&gt;全连接层−−&gt;输出层 除了 LeNet-5 模型，2012 年 ImageNet ILSVRC 图像分类挑战的第一名 AlexNet 模型、2013 年 ILSVRC 第一名 ZF Net 模型以及 2014 年第二名 VGGNet 模型的架构都满足上面介绍的正则表达式。后面我们在 CNN 进化史中会详细介绍各种流行的模型。 3.2 Inception-v3 模型上一小节通过介绍 LeNet-5 模型给出了一种经典的卷积神经网络通用架构设计。这一小节我们来看 Inception 结构以及 Inception-v3 卷积神经网络模型。 3.2.1 Inception 架构Inception 结构是一种和 LeNet-5 架构结构完全不同的卷积神经网络结构。在 LeNet-5 模型中，不同卷积层通过串联的方式连接在一起（符合卷积神经网络通用设计模式），而 Inception 结构是将不同的卷积层通过并联的方式结合在一起。下面先来详细介绍 Inception 结构： 上一小节提到一个卷积层可以使用边长为 1、3 或者 5 的过滤器，那么如何在这些边长中进行选择呢？Inception 模块给出了一个方案，那就是同时使用所有不同尺寸的过滤器，然后再将得到的矩阵拼接起来。下图给出了 Inception 模块的一个单元结构示意图： 从图中可以看出，Inception 结构会首先使用不同尺寸的过滤器处理输入矩阵 。如图，最上方的矩阵为使用了边长为 1 的过滤器的卷积层前向传播的结果。类似的，中间的矩阵使用的过滤器边长为 3，下方的矩阵使用的过滤器边长为 5。不同的矩阵代表了 Inception 模块中的一条计算路径。 虽然过滤器的尺寸大小不同，但如果所有的过滤器都使用全 0 填充且步长为 1，那么 前向传播得到的结果矩阵的长和宽都与输入矩阵一致 。这样经过不同过滤器处理的结果矩阵可以拼接成一个更深的矩阵。如上图所示，可以将它们再深度这个维度上组合起来。 –&gt; 结论： Inception 模块得到的结果矩阵的长和宽与输入矩阵一样，深度为 RGB 三个矩阵深度的和，它展示的是 Inception 模块的核心思想。 3.2.2 Inception-v3通过 Inception 模块的不同组合，我们可以搭建不同架构的神经网络结构。而真正地在后续的 Inception-v3 模型中使用到的 Inception 模块更加复杂多样。下面我们来看 Inception-v3 模型讲解。 首先给出 Inception-v3 模型的架构图: Inception-v3 模型总共有 46 层，由 11 个 Inception 模块组成。如图中最后一个单元模块就是一个 Inception 模块（比前面介绍的 Inception 模块更复杂）。 3.2.3 TensorFlow-Slim在 Inception-v3 模型中有 96 个卷积层，如果将上一章节中卷积层实现代码直接搬过来，那么一个卷积层就需要 5 行代码，于是 Inception-v3 模型总共需要至少 480 行代码来实现，这样导致代码的可读性非常差。 为了更好地实现类似 Inception-v3 模型这样复杂的卷积神经网络，下面先介绍 TensorFlow-Slim 工具来更加简洁的实现一个卷积层。以下代码对比了直接使用 TensorFlow 实现一个卷积层和使用 TensorFlow-Slim 实现同样结构的神经网络的代码量： 1 –&gt; 直接使用 TensorFlow 原始 API 实现卷积层 123456# 直接使用 TensorFlow 原始 API 实现卷积层：with tf.variable_scope(scope_name): weights = tf.get_variable("weight", ...) biases = tf.get_variable("bias", ...) conv = tf.nn.conv2d(...)relu = tf.nn.relu(tf.nn.bias_add(conv, biases)) 2 –&gt; 使用 TensorFlow-Slim 使用 TensorFlow-Slim 实现上述同样结构的神经网络。TensorFlow-Slim 提供了 slim.conv2d() 函数可以在一行中实现一个卷积层的前向传播算法。slim.conv2d() 函数有 3 个参数是必须的： 第一个参数：输入的节点矩阵； 第二个参数：当前卷积层过滤器的深度； 第三个参数：过滤器的尺寸。 可选的参数有过滤器移动的步长、是否使用全 0 填充、激活函数以及变量命名空间等 1net = slim.conv2d(input, 32, [3,3]) 完整的 Inception-v3 模型实现较长，源码见：[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets/] 。这里只提供 Inception-v3 模型中结构相对复杂的一个 Inception 模块的代码实现。以下代码实现了上图中最后一个单元模块（ Inception 模块） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# slim.arg_scope 函数可以用于设置默认的参数取值：# [1] slim.arg_scope() 第一个参数是一个函数列表，这个列表中的函数将使用默认的参数取值。比如以下定义： 使用 slim.conv2d(net, 320, [1,1]) 函数# 时自动加上 stride=1 和 padding='SAME' 的参数。# 如果在函数调用时指定了 stride，那么这里设置的默认值就不会再使用。通过这种方式可以进一步减少冗余代码。with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'): ... # 此处省略了 Inception-v3 模型中其它网络结构而直接实现最后 Inception 模块。 # 假设输入图片经过之前的神经网络前向传播的结果保存在变量 net 中。 net = 上一层的输出节点矩阵 # 为一个 Inception 模块声明一个统一的变量命名空间： with tf.variable_scope('Mixed_7c'): # 给 Inception 模块中每一条路径声明一个命名空间（第一条路径）： with tf.variable_scope('Branch_0'): # 实现一个过滤器边长为 1，深度为 320 的卷积层： branch_0 = slim.conv2d(net, 320, [1,1], scope='Conv2d_0a_1x1') # Inception 模块中第二条路径。这条计算路径上的结构本身也是一个 Inception 结构： with tf.variable_scope('Branch_1'): branch_1 = slim.conv2d(net, 384, [1,1], scope='Conv2d_0a_1x1') # tf.concat 函数可以将多个矩阵拼接起来： # tf.concat 函数的第一个参数指定拼接的维度，这里给出的 ‘3’代表了矩阵是在深度这个维度上进行拼接 branch_1 = tf.concat(3, [ # 如Inception-v3 模型架构图所示，此处 2 层卷积层的输入都是 branch_1 而不是 net slim.conv2d(branch_1, 384, [1,3], scope='Conv2d_0b_1x3'), slim.conv2d(branch_1, 384, [3,1], scope='Conv2d_0c_3x1')]) # Inception 模块中第三条路径。此路径上的结构本身也是一个 Inception 结构： with tf.variable_scope('Branch_2'): branch_2 = slim.conv2d( net, 448, [1,1], scope='Conv2d_0a_1x1') branch_2 = slim.conv2d( branch_2, 384, [3,3], scope='Conv2d_0b_3x3') branch_2 = tf.concat(3, [ slim.conv2d(branch_2, 384, [1,3], scope='Conv2d_0c_1x3'), slim.conv2d(branch_2, 384, [3,1], scope='Conv2d_0d_3x1')]) # Inception 模块中第四条路径。 with tf.variable_scope('Branch_3'): branch_3 = slim.avg_pool2d(net, [3,3], scope='AvgPool_0a_3x3') branch_3 = slim.conv2d(branch_3, 192, [1,1], scope='Conv2d_0b_1x1') # 当前 Inception 模块的最后输出是由上面四个计算结果拼接得到的： net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3]) 至此，卷积神经网络基础部分已经讲解完成，后续我们还会继续开展拓展学习，进一步学习 CNN 发展史以及各种经典 CNN 模型。 Part 4 迁移学习（Transfer Learning）]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 系列之初识卷积神经网络（CNN）与图像识别]]></title>
    <url>%2FTensorFlow%2FTensorFlow-%E7%B3%BB%E5%88%97%E4%B9%8B%E5%88%9D%E8%AF%86%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%E4%B8%8E%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 在 前面的章节 中，我们通过最简单的 MNIST 手写数字图像识别问题验证了神经网络结构和优化方法对神经网络模型的影响，我们知道，神经网络结构设计对神经网络模型预测准确率有着本质影响。前面的章节我们主要是基于 全连接神经网络结构（传统神经网络） 来学习 TensorFlow 实现深度学习算法（MNIST 手写数字图像识别等）的一般过程。 本文我们将详细地介绍深度学习中的图像识别问题以及一个非常经典有用的神经网络结构：卷积神经网络（Convolutional Neural Network）。 卷积神经网络的应用非常广泛，在自然语言处理、医疗、灾难气候发现甚至围棋人工智能程序都有应用，特别是在计算机视觉相关领域的使用。 卷积神经网络（CNN）近年来取得了爆发式的发展，是深度学习中的一颗耀眼明珠。CNN 不仅能用来对图像进行分类，还在图像分割（目标检测）任务中有着广泛的应用。CNN 已经成为了 图像分类的黄金基准 ，一直在不断的发展和改进，成为了图像识别领域取得突破性进展的主要技术支持。 图像识别与卷积神经网络 本文我们将主要通过卷积神经网络在图像识别领域的应用来讲解卷积神经网络的基本原理以及 TensorFlow 对卷积神经网络实现的支持。这一部分的内容我们主要分为四个部分： 第一部分，将介绍图像识别领域解决的问题以及图像识别领域中常用的经典数据集； 第二部分，我们将介绍卷积神经网络的主体思想和整体架构，以及详细讲解卷积神经网络中的卷积层和池化层网络结构； 第三部分，将通过两个经典的卷积神经网络模型来介绍如何设计卷积神经网络架构以及如何设置每一层神经网络配置； 第四部分，我们将介绍如何通过 TensorFlow 实现卷积神经网络的迁移学习。 Part 1：图像识别1. 图像识别问题以及经典数据集视觉是人类认识世界非常重要的一种知觉。对于人类来说，通过视觉来识别手写体数字（图像识别）、识别图片中的物体（图像识别）或者找出图片中人脸的轮廓（人脸识别）都是非常简单的任务。 1.1 图像识别任务然而对于计算机而言，让计算机识别图片中的内容就不是一件容易的事情了，这也是 计算机视觉 领域的难题。图像识别问题是人工智能的一个重要领域。 图像识别就是：希望计算机程序来处理、分析和理解图像中的内容，使得计算机可以从图片中自动识别各种不同模式的目标或对象。 就像我们在前面博文中 MNIST 手写数字图像识别任务就是利用计算机来识别图片中的手写体数字。下图展示了 2013 年之前图像识别的主流技术在 MNIST 数据集上的错误率随年份的发展趋势图： 上图最下方的虚线表示人工标注的错误率，其他不同样式的线段表示了不同算法的错误率。可以看出：相比其它传统算法，卷积神经网络可以得到更低的错误率。而且通过卷积神经网络达到的错误率以及非常接近人工标注的错误率。 1.2 计算机视觉领域经典数据集MNIST 手写体识别数据集是一个入门级的简单数据集，在其它更加复杂的图像识别数据集上，卷积神经网络有更加突出的表现。下面我们来看一下都有哪些经常使用的经典图像数据集： 1）CIFAR Cifar 数据集是一个影响力很大的图像分类数据集。Cifar 数据集分为两种：Cifar-10 和 Cifar-100 ，它们都是图像词典项目（Visual Dictionary）中 800 万张图片的子集。 –&gt; Cifar-10 Cifar-10 数据集收集了来至 10 个不同种类的 60000 张图片（每类 6000 张），类别包含有：airplane、automobile、bird、cat、deer、dog、frog、horse、ship、truck。这些图片像素都是 32 * 32 的彩色图像（3 通道）。和 MNIST 数据集类似，Cifar-10 中的每一张图片仅包含一个种类实体（一张图片对应分类）。 和 MNIST 数据集相比，Cifar 数据集的最大区别在于图片由单通道的黑白图像变成三通道的彩色图像。 另外，分类的难度也相对更高（由手写数字变成了现实中的物体）。在 Cifar-10 数据集上，人工标注的正确率大概为 94%，这比 MNIST 数据集上的人工表现要低的多。下图给出了 MNIST 和 CIFAR-10 数据集上比较难以分类的图片。 Cifar-10 数据集中，直接从图片上看，人类也很难判断图片中的实体类别。而在 MNIST 数据集上，人类还可以有一个比较准确的猜测。目前，在 Cifar-10 数据集上最好的算法的图像识别正确率为 95.59%，达到这个准确率的算法同样都使用了卷积神经网络。 –&gt; MNIST 和 CIFAR 局限性 无论是 MNIST 数据集还是 CIFAR 数据集，相比真实环境下的图像识别问题，有两个最大的问题： 现实生活中的图片分辨率要远高于 32 * 32，而且图像的分辨率也不会是固定的； 现实生活中的物体类别很多，无论是 10 种还是 100 种都远远不够，而且图片种类不会仅仅对应一个种类的物体。 为了更贴近自然场景（真实环境）下的图像识别问题，由斯坦福大学的李飞飞教授带头整理的 ImageNet 很大程度地解决了这两个问题。（2018 年底腾讯 AI Lab 公布了一个 ML-Images（1800W）常见物体类别数据集，比 ImageNet 还要大。） 2）ImageNet ImageNet 是一个基于 WordNet 的大型图像数据库。在 ImageNet 中，将近 1500W 图片被关联到了 WordNet 的大约 2W 个名词同义词集上。目前每一个与 ImageNet 相关的 WordNet 同义词集都代表了现实世界中的一个实体，可以被认为是分类问题中的一个类别。 ImageNet 中的图片都是从互联网上爬去下来的，并通过亚马逊的人工标注服务将图片分类到 WordNet 的同义词集。在 ImageNet 的图片中，一张图片中可能出现多个同义词集所代表的实体。 下图展示了 ImageNet 中的一张包含多个实体的图片，这张图片上用几个矩形框出了不同实体的轮廓。在物体识别问题中，一般将用于框出实体的矩形称为： bounding box。例如在图片中标出了四个实体，其中有两把椅子、一个人和一条狗。 –&gt; ILSVRC 竞赛 mageNet 每年都举办图像识别相关的竞赛（ImageNet Large Scala Visual Recognition Challenge），而且每年的竞赛都会有一些不同的问题（目标定位、识别、检测以及分割等），这些问题基本涵盖了图像识别的主要研究方向。ImageNet 官网：http://www.image-net.org/challenges/LSVRC 列出了历届 ILSVRC 竞赛的题目和数据集。不同年份的 ImageNet 竞赛提供不同的数据集。 其中，ILSVRC2012、ILSVRC2014 是使用最多的图像分类数据集。 ILSVRC2012 数据集： ILSVRC2012 图像分类数据集的任务和 Cifar 数据集是基本一致的，也是识别图像中的主要物体。ILSVRC2012 图像分类数据集包含了来至 1000 个类别的 120W 张图片，其中每张图片属于且只属于一个类别，分辨率大小不固定。 下图给出了不同算法在 ImageNet 图像分类数据集上的 Top-5 的正确率。从图中可以看出，在更加复杂的 ImageNet 问题上，深度学习，特别是卷积神经网络，给图像识别问题带来了质的飞跃，基于卷积神经网络的图像识别算法可以远远超过人类的表现。 –&gt; top-N top-N 正确率指的是：图像识别算法给出的前 N 个答案中有一个是正确的概率。在图像分类问题上，很多学术论文都将前 N 个答案的正确率作为比较的方法，其中，N 的取值一般为 3 或 5。 当然，除了上述我们简单介绍到的MNIST， Cifar，ImageNet 数据集，还有一些其它比较经典的图像数据集可以使用。如：Pascal VOC、COCO 等，使用到时会进行详细介绍。 Part 2：卷积神经网络（CNN）在介绍图像识别领域经典数据集时，我们给出了不同算法（包括传统算法以及卷积神经网络）的识别表现。我们发现，在上述的所有图像分类数据集上卷积神经网络都有非常突出的表现。 图像分类的黄金基准 果然名不虚传！！！ 在前面的博文所介绍的神经网络结构每两层之间的所有节点都是有边相连的，我们称这种网络结构为全连接层网络结构（传统神经网络），这里是为了与卷积神经网络、循环神经网络等做区分。 Part2 部分我们将分为三个小节内容： 从全连接网络结构到卷积神经网络结构； 介绍卷积神经网络的整体架构； 我们将详细介绍卷积神经网络中的卷积层、池化层网络结构以及其前向传播过程，并介绍如何通过 TensorFlow 实现这些网络结构。 话不多说，开始了… 2.1 从全连接神经网络结构到卷积神经网络架构卷积神经网络结构可以看作是传统的全连接神经网络的一个改进。 有参照才能看出不同，这一小节将讲解卷积神经网络与全连接神经网络的差异。下面先给出一个显示了全连接神经网络与卷积神经网络结构的对比图： 2.1.1 –&gt; 你能看出啥？–&gt; 连接结构 如上图所示，尽管全连接神经网络结构和卷积神经网络结构直观上看差异比较大，但实际上它们的整体架构是非常相似的。啥，看不出来？ 卷积神经网络也是通过一层一层的节点组织起来的。在全连接神经网络结构中，每相邻两层之间的节点都有边相连，于是一般会将每一层全连接层中的节点组织成一列，这样方便显示连接结构。 而对于卷积神经网络，相邻两层之间只有部分节点相连，为了展示每一层神经元的维度，一般会将每一层卷积层的节点组织成一个三维矩阵。 –&gt; 输入输出以及优化 事实上，除了结构相似，卷积神经网络的输入输出以及训练流程与全连接神经网络也基本一致（意味着 TensorFlow 实现流程也基本相同）。 以图像分类为例：卷积神经网络的输入层就是图像的原始像素矩阵，而输出层的每一个节点代表了不同类别的可信度。这和全连接神经网络的输入和输出是一致的。类似的，在前面博文介绍的损失函数以及参数优化过程也适用于卷积神经网络。 在后面我们还会介绍到，在 TensorFlow 中训练一个卷积神经网络的流程和训练一个全连接神经网络没有任何区别。卷积神经网络和全连接神经网络的唯一区别在于神经网络中相邻两层之间的连接方式。 2.1.2 –&gt; 全连接神经网络结构存在的缺点我们知道，相较于传统的全连接网络结构，卷积神经网络可以更好的处理图像问题。在进一步介绍卷积神经网络结构之前，我们先说明相较于卷积神经网络，为什么全连接神经网络无法很好地处理图像数据？！（事实上，导致这种差异取决于网络层的连接方式，导致了图像对全连接结构的不友好） –&gt; 参数太 TM 多了！ 使用全连接神经网络处理图像的最大问题在于全连接层的参数太多： 对于 MNIST 数据，每一张图片的大小都是 $28∗28∗1$，其中 $28∗28$ 是图片的大小，$∗1$ 表示图像是黑白（单通道）的，只有一个颜色通道。假设第一次隐藏层的节点数为 500 个，那么一个全连接层的神经网络将有 $28∗28∗500+500=392500$ 个参数。 当图片尺寸更大时，比如在 Cifar-10 数据集中，每一张图片的大小均为：$32∗32∗3$ ，其中 $32∗32$ 表示图片的大小， $∗3$ 表示图片是三通道彩色图像。这样输入层就有 3072 个节点，如果第一层全连接层仍然是 500 个节点，那么这一层全连接神经网络就将有 $3072∗500+500$ 近似 $150W$ 个参数，这还是仅一层隐藏层的前提下，相当可怕啊。 –&gt; 参数太多，怎么了？ 参数增多除了导致计算速度减慢，还很容易导致过拟合（overfitting）的问题。所以，从计算资源和调参的角度都不建议用传统的全连接神经网络，我们需要的是一个更加合理的神经网络结构来有效减少神经网络中的参数个数。卷积神经网络就可以达到这个目的。 2.1.3 –&gt; 图像的局部特征既然全连接结构导致了参数过多，那么如何能够减少参数量？思考一下：全连接的方式是必须的吗？ 全连接层的方式对于图像数据来说似乎显得不这么友好，因为图像本身拥有 局部特性。举个例子，譬如我们看一张猫的图片，可能看到猫的眼镜或者嘴巴就已经知道这是张猫片，而不需要说每个部分都看完了才知道是猫。 所以，如果我们可以使用某种方式可以对一张图片的某个局部典型特征进行提取，然后识别，那么这张图片的类别也就知道了。这个时候就产生了卷积的概念。 下面我们来详细讲解卷积神经网络结构： 2.2 卷积神经网络架构详解首先给出一个更加具体的卷积神经网络架构图： 2.2.1 –&gt; 卷积神经网络的基本结构可以看到，在卷积神经网络的前几层中，每一层的节点都被组织成一个三维矩阵。比如处理 Cifar-10 数据集中的图片时，可以将输入层组织成一个 $32∗32∗3$ 的三维矩阵。 图中虚线部分展示了卷积神经网络的一个连接示意图，从图中可以看出卷积神经网络中前几层中每一个节点只有和上一层中部分的节点相连。 一个通用的卷积神经网络主要由以下 5 种结构组成： –&gt; 1）输入层 输入层是整个神经网络的输入，在处理图像的卷积神经网络中，它一般代表了一张图片的像素矩阵。 例如图中最左侧的三维矩阵（输入层）就可以代表一张图片。其中三维矩阵的长和宽代表了图像的大小，三维矩阵的深度代表了图像的色彩通道（channel）。比如灰度图像（H, W, 1）的深度为 1，而在 RGB 色彩模式（H, W, 3）下，图像的深度为 3。 从输入层开始，卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵，直到最后的全连接层。 –&gt; 2）卷积层 卷积层，从名字可以看出，卷积层是一个卷积神经网络中最为重要的部分。 和传统全连接层不同的是， 卷积层中每一个节点的输入只是对应上一层神经网络的一小块，这个小块常用的大小有 $3∗3$ 或者 $5∗5$。 卷积层试图将神经网络中的每一小块进行更加深入地分析从而得到 某一小块抽象程度更高的特征 （是不有点局部特征提取的感觉了）。一般来说，通过卷积层处理过的节点矩阵会变得更深，所以在图中可以看到经过卷积层之后的节点矩阵的深度会增加。 –&gt; 3）池化层 Pooling（采样层 Sample / Subsample） 池化层神经网络不会（而非不能）改变三维矩阵的深度，但是它可以缩小矩阵的大小。池化操作可以认为是将一张分辨率较高的图片转化为分辨率较低的图片。 通过池化层，可以进一步缩小最后全连接层中节点的个数（减小全连接层输入矩阵大小），从而达到减少整个神经网络中参数的目的。 –&gt; 4）全连接层 如图所示，在经过多轮卷积层和池化层处理后，在卷积神经网络的最后一般会是由 1 到 2 个全连接层来给出最后的分类结果。经过几轮卷积层和池化层处理后，可以认为图像中的信息已经被抽象成了信息含量更高的特征。 我们可以将卷积层和池化层看成自动图像特征提取的过程。在特征提取完成之后，仍然需要使用全连接层来完成分类任务。 –&gt; 5）Softmax 层 和全连接中一样，Softmax 层主要用于分类问题。通过 Softmax 层，可以得到当前样例属于不同种类的概率分布情况。 至此，卷积神经网络的整体架构已经介绍完了。 输出层，全连接层以及 Softmax 层很好理解，下面我们将详细介绍卷积神经网络中特殊的两个网络结构：卷积层和池化层。 2.2.2 –&gt; CNN 中的卷积层和池化层这一小节将详细介绍卷积层和池化层网络结构以及其前向传播过程，并介绍它们在 TensorFlow 中支持。 1 –&gt; 卷积层网络结构下面首先给出卷积神经网络结构中最重要的部分：卷积核（convolution kerner）或者也称为过滤器（filter）。 1.1 –&gt; 卷积核以及其运算 –&gt; 1）卷积核（过滤器） 过滤器可以将当前网络层上的一个 子节点矩阵（N × N × C1） 转化为下一网络层（卷积层）上的一个 单位节点矩阵（1 × 1 × C2）。单位节点矩阵指的是一个长和宽都为 1，但深度不限的节点矩阵。如上图所示，表示了一个过滤器结构。左侧表示当前层的子节点矩阵，右侧表示下一层的单位节点矩阵。 噢~原来卷积核就是指卷积层的连接结构。可以看出，卷积层上的一个连接就是一个卷积核结构。 –&gt; 2）设置卷积核（过滤器） –&gt; 卷积核尺寸 在一个卷积层结构中，过滤器所处理的子节点矩阵的长和宽都是由人工指定的，这个节点矩阵的尺寸也称之为 过滤器的尺寸 。常用的过滤器尺寸有 $3×3​$ 或 $5×5​$。注意，因为过滤器处理的矩阵深度和当前层神经网络节点矩阵的深度是一致的，所以虽然当前节点矩阵是三维的，但过滤器的尺寸只需要指定两个维度。 –&gt; 卷积核深度 过滤器中另外一个需要人工指定的设置是处理得到的单位节点矩阵的深度，这个设置称之为 过滤器的深度 。注意过滤器的尺寸指的是一个过滤器输入节点矩阵的大小，而深度指定是输出单位节点矩阵的深度。 –&gt; 3） 卷积核（过滤器）运算 过滤器的运算过程也称之为过滤器的前向传播过程。 我们知道，卷积层上的每一个单位节点矩阵都对应一个过滤器结构。所以，在介绍卷积层的前向传播过程之前，我们必须先搞明白每一个单位节点矩阵，即每一个过滤器（卷积核）的前向传播过程是什么样子的？！ 过滤器的前向传播过程就是通过左侧小矩阵中的节点计算出右侧单位矩阵中节点的过程。 为了直观地解释过滤器的前向传播过程，下面将给出一个具体的样例，这个样例中将展示如何通过过滤器将一个 $2∗2∗3$ 的子节点矩阵变化为一个 $1∗1∗5$ 的单位节点矩阵。 ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ 一个过滤器的前向传播过程和全连接层类似，可以将其想象成一个全连接的前向传播过程： 它总共需要 $2∗2∗3∗5+5=65​$ 个参数，其中最后的 $+5​$ 为偏置项参数的个数。假设使用 $𝑊^𝑖_{𝑥,𝑦,𝑧} ​$来表示对于输出单位节点矩阵中的第 $𝑖​$ 个节点，过滤器输入的子节点矩阵中节点 $(𝑥, 𝑦, 𝑧)​$ 的对应的权重，使用 $𝑏^𝑖​$ 表示第 $𝑖​$ 个输出节点对应的偏置项参数，那么单位矩阵中的第 $𝑖​$ 个节点的取值 $𝑔(𝑖)​$ 为： $$ g(i) = f(\sum_{x=1}^{2}\sum_{y=1}^{2}\sum_{z=1}^{3}a_{x,y,z} · W^i_{x,y,z} + b^i) $$ 可以看出，$a_{x,y,z}$ 对应的是过滤器中输入的子节点矩阵中节点 $(𝑥,𝑦,𝑧)$ 的取值，f 为激活函数。下图展示了给定 $𝑎$, $𝑊^0$ 和 $𝑏^0$ 的情况下，使用 Relu 作为激活函数时 $𝑔(0)$的计算过程: 左侧给出了 $𝑎$, $𝑊^0$ 的取值，这里通过 3 个二维矩阵来表示一个三维矩阵的取值，其中每一个二维矩阵表示三维矩阵在某一个深度上的取值。 $·$ 符号表示点积，也就是矩阵中对应元素乘积的和。右侧显示了 $𝑔(0)$ 的计算过程。 ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ 同理，如果给出 $w^1$ 到 $w^4$ 和 $b^1$ 到 $b^4$，那么也可以类似地计算出 $g(1)$ 到 $g(4)$ 的取值。如果将 $a$ 和 $W^i$ 组织成两个向量，那么一个过滤器的计算过程完全可以通过向量点乘来完成。 –&gt; 4） 为什么卷积核有效？ 前面我们一直在说，卷积核可以提取出图像某一小块区域的典型特征….什么什么地。既然现在我们以及知道了卷积核是如何计算的，那现在考虑一下：为什么使用卷积核计算后分类效果要由于普通的神经网络呢？ 请看下图，原始图像经过第一个卷积核计算后的 feature_map 是一个三维数据，在第三列的绝对值最大，说明原始图片上对应的地方有一条垂直方向的特征，即像素数值变化较大；而通过第二个卷积核计算后，第三列的数值为 0，第二行的数值绝对值最大，说明原始图片上对应的地方有一条水平方向的特征。 哎呀，两个卷积核分别能够提取，或者说检测出原始图片的特定的特征。把卷积核就理解为特征提取器没毛病啊！ 1.2 –&gt; 卷积层的前向传播过程 好，前面我们已经完成了卷积层上一个单位节点矩阵，即一个过滤器的前向传播。事实上，卷积层结构的前向传播过程就是通过将一个过滤器从神经网络当前层的左上角移动到右下角，并且在移动中计算每一个对应的单位矩阵得到的。 下图展示了卷积层结构前向传播的过程。为了更好地可视化过滤器的移动过程，节点矩阵深度均为 1。图中展示了在 $4∗4​$ 子节点矩阵上使用 $2∗2​$ 过滤器的卷积层前向传播过程。 注意，原始图像经过卷积运算提取特征后的卷积层也称为：feature map（特征映射图）。 在这个过程中，这个过滤器会被从当前层的左上角移动到右下角。并且过滤器每移动一次，可以计算得到一个值（当过滤器深度为 K 时会计算出 K 个值）。将这些数值拼接成一个新的矩阵，就完成了卷积层前向传播的过程。 下面给出一个输入层为 $7 × 7 × 3​$，卷积核尺寸为 $3 × 3​$，输出卷积层深度为 2 的前向传播过程演示： 可以看出，卷积核的深度也为 2，并且各个深度上的参数矩阵分别为：$W0$，$W1$ ，且均为 3 × 3 × 3 的矩阵。 –&gt; 1）卷积网络层矩阵尺寸变化 –&gt; 使用填充 可以看出，当过滤器的大小不为 $1∗1$时，前向传播得到的卷积层矩阵的尺寸要小于当前层矩阵的尺寸。正如上面演示，当前层（Input Volume）矩阵的大小为 7 × 7 的，而通过前向传播算法之后，得到的卷积层矩阵大小为 3 × 3。 为了避免尺寸的变化，可以在当前层矩阵的边界上加入全 0 填充（zero-padding）。这样可以使得卷积层前向传播结果矩阵的大小和当前层矩阵保持一致。 给出一个节点矩阵深度均为 1 的在 $3 × 3​$ 子节点矩阵上使用 $2 × 2​$ 过滤器的卷积层前向传播过程： 此时我们给 $3 × 3$ 子节点矩阵边界增加全 0 填充，再来看卷积层矩阵尺寸的变化： 可以看到结果矩阵的大小和当前层矩阵保持一致。 –&gt; 设置过滤器移动步长 除了使用全 0 填充，还可以通过设置过滤器移动的步长来调整结果矩阵的大小。 可以看到，在图 6-10 和图 6-11 中，过滤器每次都只移动一格，事实上，我们还可以对其设置其它的移动步数。 下图显示了当移动步长为 2 且使用全 0 填充时，卷积层前向传播过程： 从图 6-12 上可以看出，当矩阵长和宽的步长均为 2 时，即过滤器每隔 2 步计算一次结果，所以得到的结果矩阵的长和宽也就都只有原来的一半。下面的公式给出了在同时使用全 0 填充时结果矩阵的大小： $$ out_{length} = [in_{length} / stride_{length}] ​$$ $$ out_{width} = [in_{width} / stride_{width}] ​$$ 也就是说：在使用全 0 填充，长和宽步长均为 2 时，$ out_length / out_width $ （表示输出层矩阵的长度或宽度）等于输入层矩阵长度（宽度）除以该长度方向上的步长的向上取整值。 如果不使用全 0 填充，下面的公式给出了结果矩阵的大小： $$ out_{length} = [（in_{length} - filter_length + 1） / stride_{length}] $$ $$ out_{width} = [(in_{width} - filter_width + 1) / stride_{width}] $$ –&gt; 2）卷积核（过滤器）参数共享 这里再补充一个卷积神经网络中的非常重要的一个关于卷积核参数概念。 在卷积神经网络中，同一个卷积层中使用的过滤器中的参数都是一样的。这是卷积神经网络中一个非常重要的性质。也就是说，过滤器在网络层移动过程中其内部的参数，对所有单位节点矩阵都是都是共享，训练时优化的是同一套参数（参考前面给出的卷积层前向传播过程演示理解）。 –&gt; 卷积核参数共享有什么好处？ 1 –&gt; 从直观上理解，共享过滤器参数可以使得图像上的内容不受位置的影响。 以 MNIST 手写数字识别为例，无论数字 “1”出现在左上角还是右下角，图片中的种类都是不变的。这是因为在左上角和右下角使用的过滤器参数相同，所以通过卷积层之后无论数字在图像的哪个位置，得到的结果都是一样的。 想想这也是合理的，不然对于原始图像中的不同内容，由于位置不同，竟然生成了不同的特征图，那还干个luan… 2 –&gt; 共享每一个卷积层中过滤器中的参数可以巨幅减少神经网络上的参数。 以 Cifar-10 问题为例，输入层矩阵的维度是 $32 × 32 × 3$。假设第一层卷积层使用尺寸为 $5 × 5$，深度为 16 的过滤器，那么这个卷积层的参数个数为 $5 × 5 × 3 × 16 + 16 = 1216$ 个。这和前面提到过的，使用 500 个隐藏节点的全连接层将近 150W 个参数相比，卷积层的参数个数要远远小于全连接层。 3 –&gt; 而且卷积层的参数个数和图片的大小无关，它只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关系。这使得卷积神经网络可以很好的扩展到更大的图像数据集上。 1.3 –&gt; 卷积层前向传播过程的 TensorFlow 实现 我们知道，卷积层结构的前向传播过程就是通过将一个过滤器从神经网络当前层的左上角移动到右下角，并且在移动中计算每一个对应的单位矩阵得到的。 TensorFlow 对卷积神经网络提供了非常好的支持，通过它实现卷积层网络是非常方便的。下面程序实现了一个卷积层的前向传播过程。 123456789101112131415161718192021222324252627# 通过 tf.get_variable() 函数创建过滤器权重变量和偏置项变量。# 上面介绍了卷积层的参数个数只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关。# 所以这里声明的参数变量是一个四维矩阵，其依次存储了过滤器尺寸（length / width）、当前层深度、过滤器深度共四个维度信息：filter_weight = tf.get_variable( 'weights', [5, 5, 3, 16], initializer=tf.truncated_normal_initializer(stddev=0.1))# 和卷积层的权重类似，当前层矩阵上不同位置的偏置项也是共享的，所以总共有下一层深度个不同的偏置项：biases = tf.get_variable( 'biases', [16], initializer=tf.constant_initializer(0.1))# TensorFlow 提供了一个非常方便的函数来实现卷积层前向传播的算法：# tf.nn.conv2d 第一个参数为当前层的节点矩阵。注意这个矩阵是一个四维矩阵：[训练时一个batch图像的数量，图像高度，图像宽度， 图像通道数]# 例如：输入层，input[0,:,:,:]表示一张图片，input[1,:,:,:]表示第二张图片。# tf.nn.conv2d 第二个参数提供了卷积层的权重# tf.nn.conv2d 第三个参数为不同维度上的步长，是一个长度为 4 的数组。但第一维（对应 batch_size）和最后一维度（深度）的数字要求一定是“1”，# 这是因为卷积层步长只对过滤器尺寸有效# tf.nn.conv2d 最后一个参数是表示填充方法。TensorFlow 提供了两种填充方式：SAME（全 0 填充）、VALID（有效填充）conv = tf.nn.conv2d( input, filter_weight, strides=[1,1,1,1], padding='SAME')# tf.nn.bias_add 提供了一个方便的函数给每一个节点添加偏置项bias = tf.nn.bias_add(conv, biases)# ReLU 去线性化：actived_conv = tf.nn.relu(bias) 2 –&gt; 池化层网络结构在讲解卷积神经网络的架构时，我们知道，在卷积层之间往往会加一个池化层（Pooling layer）。 池化层可以通过降采样（Subsample）的方式，在不影响特征图质量的情况下，非常有效地缩小矩阵的尺寸，从而减少最后全连接层中的参数。虽然池化层也可以减少矩阵的深度，但是实践中一般不会这样使用。 和上一小节中介绍的卷积层类似，池化层前向传播的过程也是通过移动一个类似卷积核（过滤器）的结构完成的，也可以称为滑动窗口。不过池化层过滤器中的计算的不是节点的加权和，而是采用更加简单的最大值或者平均值运算。 使用最大值操作的池化层被称之为最大池化层（max pooling），这是被使用得最多的池化层结构。使用平均操作的池化层被称之为平均池化层（average pooling）。 2.1 –&gt; 池化层中的过滤器 和卷积层中的过滤器类似，池化层的过滤器也需要人工设定过滤器的尺寸、是否使用全 0 填充以及过滤器移动的步长设置等，而且这些设置的意义也是一样的。 卷积层和池化层中过滤器移动的方式也是相似，唯一的区别在于卷积层使用的过滤器是横跨整个深度的，而池化层使用的过滤器只影响一个深度上的节点。故，池化层的过滤器除了在长和宽两个维度上移动之外，故它还需要在深度这个维度移动。 下面给出一个最大池化层前向传播计算过程示意图： 图中，不同颜色或者不同线段（虚线或实线）代表了不同的池化层过滤器。可以看出，池化层的过滤器除了在长和宽的维度上移动，它还需要在深度的维度上移动。 2.2 –&gt; 为什么可以 Max Pooling？ 从计算方式来看，算是最简单的一种了，对滑动窗口取 max 即可，但是这也引发一个思考：取 Max Pooling 意义在哪里？如果我们只取最大值，那其他的值被舍弃难道就没有影响吗？不会损失这部分信息吗？能取就说明这些信息是可损失的，那么是否意味着我们在进行卷积操作后仍然产生了一些不必要的冗余信息呢？ 其实从上文分析卷积核为什么有效的原因来看，每一个卷积核可以看做一个特征提取器，不同的卷积核负责提取不同的特征。我们例子中设计的第一个卷积核能够提取出“垂直”方向的特征，第二个卷积核能够提取出“水平”方向的特征，那么我们对其进行Max Pooling操作后，提取出的是真正能够识别特征的数值，其余被舍弃的数值，对于我提取特定的特征并没有特别大的帮助。 同时，在进行后续计算时，由于减小了 feature map 的尺寸，从而减少参数，达到减小计算量，却不损失效果的情况。 当然，并不是所有情况 Max Pooling 的效果都很好，有时候有些周边信息也会对某个特定特征的识别产生一定效果，那么这个时候舍弃这部分“不重要”的信息，就不划算了。所以具体情况得具体分析，如果加了 Max Pooling 后效果反而变差了，不如把卷积后不加 Max Pooling 的结果与卷积后加了 Max Pooling 的结果输出对比一下，看看 Max Pooling 是否对卷积核提取特征起了反效果。 2.3 –&gt; 池化层前向传播过程的 TensorFlow 实现 池化层的前向传播也很简答，下面的 TensorFlow 程序实现了池化层的前向传播算法： 1 –&gt; 最大池化层 1234# tf.nn.max_pool 实现了最大池化层的前向传播过程：# ksize 提供了过滤器的尺寸，strides 提供了步长信息，padding 提供了是否使用全 0 填充：pool = tf.nn.max_pool(actived_conv, ksize=[1,3,3,1], strides=[1,2,2,1], padding=&apos;SAME&apos;) 对比池化层和卷积层前向传播在 TensorFlow 中的实现，可以发现函数的参数形式是相似的: tf.nn.max_pool 函数中首先需要传入当前层的节点矩阵，这个矩阵是一个四维矩阵，格式和 tf.nn.conv2d 函数中的第一个参数一致。 第二个参数为过滤器的尺寸，虽然给出的是一个长度为 4 的一维数组，但是这个数组的第一个和最后一个数必须为 “1”。这意味着池化层的过滤器是不可以跨不同输入样例或者节点矩阵深度的。在实际应用中使用的最多的池化层过滤器尺寸为:[1,2,2,1] 或者 [1,3,3,1]。 第三个参数为步长，它和 tf.nn.conv2d 函数中步长的意义是一样的。而且第一维和最后一维也只能维“1”。这意味着在 TensorFlow 中，池化层不能减少节点矩阵的深度或者输入样例的个数 2 –&gt; 平均池化层 TensorFlow 还提供了 tf.nn.avg_pool 来实现平均池化层。tf.nn.avg_pool 函数的调用格式和 tf.nn.max_pool 函数是一致。 3 –&gt; Reading （Recommended）如果你对上面的讲述的理解的产不多的化，其实下面几个问题并不难回答： 卷积核的尺寸必须为正方形吗？可以为长方形吗？如果是长方形应该怎么计算？ 搭建卷积神经网络时，卷积核的个数如何确定？ Feature Map 如何理解？ 3.1 –&gt; 卷积核的尺寸不一定非得为正方形 长方形也可以，只不过通常情况下为正方形。如果要设置为长方形，那么首先得保证卷积层的输出形状是整数，不能是小数。比如你的图像是边长为 28 的正方形。那么卷积层的输出就满足 [ (28 - kernel_size)/ stride ] + 1 ，这个数值得是整数才行，否则没有物理意义。 Pooling 层同理。 而 FC 层的输出形状总是满足整数，其唯一的要求就是整个训练过程中 FC 层的输入得是定长的（全连接层无法改变网络层尺寸）。 如果你的图像不是正方形。那么在制作数据时，可以缩放到统一大小（非正方形），再使用非正方形的 kernel_size 来使得卷积层的输出依然是整数。 总之，撇开网络结果设定的好坏不谈，其本质上就是在做算术应用题：如何使得各层的输出是整数。 3.2 –&gt; 卷积核的个数 我们知道，卷积核的深度与输出卷积层的深度相同，而卷积核的个数就等于输出卷积层的深度。 那么产生一个问题：如何更加合理的设置卷积核数（卷积核深度）？ 通常情况下，靠近输入的卷积层，譬如第一层卷积层，会找出一些共性的特征，如手写数字识别中第一层我们设定卷积核个数为 5 个，一般是找出诸如”横线”、“竖线”、“斜线”等共性特征，我们称之为 basic feature； 然后经过 max pooling 后，一般卷积层尺寸会缩小，可以看作在不影响其质量的情况下进行压缩； 在第二层卷积层，设定卷积核个数为 20 个，可以找出一些相对复杂的特征，如“横折”、“左半圆”、“右半圆”等特征；并且越往后，卷积核设定的数目越多，越能体现 label 的特征就越细致，就越容易分类（一般是成倍增加，不过具体论文会根据实验情况具体设置）。 3.3 –&gt; Feature Map 如何理解 Feature Map？ 我们知道，在 CNN 中的每个卷积层，数据都是以三维数据形式存在。我们可以把它看作是“深度”个二维图片叠在一起构成，其中每个二维图片就是一个 Feature Map。 –&gt; Feature Map 在卷积神经网络中的生成 对于输入层：如果是灰度图片，那就只有一个feature map；而对于彩色图片一般就是 3 个feature map（三通道）； 对于其他层：层与层之间会有若干个卷积核，上一层所有 feature map 会跟每个卷积核做卷积，都会产生下一层的一个 feature map。也即是说，有几个卷积核就会生成几个 feature map（每个 feature map 都是提取到的典型特征）。 篇幅原因，更详细的关于卷积神经网络中卷积层以及池化层的理解可以见：一文理解卷积神经网络结构 。 如果，你已确实了解了卷积神经网络结构，Part 3 和 Part 4 内容参见 TensorFlow 系列之经典卷积神经网络模型（LeNet-5 &amp;&amp; Inception-v3） 。 2.3 Reference Linkshttps://www.cnblogs.com/charlotte77/p/7759802.html]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 之模型持久化]]></title>
    <url>%2FTensorFlow%2FTensorFlow-%E4%B9%8B%E6%A8%A1%E5%9E%8B%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 开篇： 前面章节（TensorFlow 之 MNIST 手写体数字图像识别问题）中给出的神经网络模型优化样例代码在训练完成后就直接退出了，并没有将学得的模型（训练成果物）保存下来以便下次使用（测试、预测），这样模型的训练是没有实际意义的。为了让学得模型可以复用，我们需要将训练的神经网络模型持久化。 这一章节我们总共安排了 两个部分 来进行学习： 第一部分：针对 TensorFlow 之 MNIST 手写体数字图像识别问题 章节中给出的 MNIST 手写体数字图像识别 TensorFlow 程序实现，介绍如何将一个神经网络模型持久化，使得之后可以直接使用（复用）训练好的模型； 第二部分：综合之前学过的所有 TensorFlow 知识，给出一个目前掌握程度下使用 TensorFlow 训练神经网络的最佳实践，这是必要的。 1. TensorFlow 模型持久化这一部分内容我们分为两个小节来介绍：第一小节将介绍如何通过 TensorFlow 程序来持久化一个训练好的模型，并从持久化之后的模型文件中还原被保存的模型；第二小节将介绍 TensorFlow 持久化的工作原理和持久化之后文件中的数据格式。 1.1 模型持久化实现TensorFlow 中提供了两种一般的持久化模型方法，分别是：CKPT（checkpoint） 模型以及 PB 模型。 1.1.1 持久化 CKPT（checkpoint）模型TensorFlow 提供了一个非常简单的 API 来保存和还原神经网络模型的方法：使用 tf.train.Saver() 对象。采用这种方法，下面将分别各给出一个简单的模型保存和还原的样例: 1-1 模型保存123456789101112131415161718import tensorflow as tf# 为了更加关注 tf.train.Saver 的使用：# 这里简化神经网络模型，使用变量和来代表其它复杂神经网络模型的训练过程：v1 = tf.Variable(tf.constant(1.0, shape=[1]), name='v1')v2 = tf.Variable(tf.constant(2.0, shape=[1]), name='v2')result = v1 + v2# 定义模型持久化以及日志存储路径：Model_And_Logs = './Output/'checkpoints_path = Model_And_Logs + 'Checkpoints'init_op = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess: sess.run(init_op) saver.save(sess, checkpoints_path + '/model.ckpt') 上面的代码实现了持久化一个简单的 TensorFlow 模型的方法，我们通过 tf.train.Saver() 函数将 TensorFlow 模型信息保存到了 ./Output/Checkpoints/model.ckpt 文件中。虽然上面程序只指定了一个文件路径，但是你可以在存储路径下分别看到四个文件： checkpoint model.ckpt.data-00000-of-00001 model.ckpt.index model.ckpt.meta –&gt; （1）模型持久化文件说明： 上述我们知道 tf.train.Saver() 函数将 TensorFlow 模型信息保存到了 model.ckpt 持久化系列文件中，那么这些模型持久化文件是如何存储模型信息的？我们知道：Tensorflow 神经网络模型主要包含网络结构或者计算图（Graph）和我们已经训练好的网络参数的值。因此，Tensorflow 模型持久化结果应当两个主要的部分： [1] 元图（Meta Graph）: 保存完整的 Tensorflow graph 结构。比如说，所有的 variables, operations, collections 等等。持久化图的文件后缀: .meta 。 [2] 检查点文件（checkpoint file）： 保存了所有权重（weights），偏置（biases），梯度（gradients）和所有其他保存的变量（variables）的二进制文件。它由两个文件构成： mymodel.data ：保存了模型的所有变量的值。 mymodel.index ：是一个 string-string table，table 的 key 值为 tensor 名，value 为 BundleEntryProto。 注意： Tensorflow 0.11 版本之前版本，检查点文件（checkpoint file）是一个后缀名为 .ckpt 的文件。其模型持久化结果文件如下所示： checkpoint model.ckpt model.ckpt.meta 除了上述两个文件之外，我们还发现，Tensorflow 模型持久化结果中还有一个叫做 checkpoint 的文件: [3] 检查点记录文件（checkpoint）: 这个文件保存了目录下所有已存储的模型文件列表和当前状态最新保存的模型文件。checkpoint 文件有 model_checkpoint_path 和 all_model_checkpoint_paths 两个属性： model_checkpoint_path：保存了最新的 TensorFlow 模型文件的文件名； all_model_checkpoint_paths：保存未被删除的所有 TensorFlow 模型文件的文件名。 –&gt; （2）tf.train.Saver() 函数实际应用： [1] 根据训练时间保存最新的几个模型： # 如果你只想要保存最新的4个模型，并且想要在训练的时候每2个小时保存一个模型： # 使用 max_to_keep 和 keep_checkpoint_every_n_hours： # saves a model every 2 hours and maximum 4 latest models are saved. saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2) [2] 根据迭代次数保存模型： # 为了防止程序异常退出而导致模型没有成功持久化，可以采取边训练边保存模型： # 但实际不会迭代一次后就保存模型（资源开销很大），我们可以设置每多少次迭代存储一次模型: if iter % 1000 == 0: saver.save(sess, &apos;path/mymodel.ckpt&apos;,global_step=iter) 这种方法将在模型名字的后面追加上类似于 -1000 的迭代版本区分，下面的文件将会被创建： mymodel.ckpt-1000.index mymodel.ckpt-1000.meta mymodel.ckpt-1000.data-00000-of-00001 checkpoint [3] write_meta_graph 参数 # 由于网络的图（graph）在训练的时候是不会改变的，因此，我们没有必要每次都重复保存.meta文件，可以使用如下方法： saver.save(sess, &apos;path/mymodel.ckpt&apos;,global_step=step, write_meta_graph=False) [4] 一个实际使用策略 我们再来谈一点实际中的应用：在实际优化模型时，我们一般不会只保存最新的模型（最新的模型不一定是验证精度最高的模型），故通常会保存多个模型结果。但是保存模型多过是消耗磁盘存储的，我们更加倾向的是存储固定个数的精确度最高的几个模型。需要加一个变量以及条件语句，如下实现： saver=tf.train.Saver(max_to_keep=3) max_acc=0 for iter in range(100): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_op, feed_dict={x: batch_xs, y_: batch_ys}) val_loss, val_acc=sess.run([loss,acc], feed_dict={x: mnist.test.images, y_: mnist.test.labels}) print(&apos;epoch:%d, val_loss:%f, val_acc:%f&apos;%(i,val_loss,val_acc)) # 存储正确率排名最高的3个模型： if val_acc &gt; max_acc: max_acc=val_acc saver.save(sess,&apos;ckpt/mmodel.ckpt&apos;,global_step=iter+1) sess.close() 1-2 加载持久化模型这里，首先我们给出一个加载 1-1 模型保存 中 TensorFlow 模型的样例： import tensorflow as tf # 重新定义持久化模型中神经网络结构： v1 = tf.Variable(tf.constant(1.0, shape=[1]), name=&apos;v1&apos;) v2 = tf.Variable(tf.constant(2.0, shape=[1]), name=&apos;v2&apos;) result = v1 + v2 saver = tf.train.Saver() with tf.Session() as sess: # 加载已经保存好的模型，并且通过保存的模型中变量的值来计算上述神经网络值： saver.restore(sess, &apos;./Output/Checkpoints/model.ckpt&apos;) print (sess.run(result)) 样例输出如下： INFO:tensorflow:Restoring parameters from ./Output/Checkpoints/model.ckpt [3.] 前面我们已经知道，在神经网络训练过程中我们可以保存不同迭代次数以及不同时刻下的模型，而 checkpoint 检查点记录文件中维护了所有已存储的模型文件列表（all_model_checkpoint_paths）和最新的保存的模型文件（model_checkpoint_path）。那么，我们如何加载存储的不同模型： import tensorflow as tf # 重新定义持久化模型中神经网络结构： v1 = tf.Variable(tf.constant(1.0, shape=[1]), name=&apos;v1&apos;) v2 = tf.Variable(tf.constant(2.0, shape=[1]), name=&apos;v2&apos;) result = v1 + v2 saver=tf.train.Saver() with tf.Session() as sess: # 获取所有存储的模型信息 ckpt ： ckpt = tf.train.get_checkpoint_state(&apos;./Output/Checkpoints&apos;) # 1. 加载最新模型： if ckpt and ckpt.model_checkpoint_path: saver.restore(sess, ckpt.model_checkpoint_path) else: print (&apos;Please Checking your all_model_checkpoint_paths list !&apos;) # 2. 遍历加载所有已保存模型： if ckpt and ckpt.all_model_checkpoint_paths: for model_path in ckpt.all_model_checkpoint_paths: saver.restore(sess, model_path) –&gt; （1） tf.train.latest_checkpoint()：函数 # 使用 tf.train.latest_checkpoint() 函数也可以加载最新模型： checkpoint_path = tf.train.latest_checkpoint(./Output/Checkpoints&apos;) saver.restore(sess, checkpoint_path) 1-3 直接加载持久化后的图（Graph）在 1-2 加载持久化模型 加载模型的过程中，我们需要重新定义 Tensorflow 计算图上的所有运算（唯一不同的区别：加载模型时没有明确调用变量初始化过程，而是将变量的值通过已经保存的模型直接加载进来）。这样我们需要加载模型时必须根据原有神经网络重新定义一遍，这是很繁琐的。 如果你仔细想的话，持久化模型的时候我们已经将模型的 Graph 保存在了 .meta 文件中， TensorFlow 中还提供了 tf.train.import_meta_graph() 函数用于直接加载持久化的计算图。如下： saver = tf.train.import_meta_graph(&apos;mymodel.ckpt-1000.meta&apos;) 但是注意，这仅仅是将已经定义的网络导入到当前的Graph中，但是我们还是需要加载网络中关注的参数值。TensorFlow 支持 graph.get_tensor_by_name() 函数来获取关注的张量（变量、运算等）。如下样例： import tensorflow as tf # 直接加载持久化的图 saver = tf.train.import_meta_graph(&apos;./Output/Checkpoints/model.ckpt.meta&apos;) with tf.Session() as sess: saver.restore(sess,&apos;./Output/Checkpoints/model.ckpt&apos;) # 需要通过张量的名称来获取关注的张量： print(sess.run(tf.get_default_graph().get_tensor_by_name(&apos;add:0&apos;))) 样例结果输出如下: INFO:tensorflow:Restoring parameters from ./Output/Checkpoints/model.ckpt [3.] 注意：我们知道定义神经网络时我们会使用 placeholder 占位符来准备输入，当网络被保存时，给 placeholder feed_dict 的值是不会被保存。预测新数据（或补充训练）时需要使用 feed_dict 来将新的数据传递给网络。 结论： 现在我们已经知道，如果想要通过直接加载持久化图的方法恢复存储的网络，我们不仅需要恢复图（Graph）和权重，而且也需要准备一个新的 feed_dict，将新的训练数据喂给网络（迁移学习）。同时我们可以通过使用 graph.get_tensor_by_name() 方法来获得已经保存的变量、操作（operations）以及 placeholder variables。 =================================================================== 至此，如 [1-2 加载持久化模型]、[1-3 直接加载持久化后的图（Graph）] 小节， 我们已经了解了两种加载学得模型的方法。那么实际应用时，我们如何做出选择呢？！这里谈一些我自己的想法：如果你对于使用的模型十分了解的话（熟知网络结构、操作节点名称等），你可以使用第二种方法。无需重新定义网络结构，精简代码产量；如果你对学得模型使用不太熟练，不能很好指出各操作节点名称，推荐第一种方法。此时，你需要重构网络结构，但可以直接在代码中使用定义的各种操作节点。 =================================================================== 1-4 方便使用滑动平均的变量重命名tf.train.Saver() 类默认情况下保存和加载 TensorFlow 计算图上定义的全部变量。还支持保存和加载模型的部分变量或在保存和加载模型时给变量重命名。 （1）保存或加载部分变量： 在声明 tf.train.Saver 类时，可以提供其一个列表来指定需要保存或者加载的变量。但注意，不常用！模型复杂时，你不可能把需要的那部分全部罗列出来（了解即可）。 比如，我们之前已经训练好了一个五层神经网络模型，但现在想尝试一个六层的神经网络，那我们就可以将前面五层神经网络中的参数直接加载到新的模型，而仅仅将最后一层神经网络重新训练（需要我们将前五层的所有变量以列表的形式提供，实际中这是不太现实的）。 例如加载模型时，使用 tf.train.Saver([v1]) 命令来构建 tf.train.Saver 类。此时只有变量 v1 被加载进来，下面程序会报错。代码如下： import tensorflow as tf # 重新定义持久化模型中神经网络结构： v1 = tf.Variable(tf.constant(1.0, shape=[1]), name=&apos;v1&apos;) v2 = tf.Variable(tf.constant(2.0, shape=[1]), name=&apos;v2&apos;) result = v1 + v2 saver = tf.train.Saver([v1]) with tf.Session() as sess: # 加载已经保存好的模型，并且通过保存的模型中变量的值来计算上述神经网络值： saver.restore(sess, &apos;./Output/Checkpoints/model.ckpt&apos;) # 报错：tensorflow.python.framework.error.FailedPreconditionError:Attempting to use uninitialized value v2 and value add:0 # 这是由于 v2 ，add:0没有加载进来（没有被初始化）。 print (sess.run(result)) （2）变量重命名： 除了可以选取需要被保存或加载的变量， tf.train.Saver 类也支持通过字典（dictionary）在保存和加载模型时给模型的变量重命名。如下代码展示变量重命名是如何被使用的： import tensorflow as tf # 这里声明的变量名称和已经保存的模型中变量的名称不同。 v1 = tf.Variable(tf.constant(1.0, shape=[1]), name=&apos;new-v1&apos;) v2 = tf.Variable(tf.constant(2.0, shape=[1]), name=&apos;new-v2&apos;) # 直接使用tf.train.Saver()加载模型会提示变量找不到的错误# 需要使用一个字典来重命名变量。这个字典指定 # 原来名称为v1的变量现在加载在变量v1中(&apos;new-v1&apos;),名称为v2的变量加载到变量v2中(&apos;new-v2&apos;) saver=tf.train.Saver({&apos;v1&apos;:v1,&apos;v2&apos;:v2}) with tf.Session() as sess: saver.restore(sess, &apos;./Output/Checkpoints/model.ckpt&apos;) print (sess.run(v1) , sess.run(v2)) 样例输出结果如下： INFO:tensorflow:Restoring parameters from ./Output/Checkpoints/model.ckpt [1.] [2.] [2.1] 变量重命名的作用 在保存或加载模型时给模型中变量重命名主要目的之一是方便使用变量的滑动平均值。 使用变量的滑动平均值可以让神经网络模型更加健壮。在 TensorFlow 中，每一个变量的滑动平均值是通过影子变量维护的，获取变量的滑动平均值实际上就是获取这个影子变量的取值。如果在保存或加载模型时直接将影子变量映射到变量自身，那么在使用训练好的模型时就不需要再调用函数来获取变量的滑动平均值了。这样方便了滑动平均模型的使用。 以下代码先给出了一个保存滑动平均模型的样例： import tensorflow as tf v1 = tf.Variable(0, dtype=tf.float32, name=&apos;v1&apos;) v2 = tf.Variable(1, dtype=tf.float32, name=&apos;v2&apos;) # 没有声明滑动平均模型时，有两个变量 v1\v2，所以下面语句会输出&apos;v1:0\v2:0&apos; for variables in tf.global_variables(): print(variables.name) ema=tf.train.ExponentialMovingAverage(0.99) maintain_averages_op=ema.apply(tf.global_variables()) # 在声明滑动平均模型后，对于每一个变量 TensorFlow 会自动生成一个影子变量 # 下面语句会输出：&apos;v1:0&apos;和&apos;v1/ExponentialMovingAverage:0&apos;\&apos;v2:0&apos;和&apos;v2/ExponentialMovingAverage:0&apos; for variables in tf.global_variables(): print(variables.name) saver=tf.train.Saver() with tf.Session() as sess: init_op=tf.global_variables_initializer() sess.run(init_op) sess.run(tf.assign(v1,10)) sess.run(tf.assign(v2,5)) sess.run(maintain_averages_op) # 保存时，tensorflow会将&apos;v:0&apos;和&apos;v/ExponentialMovingAverage:0&apos;两个变量都保存下来 saver.save(sess,&apos;./Output/Checkpoints_avg/model.ckpt&apos;) print(sess.run([v1,ema.average(v1)])) # 输出：[10.0, 0.099999905] print(sess.run([v2,ema.average(v2)])) # 输出：[10.0, 0.099999905] 样例输出结果如下： v1:0 v2:0 v1:0 v2:0 v1/ExponentialMovingAverage:0 v2/ExponentialMovingAverage:0 [10.0, 0.099999905] [5.0, 1.04] 下面我们来加载上面模型并对滑动平均变量重命名： import tensorflow as tf v1 = tf.Variable(0, dtype=tf.float32, name=&apos;v1&apos;) saver = tf.train.Saver({&apos;v1/ExponentialMovingAverage&apos;:v1}) with tf.Session() as sess: saver.restore(sess,&apos;./Output/Checkpoints_avg/model.ckpt&apos;) print (sess.run(v1)) 样例结果输出如下: INFO:tensorflow:Restoring parameters from ./Output/Checkpoints_avg/model.ckpt 0.099999905 –&gt; 正常情况下，你会有一个疑问？ 就如保存或加载部分变量中一样，给出所有关注的滑动平均变量重命名字典以提供给 tf.train.Saver() 是不现实的。为了方便加载时重命名滑动平均变量，tf.train.ExpoentialMovingAverage 类提供了 variables_to_restore 函数来生成 tf.train.Saver 类所需要的变量重命名字典。示例代码如下： import tensorflow as tf v1 = tf.Variable(0, dtype=tf.float32, name=&apos;v1&apos;) v2 = tf.Variable(1, dtype=tf.float32, name=&apos;v2&apos;) # 没有声明滑动平均模型时，有两个变量 v1\v2，所以下面语句会输出&apos;v1:0\v2:0&apos; for variables in tf.global_variables(): print(variables.name) ema = tf.train.ExponentialMovingAverage(0.99) # 自动维护重命名字典： print (ema.variables_to_restore()) saver = tf.train.Saver(ema.variables_to_restore()) with tf.Session() as sess: saver.restore(sess,&apos;./Output/Checkpoints_avg/model.ckpt&apos;) print (sess.run(v1)) print (sess.run(v2)) 样例结果输出如下： v1:0 v2:0 {&apos;v2/ExponentialMovingAverage&apos;: &lt;tf.Variable &apos;v2:0&apos; shape=() dtype=float32_ref&gt;, &apos;v1/ExponentialMovingAverage&apos;: &lt;tf.Variable &apos;v1:0&apos; shape=() dtype=float32_ref&gt;} INFO:tensorflow:Restoring parameters from ./Output/Checkpoints_avg/model.ckpt 0.099999905 1.04 1.1.2 持久化 PB 模型我们知道，使用 tf.train.Saver 会保存运行 TensorFlow 程序所需要的全部信息，然而有时我们并不需要某些信息。例如：在测试或者离线预测时，只需要知道如何从神经网络的输入层经过前向传播过程计算得到输出层即可（不需要变量初始化、模型保存、损失函数、反向传播算法等），类似的还有我们即将学习的迁移学习，你会发现我们只需要原神经网络模型的一部分计算图结构。而且，将变量和计算图结构分成不同的文件存储有时也不方便。 TensorFlow 提供了 convert_variables_to_constants() 函数。通过这个函数可以将计算图中的变量以及其取值通过常量的形式保存。这样使得整个 TensorFlow 计算图可以统一存放在一个文件中。示例代码如下： import tensorflow as tf from tensorflow.python.framework import graph_util v1 = tf.Variable(tf.constant(1.0, shape=[1]), name=&apos;v1&apos;) v2 = tf.Variable(tf.constant(2.0, shape=[1]), name=&apos;v2&apos;) result = v1 + v2 init_op = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init_op) # 导出当前计算图的 GraphDef 部分，这需要计算图中的这一部分即可完成从输入层到输出层的计算过程 graph_def = tf.get_default_graph().as_graph_def() # 将图中的变量及其取值转化为常量，同时将图中不必要的结点去掉。 # 在下面一行代码中，最后一个参数[&apos;add&apos;]给出了需要保存的节点名称（需要计算到该节点，即输出节点）。 # add 节点是上面定义的两个变量相加操作。注意，&apos;add:0&apos;表示某个计算节点的第一个输出，是一个张量名。 output_graph_def = graph_util.convert_variables_to_constants(sess, graph_def, [&apos;add&apos;]) # 将导出的模型存入文件 with tf.gfile.GFile(&apos;./Output/combined_pb/model.pb&apos;,&apos;wb&apos;) as f: f.write(output_graph_def.SerializeToString()) # 序列化输出 # for op in tf.get_default_graph().get_operations(): 打印模型节点信息 # print (op.name) 样例结果输出如下： INFO:tensorflow:Froze 2 variables. INFO:tensorflow:Converted 2 variables to const ops. 上面我们将神经网络学得模型持久化为 pb 文件（计算图以及常量固化为常量）。通过下面的程序可以直接计算模型中定义的加法运算的结果。当只需要得到计算图中某个节点的的取值时，TensorFlow 提供了一个更加方便的方法: tf.import_graph_def() 。后面基于此方法来使用训练好的模型完成迁移学习。 import tensorflow as tf with tf.Session() as sess: model_filename = &apos;./Output/combined_pb/model.pb&apos; # 读取保存的 pb 模型文件，将文件解析成对应的 GraphDef Protocol Buffer 格式： with tf.gfile.FastGFile(model_filename,&apos;rb&apos;) as f: graph_def=tf.GraphDef() graph_def.ParseFromString(f.read()) # 将 graph_def 中保存的图加载到当前图中。return_elements=[&apos;add:0&apos;]给出了返回的 # 张量的名称。在保存时给出的是计算节点的名称，所以为 &apos;add&apos; 。 # 在加载时给出的是张量的名称，所以是 add:0 result = tf.import_graph_def(graph_def,return_elements=[&apos;add:0&apos;]) print (sess.run(result)) 样例结果输出如下： [array([3.], dtype=float32)] 1.1.3 CKPT 转换成 PB 格式 1.2 持久化原理以及数据格式化1.2.1 图的持久化文件详解通过前面章节介绍，我们知道 TensorFlow 是一个通过图的形式来表述计算的编程系统，TensorFlow 中所有的计算都会被表达成计算图上的节点。事实上，TensorFlow 通过元图（MetaGraph）来记录计算图中的信息，以及运行计算图中节点所需要的元数据。 而 TensorFlow 中元图是由 MetaGraphDef Protocol Buffer 来定义的，以下代码给出了 MetaGraphDef 类型的定义： message MetaGraphDef{ MeatInfoDef meta_info_def = 1; GraphDef graph_def = 2; SaverDef saver_def = 3; map&lt;string, CollectionDef&gt; collection_def = 4; map&lt;string, SignatureDef&gt; signature_def = 5; } 上面小节中我们在介绍 tf.train.Saver 类时知道模型的持久化结果会生成一个 .meta 图持久化文件。这里 MetaGraphDef 中的内容（元图信息）构成了 TensorFlow 持久化的一个文件，以 .meta 为后缀名，也就是我们上面所介绍的图持久化文件。也就是说：文件 model.ckpt.meta 中存储的就是元图的数据。 下面我们来结合上面 [1.x] 小节中变量相加样例的持久化结果，逐一介绍 MetaGraphDef 类型的每一个属性中存储的信息，存储于 model.ckpt.meta 文件。 （1）查看 MetaGraphDef Protocol Buffer 信息 由于持久化模型之后得到的是二进制文件，不方便查看。为了方便调试，Tensorflow提供了 export_meta_graph() 函数，这个函数支持以 JSON 格式导出 MetaGraphDef（计算图元图）,并将其存储在 model.ckpt.meta.json 文件。下面为实现的代码: import tensorflow as tf v1 = tf.Variable(tf.constant(1.0, shape=[1]), name =&quot;v1&quot;) v2 = tf.Variable(tf.constant(2.0, shape=[1]), name =&quot;v2&quot;) result1 = v1 + v2 saver = tf.train.Saver() #通过 export_meta_graph 函数导出 TensorFlow 的计算图的元图，并保存为json格式 saver.export_meta_graph(&quot;./Output/Checkpoints/model.ckpt.meta.json&quot;, as_text=True) 查看 model.ckpt.meta.json 文件内容如下（只给出一部分）： meta_info_def { stripped_op_list { op { name: &quot;Add&quot; input_arg { name: &quot;x&quot; type_attr: &quot;T&quot; } input_arg { name: &quot;y&quot; type_attr: &quot;T&quot; } output_arg { name: &quot;z&quot; type_attr: &quot;T&quot; } attr { name: &quot;T&quot; type: &quot;type&quot; allowed_values { list { type: DT_BFLOAT16 type: DT_HALF type: DT_FLOAT type: DT_DOUBLE type: DT_UINT8 type: DT_INT8 type: DT_INT16 type: DT_INT32 type: DT_INT64 type: DT_COMPLEX64 type: DT_COMPLEX128 type: DT_STRING } } } } ...... （2）MetaGraphDef Protocol Buffer 信息解读 上面我们已经获取到了 MetaGraphDef Protocol Buffer 中定义的信息文件，下文将结合 model.ckpt.meta.json 文件具体介绍 TensorFlow 元图中存储的信息： [2.1] meta_info_def 属性 meta_info_def 属性通过 MetaInfoDef 定义，它记录了 TensorFlow 计算图中的 元数据 以及 TensorFlow 程序中 所有使用到的运算方法 的信息。下面给出 MetaInfoDef 的定义： message MetaInfoDef{ string meta_graph_version = 1; OpList stripped_op_list = 2; google.protobuf.Any any_info = 3; repeated string tags = 4; } –&gt; meta_graph_version 属性和 tags 属性： TensorFlow 计算图中元数据包括了计算图的版本号（meta_graph_version 属性）以及用户指定的一些标签（tags 属性）。如果没有在 Saver 中特殊指定，那么这些属性都默认为空。 –&gt; stripped_op_list 属性： 在 model.ckpt.meta.json 中，meta_info_def 属性里只有 stripped_op_list 属性不为空。stripped_op_list 属性记录了 TensorFlow 计算图上使用到的所有运算方法信息。 注意 stripped_op_list 属性保存的是 TensorFlow 运算方法的信息，所有如果某一个运算在 TensorFlow 计算图中出现了很多次，那么在 stripped_op_list 中也只会出现一次。（比如：在 model.ckpt.meta.json 文件的 stripped_op_list 属性中只有一个 Variable 运算，但这个运算在程序中被使用了两次。） stripped_op_list 属性的类型是 OpList。OpList 类型是一个 OpDef 类型的列表。一下代码给出了 OpDef 类型的定义： message opDef{ string name = 1; repeated ArgDef input_arg = 2; repeated ArgDef output_arg =3; repeated AttrDef attr = 4; string summary = 5; string description = 6; OpDeprecation deprecation = 8; bool is_commutative = 18; bool is_aggregate = 16 bool is_stateful = 17; bool allows_uninitialized_input = 19; }; OpDef 类型中前四个属性定义了一个运算最核心的信息： OpDef 中的第一个属性 name 定义了运算的名称，这也是一个运算唯一的标识符。在 TensorFlow 计算图元图的其他属性，比如下面要介绍的 GraphDef 属性，将通过运算名称来引用不同的运算；OpDef 的第二个和第三个属性为：input_arg 和 output_arg，它们定义了运算的输入和输出。因为输入和输出可以有多个，所有这两个属性都是列表(repeated)；OpDef 的四个属性为：attr，它给出了运算参数的其他信息； 在 model.ckpt.meta.json 文件中总共定义了 7 个运算，下面将给出一个比较有代表性的运算来辅助说明 OpDef 的数据结构： op { name: &quot;Add&quot; input_arg{ name: &quot;x&quot; type_attr:&quot;T&quot; } input_arg{ name: &quot;y&quot; type_attr:&quot;T&quot; } output_arg{ name: &quot;z&quot; type_attr:&quot;T&quot; } attr{ name:&quot;T&quot; type:&quot;type&quot; allow_values{ list{ type:DT_HALF type:DT_FLOAT ... } } } ​​ } 如上所示：给出了名称为 &quot;Add&quot; 的运算。这个运算有两个输入 &quot;x&quot; , &quot;y&quot; 和一个输出 &quot;z&quot;。输入输出属性都指定了属性 type_attr，并且 type_attr 属性值为：T。在 OpDef 的 attr 属性中，必须要出现名称(name)为 T 的属性。在以上样例，attr 属性给出了运算输入、输出允许的参数类型（allowed_values）。 [2.2] graph_def 属性 graph_def 属性主要记录了计算图中的节点信息。TensorFlow 计算图中的一个节点对应 TensorFlow 中的一个运算。因为 meta_info_def 中已经包含所有运算的具体信息，所以 graph_def 属性只关注节点（运算）的连接结构。 graph_def 属性是通过 GraphDef Protocol Buffer 定义的。GraphDef 主要包含了一个 NodeDef 类型的列表。以下代码给出 GraphDef 和 NodeDef 类型中包含的信息： message GraphDef{ repeated NodeDef node = 1; VersionDef versions = 4; }; message NodeDef{ string name = 1; string op = 2; repeated string input = 3; string device = 4; map&lt;string, AttrValue&gt; attr = 5; }; GraphDef 类型中的 versions 属性比较简单，它主要存储了 TensorFlow 的版本号。GraphDef 的主要信息都存在 node 属性中，它记录了 TensorFlow 计算图上所有的节点信息。 –&gt; node 属性 GraphDef 中的 node 是由 NodeDef 类型定义的。NodeDef 类型中有一个名称属性，它是一个节点的唯一标识符。在 TensorFlow 程序中可以通过节点的名称来获取相应的节点。 NodeDef 类型中的 op 属性给出了该节点使用的 TensorFlow 运算方法的名称，通过这个名称可以在 TensorFlow 计算图元图的 meta_info_def 属性中找到该运算的具体信息。 NodeDef 类型中的 input 属性是一个字符串列表，它定义了运算的输入。input 属性中每个字符串的取值格式为 node:src_output（是不很熟悉~），表示 node 节点的第 src_outpu 个输出结果。 NodeDef 类型中的 device 属性指定了处理这个运算的设备。运行 TensorFlow 运算的设备可以是本地机器的 CPU 或者 GPU，也可以是另一台远程机器的 CPU 或者 GPU。后面我们会介绍如何给 TensorFlow 运算指定运行设备。当 device 属性为空时，TensorFlow 在运行时会自动选择一个最合适的设备来运行这个运算。 NodeDef 类型中的 attr 属性指定了和当前运算相关的配置信息。 下面列举了 model.ckpt.meta.json 文件中的一些计算节点来更加详细地介绍 graph_def 属性： graph def { node { name: &quot;v1&quot; op: &quot;Variable&quot; attr { key:&quot;_output_shapes&quot; value { list{ shape { dim { size: 1 } } } } } } attr { key :&quot;dtype&quot; value { type: DT_FLOAT } } ... } node { name :&quot;add&quot; op :&quot;Add&quot; input :&quot;v1/read&quot; input: &quot;v2/read&quot; ... } node { name: &quot;save/control_dependency&quot; op:&quot;Identity&quot; ... } versions { producer :12 } } 上面给出了 model.ckpt.meta.json 文件中 graph_def 属性中 比较有代表性的几个节点： 第一个节点给出的是变量定义的运算。在 TensorFlow 中变量定义也是一个运算，合格运算的名称为: v1( name:&quot;v1&quot; )，运算方法的名称为 Variable( op:&quot;Variable&quot; )。但定义变量的运算方法只用到了一个，于是在 MetaInfoDef 类型的 stripped_op_list 属性中只有一个名称为 Variable 的运算方法。除了指定计算图中节点的名称和运算方法， NodeDef 类型中还定义了运算相关的属性。在节点 v1中，attr 属性指定了这个变量的维度以及类型。 第二个节点是代表加法运算的节点。它指定了两个输入：一个为 v1/read ，另一个为 v2/read。其中，v1/read 代表的节点可以读取变量 v1 的值。因为 v1 的值是节点 v1/read 的第一个输出，所以后面的 :0 就可以省略了。同理，v2/read 也代表了变量 v2 的值。 以上样例给出的最后一个节点名称为 save/contro_dependency，该节点是系统完成 TensorFlow 模型持久化过程中自动生成的一个运算。 在样例文件最后的属性 versions 给出了生成 model.ckpt.meta.json 文件时使用的 TensorFlow 版本。 [2.3] saver_def 属性 saver_def 属性中记录了持久化模型时需要用到的一些参数，比如保存到文件的文件名、保存操作和加载操作的名称以及保存频率、清理历史纪录等。saver_def 属性的类型为：SaverDef,其定义如下: message SaverDef { string filename_tensor_name = 1; string save_tensor_name = 2; string restore_op_name = 3; int32 max_to_keep = 4; bool sharded = 5; float keep_checkpoint_every_n_hours = 6; enum CheckpointFormatVersion { LEGACY = 0; V1 = 1; V2 = 2; } CheckpointFormatVersion version = 7; } 下面给出 model.ckpt.meta.json 文件中 saver_def 属性的内容: saver_def { filename_tensor_name: &quot;save_1/Const:0&quot; save_tensor_name: &quot;save_1/control_dependency:0&quot; restore_op_name: &quot;save_1/restore_all&quot; max_to_keep: 5 keep_checkpoint_every_n_hours: 10000.0 version: V2 } filename_tensor_name 属性给出了保存文件名的张量名称，这个张量就是节点 save/Const 的第一个输出； save_tensor_name 属性给出了持久化 TensorFlow 模型的运算所对应的节点名称。从上面的文件可以看出，这个节点就是在 graph_def 属性中给出的 save/control_dependency 节点； 和持久化 TensorFlow 模型对应的是加载 TensorFlow 模型的运算，这个运算的名称由 restore_op_name 属性指定； max_to_keep 属性和 keep_checkpoint_every_n_hours 属性设定了 tf.train.Saver 类清理之前保存的模型的策略。比如 max_to_keep=5 时，在第六次调用 saver.save() 函数时，第一次保存的模型就会被自动删除。通过设置 keep_checkpoint_every_n_hours=5 时，表示每 5 个小时可以在 max_to_keep 的基础上多保存一个模型。 [2.4] collection_def 属性 我们知道，在 TensorFlow 的计算图(tf.Graph)中可以维护不同的集合，而维护这些集合的底层实现就是通过 collection_def 这个属性。collection_def 属性是一个集合名称到集合内容的映射，其中集合的名称为字符串；而集合内容为 CollectionDef Protocol Buffer。以下代码先给出 CollectionDef 类型的定义： message CollectionDef { message Nodelist { repeated string value = 1; } message BytesList { repeated bytes value = 1 ; } message Int64List { repeated int64 value = 1[packed = true]; } message FloatList { repeated float value = 1[packed = true] ; } message AnyList { repeated google.protobuf.Any value= 1; } oneof kind { NodeList node_list = 1; BytesList bytes_lista = 2; Int64List int64_list = 3; Floatlist float_list = 4; AnyList any_list = 5; } } 通过上面的定义可以看出，TensorFlow 计算图上的集合主要可以维护 4 种不同的集合： Nodelist 用于维护计算图上节点的集合； BytesList 用于维护字符串或者系列化之后的 Protocol Buffer 的集合。比如张量是通过 Protocol Buffer 表示的，而张量的集合是通过 BytesList 维护的； Int64List 用于维护整数集合； Floatlist 用于维护实数集合； 下面给出了 model.ckpt.meta.json 文件中 collecion_def 属性的内容： collection_def { key: &quot;trainable_variables&quot; value { bytes_list { value: &quot;\n\004v1:0\022\tv1/Assign\032\tv1/read:02\007Const:08\001&quot; value: &quot;\n\004v2:0\022\tv2/Assign\032\tv2/read:02\tConst_1:08\001&quot; value: &quot;\n\006v1_1:0\022\013v1_1/Assign\032\013v1_1/read:02\tConst_2:08\001&quot; value: &quot;\n\006v2_1:0\022\013v2_1/Assign\032\013v2_1/read:02\tConst_3:08\001&quot; } } } collection_def { key: &quot;variables&quot; value { bytes_list { value: &quot;\n\004v1:0\022\tv1/Assign\032\tv1/read:02\007Const:08\001&quot; value: &quot;\n\004v2:0\022\tv2/Assign\032\tv2/read:02\tConst_1:08\001&quot; value: &quot;\n\006v1_1:0\022\013v1_1/Assign\032\013v1_1/read:02\tConst_2:08\001&quot; value: &quot;\n\006v2_1:0\022\013v2_1/Assign\032\013v2_1/read:02\tConst_3:08\001&quot; } } } 可以看到，样例程序维护了两个集合。一个是所有变量的集合，这个集合的名称为：&quot;variables&quot;。另外一个是可训练变量的集合，名称为：&quot;trainable_variables&quot;。在样例程序中，这两个集合中的元素是一样的，都是变量 v1、v2 。它们都是系统自动维护的。 1.2.2 变量文件和 checkpoint 文件介绍（1）.data 和 .index 文件（或 .ckpt 文件） 通过读 MetaGraphDef 类型中主要属性的讲解。上面我们已经理解了 TensorFlow 模型持久化得到的 .meta 文件内容。 除了持久化 TensorFlow 计算图的结构，持久化 TensorFlow 中变量的取值也是非常重要的一部分，由 tf.Saver() 得到的 model.ckpt.data 和 model.ckpt.index 文件保存了所有变量的取值，我们可以大致理解为：一个 （key,value） 的变量取值列表。 （2）checkpoint 文件 checkpoint 文件是 tf.train.Saver 类自动生成并且自动维护的。在 checkpoint 文件中维护由一个 tf.train.Saver 类持久化的所有 TensorFlow 模型文件名。当某个保存的 TensorFlow 模型文件被删除时，这个模型所对应的文件名也会从 checkpoint 文件列表中删除。 至此，神经网络模型持久化介绍终于完成（撒花…）！！！ 2. TensorFlow 最佳实践样例综合之前学过的所有 TensorFlow 知识，给出一个目前掌握程度下使用 TensorFlow 训练神经网络的最佳实践，这是必要的。 –&gt; 前情回顾： 在上一章节 TensorFlow-之-MNIST-手写体数字图像识别问题 中我们结合变量命名空间，给出了一个使用了指数衰减学习率、权重正则化以及变量滑动平均的多层全连接神经网络模型来解决 MNIST 手写数字识别问题的“近最优”实现（我不会告诉你还差那两步就达到目前最优了）。 –&gt; 引发问题： 模型优化样例代码在训练完成后就直接退出了，并没有将学得的模型（训练成果物）保存下来以便下次使用（测试、预测），训练了一个多月，成果 Game Over 了~ 另外，模型组件使用还是不太灵活。思考，假如我们能够将训练和测试分为两个单独的程序模块，这样，训练神经网络的程序可以持续输出训练好的模型；而测试程序可以每隔一段时间检查一次最新模型的准确率，如果模型效果更好，则将这个最新的模型提供给产品使用。这不是更加灵活么？ 此外，由于神经网络的前向传播过程在训练以及测试中都要使用到，故可以将前向传播的过程抽象成一个单独的库函数，使用上更加灵活。 2.1 Get Start出于以上想法，重构之后的程序分被拆分为两个模块（程序）： MNIST_FCNN_FP_Model.py：定义了神经网络中的参数以及前向传播过程； MNIST_FCNN_Train_And_Evaluate.py：定义了神经网络的训练以及测试模块。 1 – &gt; MNIST_FCNN_FP_Model.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# -*- coding:utf-8 -*-''' TensorFlow_Hello : MNIST 手写数字图片识别示例 Model : Full Connection Code For : Definition of forward Propagation process in Neural Networks'''import tensorflow as tf###################### Build Forward Propagation Prcess ########################## 1. MNIST 数据集相关的常数 ##### 图像像素（28px * 28px像素矩阵转化），输入层的节点数；INPUT_NODE = 784# 输出节点数，等于类别数目。分别对应 0~9 10个数字类别；OUTPUT_NODE = 10#### 2. 配置神经网络中的参数 ##### 定义神经网络模型中每一层网络中的节点个数：LAYERS_DIMENSION = [INPUT_NODE, 200, OUTPUT_NODE]# 获取神经网络的层数NUM_LAYERS = len(LAYERS_DIMENSION)#### 3. 通过tf.get_variable函数创建变量：在训练 ANN 时会创建这些变量；在测试时会通过保存的模型加载这些变量 ####'''## get_weight_variable() :Function ：1] 因为可以在变量加载时将滑动平均变量重命名，故在训练过程中可以通过相同的名字使用变量自身, 而在测试时使用变量的滑动平均值 2] 根据 regularizer 会将变量的正则化损失加入损失集合'''def get_weight_variable(shape, regularizer_class): weights = tf.get_variable("weights", shape, initializer= tf.truncated_normal_initializer(stddev=0.1)) # 当给出正则化生成函数时，将当前参数的正则化损失加入到 losses 集合。losses是自定义集合，不在 Tensorflow 自动管理的集合列表 if regularizer_class != None: tf.add_to_collection('losses', regularizer_class(weights)) return weights#### 3. 构建神经网络前向传播过程 ####'''## inference(): ##Function ：1] Build Forward Propagation Prcess 2] Build Forward Propagation Prcess Using ExponentialMovingAverage'''def inference(input_tensor, regularizer_class, movingAvg_class): # 定义一个变量，用于维护前向传播时当前层的节点，开始的时候就是输入层： cur_layer = input_tensor # 获取当前层节点个数： in_dimension = LAYERS_DIMENSION[0] # 当没有提供滑动平均类时，直接使用参数当前的取值 if movingAvg_class == None: for layer_index in range(1, NUM_LAYERS): # 获取下一层节点的个数：，对应当前层输出节点 out_dimension = LAYERS_DIMENSION[layer_index] # 声明当前层神经网络的变量，并完成前向传播过程： with tf.variable_scope('layer' + str(layer_index), reuse=False): # 使用 tf.get_variable 或 tf.Variable 创建变量（无区别），因为在训练或测试中没有在同一程序多次调用该函数。 # 如果在同一程序多次调用，需要在第一次调用之后将 reuse 参数设置为 True。 weights = get_weight_variable([in_dimension, out_dimension], regularizer_class) biases = tf.get_variable("biases", [out_dimension], initializer=tf.constant_initializer(0.1)) if layer_index != NUM_LAYERS-1: cur_layer = tf.nn.relu(tf.matmul(cur_layer, weights) + biases) # 进入下一层之前，更新下一层节点的输入节点数 in_dimension = LAYERS_DIMENSION[layer_index] else: cur_layer = tf.matmul(cur_layer, weights) + biases return cur_layer # 使用 movingAvg_class.average 函数计算变量（参数）的滑动平均值 else: for layer_index in range(1, NUM_LAYERS): # 获取下一层节点的个数：，对应当前层输出节点 out_dimension = LAYERS_DIMENSION[layer_index] # 声明当前层神经网络的变量，并完成前向传播过程： with tf.variable_scope('layer' + str(layer_index), reuse=True): # 使用 tf.get_variable 或 tf.Variable 创建变量（无区别），因为在训练或测试中没有在同一程序多次调用该函数。 # 如果在同一程序多次调用，需要在第一次调用之后将 reuse 参数设置为 True。 weights = get_weight_variable([in_dimension, out_dimension], regularizer_class) biases = tf.get_variable("biases", [out_dimension], initializer=tf.constant_initializer(0.1)) if layer_index != NUM_LAYERS-1: cur_layer = tf.nn.relu(tf.matmul(cur_layer, movingAvg_class.average(weights)) + movingAvg_class.average(biases)) # 进入下一层之前，更新下一层节点的输入节点数 in_dimension = LAYERS_DIMENSION[layer_index] else: cur_layer = tf.matmul(cur_layer, movingAvg_class.average(weights)) + movingAvg_class.average(biases) return cur_layer 2 –&gt; MNIST_FCNN_Train_And_Evaluate.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292# -*- coding:utf-8 -*-''' ### 神经网络模型优化以及模型评估 TensorFlow_Hello : MNIST 手写数字图片识别示例 Model : Full Connection Code For : Neural network training and testing'''import osimport shutilimport sysimport timefrom datetime import timedeltaimport MNIST_FCNN_FP_Modelimport tensorflow as tf# 导入用于下载和读取 MNIST 数据集的 python 源文件：见 input_data.pyfrom tensorflow.examples.tutorials.mnist import input_data###################### FCNN Model Training and Test ########################### 1. 定义神经网络结构的相关参数 ###### 一个训练 batch 块中的训练数据个数。数值越小，训练过程越接近随机梯度下降；数值越大，训练越接近梯度下降。BATCH_SIZE = 100# 设置基础学习率LEARNING_RATE_BASE = 0.8# 设置学习率的衰减率LEARNING_RATE_DECAY = 0.99# 设置描述模型复杂度（结构风险）的正则化项在损失函数中的系数REGULARIZATION_RATE = 0.0001# 设置训练轮数TRAINING_STEPS = 100000# 设置滑动平均衰减率MOVING_AVERAGE_DECAY = 0.99# 设置模型的保存路径MODEL_SAVE_PATH = "Save_Model/"MODEL_NAME = "model.ckpt"# 指定 MNIST 数据集的下载和读取的路径：MNIST_DATA_PATH = "../../MNIST_data"# 每 10 秒加载一次最新模型，并在测试数据集上测试最新模型正确率EVAL_INTERVAL_SECS = 5##### 2. Referenced Function Define ####### [1]. get_time_dif：获取已使用时间def get_time_dif(start_time): end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=time_dif)##### 3. Model Training #####def train(mnist): # 定义神经网络输入输出 x = tf.placeholder(tf.float32, [None, MNIST_FCNN_FP_Model.INPUT_NODE], name='input_x') y_ = tf.placeholder(tf.float32, [None, MNIST_FCNN_FP_Model.OUTPUT_NODE], name='input_y') # 定义 L2 正则化损失函数（结构风险）： regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE) ### 1. avg_class = None ### # 使用 MNIST_FCNN_FP_Model.py 中定义的前向传播函数计算 FP 结果(不会使用参数的滑动平均值): y = MNIST_FCNN_FP_Model.inference(x, regularizer, None) ### 2. avg_class = variable_averages ### # 定义存储训练轮数的变量，无需计算滑动平均值。所以一般需要指定训练轮数变量为不可训练变量(trainable=False) global_step = tf.Variable(0, trainable=False) # 通过滑动平均衰减率和训练轮数变量，初始化滑动平均类。给定训练轮数可以加快训练早期变量的更新速度 variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) # 在所有（可训练，Except global_step）神经网络变量上使用滑动平均 # tf.trainable_variables 可返回 GraphKeys.TRAINABLE_VARIABLES 集合中元素，即未指定 trainable=False 的变量 variables_averages_op = variable_averages.apply(tf.trainable_variables()) # 计算使用了滑动平均之后的 FP 结果。滑动平均之后不会改变变量本身取值，会维护一个 shadow_variable 来记录其滑动平均值 movingAvg_y = MNIST_FCNN_FP_Model.inference(x, regularizer, variable_averages) ### 3. Loss Function ### # 定义交叉熵损失函数（经验风险）:该函数第二个参数需要提供的是正确答案的数字，tf.argmax(y_, 1)可以获取 y 对应的类别编号 cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1)) # 计算当前 batch 中所有样例的交叉熵平均值 cross_entropy_mean = tf.reduce_mean(cross_entropy) # 计算 Loss Function loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses')) ### 4. ANN BP ### # 设置指数衰减的学习率:随着迭代进行，更新变量的学习率在这个基础上递减 learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, # 过完所有训练数据需要的迭代次数 LEARNING_RATE_DECAY) # 使用优化器来优化损失函数 train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # 在 ANN 中，每迭代一次数据就需要通过 BP 来更新 ANN 中参数，又要更新每一个参数的滑动平均值。Tensorflow提供了两种机制：tf.control_dependencie和tf.group： # train_op = tf.group(train_step, variables_averages_op) 等价于 == with tf.control_dependencies([train_step, variables_averages_op]): train_op = tf.no_op(name='train') # 检验不使用滑动平均模型的 ANN FP 结果是否正确 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # correct_prediction = tf.equal(tf.argmax(movingAvg_y, 1), tf.argmax(y_, 1)) # tf.cast(x, dtype) 会将x数据格式转化成 dtype 数据格式。这里，将 bool 型数值转为 float 后，x会变为0/1序列。 # 计算正确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) ### 5. TensorFlow ANN Model 持久化 ### print("Configuring Saver ----&gt;") # 初始化 TensorFlow 持久化类 # saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2) saver = tf.train.Saver(max_to_keep=4) if not os.path.exists(MODEL_SAVE_PATH): os.makedirs(MODEL_SAVE_PATH) else: shutil.rmtree(MODEL_SAVE_PATH) os.makedirs(MODEL_SAVE_PATH) ### 6. Begin To Training ### with tf.Session() as sess: # 初始化所有变量 tf.global_variables_initializer().run() # 准备验证数据集，一般用于在训练过程中大致判断模型是否收敛 validate_feed = &#123;x: mnist.validation.images, y_: mnist.validation.labels&#125; print('Begin to train and Validate ----&gt;') # 迭代训练神经网络 for epoch in range(TRAINING_STEPS): start_time = time.time() # 产生当前迭代使用的一个 batch 的训练数据，并进行训练 xs, ys = mnist.train.next_batch(BATCH_SIZE) sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;) # 每 1000 次迭代输出一次模型训练结果以及验证集上的结果 if epoch % 1000 == 0: ''' # 计算滑动平均模型在验证集上的结果。这里，由于 MNIST 数据集较小，故一次处理所有验证数据，没有将验证数据划分为更小的batch。 # 当神经网络模型比较复杂或者验证数据集比较大时，太大的 batch 会导致运行时间过长甚至发生内存溢出的 Error ''' loss_train, acc_train = sess.run([loss, accuracy], feed_dict=&#123;x: xs, y_: ys&#125;) loss_val, acc_val = sess.run([loss, accuracy], feed_dict=validate_feed) # todo time_dif = get_time_dif(start_time) msg = 'Iter: &#123;0:&gt;6&#125;, Train Loss: &#123;1:&gt;6.2&#125;, Train Acc: &#123;2:&gt;7.2%&#125;,' \ + ' Val Loss: &#123;3:&gt;6.2&#125;, Val Acc: &#123;4:&gt;7.2%&#125;, Time: &#123;5&#125;' print(msg.format(epoch, loss_train, acc_train, loss_val, acc_val, time_dif)) # 每 2000 次迭代持久化一次模型，防止程序断点 if epoch % 2000 == 0: # 保存当前模型，注意这里给出 global_step 参数，可以使得每个保存模型的文件末尾加上训练轮数：model.ckpt-1000 saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step, write_meta_graph=False) print("----- %s has been writen local dics -----" % tf.train.latest_checkpoint(MODEL_SAVE_PATH)) ##### 3. Model Evaluating #####def evaluate(mnist): with tf.Graph().as_default() as g: # 定义神经网络输入输出 x = tf.placeholder(tf.float32, [None, MNIST_FCNN_FP_Model.INPUT_NODE], name='input_x') y_ = tf.placeholder(tf.float32, [None, MNIST_FCNN_FP_Model.OUTPUT_NODE], name='input_y') # 准备测试数据集，一般用于评价模型优劣的标准 test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125; # 使用 inference() 函数计算 FP 结果。由于测试时不关注正则化损失的值，所以设置 regularizer_class：None y = MNIST_FCNN_FP_Model.inference(x, None, None) # 检验不使用了滑动平均模型的 ANN FP结果是否正确 correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # tf.cast(x, dtype) 会将x数据格式转化成 dtype 数据格式。这里，将 bool 型数值转为 float 后，x会变为0/1序列。 # 计算正确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 通过变量重命名的方式加载模型，故在 FP 过程中不需要调用滑动平均函数来获取平均值，可以完全共用 MNIST_FCNN_FP_Model 中定义的 FP 过程 variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY) variables_to_restore = variable_averages.variables_to_restore() saver = tf.train.Saver(variables_to_restore) # 每隔 EVAL_INTERVAL_SECS 秒调用计算一次正确率的过程以检测训练过程中正确率的变化 while True: with tf.Session() as sess: # tf.train.get_checkpoint_state() 会通过 checkpoint 文件自动找到目录中最新模型的文件名 ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH) if ckpt and ckpt.model_checkpoint_path: # Load lastest Model saver.restore(sess, ckpt.model_checkpoint_path) # 通过文件名得到模型保存时的迭代轮数 global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1] accuracy_score = sess.run(accuracy, feed_dict=test_feed) # 输出模型当前训练情况： print("After %s training step(s), test " "accuracy = %g." % (global_step, accuracy_score)) else: print('No checkpoint file found.Please check again!') return time.sleep(EVAL_INTERVAL_SECS) ################ Main Function as the begin of program #################def main(argv=None): ###################### Functions for downloading and reading MNIST data. ###################### ''' ## 初始化：下载或读取用于训练、测试以及验证的 MNIST 手写数字图片（28px * 28px）数据集 ## MNIST 数据集分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。每一个 MNIST 数据单 元有两部分组成：一张包含手写数字的图片和一个对应的标签。比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。 mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。 mnist.train.labels 是一个形状为 [60000, 10] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的分类标签（one-hot vectors）。在此张量里的每一个元素，都表示某张图片对应分类的 one-hot vectors 向量，数值只有0和1 。 实际，read_data_sets 类会将数据从原始数据包格式解析成训练和测试神经网络时的数据格式 。read_data_sets会自动将 MNIST 数据集 划分为 train（55000）、test（10000） 以及 validation（5000）三个数据集。 ''' print("Loading training and validation data...") start_time = time.time() mnist = input_data.read_data_sets(MNIST_DATA_PATH, one_hot=True) # print mnist.train dataSet size : print("Training data size : ", mnist.train.num_examples) # print mnist.validation dataSet size : print("Validating data size : ", mnist.validation.num_examples) # print mnist.test dataSet size : print("Testing data size : ", mnist.test.num_examples) time_dif = get_time_dif(start_time) print("Time usage:", time_dif) if len(sys.argv) != 2 or sys.argv[1] not in ['train', 'eval']: raise ValueError("""usage: Please use the format of : python MNIST_FCNN_Train_And_Evaluate.py [train or eval]""") if sys.argv[1] == 'train': train(mnist) else: evaluate(mnist)# TensorFlow 提供的一个主程序入口，tf.app.run 会调用上面定义的 main 函数：if __name__ == '__main__': # tf.app.run() main()]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>MNIST</tag>
        <tag>Checkpoint</tag>
        <tag>Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 之 MNIST 手写体数字图像识别问题]]></title>
    <url>%2FTensorFlow%2FTensorFlow-%E4%B9%8B-MNIST-%E6%89%8B%E5%86%99%E4%BD%93%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 上一章节 02. 深层神经网络和深度学习 我们介绍了训练神经网络模型时需要考虑的主要问题以及如何解决这些问题的常用方法。这一章节，我们将通过一个实际的问题（MNIST 手写体数字识别）来验证 [02] 章节中介绍的解决方法。 你应该记得，在 [02] 章节中我们是通过 Python Numpy 库中的随机方法来产生的训练数据集，没有实际意义。本章我们将针对实际的图像识别问题，使用 MNIST 手写体数字识别数据集，结合 [02] 章节涉及到的方法给出一个使用 TensorFlow 训练神经网络的最佳实践。 这一章节我们总共安排了 三个部分 来进行学习： 第一部分：简单介绍 MNIST 手写体数字图像识别数据集，并给出 TensorFlow 程序如何处理 MNIST 数据集数据； 第二部分：基于 MNIST 手写体数字图像识别问题对比 [02] 章节中提到的神经网络结构设计和参数优化的不同方法，从实际问题中展示不同优化方法带来的性能提升； 第三部分：针对第二部分给出的 MNIST 手写体数字图像识别 TensorFlow 程序实现的不足之处，引出 TensorFlow 变量重用的问题和变量命名空间的使用，并且最终给出 MNIST 完整样例； 1. MNIST 手写体数字图像数据集这一小节，先让我们来认识一下 MNIST 手写体数字图像识别数据集。MNIST(Mixed National Institute of Standards and Technology database) 是一个非常有名的手写数字体图像识别数据集，也是一个入门级的计算机视觉数据集（很多资料会将其作为深度学习入门样例）。就好比编程入门有 Hello World，机器学习入门有 MNIST。 MNIST 数据集中包含各种手写数字图片： MNIST 数据集是 NIST 数据集的一个子集。MNIST 官方数据集分成两部分：60000 行的训练数据集（mnist.train）和 10000 行的测试数据集（mnist.test）。每一个 MNIST 数据单元（数据对象）有两部分组成：一张包含手写数字的图片和一个对应的标签。 –&gt; MNIST 数据单元： 手写图像：每一张图片都代表了一个 0~9 中的数字灰度图（单通道），图片大小为 28像素 × 28像素。我们可以用一个像素矩阵来表示这张数字图片。标签：图片对应的一个 0~9 中的数字。 下图展示了一张数字图片以及它所对于的像素矩阵： MNIST 数据集更加详细说明以及使用参见文档：MNIST Introduce 。 虽然这个数据集只提供了训练和测试数据，但是为了验证模型训练的效果，使用时一般会从训练数据中划分出一部分数据作为验证数据集（后面会更加详细地介绍验证数据集的作用，这里不用深究）。 1.1 MNIST 数据集的 TensorFlow 支持为了在 TensorFlow 中使用方便，TensorFlow 对 MNIST 数据集进行了内部封装，提供了一个类来处理 MNIST 数据集。这个类会自动下载并转换 MNIST 数据格式（可能由于网络原因下载失败，你可以参考 这里 进行手动下载），将其从原始的数据包中解析成训练和测试神经网络时可以使用的格式。下面程序给出了使用这个函数的样例： 1234567891011121314151617# 导入用于下载和读取 MNIST 数据集的模块 from tensorflow.examples.tutorials.mnist import input_data# 指定 MNIST 数据集的下载和读取的路径：MNIST_data_Path = "./MNIST_data/"# 获取 MNIST 数据集对象mnist = input_data.read_data_sets(MNIST_data_Path, one_hot=True)# print mnist.train dataSet size :print("Training data size : ", mnist.train.num_examples)# print mnist.validation dataSet size :print("Validating data size : ", mnist.validation.num_examples)# print mnist.test dataSet size :print("Testing data size : ", mnist.test.num_examples)# print mnist.train.images[0] / mnist.train.labels[0] Formatprint("Example training data（image）: ", "\n", mnist.train.images[0])print("Example training data lable : ", mnist.train.labels[0]) 样例程序输出结果如下： 1234567891011121314151617181920212223242526272829303132Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.Extracting ./MNIST_data/train-images-idx3-ubyte.gzSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.Extracting ./MNIST_data/train-labels-idx1-ubyte.gzSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.Extracting ./MNIST_data/t10k-images-idx3-ubyte.gzSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gzTraining data size : 55000Validating data size : 5000Testing data size : 10000Example training data : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ................. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.34901962 0.9843138 0.9450981 0.3372549 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.01960784 0.8078432 0.96470594 0.6156863 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ...................... 0. 0. 0. 0. 0.01568628 0.45882356 0.27058825 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ]Example training data lable : [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] 从上面代码运行输出可以看出，input_data.read_data_sets() 函数生成的类会自动将 MNIST 数据集划分为 train、validation、test 三个数据集。其中，train 这个集合有 55000 张图片，validation 集合中有 5000 张图片，这两个数据集共同构成了 MNIST 本身提供的训练数据集。 test 集合中划分了 10000 张图片，这些图片都来自于 MNIST 提供的测试数据集。 我们知道，每张图片可以看作一个像素矩阵，实际上所有的图像处理工具都是这么做的（样例参见文档：Image PixMatrix）。 这里，因为神经网络的输入是一个特征向量，所以把一张二维图像的像素矩阵变换为一个一维数组可以方便 TensorFlow 将图片的像素矩阵提供给神经网络的输入层。 故，TensorFlow 程序封装的类处理后的每张图片都是一个长度为 784 的一维数组，这个数组中的元素对应了图片像素矩阵中的每个数值（28 * 28 = 784）。像素矩阵中元素的取值范围为 [0,1]，它代表了颜色的深浅。其中 0 表示白色背景（background），1 表示黑色前景（foreground）。 同样，为了方便使用随机梯度下降，input_data.read_data_sets()函数生成的类还提供了 mnist.train.next_batch() 函数，他可以从所有的训练数据中读取一小部分数据作为一个训练 batch。以下代码显示了如何使用这个功能： 123456789101112# 取 batch_size = 100 大小的训练数据：batch_size = 100xs, ys = mnist.train.next_batch(batch_size)print ('X Shape: ', xs.shape)print ('Y Shape: ', ys.shape)% 输出：X Shape: (100, 784)Y Shape: (100, 10) 2. MNIST 神经网络模型训练以及不同模型结果对比这一部分，我们首先基于 MNIST 手写体数字图像识别问题给出一个 TensorFlow 实现，这个程序整合了 [02] 章节中介绍的所有优化方法。 接着，介绍验证数据集在神经网络训练过程中的作用。通过实现数据来证明，神经网络在验证数据集上的表现可以近似地作为评价不同神经网络模型的标准或者作为迭代轮数的依据。 最后，通过模型在测试集上的表现对比 [02] 章节中提到的神经网络结构设计和参数优化的不同方法，从实际问题中展示不同优化方法带来的性能提升。 2.1 TensorFlow 实现 MNIST 手写体数字图像识别问题这一小节将给出一个 MNIST 手写体数字图像识别的 TensorFlow 实现。这个程序整合了 [02] 章节中介绍的所有优化方法：在神经网络结构设计上采用全连接结构，引入隐藏层、激活函数、偏置项；在训练神经网络上，引入设置学习率指数衰减、正则化以及滑动平均模型。训练好的神经网络模型在 MNIST 测试数据集上可以达到 98.4% 左右的准确率。下面给出完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242''' TensorFlow_Hello : MNIST 手写数字图片识别示例 Neural Network Structure : Full Connection'''import timeimport osfrom datetime import timedeltaimport tensorflow as tf# 导入用于下载和读取 MNIST 数据集的 python 源文件：见 input_data.pyfrom tensorflow.examples.tutorials.mnist import input_data###################### Part 1 : Macros Variables Define ######################## 1. MNIST 数据集相关的常数 ### 图像像素（28px * 28px像素矩阵转化），输入层的节点数；INPUT_NODE = 784# 输出节点数，等于类别数目。分别对应 0~9 10个数字类别；OUTPUT_NODE = 10## 2. 配置神经网络中的参数 ### Layes1: 隐藏层节点数，这里设置只有一层隐藏层（hidden layers）的网络结构LAYER1_NODE = 200# 一个训练 batch 块中的训练数据个数。# batch 数值越小，训练过程越接近随机梯度下降；数值越大，训练越接近梯度下降。BATCH_SIZE = 100# 设置基础学习率LEARNING_RATE_BASE = 0.8# 设置学习率的衰减率LEARNING_RATE_DECAY = 0.96# 设置描述模型复杂度（结构风险）的正则化项在损失函数中的系数(lambda)REGULARIZATION_RATE = 0.0001# 设置滑动平均衰减率MOVING_AVERAGE_DECAY = 0.99# 设置训练轮数TRAINING_STEPS = 30000###################### Part 2 : Referenced Function Define ######################## [1] get_time_dif：获取已使用时间def get_time_dif(start_time): end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=time_dif)''' ## 2. inference : 通过给定的 ANN 输入和所有参数，计算 ANN FP 的结果 ## 定义了一个 ReLU（非线性） 激活函数的三层全连接神经网络，实现了多层网络结构以及去线性化。同时，支持：传入用于计算参数滑动平均值的类，方便在测试时使用滑动平均模型。'''def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2): # 当没有提供滑动平均类时，直接使用参数当前的取值 if avg_class == None: # 计算隐藏层的 FP 结果，激活函数为 ReLU: layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1) return tf.matmul(layer1, weights2) + biases2 else: layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1)) return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2) def train(mnist): x = tf.placeholder(dtype=tf.float32, shape=(None, INPUT_NODE), name="input_x") y_ = tf.placeholder(dtype=tf.float32, shape=(None, OUTPUT_NODE), name="input_y") weights1 = tf.Variable(tf.random_normal([INPUT_NODE, LAYER1_NODE], dtype=tf.float32, stddev=0.1)) biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE])) # 输出层参数(First layer)： weights2 = tf.Variable(tf.random_normal([LAYER1_NODE, OUTPUT_NODE], dtype=tf.float32, stddev=0.1)) biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE])) # 定义存储训练轮数的变量，该变量无需计算滑动平均值。所以一般需要指定训练轮数变量为不可训练变量(trainable=False) global_step = tf.Variable(0, trainable=False) ## 1. avg_class = None ## # 计算当前参数下 ANN FP 的结果，这里调用计算滑动平均的类参数为 None ，所以不会使用参数的滑动平均值 y = inference(x, None, weights1, biases1, weights2, biases2) ## 2. avg_class = variable_averages ## variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) # 在所有（可训练，Except global_step）神经网络变量上使用滑动平均 # tf.trainable_variables 可返回 GraphKeys.TRAINABLE_VARIABLES 集合中元素，即未指定 trainable=False 的变量 variables_averages_op = variable_averages.apply(tf.trainable_variables()) # 计算使用了滑动平均之后的 FP 结果。滑动平均之后不会改变变量本身取值，会维护一个 shadow_variable 来记录其滑动平均值 movingAvg_y = inference(x, variable_averages, weights1, biases1, weights2, biases2) ## 3. Loss Function ## # 定义交叉熵损失函数（经验风险）:该函数第二个参数需要提供的是正确答案的数字，tf.argmax(y_, 1)可以获取 y 对应的类别编号 # 注意，这里的 logits 参数值只能是不使用滑动平均的变量，不能是影子变量： cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1)) # 计算当前 batch 中所有样例的交叉熵平均值 cross_entropy_mean = tf.reduce_mean(cross_entropy) # 定义 L2 正则化损失函数（结构风险）： regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE) # 计算模型的L2正则化损失，一般只计算权重的正则化损失，而不使用偏置项 regularization = regularizer(weights1) + regularizer(weights2) # 计算 Loss Function：经验风险和结构分析 loss = cross_entropy_mean + regularization ## 4. ANN BP ## # 设置指数衰减的学习率:随着迭代进行，更新变量的学习率在这个基础上递减 learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, # 遍历一次所有训练数据需要的迭代次数 LEARNING_RATE_DECAY) # 使用优化器来优化损失函数 train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # 在 ANN 中，每迭代一次数据就需要通过 BP 来更新 ANN 中参数，又要更新每一个参数的滑动平均值。Tensorflow提供了两种机制：tf.control_dependencie和tf.group： # train_op = tf.group(train_step, variables_averages_op) 等价于 == with tf.control_dependencies([train_step, variables_averages_op]): train_op = tf.no_op(name='train') ## 5. Accuracy ## # 检验使用了滑动平均模型的 ANN FP 结果是否正确（滑动平均或不使用滑动平均） # correct_prediction = tf.equal(tf.argmax(movingAvg_y, 1), tf.argmax(y_, 1)) correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # tf.cast(x, dtype) 会将x数据格式转化成 dtype 数据格式。这里，将 bool 型数值转为 float 后，x会变为0/1序列。 # 计算正确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) ############################ Begin To Training ########################### config = tf.ConfigProto(#device_count=&#123;"CPU": 4&#125;, # limit to num_cpu_core CPU usage #inter_op_parallelism_threads = 1, #intra_op_parallelism_threads = 1, allow_soft_placement=True, log_device_placement=True ) with tf.Session(config=config) as sess: start_time = time.time() # 初始化所有变量 tf.global_variables_initializer().run() print('Training and evaluating...') # 准备验证数据集，一般用于在训练过程中大致判断停止 validate_feed = &#123;x: mnist.validation.images, y_: mnist.validation.labels&#125; # 准备测试数据集，一般用于评价模型优劣的标准 test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125; # 迭代训练神经网络 for step in range(TRAINING_STEPS): # 产生这一轮使用的一个 batch 的训练数据，并进行训练 xs, ys = mnist.train.next_batch(BATCH_SIZE) sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;) # 每 1000 次输出一次在验证数据上的测试结果 if step % 1000 == 0: # print('Epoch:', epoch + 1) loss_train, acc_train = sess.run([loss, accuracy], feed_dict=&#123;x: xs, y_: ys&#125;) loss_val, acc_val = sess.run([loss, accuracy], feed_dict=validate_feed) # todo time_dif = get_time_dif(start_time) msg = 'Iter: &#123;0:&gt;6&#125;, Train Loss: &#123;1:&gt;6.2&#125;, Train Acc: &#123;2:&gt;7.2%&#125;,' \ + ' Val Loss: &#123;3:&gt;6.2&#125;, Val Acc: &#123;4:&gt;7.2%&#125;, Time: &#123;5&#125;' print(msg.format(step, loss_train, acc_train, loss_val, acc_val, time_dif)) # 训练结束后，在测试数据集上检测 ANN 模型的最终正确率 test_acc = sess.run(accuracy, feed_dict=test_feed) print ("After %d training step(s), test accuracy " "using average Model is %g " % (TRAINING_STEPS, test_acc)) def main(arg=None): ###################### Functions for downloading and reading MNIST data. ###################### ''' ## 初始化：下载或读取用于训练、测试以及验证的 MNIST 手写数字图片（28px * 28px）数据集 ## MNIST 数据集分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。每一个 MNIST 数据单 元（数据对象）有两部分组成：一张包含手写数字的图片和一个对应的标签。比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。 mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。 mnist.train.labels 是一个形状为 [60000, 10] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的分类标签（one-hot vectors）。在此张量里的每一个元素，都表示某张图片对应分类的 one-hot vectors 向量，数值只有0和1 。 实际，read_data_sets 类会将数据从原始数据包格式解析成训练和测试神经网络时的数据格式 。read_data_sets会自动将 MNIST 数据集 划分为 train（55000）、test（10000） 以及 validation（5000）三个数据集。 ''' print("Loading training and validation data...") start_time = time.time() # 指定 MNIST 数据集的下载和读取的路径： MNIST_data_Path = "../../MNIST_data" mnist = input_data.read_data_sets(MNIST_data_Path, one_hot=True) # print mnist.train dataSet size : print("Training data size : ", mnist.train.num_examples) # print mnist.validation dataSet size : print("Validating data size : ", mnist.validation.num_examples) # print mnist.test dataSet size : print("Testing data size : ", mnist.test.num_examples) # print mnist.train.images[0] / mnist.train.labels[0] Format # print("Example training data[0] pix-matirx : ", "\n", mnist.train.images[0]) # print("Example training picture shape : " + str(len(mnist.train.images[0]))) # print("Example training data lable : ", mnist.train.labels[0]) # print("Example training lable shape : " + str(len(mnist.train.labels[0]))) time_dif = get_time_dif(start_time) print("Time usage:", time_dif) train(mnist)if __name__=="__main__": # tf.app.run() main() 样例程序输出信息如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Loading training and validation data...Extracting MNIST_data\train-images-idx3-ubyte.gzExtracting MNIST_data\train-labels-idx1-ubyte.gzExtracting MNIST_data\t10k-images-idx3-ubyte.gzExtracting MNIST_data\t10k-labels-idx1-ubyte.gzTraining data size : 55000Validating data size : 5000Testing data size : 10000Time usage: 0:00:01Training and evaluating...Iter: 0, Train Loss: 3.4, Train Acc: 19.00%, Val Loss: 4.1, Val Acc: 12.78%, Time: 0:00:00Iter: 1000, Train Loss: 0.081, Train Acc: 100.00%, Val Loss: 0.18, Val Acc: 96.64%, Time: 0:00:02Iter: 2000, Train Loss: 0.077, Train Acc: 100.00%, Val Loss: 0.14, Val Acc: 97.88%, Time: 0:00:04Iter: 3000, Train Loss: 0.068, Train Acc: 100.00%, Val Loss: 0.14, Val Acc: 97.92%, Time: 0:00:06Iter: 4000, Train Loss: 0.064, Train Acc: 100.00%, Val Loss: 0.12, Val Acc: 98.18%, Time: 0:00:08Iter: 5000, Train Loss: 0.06, Train Acc: 100.00%, Val Loss: 0.12, Val Acc: 98.30%, Time: 0:00:10Iter: 6000, Train Loss: 0.057, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.26%, Time: 0:00:12Iter: 7000, Train Loss: 0.055, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.22%, Time: 0:00:14Iter: 8000, Train Loss: 0.052, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.24%, Time: 0:00:16Iter: 9000, Train Loss: 0.052, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.14%, Time: 0:00:18Iter: 10000, Train Loss: 0.052, Train Acc: 100.00%, Val Loss: 0.1, Val Acc: 98.26%, Time: 0:00:21Iter: 11000, Train Loss: 0.05, Train Acc: 100.00%, Val Loss: 0.1, Val Acc: 98.26%, Time: 0:00:23Iter: 12000, Train Loss: 0.048, Train Acc: 100.00%, Val Loss: 0.1, Val Acc: 98.26%, Time: 0:00:25Iter: 13000, Train Loss: 0.044, Train Acc: 100.00%, Val Loss: 0.098, Val Acc: 98.32%, Time: 0:00:27Iter: 14000, Train Loss: 0.043, Train Acc: 100.00%, Val Loss: 0.098, Val Acc: 98.26%, Time: 0:00:29Iter: 15000, Train Loss: 0.045, Train Acc: 100.00%, Val Loss: 0.097, Val Acc: 98.30%, Time: 0:00:32Iter: 16000, Train Loss: 0.044, Train Acc: 100.00%, Val Loss: 0.095, Val Acc: 98.32%, Time: 0:00:34Iter: 17000, Train Loss: 0.044, Train Acc: 100.00%, Val Loss: 0.094, Val Acc: 98.32%, Time: 0:00:36Iter: 18000, Train Loss: 0.041, Train Acc: 100.00%, Val Loss: 0.094, Val Acc: 98.30%, Time: 0:00:38Iter: 19000, Train Loss: 0.043, Train Acc: 100.00%, Val Loss: 0.094, Val Acc: 98.36%, Time: 0:00:40Iter: 20000, Train Loss: 0.042, Train Acc: 100.00%, Val Loss: 0.092, Val Acc: 98.40%, Time: 0:00:42Iter: 21000, Train Loss: 0.038, Train Acc: 100.00%, Val Loss: 0.092, Val Acc: 98.36%, Time: 0:00:44Iter: 22000, Train Loss: 0.041, Train Acc: 100.00%, Val Loss: 0.091, Val Acc: 98.34%, Time: 0:00:47Iter: 23000, Train Loss: 0.041, Train Acc: 100.00%, Val Loss: 0.091, Val Acc: 98.32%, Time: 0:00:49Iter: 24000, Train Loss: 0.04, Train Acc: 100.00%, Val Loss: 0.09, Val Acc: 98.28%, Time: 0:00:51Iter: 25000, Train Loss: 0.039, Train Acc: 100.00%, Val Loss: 0.09, Val Acc: 98.32%, Time: 0:00:53Iter: 26000, Train Loss: 0.04, Train Acc: 100.00%, Val Loss: 0.09, Val Acc: 98.36%, Time: 0:00:56Iter: 27000, Train Loss: 0.039, Train Acc: 100.00%, Val Loss: 0.089, Val Acc: 98.32%, Time: 0:00:58Iter: 28000, Train Loss: 0.038, Train Acc: 100.00%, Val Loss: 0.089, Val Acc: 98.28%, Time: 0:00:59Iter: 29000, Train Loss: 0.041, Train Acc: 100.00%, Val Loss: 0.089, Val Acc: 98.34%, Time: 0:01:01After 30000 training step(s), test accuracy using average Model is 0.9828 2.2 使用验证数据集判断模型效果上一小节我们给出了解决 MNIST 手写体数字图像识别问题的一个 TensorFlow 实现。在程序的开始我们设置了一系列的参数：初始学习率、学习率衰减率、隐藏层节点数、迭代轮数等，并且我们知道神经网络模型的最终表达效果是受上述参数影响的。 那么如何设置这些参数的取值？！大部分情况下，配置神经网络的参数是需要通过不断实验来进行调整以达到最佳效果神经网络模型模型。那么如何评判不同参数设置下的模型效果？ 1）测试数据集： 神经网络模型的效果好坏是通过测试数据（模型对未知数据的预测）来评判的。 但是我们不能直接通过模型在测试数据上的效果（测试数据集变成了训练数据集的一部分参与训练，这时设置测试集是没有意义的）来选择参数，这可能导致神经网络过度拟合测试数据，从而失去了对未知数据的预判能力。 也就是说我们需要保证测试数据在训练过程是不可见的（即：测试数据不应该对我们最终学得（/训练得到）的神经网络模型提供任何支持），这样才能保证通过测试数据评估出来的模型效果和在真实场景下模型对未知数据的预测效果是最接近的（测试数据集对应的是部分未知数据，所以需要保证训练过程中测试数据不可见。）。 由于测试数据的训练过程的不可见，为了评测 优化过程 神经网络模型在不同参数下的效果，第一种方法是引入了 验证数据（代表测试数据） 。一般会从训练数据中抽取一部分数据作为验证数据集，通过训练模型在验证数据集上的表现评价优化过程中不同参数选取下模型的好坏。 第二张方法是采用 交叉验证（cross validation） ，一般是将数据集分成 $k$ 个子集，每个子集均做一次测试集，其余的作为训练集。交叉验证重复 $k$ 次，每次选择一个子集作为测试集，并将 $k$ 次的平均交叉验证识别正确率作为结果。但是神经网络本身训练时间就比较长，采用交叉验证会花费大量的时间（适合小数据集）。所以在海量数据下，一般会更多的采用验证数据集的形式来评测模型的效果。 2）验证数据集： 为了说明验证数据可以近似作为模型效果的评价标准，我们将对比不同迭代轮数情况下，模型在验证数据和测试数据上的正确率。如下图所示： 上图给出了每 1000 轮迭代滑动平均模型在不同数据集上的正确率变换曲线。可以看出，虽然两条曲线不完全重合，但这两条曲线的趋势基本一致，而且他们的相关系数（correlation coefficient）大于 0.9999。 实验说明： 模型在验证数据集上的表现完全可以近似作为评价不同神经网络模型的标准或者作为迭代轮数的依据！ 注意： 但请注意：上面我们所说的测试数据集训练过程不可见并不是严格不可见。你会看到很多资料不会划分验证数据集，直接使用测试数据集作为模型训练过程中是否收敛的评价标准。个人理解：主要由于不同问题的数据分布不一样，理想情况下我们希望选取的未划分的样本数据集样本对于问题的数据分布是均匀的。但如果验证数据分布不能很好地代表测试数据分布，那么模型在这两个数据集上的表现就可能不一样。 一般来说选取的验证数据分布越接近测试数据分布，模型在验证数据上的表现越可以体现神经网络模型在测试数据（未知数据）下的效果！ 2.3 使用不同优化方法模型效果对比这一小节将以 MNIST 手写数字图像识别实例来对比 [02] 章节中提到的不同优化方法对神经网络模型正确率的影响。既然是最终的学得模型，这里我们使用神经网络模型在 MNIST 测试数据集上的正确率作为评价不同优化方法的标准。 在下图 5-3 中，给出了在相同神经网络参数下，使用不同优化方法，经过 30000 轮训练迭代后，得到的最终模型的正确率（10 次运行结果的平均值）。 实验数据表明： （1）从上图中很明显看出：调整神经网络模型的结构（不使用隐藏层或没有激活函数）对最终的正确率有非常大的影响。这说明神经网络的结构设计对最终模型的效果有本质的影响。后面会介绍一种更加特殊的神经网络结构 CNN（卷积神经网络），它可以更加有效的处理图像信息。通过 CNN 可以进一步将模型正确率提高到 99.5%。 （2）从上图数字中发现：使用滑动平均模型、指数衰减学习率和使用正则化带来的正确率提升并不是特别明显。其中使用了所有优化方法的模型和不使用滑动平均模型以及不使用指数衰减学习率的模型正确率都可以达到约 98.4% 。那么是不是意味着这些优化方法对模型正确率提升不大？！ 答案肯定是否定的。这里正确率提升不是特别明显是由于 MNIST 数据集简单，模型收敛速度很快，而滑动平均模型以及指数衰减学习率在一定程度上都是限制神经网络中参数的更新速度，所以这两种优化对最终模型影响不大。从图 5-2 中可以看出，在迭代 4000 次以后就已经接近最终的正确率了。 下面我们将进一步分析滑动平均模型、指数衰减学习率和使用正则化对训练模型的影响： 3. TensorFlow 变量管理由于编程习惯，我们通常喜欢采用模块化编程风格，提高代码可读性。例如在 2.1 TensorFlow 实现 MNIST 手写体数字图像识别问题 中将计算神经网络前向传播结果的过程抽象成了一个函数（模块）。这样的编程习惯给我们带来的一个好处就是：在训练、测试以及预测的过程中可以统一调用同一个函数来得到模型的前向传播结果。这个函数被定义为： def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2) 3.1 变量重用以及变量的命名空间从上述前向传播过程定义中可以看到，这个函数的形参中包括了神经网络中的所有参数。然而，当神经网络的结构更加复杂，引入的参数更多时，就需要一个更好的方式来传递和管理神经网络中的参数了。TensorFlow 提供了通过变量名称来创建或者获取一个变量的机制。 通过这种机制，我们可以在不同函数中可以直接通过变量的名称来使用变量，而不要将变量通过参数的形式到处传递。TensorFlow中主要通过 tf.get_variable() 函数和 tf.variable_scope() 函数来实现。下面将分别介绍如何使用这两个函数： 3.1.1 tf.get_variable() 函数和 tf.variable_scope() 函数除了之前我们提到过的 tf.Variable() 函数，TensorFlow 中还支持通过 tf.get_variable() 函数来创建或者获取变量。 1）tf.get_variable() 创建变量 前面我们介绍过通过 tf.Variable() 函数来创建一个变量。除了 tf.Variable() 函数，TensorFlow 可以使用 tf.get_variable() 函数来创建变量。当用于创建变量时，两个函数的功能是基本等价的。下面代码给出了通过这两个函数创建同一个变量的的样例： import tensorflow as tf v = tf.get_variable(&apos;v&apos;, shape=[1], initializer=tf.constant_initializer(1.0)) # 和上面函数的定义是等价的 # v = tf.Variable(tf.constant(1.0, shape=[1]), name=&apos;v&apos;) 从上面的代码中可以看出，通过 tf.Variable() 和 tf.get_variable() 函数创建变量的过程基本是一样的。TensorFlow 提供的 initializer() 函数和前面章节中介绍的随机数以及常量生成函数大部分是一一对应的。下面给出 TensorFlow 提供的 7 种 initializer 函数： tf.Variable() 和 tf.get_variable() 创建变量的区别 tf.Variable() 函数和 tf.get_variable() 函数的一个区别在于指定 变量名称的参数。对于 tf.Variable() 函数，变量名称是一个可选的参数，可以通过 name=&#39;v&#39; 的形式给出。但对于 tf.get_variable() 函数，变量名称是一个必须的参数，tf.get_variable() 会根据这个给定的名称去创建或获取变量。 以我们上面给出的样例程序为例: tf.get_variable() 首先会试图取创建一个名字为 &#39;v&#39; 的参数，如果创建失败（比如已经有了同名的参数），那么这个程序就会报错，这是为了避免无意识的变量复用造成的错误。比如在定义神经网络参数时，第一层网络的权重已经叫了 weights 了，那么在创建第二层神经网络时，如果这个参数名称仍然叫 weights ，就会发生变量重用错误，否则两个神经网络层共用一个权重会出现一些比较难以发现的问题。 也就是说，tf.get_variable() 会进行命名检查。而对于 tf.Variable(), 系统如果检测到命名冲突，系统会自己处理。不会报错！ 下面我们通过相应应用代码来说明上述区别： –&gt; 使用 tf.get_variable() ： import tensorflow as tf w_1 = tf.get_variable(name=&quot;w_1&quot;,initializer=1) w_2 = tf.get_variable(name=&quot;w_1&quot;,initializer=2) # 错误信息: # ValueError: Variable w_1 already exists, disallowed. Did you mean to set reuse=True in VarScope? –&gt; 使用 tf.Variable() ： import tensorflow as tf w_1 = tf.Variable(3,name=&quot;w_1&quot;) w_2 = tf.Variable(1,name=&quot;w_1&quot;) print (w_1.name) print (w_2.name) # 语句输出： w_1:0 w_1_1:0 2）tf.variable_scope() 命名空间的使用 在介绍如何使用 tf.get_variable() 获取变量之前，我们先介绍一下 tf.variable_scope() 函数。因为 tf.get_variable() 获取变量必须 tf.variable_scope() 函数配合使用！ tf.variable_scope() 函数需要和 Python 上下文管理器一起使用，创建一个 TensorFlow 命名空间。它有一个 reuse 参数，用于表示是创建变量还是获取已经存在的变量。如果不设置 reuse（即默认下，reuse 的取值为：reuse=None 或 reuse=False），tf.get_variable() 操作将创建新的变量。此时，如果变量已经存在，则报错；如果设置 reuse=True，则这个上下文管理器中的所有 tf.get_variable() 函数会取获取已经创建的变量。此时，如果变量不存在，则报错。下面给出一个程序来说明其用法： import tensorflow as tf # 在名字为 scope1 的命名空间内创建变量； with tf.variable_scope(&quot;scope1&quot;): w1 = tf.get_variable(&quot;w1&quot;, shape=[]) # 在名字为 scope1 的命名空间内获取变量： with tf.variable_scope(&quot;scope1&quot;, reuse=True): w1_p = tf.get_variable(&quot;w1&quot;, shape=[]) print (w1 is w1_p) print (w1, w1_p) 样例程序输出如下： True &lt;tf.Variable &apos;scope1/w1:0&apos; shape=() dtype=float32_ref&gt; &lt;tf.Variable &apos;scope1/w1:0&apos; shape=() dtype=float32_ref&gt; 从上面代码可以看出，通过 tf.variable_scope() 函数创建的命名空间，在命名空间内创建的变量名称都会带上这个命名空间名作为前缀。 –&gt; 命名空间嵌套 TensorFlow 中的 tf.variable_scope() 函数是可以嵌套使用的，那么嵌套时，reuse 的取值如何确定： 3）tf.get_variable() 获取变量 上面已经介绍了如何使用 tf.get_variable() 创建变量以及 tf.variable_scope() 函数的用法。 这里，如果需要通过 tf.get_variable() 来获取一个已经创建的变量，需要通过 tf.variable_scope() 以及 Python 上下文管理生成一个 变量命名空间 ，并且 明确指定在这个上下文管理器 中使用 tf.get_variable() 来获取变量。 下面给出一段代码说明 tf.variable_scope() 如何控制 tf.get_variable() 获取已经创建过的变量： 1234567891011121314151617181920212223# 在名字为 foo 的命名空间内创建变量 v ；with tf.variable_scope('foo'): v = tf.get_variable('v', [1], initializer=tf.constant_initializer(1.0))# 在名字为 foo 的命名空间内已经存在变量 v ，所以下面代码会报错；# Variable foo/v already exists,disallowed. Did you mean to set reuse=True in VarScope?'''with tf.variable_scope('foo'): v = tf.get_variable('v', [1])'''# 重新获取变量 v：with tf.variable_scope('foo', reuse=True): v = tf.get_variable('v', [1]) print (v)# 当 reuse=True时，tf.get_variable()函数自能获取已经创建的变量，所以下面代码会报错；# Variable bar/v does not exists,disallowed. Did you mean to set reuse=None in VarScope?'''with tf.variable_scope('bar', reuse=True): v = tf.get_variable('v', [1])''' 样例程序输出如下: &lt;tf.Variable &apos;foo/v:0&apos; shape=(1,) dtype=float32_ref&gt; 3.2 使用命名空间来管理变量名称下面我们直接给出一个样例来显示如何通过 tf.variable_scope() 来管理变量名称： v1 = tf.get_variable(&apos;v&apos;, [1]) # 输出 v:0。&apos;v&apos; 为变量的名称，&apos;:0&apos; 表示这个变量是生成变量运算的第一个输出结果： print (v1.name) with tf.variable_scope(&apos;foo&apos;): v2 = tf.get_variable(&apos;v&apos;, [1]) # 输出 foo/v:0。在 tf.variable_scope 中创建变量，通过 / 来分割命名空间的名称和变量的名称 print (v2.name) with tf.variable_scope(&apos;foo&apos;): with tf.variable_scope(&apos;bar&apos;): v3 = tf.get_variable(&apos;v&apos;, [1]) # 输出 foo/bar/v:0。命名空间嵌套 print (v3.name) v4 = tf.get_variable(&apos;v1&apos;, [1]) # 输出 foo/v:0。 print (v4.name) # 创建一个命名为空的命名空间，并设置 reuse=True: with tf.variable_scope(&apos;&apos;, reuse=True): # 可以直接通过带命名空间名称的变量名称来获取其它命名空间下的变量 v5 = tf.get_variable(&apos;foo/bar/v&apos;, [1]) print (v5 == v3) # True v6 = tf.get_variable(&apos;foo/v1&apos;, [1]) print (v6 == v4) 样例程序结果输出如下： v:0 foo/v:0 foo/bar/v:0 foo/v1:0 True True 3.3 使用命名空间管理 FP 实例通过 tf.get_variable() 和 tf.variable_scope() 函数，对上一小节中定义的计算神经网络模型进行改进： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313''' TensorFlow_Hello : MNIST 手写数字图片识别示例 Neural Network Structure : Full Connection'''import timefrom datetime import timedeltaimport tensorflow as tf# 导入用于下载和读取 MNIST 数据集的 python 源文件：见 input_data.pyfrom tensorflow.examples.tutorials.mnist import input_data###################### Part 1 : Macros Variables Define ######################## 1. MNIST 数据集相关的常数 ### 图像像素（28px * 28px像素矩阵转化），输入层的节点数；INPUT_NODE = 784# 输出节点数，等于类别数目。分别对应 0~9 10个数字类别；OUTPUT_NODE = 10## 2. 配置神经网络中的参数 ### Layes1: 隐藏层节点数，这里设置只有一层隐藏层（hidden layers）的网络结构LAYER1_NODE = 200# 一个训练 batch 块中的训练数据个数。数值越小，训练过程越接近随机梯度下降；数值越大，训练越接近梯度下降。BATCH_SIZE = 100# 设置基础学习率LEARNING_RATE_BASE = 0.8# 设置学习率的衰减率LEARNING_RATE_DECAY = 0.96# 设置描述模型复杂度（结构风险）的正则化项在损失函数中的系数(lambda)REGULARIZATION_RATE = 0.0001# 设置滑动平均衰减率MOVING_AVERAGE_DECAY = 0.99# 设置训练轮数TRAINING_STEPS = 30000###################### Part 2 : Referenced Function Define ########################## [1]. 通过tf.get_variable函数创建变量：在训练 ANN 时会创建这些变量；在测试时会通过保存的模型加载这些变量 ####'''## get_weight_variable() :Function ： 1] 因为可以在变量加载时将滑动平均变量重命名，故在训练过程中可以通过相同的名字使用变量自身, 而在测试时使用变量的滑动平均值 2] 根据 regularizer 会将变量的正则化损失加入损失集合'''def get_weight_variable(shape, regularizer_class): weights = tf.get_variable("weights", shape, initializer= tf.truncated_normal_initializer(stddev=0.1)) # 当给出正则化生成函数时，将当前参数的正则化损失加入到 losses 集合。losses是自定义集合，不在 Tensorflow 自动管理的集合列表 if regularizer_class != None: tf.add_to_collection('losses', regularizer_class(weights)) return weights#### [2]. 构建神经网络前向传播过程 ####'''## inference(): ##Function ： 1] Build Forward Propagation Prcess 2] Build Forward Propagation Prcess Using ExponentialMovingAverage'''def inference(input_tensor, regularizer_class, movingAvg_class): # 当没有提供滑动平均类时，直接使用参数当前的取值 if movingAvg_class == None: # 声明第一层神经网络的变量，并完成前向传播过程： with tf.variable_scope('layer1', reuse=False): # 使用 tf.get_variable 或 tf.Variable 创建变量（无区别），因为在训练或测试中没有在同一程序多次调用该函数。如果在同一程 # 序多次调用，需要在第一次调用之后将 reuse 参数设置为 True。 weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer_class) biases = tf.get_variable("biases", [LAYER1_NODE], initializer=tf.constant_initializer(0.1)) layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases) # 声明第二层神经网络的变量，并完成前向传播过程： with tf.variable_scope('layer2', reuse=False): weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer_class) biases = tf.get_variable("biases", [OUTPUT_NODE], initializer=tf.constant_initializer(0.1)) layer2 = tf.matmul(layer1, weights) + biases # 返回最终前向传播的结果 return layer2 # 使用 movingAvg_class.average 函数计算变量（参数）的滑动平均值 else: # Layer1 ：定义第一层神经网络的变量和前向传播过程 with tf.variable_scope('layer1', reuse=True): # 使用 tf.get_variable 或 tf.Variable 创建变量（无区别），因为在训练或测试中没有在同一程序多次调用该函数。如果在同一程 # 序多次调用，需要在第一次调用之后将 reuse 参数设置为 True。 weights = tf.get_variable("weights", [INPUT_NODE, LAYER1_NODE]) biases = tf.get_variable("biases", [LAYER1_NODE]) layer1 = tf.nn.relu(tf.matmul(input_tensor, movingAvg_class.average(weights)) + movingAvg_class.average(biases)) # Layer2 ：类似的定义第二层神经网络的变量和前向传播过程 with tf.variable_scope('layer2', reuse=True): weights = tf.get_variable("weights", [LAYER1_NODE, OUTPUT_NODE]) biases = tf.get_variable("biases", [OUTPUT_NODE]) # 计算隐藏层的 FP 结果，激活函数为 ReLU : layer2 = tf.matmul(layer1, movingAvg_class.average(weights)) + movingAvg_class.average(biases) return layer2 ### [3]. get_time_dif：获取已使用时间def get_time_dif(start_time): end_time = time.time() time_dif = end_time - start_time return timedelta(seconds=int(round(time_dif)))############################ Part 3 : Model Training Function ###########################def train(mnist): # 神经网络输入输出 x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='input_x') y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='input_y') # 定义存储训练轮数的变量，该变量无需计算滑动平均值。所以一般需要指定训练轮数变量为不可训练变量(trainable=False) global_step = tf.Variable(0, trainable=False) # 定义 L2 正则化损失函数（结构风险）： regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE) ## 1. avg_class = None ## # 计算当前参数下 ANN FP 的结果，这里调用计算滑动平均的类参数为 None ，所以不会使用参数的滑动平均值 y = inference(x, regularizer, None) ## 2. avg_class = variable_averages ## # 通过滑动平均衰减率和训练轮数变量，初始化滑动平均类。给定训练轮数可以加快训练早期变量的更新速度 variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) # 在所有（可训练，Except global_step）神经网络变量上使用滑动平均 # tf.trainable_variables 可返回 GraphKeys.TRAINABLE_VARIABLES 集合中元素，即未指定 trainable=False 的变量 variables_averages_op = variable_averages.apply(tf.trainable_variables()) # 计算使用了滑动平均之后的 FP 结果。滑动平均之后不会改变变量本身取值，会维护一个 shadow_variable 来记录其滑动平均值 movingAvg_y = inference(x, regularizer, variable_averages) ## 3. Loss Function ## # 定义交叉熵损失函数（经验风险）:该函数第二个参数需要提供的是正确答案的数字，tf.argmax(y_, 1)可以获取 y 对应的类别编号 cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1)) # 计算当前 batch 中所有样例的交叉熵平均值 cross_entropy_mean = tf.reduce_mean(cross_entropy) # 计算 Loss Function：经验风险和结构分析 loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses')) ## 4. ANN BP ## # 设置指数衰减的学习率:随着迭代进行，更新变量的学习率在这个基础上递减 learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, # 遍历一次所有训练数据需要的迭代次数 LEARNING_RATE_DECAY) # 使用优化器来优化损失函数 train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) # 在 ANN 中，每迭代一次数据就需要通过 BP 来更新 ANN 中参数，又要更新每一个参数的滑动平均值。Tensorflow提供了两种机制：tf.control_dependencie和tf.group： # train_op = tf.group(train_step, variables_averages_op) 等价于 == with tf.control_dependencies([train_step, variables_averages_op]): train_op = tf.no_op(name='train') # 检验使用了滑动平均模型的 ANN FP 结果是否正确 # correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) correct_prediction = tf.equal(tf.argmax(movingAvg_y, 1), tf.argmax(y_, 1)) # tf.cast(x, dtype) 会将x数据格式转化成 dtype 数据格式。这里，将 bool 型数值转为 float 后，x会变为0/1序列。 # 计算正确率 accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) ############################ Begin To Training ########################### config = tf.ConfigProto(#device_count=&#123;"CPU": 4&#125;, # limit to num_cpu_core CPU usage #inter_op_parallelism_threads = 1, #intra_op_parallelism_threads = 1, allow_soft_placement=True, log_device_placement=True ) with tf.Session(config=config) as sess: # 初始化所有变量 tf.global_variables_initializer().run() print('Training and evaluating...') start_time = time.time() # 准备验证数据集，一般用于在训练过程中大致判断停止 validate_feed = &#123;x: mnist.validation.images, y_: mnist.validation.labels&#125; # 准备测试数据集，一般用于评价模型优劣的标准 test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125; # 迭代训练神经网络 for epoch in range(TRAINING_STEPS): # 产生这一轮使用的一个 batch 的训练数据，并进行训练 xs, ys = mnist.train.next_batch(BATCH_SIZE) sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;) # 每 1000 次输出一次在验证数据上的测试结果 if epoch % 1000 == 0: # print('Epoch:', epoch + 1) ''' # 计算滑动平均模型在验证集上的结果。这里，由于 MNIST 数据集较小，故一次处理所有验证数据，没有将验证数据划分为更小的batch。 # 当神经网络模型比较复杂或者验证数据集比较大时，太大的 batch 会导致运行时间过长甚至发生内存溢出的 Error ''' loss_train, acc_train = sess.run([loss, accuracy], feed_dict=&#123;x: xs, y_: ys&#125;) loss_val, acc_val = sess.run([loss, accuracy], feed_dict=validate_feed) # todo time_dif = get_time_dif(start_time) msg = 'Iter: &#123;0:&gt;6&#125;, Train Loss: &#123;1:&gt;6.2&#125;, Train Acc: &#123;2:&gt;7.2%&#125;,' \ + ' Val Loss: &#123;3:&gt;6.2&#125;, Val Acc: &#123;4:&gt;7.2%&#125;, Time: &#123;5&#125;' print(msg.format(epoch, loss_train, acc_train, loss_val, acc_val, time_dif)) # 训练结束后，在测试数据集上检测 ANN 模型的最终正确率 test_acc = sess.run(accuracy, feed_dict=test_feed) print ("After %d training step(s), test accuracy " "using average Model is %g " % (TRAINING_STEPS, test_acc)) ############################ Main Function as the begin of program ###########################def main(arg=None): ###################### Functions for downloading and reading MNIST data. ###################### ''' ## 初始化：下载或读取用于训练、测试以及验证的 MNIST 手写数字图片（28px * 28px）数据集 ## MNIST 数据集分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。每一个 MNIST 数据单 元（数据对象）有两部分组成：一张包含手写数字的图片和一个对应的标签。比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。 mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。 mnist.train.labels 是一个形状为 [60000, 10] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的分类标签（one-hot vectors）。在此张量里的每一个元素，都表示某张图片对应分类的 one-hot vectors 向量，数值只有0和1 。 实际，read_data_sets 类会将数据从原始数据包格式解析成训练和测试神经网络时的数据格式 。read_data_sets会自动将 MNIST 数据集 划分为 train（55000）、test（10000） 以及 validation（5000）三个数据集。 ''' print("Loading training and validation data...") start_time = time.time() # 指定 MNIST 数据集的下载和读取的路径： MNIST_data_Path = "./MNIST_data/" mnist = input_data.read_data_sets(MNIST_data_Path, one_hot=True) # print mnist.train dataSet size : print("Training data size : ", mnist.train.num_examples) # print mnist.validation dataSet size : print("Validating data size : ", mnist.validation.num_examples) # print mnist.test dataSet size : print("Testing data size : ", mnist.test.num_examples) # print mnist.train.images[0] / mnist.train.labels[0] Format # print("Example training data[0] pix-matirx : ", "\n", mnist.train.images[0]) # print("Example training picture shape : " + str(len(mnist.train.images[0]))) # print("Example training data lable : ", mnist.train.labels[0]) # print("Example training lable shape : " + str(len(mnist.train.labels[0]))) time_dif = get_time_dif(start_time) print("Time usage:", time_dif) train(mnist) # TensorFlow 提供的一个主程序入口，tf.app.run 会调用上面定义的 main 函数：if __name__ == '__main__': tf.app.run() # main() 样例运行结果如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041Loading training and validation data.........Instructions for updating:Please use alternatives such as official/mnist/dataset.py from tensorflow/models.Training data size : 55000Validating data size : 5000Testing data size : 10000Time usage: 0:00:01Training and evaluating...Iter: 0, Train Loss: 2.6, Train Acc: 6.00%, Val Loss: 3.6, Val Acc: 11.18%, Time: 0:00:00Iter: 1000, Train Loss: 0.097, Train Acc: 95.00%, Val Loss: 0.16, Val Acc: 97.42%, Time: 0:00:02Iter: 2000, Train Loss: 0.081, Train Acc: 97.00%, Val Loss: 0.14, Val Acc: 97.98%, Time: 0:00:03Iter: 3000, Train Loss: 0.069, Train Acc: 100.00%, Val Loss: 0.13, Val Acc: 98.18%, Time: 0:00:05Iter: 4000, Train Loss: 0.064, Train Acc: 99.00%, Val Loss: 0.12, Val Acc: 98.28%, Time: 0:00:07Iter: 5000, Train Loss: 0.06, Train Acc: 100.00%, Val Loss: 0.12, Val Acc: 98.28%, Time: 0:00:08Iter: 6000, Train Loss: 0.058, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.30%, Time: 0:00:10Iter: 7000, Train Loss: 0.056, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.26%, Time: 0:00:12Iter: 8000, Train Loss: 0.052, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.26%, Time: 0:00:14Iter: 9000, Train Loss: 0.052, Train Acc: 100.00%, Val Loss: 0.11, Val Acc: 98.18%, Time: 0:00:16Iter: 10000, Train Loss: 0.049, Train Acc: 100.00%, Val Loss: 0.1, Val Acc: 98.26%, Time: 0:00:18Iter: 11000, Train Loss: 0.048, Train Acc: 100.00%, Val Loss: 0.1, Val Acc: 98.28%, Time: 0:00:20Iter: 12000, Train Loss: 0.047, Train Acc: 100.00%, Val Loss: 0.099, Val Acc: 98.26%, Time: 0:00:22Iter: 13000, Train Loss: 0.05, Train Acc: 100.00%, Val Loss: 0.099, Val Acc: 98.24%, Time: 0:00:24Iter: 14000, Train Loss: 0.046, Train Acc: 100.00%, Val Loss: 0.098, Val Acc: 98.26%, Time: 0:00:26Iter: 15000, Train Loss: 0.043, Train Acc: 100.00%, Val Loss: 0.097, Val Acc: 98.26%, Time: 0:00:28Iter: 16000, Train Loss: 0.044, Train Acc: 100.00%, Val Loss: 0.096, Val Acc: 98.24%, Time: 0:00:30Iter: 17000, Train Loss: 0.048, Train Acc: 100.00%, Val Loss: 0.095, Val Acc: 98.26%, Time: 0:00:32Iter: 18000, Train Loss: 0.045, Train Acc: 100.00%, Val Loss: 0.095, Val Acc: 98.28%, Time: 0:00:34Iter: 19000, Train Loss: 0.046, Train Acc: 100.00%, Val Loss: 0.094, Val Acc: 98.20%, Time: 0:00:35Iter: 20000, Train Loss: 0.043, Train Acc: 100.00%, Val Loss: 0.093, Val Acc: 98.26%, Time: 0:00:37Iter: 21000, Train Loss: 0.039, Train Acc: 100.00%, Val Loss: 0.092, Val Acc: 98.26%, Time: 0:00:39Iter: 22000, Train Loss: 0.037, Train Acc: 100.00%, Val Loss: 0.092, Val Acc: 98.32%, Time: 0:00:40Iter: 23000, Train Loss: 0.04, Train Acc: 100.00%, Val Loss: 0.091, Val Acc: 98.34%, Time: 0:00:42Iter: 24000, Train Loss: 0.044, Train Acc: 100.00%, Val Loss: 0.091, Val Acc: 98.26%, Time: 0:00:43Iter: 25000, Train Loss: 0.041, Train Acc: 100.00%, Val Loss: 0.091, Val Acc: 98.26%, Time: 0:00:45Iter: 26000, Train Loss: 0.038, Train Acc: 100.00%, Val Loss: 0.09, Val Acc: 98.28%, Time: 0:00:47Iter: 27000, Train Loss: 0.04, Train Acc: 100.00%, Val Loss: 0.09, Val Acc: 98.32%, Time: 0:00:48Iter: 28000, Train Loss: 0.037, Train Acc: 100.00%, Val Loss: 0.09, Val Acc: 98.30%, Time: 0:00:50Iter: 29000, Train Loss: 0.044, Train Acc: 100.00%, Val Loss: 0.09, Val Acc: 98.32%, Time: 0:00:52After 30000 training step(s), test accuracy using average Model is 0.9823 结语 我们在本文第一部分提出了一个非常有名的深度学习入门样例——MNIST 手写数字体图像识别。而第二部分就已经结合前面章节提到过的神经网络结构设计和参数优化的不同方法给出了一个完整的解决 TensorFlow 程序来解决 MNIST 手写数字图像识别的问题。 然而这个实现的可扩展性能不是很好。正如第三部分提到的，计算神经网络前向传播的函数需要将所有变量都传入，当神经网络的结构变得复杂、参数更多时，程序会有大量的冗余代码，可读性会变得非常差，编程效率低下。变量命名空间的引入合理的解决了这个问题。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>MNIST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深层神经网络和深度学习]]></title>
    <url>%2FTensorFlow%2F%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 读取须知： 上一章节，我们介绍了 TensorFlow 基本工作原理，并且给出了一个完整的 TensorFlow 样例来训练一个简单的三层全连接神经网络模型。本章我们将进一步介绍如何更加合理的设计和优化神经网络，使得学得的模型能够更好的对未知样本进行更加精确的预测。 这一章节我们总共安排了四个部分来进行学习： 第一部分：针对网络结构，介绍深度学习与深层网络 (DNN) 的概念，并给出一个样例来说明深层网络可以解决部分浅层网络解决不了的问题； 第二部分：针对损失函数，将介绍如何设定神经网络的优化目标（即：损失函数）。这一节将分别介绍分类问题和回归问题中比较常使用的几种经典的损失函数，以及如何通过自定义损失函数，使得神经网络优化的目标更加接近实际问题的需求； 第三部分：针对反向传播算法，将更加详细的介绍神经网络的 BP 算法，并且给出一个 TensorFlow 实现反向传播过程的通用框架； 第四部分：针对神经网络优化，将介绍在神经网络优化过程中经常遇到的几个问题，并给出解决方案。 1. 深度学习和深层神经网络维基百科中对深度学习的精确定义为：“一类通过多层非线性变换对高复杂性数据建模算法的合集”。 深层神经网络是实现“多层非线性”最常用的一种方法，所以在实际中基本上可以认为深度学习就是深层神经网络的代名词。从上面描述可以看出深度学习的两个重要的特性： 多层 和 非线性 。为什么要强调这两个性质呢？！下面我们将通过具体样例来说明这两点在对复杂问题建模时是缺一不可的！ 1.1 线性模型的局限性这一小节我们先介绍线性变换存在的问题？以及为什么要在深度学习中强调“复杂问题”？既然线性模型存在问题，那么我们如何解决（去线性化）？ 在线性模型中，模型的输出为输入的加权和。假设一个模型的输出 $ y $ 和输入 $ x_i $ 满足以下关系，那么这个模型就是一个线性模型： $$ y = \sum_{i=0}w_ix_i + b $$ 在一个线性模型中，通过输入 $ x_i $ 得到输出 $y$ 的函数称之为一个 线性变换。上面的公式就是一个线性变换。线性模型的最大特点就是 任意线性模型的组合仍然还是线性模型。 可以发现，前面章节我们实现的三层全连接网络的前向传播过程就是一个线性模型，前向传播的计算公式： $$ a^{(1)} = xW^{(1)}, y = a^{(1)}W^{(2)} $$ 其中 $ x $ 为输入，$ W $ 为参数。由上述关系推导得： $$ y = (xW^{(1)}W^{(2)}) = x(W^{(1)}W^{(2)}) = xW’ $$ 可见，推导过程中 $ W^{(1)}W^{(2)} $ 被表示为一个新的参数（权重矩阵） $ W’ $，计算过程如下: $$ W’ = W^{(1)}W^{(2)} = \left[ \begin{array} {cccc}W_{1,1}^{(1)} &amp; W_{1,2}^{(1)} &amp; W_{1,3}^{(1)}\\W_{2,1}^{(1)} &amp; W_{2,2}^{(1)} &amp; W_{2,3}^{(1)}\\\end{array} \right]\left[ \begin{array} {cccc} W_{1,1}^{(2)}\\ W_{2,1}^{(2)}\\ W_{3,1}^{(2)}\\\end{array} \right] = \left[ \begin{array} {cccc} W_{1,1}^{(2)}W_{1,1}^{(2)}+W_{1,2}^{(1)}W_{2,1}^{(2)}+W_{1,3}^{(1)}W_{3,1}^{(2)}\\ W_{2,1}^{(2)}W_{1,1}^{(2)}+W_{2,2}^{(1)}W_{2,1}^{(2)}+W_{2,3}^{(1)}W_{3,1}^{(2)}\end{array} \right] \\= \left[ \begin{array} {cccc} W_1’\\ W_2’\end{array} \right] $$ 这样输入和输出的关系就可以表示为： $$ y = xW’ = \left[ \begin{array} {cccc} x_1 &amp; x_2\end{array} \right]\left[ \begin{array} {cccc} W_1’\\ W_2’\end{array} \right] = \left[ \begin{array} {cccc} W_1’x_1+W_2’x_2\end{array} \right] $$ 可以看出，这个前向传播算法完全符合线性模型的定义。虽然这个神经网络有两层（不算输入层，输入层节点为非神经元节点），但它和单层的神经网络并没有本质区别。 我们可以以此类推：只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别，而且线性模型能够解决的问题是有限的（只能解决线性可分问题）。 至此，我们已经知道了线性变换存在的问题：表达能力有限以及解决问题是有限的。上面我们已经说明了其表达能力有限性，下面我们给出一个样例来看为什么线性模型解决的问题是有限的： 还记得上一章节我们 PLAY 过的 TensorFlow 游乐场么？你将默认设置下网页顶部激活函数（Activation）一栏选择为线性（Linear），这和我们实现的三层神经网络结构是基本一致的，然后开始训练，你会发现当前设置下线性模型是无法解决这个二分类任务的。如果我们将默认数据集更换成第三个（线性可分数据集）便可以对数据集做区分了，也就是说线性模型只能用来解决线性可分问题。 我们再来说明深度学习中为什么强调高复杂问题：数据至少是无法通过直线（或者高维平面平面：线性模型中，当输入为 n 个时，x 和 y 就会形成 n+1 维空间中的一个平面）划分的问题。而实际问题往往都是复杂问题。 最后，我们再来看深度学习的定义：事实上，强调“非线性”、“高复杂问题”其实是很有道理的！既然线性模型存在上述阐述的局限性，那么如何解决这个问题呢？？？ 1.2 激活函数去线性化（Activation Function）前面我们已经提到过了激活函数，并且通过 TensorFlow 游乐场看到了它的神奇作用。这一小节，我们来介绍激活函数如何工作？ 我们知道神经元结构的输出为所有输入的加权和，这导致了整个神经网络是一个线性模型。那么如何对这个线性模型去线性化（转化为非线性模型）呢？其实很简单，我们只需要让每个神经元的输出再通过一个非线性函数（激活函数），那么整个神经网络模型就不再是线性的了。下面给出了加入偏置项以及激活函数的神经元结构： 除了激活函数，还增加了一个新的参数–偏置项（bias）。偏置项是神经网络中非常常用的一种结构，对应线性模型中的平移变换，可以帮助模型更好的拟合数据集。 下面我们来看加入激活函数和偏置项之后的神经网络结构的前向传播算法实现原理： $$ A_1 = [a_{11},a_{12},a_{13}] = f(xW^{(1)} + b) = f([x_1, x_2]\left[ \begin{array} {cccc}W_{1,1}^{(1)} &amp; W_{1,2}^{(1)} &amp; W_{1,3}^{(1)}\\W_{2,1}^{(1)} &amp; W_{2,2}^{(1)} &amp; W_{2,3}^{(1)}\\\end{array} \right] + \left[ \begin{array} {cccc} b_1 &amp; b_2 &amp; b_3\end{array} \right] )\\ = f([W_{1,1}^{(1)}x_1+W_{2,1}^{(1)}x_2+b_1, W_{1,2}^{(1)}x_1+W_{2,2}^{(1)}x_2+b_2, W_{1,3}^{(1)}x_1+W_{2,3}^{(1)}x_2]+b_3) \\ = [f([W_{1,1}^{(1)}x_1+W_{2,1}^{(1)}x_2+b_), f(W_{1,2}^{(1)}x_1+W_{2,2}^{(1)}x_2+b_2), f(W_{1,3}^{(1)}x_1+W_{2,3}^{(1)}x_2]+b_3)] $$ 下面展示几种常用的非线性激活函数图像： TensorFlow 提供了对上述非激活函数的支持：tf.nn.relu()、tf.sigmoid() 和 tf.tanh()。除此之外 TensorFlow 还支持自定义激活函数。以下代码展示了 TensorFlow 使用 Relu 激活函数实现三层神经网络的前向传播算法： $$ a = tf.nn.relu( tf.matmul(x, w1) + biases1)\\y = tf.nn.relu( tf.matmul(w1, w2) + biases2) $$ 1.3 多层（深层）网络解决异或运算上一小节我们详细讲解了线性变换问题。这一小节我们将通过实际问题来讲解深度学习的另一个重要的性质：多层变换。 在神经网络的发展史上，一个很重要的问题就是异或问题。 何为异或问题？：直观来说就是如果两个输入的符号相同时（同时为正或同时为负）则输出 0，否则（一正一负）输出 1。 神经网络的理论模型最初是由 Warren McCulloch 和 Walter Pitts 于 1943 年提出，并在 1958 年由 Frank Tosenblatt 提出了感知机模型，从数学上完成了对神经网络的精确建模。感知机可以简单理解为单层的神经网络，它会先将输入进行加权和，然后通过激活函数最终得到输出（没有隐藏层的神经网络结构）。上世纪 60 年代，神经网络作为对人类大脑的模拟算法受到了广泛的关注。 然而到了 1969 年， Marvin Minsky 和 Seymour Papert 提出感知机是无法模拟异或运算的，这里我们不对其复杂的数学求证过程做推导。，但我们可以通过 TensorFlow 游乐场来模拟一下通过感知机的结构来模拟异或运算： 可以看到，感知机模型是无法对数据集做出有效区分的。也就是说感知机无法模拟异或运算。而当加入隐藏层之后，我们发现异或问题可以得到很好的解决。这是由于隐藏层中的神经元节点可以被认为从输入的特征向量中提取出了更高维的数据特征。 结论： 实际上，深层神经网络有组合特征提取的功能，对于解决不易提取特征向量的问题（如图像识别、语音识别等）有很大的帮助，这也是深度学习在这些问题上更加容易取得突破性进展的重要原因！ 2. 损失函数（Loss Function）上一小节我们针对神经网络结构进一步了解了深度学习的概念。本节我们将介绍如何刻画不同神经网络模型的效果。神经网络模型的效果以及优化目标是通过损失函数来定义的。我们可以通过损失函数是否收敛用于评价一个模型在当前数据集下是否达到最佳。 2.1 经典损失函数这一小节我们将介绍适用于分类问题和回归问题的经典损失函数，并通过 TensorFlow 实现这些损失函数。分类问题和回归问题是监督学习两大种类，下面我们将分别做简单介绍： 2.1.1 分类问题经典损失函数分类问题希望解决的是：将不同的数据样本划分到事先定义好的类别中。比如我们之前章节接触的二分类问题，需要将样本零件划分到合格或不合格两个类别中。以及后面我们还会介绍到的手写数字识别问题，它可以被归纳为一个十分类问题。 在解决判断零件是否合格的二分类问题时，我们认为当最终输出节点值越接近 0，这个样本可能不及格；反之越接近 1 则越有可能合格。为了给出集体的类别结果，我们可以取 0.5 为阈值。我们认为凡是输出大于 0.5 的样本都认为时合格的，反之不合格。但是这样的做法时是不容易推广到多分类的。虽然理论上对于多分类设置多个阈值是可能的，但在解决实际问题的过程一般不会这么处理。 通过神经网络解决多分类问题常用的方法 是：设置 $n$（n 为类别数目） 个输出节点。对于每一个样例，神经网络都可以得到一个 $n$ 维数组（数组中每一维[每一个输出节点]对应一个类别）作为输出结果。理想情况下，如果一个样本属于类别 $k$，那么这个类别所对应的输出节点值应该为 1 ，其它的均为 0。 这里我们结合一个样例说明：以手写数字图像识别为例，数字图片需要被分类到 0~9 $10$ 个数字类别中。我们设置 $n = 10$，对于每一个手写数字图片，通过神经网络后都可以得到一个 $10$ 维（长度为 10）的数组，每一个维度都对应了 0~9 中数字中的一个。如果某个样本属于数字类别 1，那么输出的数组理想情况应该是：[0,1,0,0,0,0,0,0,0,0]，神经网络的输出越接近它表示结果越好。对于这个神经网络模型我们优化的目标是，让神经网络的输出向量尽可能的接近真实标注期望的向量。 基于上述的情况，那么我们如何判断一个输出向量和期望的向量有多接近呢？交叉熵损失函数是常用的一种评价方法。 交叉熵刻画了两个概率分布之间的距离，它是分类问题最常使用的一种损失函数。 –&gt; 交叉熵损失函数（Cross Entropy）以及 TensorFlow 实现样例 1）交叉熵损失函数 交叉熵是信息论中的概念，这里不讨论其原本的意义，这一小节主要讲解它对于评估分类效果的意义。假设给定两个概率分布 $ p $ 和 $ q $，这里我们通过 $ q $ 来表示 $ p $ 的交叉熵： $$ H(p,q) = - \sum_{x}p(x)log(q(x)) $$ 需要注意的是：交叉熵刻画的是两个概率分布之间的距离，然后神经网络的输出却不一定是一个概率分布。（概率分布刻画的是不同事件发生的概率。由概率论知识可知，当事件总数是有限的前提下，概率分布的和是 1） Softmax 回归引入： 那么，我们如何将神经网络的前向传播结果转换成一个概率分布呢？Softmax 回归是一个非常常用的方法。Softmax 回归本身可以作为一个学习算法来优化分类结果，但在 TensorFlow 中，Softmax 回归的参数被去掉了，它只是一层额外的处理层，作用是将神经网络的输出变成一个概率分布。 下面给出加上 softmax 回归的神经网络示意图： 根据上图，假设原始的神经网络的输出为 $ y_1,y_2,…,y_n $，那么经过 Softmax 回归处理之后的输出为： $$ softmax(y)_i = y_i’ = \frac{e^{yj}}{\sum_{j=1}^{n}e^{yj}} $$ 由上可知，神经网络输出经过一个 Softmax 回归变换成一个概率分布，接下来就可以通过交叉熵来刻画概率分布之间的距离了。 返回来继续看交叉熵，从交叉熵公式中可以看出交叉熵函数不是对称的（$ H(p,q) \ne H(q,p) $）。因为正确答案是期望得到的，所有当交叉熵作为损失函数时，$ p $ 代表答案标注，$ q $ 代表预测值。它刻画的是概率分布 $ q $ 和 $ p $ 的距离，交叉熵越小，概率分布越接近。 下面我们给出一个三分类样例来说明通过交叉熵确实可以判断期望值和预测值之间的距离： 假设某个样例的标注是 (1,0,0)。当某模型经过 Softmax 回归之后的预测值是 (0.5,0.4,0.1),那么这个预测值和正确答案之间的交叉熵为： $$ H((1,0,0),(0.5,0.4,0.1)) = -(1*log0.5 + 0*log0.4 + 0*log0.1) \approx 0.3 $$ 另外一个模型经过 Softmax 回归之后的预测值是 (0.8,0.1,0.1),那么这个预测值和正确答案之间的交叉熵为： $$ H((1,0,0),(0.8,0.1,0.1)) = -(1*log0.8 + 0*log0.1 + 0*log0.1) \approx 0.1 $$ 从直观上很容易知道第二个模型的答案要优于第一个答案。通过交叉熵的计算得到的结果也是一致的。 2）TensorFlow 实现样例 上面我们已经了解了交叉熵的概念，并且在之前的章节中我们介绍过 TensorFlow 中定义的交叉熵损失函数。这里我们来看其是如何实现的，下面先给出其代码实现： cross_entropy = -tf.reduce_mean( input_y * tf.log( tf.clip_by_value(y, 1e-10, 1.0)) ) 其中，input_y 代表输入的正确答案（真实标记），$y$ 代表预测结果。下面来看涉及到的几个功能函数： –&gt; [2.1] tf.clip_by_value() 函数可以将一个张量中的数值限定在一个范围之内，用于给出函数的有效域，可以避免一些运算错误（比如：log0 是无效的）。下面给出一个简单的运算样例： import tensorflow as tf v = tf.constant([ [1.0,2.0,3.0],[4.0,5.0,6.0] ]) sess = tf.InteractiveSession() print (tf.clip_by_value(v, 2.5, 4.5).eval()) 样例输出结果如下: [[2.5 2.5 3. ] [4. 4.5 4.5]] 可以看出，输入的常量 $v$ 中的数值被限制到了 $ v[i][j]\in(2.5，4.5) $,保证了在进行 tf.log() 函数时不会产生错误。 –&gt; [2.2] tf.log() 函数用于对张量中的所有元素依次计算其对数值，下面给出一个简单的运算样例： v = tf.constant([1.0,2.0,3.0]) print (tf.log(v).eval()) 样例输出结果如下: [0. 0.6931472 1.0986123] –&gt; [2.3] * 运算符用于将两个矩阵中的元素进行对应相乘，也可以称为数乘（注意与点乘的区别）。矩阵的乘法通过 tf.matmul() 函数来实现。下面给出一个样例来看一下数乘和点乘的区别： 12345v1 = tf.constant([[1.0,2.0],[3.0,4.0]])v2 = tf.constant([[4.0,5.0],[6.0,7.0]])print ((v1*v2).eval())print (tf.matmul(v1,v2).eval()) 样例输出结果如下: [[ 4. 10.] [18. 28.]] [[16. 19.] [36. 43.]] 通过上述三个运算已经可以完成了每一个样例的每一个类别的交叉熵 $ p(x)log(q(x)) $ 的计算。这三部的计算的结果是一个 $n*m$ 阶的二维矩阵，其中 n 为一个 batch 中的样本数，m 为分类的类别数。 根据交叉熵公式，应该将每行中的 m 个结果相加得到所有样例的交叉熵，然后再对这 n 行取平均值得到一个 batch 的平均交叉熵。但是，由于分类数是不变的，即每一行都是 n 个分类，所以等价于对整个矩阵做平均，下面给出 TensorFlow 中对矩阵做平均的运算： 1234v3 = tf.constant([[1.0,2.0],[3.0,4.0]])print (v3.eval())print (tf.reduce_mean(v3).eval()) 样例输出结果如下: [[1. 2.] [3. 4.]] 2.5 由于交叉熵一般会与“Softmax”回归一起使用，TensorFlow 给出了一个统一函数 tf.nn.softmax_cross_entropy_with_logits() 用来封装 Softmax 回归和交叉熵。以下代码给出了使用了 Softmax 回归 之后的交叉熵损失函数： # Old Version: # cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y, input_y) # New Version: cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=input_y) 注意，在只有一个正确答案的分类问题中（可以表示为：one-hot），TensorFlow提供了 tf.nn.sparse_softmax_cross_entropy_with_logits() 函数来加速计算过程。 2.1.2 回归问题经典损失函数上一小节我们已经知道分类问题希望解决的是：将不同的数据样本划分到事先定义好的类别中。而回归问题需要预测的不是一个事先定义好的类别，而是连续的实数，是对具体数值的预测（比如：房价预测、销量预测）。解决回归问题的神经网络一般只有一个输出节点，这个节点的输出值就是预测值。 对于回归问题，最常用的损失函数是均方误差（MSE:mean squared error）。公式定义如下： $$ MSE(y,y’) = \cfrac{\sum_{i=1}^{n}(y_i-y_i’)^2}{n} $$ 其中，$y_i$ 为 batch 中第 i 个数据的正确答案，而 $y_i’$ 是神经网络给出的预测值。下面给出使用 TensorFlow 来实现均方误差损失函数： $$mse = tf.reduce\_mean(tf.square(y - y’))$$ 注意，这里的 - 运算符是两个矩阵中对应元素做差值。 2.2 TensorFlow 自定义损失函数TensorFlow 中不仅支持经典的损失函数，还可以优化任意的自定义损失函数。下面将介绍如何通过自定义损失函数的方法来使得神经网络优化的结果更加接近实际问题的需求。这里会给出一个预测商品销量问题的样例： loss = tf.reduce_sum(tf.select( tf.greater(v1,v2), (v1-v2) * a, (v1-v2) * b)) 1）比较函数 tf.greater(v1,v2)：tf.greater(v1,v2) 的输入是两个张量，函数会比较两个张量每一个元素的大小，返回操作结果。当输入的张量维度不一样时，TensorFlow 会进行类似的 Numpy 广播操作（broadcasting）的处理。 2）选择条件函数 tf.select()：当选择条件为 True 时，tf.select() 函数会选择第二个参数中的值，否则使用第三个参数中的值。注意 tf.select() 函数判断和选择都在元素级别进行。 下面代码给出了上述两个函数的用法： 123456789101112131415import tensorflow as tfv1 = tf.constant([1.0,2.0,3.0,4.0])v2 = tf.constant([4.0,3.0,2.0,1.0])sess = tf.InteractiveSession()print (tf.greater(v1,v2).eval())# AttributeError: module 'tensorflow' has no attribute 'select# print (tf.select( tf.greater(v1,v2), v1, v2).eval())# 原因：新版本 TensorFlow API的名称做了改变，这个选择操作的tf.select()被改为tf.where()。print (tf.where( tf.greater(v1,v2), v1, v2).eval())sess.close() 样例输出结果如下: [False False True True] [4. 3. 3. 4.] 2.3 损失函数对神经网络模型的影响上面章节我们也就了解了 TensorFlow 中经典损失函数以及自定义损失函数的使用，也明白了一个损失函数使用的重要性。我们可以通过 TensorFlow 来定义不同的损失函数，那么对于一个模型来说，损失函数到底会有怎样的影响？下面我们将通过一个样例来说明选择一个合适的模型对训练结果影响的重要性。这里我们实现一个拥有两个输入节点，一个输出节点，没有隐藏层的神经网络结构： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import tensorflow as tffrom numpy.random import RandomStatebatch_size = 8input_x = tf.placeholder(tf.float32, shape=(None,2), name="input_x")input_y = tf.placeholder(tf.float32, shape=(None,1), name="input_y")w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))y = tf.matmul(input_x, w1)loss_less = 10loss_more = 1define_loss = tf.reduce_sum(tf.where( tf.greater(y,input_y), (y-input_y) * loss_more, (input_y-y) * loss_less))learning_rate = 0.001train_op = tf.train.AdamOptimizer(learning_rate).minimize(define_loss)# 通过随机数生成一个模拟数据集：rdm = RandomState(1)dataset_size = 128 # 训练数据集样本数目X = rdm.rand(dataset_size, 2)# 定义规则给出样本的标签（x1 + x2 + noise）：# noise 属于 -0.05 ~ 0.05 的值Y = [ [x1 + x2 + rdm.rand()/10.0-0.05] for (x1, x2) in X ]# 下面创建一个会话来运行程序：with tf.Session() as sess: init_op = sess.run(tf.global_variables_initializer()) # 打印训练之前的神经网络参数： print (sess.run(w1)) # 开始训练： # 定义训练轮数： STEPS = 5000 for i in range(STEPS): # 每次选取一个 batch 的数据进行训练： start = (i * batch_size) % dataset_size end = min(start + batch_size, dataset_size) data_feed = feed_dict=&#123;input_x: X[start:end], input_y: Y[start:end]&#125; # 训练神经网络参数 sess.run(train_op, data_feed) print (sess.run(w1)) 样例输出结果如下: [[-0.8113182] [ 1.4845988]] [[1.019347 ] [1.0428089]] 标准答案为：x1 + x2 + noise。此时的参数下，模型更加偏向于预测多一点。如果将 loss_less 和 loss_more 和值互换，那么最终 w1 = [0.95525807, 0.9813394]，此时模型更加偏向预测少一点。如果使用均方误差作为损失函数，w1 = [0.97437561, 1.0243336]，会让预测值离标准答案更近。 结论： 相同的神经网络，不同损失函数的使用会对训练得到的模型产生重要的影响！ 3. 神经网络优化算法（BP）这一小节我们将针对神经网络学习（优化）算法，更加具体地介绍如何通过反向传播算法和梯度下降（Gradient Decent）算法优化神经网络中的参数。梯度下降算法主要用于优化单个参数的取值，而反向传播算法给出了一个高效的方式在所有参数上使用梯度下降算法。 反向传播传播算法是学习神经网络的核心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使得神经网络模型在训练数据集上的损失函数达到一个较小值。本小节，我们将主要讲解神经网络优化过程的基本概念和主要思想，而略去算法的数学推导和证明。 神经网络的优化过程可以分为两个阶段： 第一阶段先通过前向传播算法计算得到预测值，并将预测值和真实值做对比得出两者之间的差距（loss）; 第二阶段通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数，下面我们会做详细说明。 3.1 梯度下降算法首先我们给出一个图解来探讨使用梯度下降算法优化参数取值的过程： 结合图解，这里我们再通过一个具体样例来说明梯度下降算法是如何工作的。假设要通过梯度下降算法来优化参数 $\theta$ ,使得损失函数 $J(X)=\theta^2$ 的值尽量小。梯度下降算法的第一步需要随机产生一个参数 $\theta$ 的初始值，然后再通过梯度和学习率来更新参数 $\theta$ 的取值。 样例中参数 $\theta$ 的梯度为：$\nabla=\cfrac{\partial J(\theta)}{\partial x} = 2\theta$，所以 BP 中每次使用梯度下降算法对参数 $\theta$ 的更新公式为： $$ x_{n+1} = x_n - \eta \nabla $$ 假设参数 $\theta$ 初始值为 5，学习率设置为 0.3，那么优化过程可以总结为下表： 轮数 当前轮参数值 梯度×学习率 更新后参数值 1 5 2×5×0.3=3 5-3=2 2 2 2×2×0.3=1.2 2-1.2=0.8 3 0.8 2×0.8×0.3=0.48 0.8-0.48=0.32 4 0.32 2×0.32×0.3=0.192 0.32-0.192=0.128 5 0.128 2×0.128×0.3=0.0768 0.128-0.0768=0.0512 结论： 可以发现，经过 5 迭代后，参数 $\theta$ 取值已变为 0.0512，和参数最优值 0 已经很接近了。这个样例虽然比较简单，但神经网络反向传播算法根据梯度和学习率使用梯度下降算法更新参数过程是类似的。 3.1.1 梯度下降法存在的问题1）局部最优解 需要注意的是： 梯度下降算法并不能保证被优化的函数达到全局最优解 如下如所示，图中给出的函数就有可能只能得到局部最优解而不是全局最优。在图中标记点处（小黑点），损失函数的偏导为 0，于是参数就不会在进一步更新了。 结论： 梯度下降是否能够达到全局最优取决于：待优化参数初始值落在哪个区域。参数初始值的选取很大程度影响最后的优化结果。只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优。 2）耗时 梯度下降法除了不一定能达到全局最优外，另一个存在的问题就是计算时间太长。因为要在全部训练数据上最小化损失，也就是说所有损失函数 $ J(\theta) $ 是在所有的训练数据上的损失和达到最小。这样在每一轮的迭代中都需要计算在全部训练数据上的损失。在海量训练训练数据下，要计算所有训练数据的损失函数是极其耗时的。 为了加速训练过程，提出了随机梯度下降算法（Stochastic Gradient Decent：[stəˈkastik ˈgrādēənt ˈdēsənt]，SGD）: 3.2 随机梯度下降算法（SGD）随机梯度下降算法 SGD 优化的不是在全部训练数据上的损失函数，而是在每一轮迭代中随机优化某一条训练数据上的损失函数，这样每一轮参数更新的速度就大大加快了。 但是由于 SGD 算法每次优化的只是在某一条数据上的损失函数，所以问题也非常明显：在某一条数据上损失函数更小并不能代表在全部数据上的损失函数更小，于是 使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优。 为了综合梯度下降算法和随机梯度下降算法的优缺点，在实际应用中提出了一种折中的办法：Mini Batch SGD。 3.3 Mini Batch SGDMini Batch SGD 算法的优化过程是：每次计算一小部分训练数据的损失函数，也就是我们之前提到的一个 batch 的数据。通过矩阵运算，每次在一个 batch 的数据上优化神经网络的参数并不会比单个数据慢太多；另一方面，每次使用一个 batch 的数据可以大大减少损失函数收敛所需要的迭代次数，同时可以使得收敛的结果更加接近梯度下降的效果。 注意： 当神经网络模型比较复杂或者数据本身（大尺寸图像）比较大，太大的 batch 会导致计算时间过程过长甚至发生内存溢出！ 3.4 TensorFlow 实现反向传播过程的通用框架综合上述，下面给出 TensorFlow 实现反向传播过程的通用框架： 1234567891011121314151617181920212223242526272829303132333435import tensorflow as tffrom numpy.random import RandomStatebatch_size = 8input_x = tf.placeholder(tf.float32, shape=(None,2), name="input_x")input_y = tf.placeholder(tf.float32, shape=(None,1), name="input_y")w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1)).....define_loss = ...learning_rate = 0.001train_op = tf.train.AdamOptimizer(learning_rate).minimize(define_loss)with tf.Session() as sess: init_op = sess.run(tf.global_variables_initializer()) # 打印训练之前的神经网络参数： print (sess.run(w1)) # 开始训练： # 定义训练轮数： STEPS = 5000 for i in range(STEPS): # 每次选取一个 batch 的数据进行训练： start = (i * batch_size) % dataset_size end = min(start + batch_size, dataset_size) data_feed = feed_dict=&#123;input_x: X[start:end], input_y: Y[start:end]&#125; # 训练神经网络参数 sess.run(train_op, data_feed) 4. 神经网络进一步优化这一小节，我们将针对神经网络优化过程可能遇到的问题，进一步优化神经网络。例如：如何设置梯度下降算法的学习率？如何解决过拟合，使得我们训练好的模型更加泛化？以及如何使用滑动平均模型使得学得的模型更加健壮，滑动平均模型可以一定程度上提高模型在最终测试数据集上的表现（增强模型效果）。 4.1 学习率的设置上一小节我们在讲解梯度下降算法时提到，训练（优化）神经网络模型需要通过学习率来控制参数更新的速度（幅度）。那么考虑一个问题：我们是否只需要设置一个较大的学习率，就可以让模型在很短时间内达到局部最优？ 答案肯定是否定的！学习率决定了参数每次更新的幅度。如果幅度过大，那么可能导致参数在极优值两侧来回移动，无法收敛到一个极小值。相反，如果学习率过小，每次参数更新幅度较小，虽然能够保证收敛性，但这会大大降低参数的优化（训练）速度。模型需要更多轮的迭代才可以达到一个比较理想的结果。下面来看一个实例： 还是以梯度下降算法介绍中给出的样例为例。假设：参数 $\theta$ 初始值为：5 当学习率为：0.001 时，迭代 5 次之后，$\theta$ 的取值将为 4.95。要将 $\theta$ 训练到 0.05 需要大约 2300 轮； 当学习率为：0.3 时，只需要迭代 5 轮就可以达到。 综上所述：如何更加合理的设置学习率取值为对保证模型优化的高效和精确是必要的！ 4.1.1 指数衰减法为了解决学习率的设定问题，TensorFlow 提供了一种更加灵活的学习率设置方法：指数衰减法。 TensorFlow 通过 tf.train.exponential_decay() 函数实现了指数衰减学习率。这个函数会指数级地减少学习率，让模型在训练前期快速接近较优解，又保证在模型训练后期不会波动太大（更加稳定），从而接近局部最优。 1）tf.train.exponential_decay() 函数 12345678tf.train.exponential_decay( learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None) 上述函数实现了以下代码的功能： decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) tf.train.exponential_decay() 参数说明: learning_rate：初始学习率 global_step：全局迭代次数（总迭代次数） decay_steps：衰减速度 decay_rate：衰减系数 staircase：衰减方式（阶梯式衰减 or 连续衰减） name：String. Optional name of the operation. Defaults to ‘ExponentialDecay’. |—————————————————————— staircase 参数详解： tf.train.exponential_decay() 函数可以通过设置参数 staircase 来选择不同的衰减方式。如果参数 staircase 为 True，(global_step / decay_steps) 结果会取整，此时学习率成为阶梯函数( staircase function )。 下图展示的连续的学习率曲线是 staircase 为 False，阶梯状曲线是 staircase 为 True： 当 staircase 设置为 True 时，decay_steps 通常代表完整的使用一遍训练数据所需要的迭代轮数。这时，这个迭代轮数也就是总训练样本数除以每一个 batch 中的训练样本数（即：batch_size）。这种设置的常用场景是：每完整的过完一遍训练数据，学习率就减少一次。 优点：可以使得训练数据集中的所有数据对模型训练有相等的作用。 当 staircase 默认为 False 时，即连续衰减学习率。不同的训练数据有不同的学习率，只有当学习率减小时，对应的训练数据对模型的训练结果的影响也就小了。 |—————————————————————— 下面将给出一个使用了指数衰减学习率的优化器样例： 123456789101112131415global_step = tf.Variable(0, trainable=False)starter_learning_rate = 0.1learning_rate = tf.train.exponential_decay( starter_learning_rate, global_step, 100000, 0.96, staircase=True)# Passing global_step to minimize() will increment it at each step.learning_step = ( tf.train.GradientDescentOptimizer(learning_rate) .minimize(...my loss..., global_step=global_step) 注意： 学习率决定了参数更新（损失函数下降）的速度。一般来说，初始学习率、衰减学习率以及衰减速度都是根据经验设置的。但是损失函数下降的速度和迭代结束之后总损失的大小没有必然的联系。也就是说，并不能通过前几轮损失函数下降的速度来比较不同神经网络模型的效果。 4.2 过拟合问题前面我们了解了如何在训练数据上优化一个给定的损失函数。然而在真实的应用中想要的并不是单纯让模型尽量模拟（拟合）训练数据的行为，而是希望通过训练出来的模型对未知数据也能够给出很好的预测。 所谓的过拟合：指的是当一个模型过为复杂之后，它可以很好的记忆每一个训练数据中的随机噪音的部分而忘记了去“学习”训练数据中的通用趋势。 举一个极端的样例：如果一个模型中的参数比训练数据的总数还多（一般样本数维持在参数量的10倍），那么只要训练数据不冲突，这个模型完全可以记住所有训练数据的结果从而使得损失函数为 0。可以直观的想象一个包含 n 个变量和 n 个等式的方程组：当方程不冲突时，这个方程组是可以通过数学方法求解的。然而过度拟合训练数据的随机噪音虽然可以得到非常小的损失，但是对于未知数据可能无法做出可靠的判断。 下图给出了模型训练的三种不同情况： 第一种情况下，由于模型过于简单，无法刻画问题的趋势。属于欠拟合，模型在训练数据集的准确率较低； 第二张模型是比较合理的，它既不会过于关注训练数据中的噪音，又能很好的刻画问题的整体趋势（理想情况）； 第三种模型就是过拟合了，虽然第三个模型完美地划分了不同形状的点，但这样的划分并不能很好地对未知数据做出判断，因为它过度拟合了训练数据中的噪音数据而忽略了问题的整体规律（测试集准确率远远跟不上训练集测试集）。 4.3 正则化为了避免过拟合问题 前面章节提到过使用 TensorFlow 可以优化任意形式的损失函数，所以 TensorFlow 自然也可以优化任何带正则化的损失函数。以下代码给出了一个简单的带 $L2$ 正则化的损失函数定义: 123456w = tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))y = tf.matmul(x,w)# loss = 经验风险最小化 + 结构风险最小化loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda)(w) #lambda为正则化权重 实际过程中 lambda 为关键字 loss 定义为损失函数，由两个部分组成：第一个部分是均方误差损失函数，刻画模型在训练数据上的表现。第二部分就是正则化，防止模型过度模拟训练数据中的随机噪声。类似的, tensorflow.contrib.layers.l1_regularizer(lambda)(w) 函数可以计算 $L1$ 正则化的值。以下代码给出了使用这两个函数的样例： 123456789import tensorflow as tfweights = tf.constant([[1.0, -2.0], [-3.0, 4.0]])with tf.Session() as sess: # 输出为：(|1| + |-2| + |-3| + |4|) * 0.5 = 5。其中 0.5 为正则化项的权重。 print (sess.run(tf.contrib.layers.l1_regularizer(.5)(weights))) # 输出为：(1^2 + (-2)^2 + (-3)^2 + 4^2) * 0.5 = 7.5。 print (sess.run(tf.contrib.layers.l2_regularizer(.5)(weights))) 样例输出结果为： 125.07.5 在简单的神经网络中，上述代码可以很好地计算带正则化的损失函数，但当神经网络的参数增多之后，这样的方式可能导致 loss 函数定义可读性变差。更主要的是导致：网络结构复杂之后定义网络结构的部分和计算损失函数的部分可能不在同一函数中，这样通过变量这样方式计算损失函数就不方便了。 下面我们给出一个使用 TensorFlow 中给提供的集合(Collection)解决一个 5 层神经网络带 $L2$ 正则化 的损失函数计算方法（网络结构的部分和计算损失函数的部分不在同一函数）: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import tensorflow as tf# Part 1：定义神经网络结构以及前向传播算法：# 获取单层神经网络 Edge 上的权重，并且将这个权重的 L2 正则化损失加入到名称为 'losses' 的集合中：def get_weight(shape, lamada): # 创建变量： var = tf.Variable(tf.random_normal(shape), dtype=tf.float32) # 使用 add_to_collection() 函数将这个新创建的变量的 L2 正则化损失加入到集合 # 这个函数的第一个参数 'losses' 是集合的名称；第二个参数是要加入到集合的内容 tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(lamada)(var)) return var# 带输入特征向量以及标签的占位符input_x = tf.placeholder(tf.float32, shape=(None, 2))input_y = tf.placeholder(tf.float32, shape=(None, 1))# batch size：batch_size = 8# 定义神经网络中每一层网络中的节点个数：layer_dimension = [2, 10, 10, 10, 1]# 获取神经网络的层数n_layers = len(layer_dimension)# 定义一个变量用于维护前向传播时当前层的节点，开始的时候就是输入层：cur_layer = input_x# 获取当前层节点个数：in_dimension = layer_dimension[0]# 通过一个循环生成5层全连接的神经网络结构for i in range(1, n_layers): # 获取下一层节点的个数 out_dimension = layer_dimension[i] # 获取当前计算层的权重并加入了 L2 正则化损失 weight = get_weight([in_dimension, out_dimension], 0.001) # 随机生成偏向 bias = tf.Variable(tf.constant(0.1, shape=[out_dimension])) # 计算前向传播节点，使用RELU激活函数 cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias) # 进入下一层之前，更新下一层节点的输入节点数 in_dimension = layer_dimension[i] # Part 2：定义损失函数以及反向传播算法：# 计算模型数据的均值化损失加入损失集合mse_loss = tf.reduce_mean(tf.square(input_y - cur_layer))tf.add_to_collection('losses', mse_loss)# get_collection 返回一个列表，列表是所有这个集合的所有元素# 在本例中，元素代表了其他的损失，加起来就得到了所有的损失loss = tf.add_n(tf.get_collection('losses'))global_step = tf.Variable(0)# 学习率的设置：指数衰减法，参数：初始参数，全局步骤，每训练100轮乘以衰减速度0,96(当staircase=True的时候)learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase=True)train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) 上面给出的代码给出的是一个只有 5 层的全连接神经网络，在更加复杂的网络结构中，使用这样的方式来计算损失函数将大大增强代码的可读性。 4.4 滑动平均模型上一小节我们通过神经网络训练过程可能产生的过拟合问题，提出了通过参数正则化使得模型在测试数据上更健壮（泛化）。这一小节我们来介绍另一种保证模型健壮性的方法：滑动平均模型。 在采用 随机梯度下降算法 训练神经网络时，使用滑动平均模型在很多应用中都可以在一定程度上提高最终模型在预测数据上的表现。 4.4.1 滑动平均模型原理1 –&gt; 滑动平均： 滑动平均，使变量的更新与一段时间内（非某一时刻）的取值有关。 在创建滑动平均模型后，滑动平均模型会对每一个变量维护一个影子变量(shadow_variable，影子变量的初始值为相应变量的初始值)，每当变量更新时，影子变量的值会更新为： $$ shadow\_variable = decay × shadow\_variable + ( 1 - decay ) × variable ​$$ 其中，$shadow\_variable$：为影子变量，$variable$：为待更新变量，$decay$：为衰减率。注意区分：第一个 $shadow\_variable​$ 是当前时刻影子变量的值下；而第二个是上一时刻变量的影子变量。 可以看出，$decay$ 决定了模型更新的速度，$decay$ 越大，影子变量受变量更新的影响越小，模型越趋于稳定。故，在实际的应用中，$decay$ 值一般会设成非常接近 1 的数（如：0.999、0.9999）。 注意，变量的影子变量和变量的滑动平均值是一个意思！ 事实上，滑动平均值可以看作是变量的过去一段时间（过去1/(1-decay​)个时刻）取值的均值，相比对变量直接赋值而言，采用滑动平均得到的值在图像上更加平缓光滑，抖动性更小，不会因为某次的异常取值而使得滑动平均值波动很大。 2 –&gt; 测试中使用 对神经网络边的权重 weights 使用滑动平均，并且得到其对应的影子变量 shadow_weights。但注意，在训练过程仍然使用原来不带滑动平均的权重 weights，不然无法得到 weights 下一步更新的值，又怎么求下一步 weights 所对应的影子变量 shadow_weights 呢？！ 故，之后在测试过程中使用 shadow_weights 来代替 weights 作为神经网络边的权重，这样在测试数据上效果更好。 4.4.2 TensorFlow 中的滑动平均模型定义Tensorflow 中通过 tf.train.ExponentialMovingAverage() 函数来实现滑动平均模型。 1 –&gt; 滑动平均对象初始化 在初始化 ExponentialMovingAverage() 时，需要提供一个衰减率（decay），这个衰减率将用于控制模型更新的速度。 1variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, num_updates) 为了使模型在训练前期更新得的更快，ExponentialMovingAverage 函数还提供了 num_updates 参数来动态设置 decay 的大小。如果在 ExponentialMovingAverage() 函数初始化时提供了 num_updates 参数，那么每次使用的衰减率将是： $$ min \{ {decay, \cfrac{1 + num\_updates}{10 + num\_updates}} \} $$ 可以看出，num_updates 越大，衰减率就越大。num_updates 一般会为迭代轮数，所以当迭代轮数越大，模型参数就越稳定。即： 1variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) 2 –&gt; 添加/更新变量 添加目标变量，为之维护影子变量： 1variable_averages_op = variable_averages.apply([var0, var1, ..., var_n]) 注意维护不是自动的，更新一次影子变量就需要训练中执行一次上述语句。这里提供了一种方法：一般都会使用tf.control_dependencies 使之和 train_op 绑定，使得至于每次 train_op 都会更新影子变量。 1234# Tensorflow提供了两种机制：tf.control_dependencie 和 tf.group：# train_op = tf.group(train_step, variables_averages_op) 等价于 ==with tf.control_dependencies([train_step, variables_averages_op]): train_op = tf.no_op(name='train') 3 –&gt; 获取影子变量值 从影子变量集合中提取目标变量对应的滑动平均值： 1sess.run(variable_averages.average([var0, var1])) 4 –&gt; 给一个使用样例 1234567891011121314151617181920212223242526272829303132333435363738394041import tensorflow as tf# 定义一个变量用以计算滑动平均，变量的初始值为0,手动指定类型为float32，# 因为所有需要计算滑动平均的变量必须是实数型v1 = tf.Variable(0,dtype=tf.float32)# 模拟神经网络迭代的轮数，动态控制衰减率step = tf.Variable(0,trainable=False)# 定义一个滑动平均的类，初始化时给定衰减率为0.99和控制衰减率的变量ema = tf.train.ExponentialMovingAverage(0.99,step)# 定义一个滑动平均的操作，这里需要给定一个列表，每次执行这个操作时，列表里的元素都会被更新maintain_average_op = ema.apply([v1])with tf.Session() as sess: # 初始化所有变量 init_op = tf.global_variables_initializer() sess.run(init_op) # 获取滑动平均之后变量的取值 print (sess.run([v1,ema.average(v1)])) # 输出 :[0.0, 0.0] # 更新v1的值为5 sess.run(tf.assign(v1,5)) # 更新 v1 的滑动平均值，衰减率为 min&#123;0.99,(1+step)/(10+step)=0.1&#125;=0.1, # 所以 v1 的滑动平均被更新为 0.1*0+0.9*5=4.5 sess.run(maintain_average_op) print (sess.run([v1,ema.average(v1)])) # 输出 :[5.0, 4.5] # 更新迭代的轮数 sess.run(tf.assign(step,10000)) sess.run(tf.assign(v1,10)) # 这里的衰减率变成 0.99 # v1 = 0.99*4.5+0.01*10=4.555 sess.run(maintain_average_op) print (sess.run([v1,ema.average(v1)])) #再次更新滑动平均值 sess.run(maintain_average_op) print (sess.run([v1,ema.average(v1)]))]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>DeepLearning</tag>
        <tag>Deep Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MNIST Introduce]]></title>
    <url>%2FDataSet%2FMNIST-Introduce%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 当我们开始学习编程的时候，第一件事往往是学习打印 “Hello World”。就好比编程入门有 Hello World，机器学习入门有 MNIST。这篇博文来看 MNIST 数据集的获取以及 TensorFlow 中的使用。 MNIST 是一个入门级的计算机视觉数据集，它包含各种手写数字图片： 也包含每一张图片对应的标签，告诉我们这个是数字几。比如，上面这四张图片的标签分别是5，0，4，1。 How To Download MNIST首先我们来看如何下载 MNIST 数据集到本地（提供三种下载方法）： 1 –&gt; input_data.py 脚本自动下载和安装 Tensorflow 团队对 MNIST 数据集进行了封装，给我们提供了一份 Python 源代码用于自动下载、安装以及使用 MNIST 数据集（源代码参见：input_data.py ）。 使用时直接将下面的代码写入脚本导入到你的项目里面，也可以直接复制粘贴到你的 TensorFlow 代码文件里面即可： 12345678# 导入用于下载和读取 MNIST 数据集的模块 from tensorflow.examples.tutorials.mnist import input_data# 指定 MNIST 数据集的下载和读取的路径：MNIST_data_Path = "/home/Tensorflow/MNIST/MNIST_data/"# 获取MNIST数据集对象mnist = input_data.read_data_sets(MNIST_data_Path, one_hot=True) 如果上述下载失败【Network is unreachable】，你还可以使用下面的方法进行下载： 2 –&gt; 手动下载 MNIST数据集的官网：http://yann.lecun.com/exdb/mnist/ 找到相应的下载链接即可下载，数据集如下： train-images-idx3-ubyte.gz 训练集图片 ：55000 张 训练图片, 5000 张 验证图片 train-labels-idx1-ubyte.gz 训练集图片对应的数字标签 t10k-images-idx3-ubyte.gz 测试集图片 ：10000 张 图片 t10k-labels-idx1-ubyte.gz 测试集图片对应的数字标签 3 –&gt; input_data_google.py 脚本自动下载和安装 Google 官方提供的下载脚本，和 Tensorflow 中的比较可知，分装方法一致。(其实一回事，Tensorflow 就是由 Google 发布)。 input_data_google.py 脚本内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189# Copyright 2015 Google Inc. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""Functions for downloading and reading MNIST data."""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport gzipimport osimport numpyfrom six.moves import urllibfrom six.moves import xrange # pylint: disable=redefined-builtinSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'def maybe_download(filename, work_directory): """Download the data from Yann's website, unless it's already here.""" if not os.path.exists(work_directory): os.mkdir(work_directory) filepath = os.path.join(work_directory, filename) if not os.path.exists(filepath): filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath) statinfo = os.stat(filepath) print('Successfully downloaded', filename, statinfo.st_size, 'bytes.') return filepathdef _read32(bytestream): dt = numpy.dtype(numpy.uint32).newbyteorder('&gt;') return numpy.frombuffer(bytestream.read(4), dtype=dt)[0] # 增加 [0]def extract_images(filename): """Extract the images into a 4D uint8 numpy array [index, y, x, depth].""" print('Extracting', filename) with gzip.open(filename) as bytestream: magic = _read32(bytestream) if magic != 2051: raise ValueError( 'Invalid magic number %d in MNIST image file: %s' % (magic, filename)) num_images = _read32(bytestream) rows = _read32(bytestream) cols = _read32(bytestream) buf = bytestream.read(rows * cols * num_images) data = numpy.frombuffer(buf, dtype=numpy.uint8) data = data.reshape(num_images, rows, cols, 1) return datadef dense_to_one_hot(labels_dense, num_classes=10): """Convert class labels from scalars to one-hot vectors.""" num_labels = labels_dense.shape[0] index_offset = numpy.arange(num_labels) * num_classes labels_one_hot = numpy.zeros((num_labels, num_classes)) labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1 return labels_one_hotdef extract_labels(filename, one_hot=False): """Extract the labels into a 1D uint8 numpy array [index].""" print('Extracting', filename) with gzip.open(filename) as bytestream: magic = _read32(bytestream) if magic != 2049: raise ValueError( 'Invalid magic number %d in MNIST label file: %s' % (magic, filename)) num_items = _read32(bytestream) buf = bytestream.read(num_items) labels = numpy.frombuffer(buf, dtype=numpy.uint8) if one_hot: return dense_to_one_hot(labels) return labelsclass DataSet(object): def __init__(self, images, labels, fake_data=False): if fake_data: self._num_examples = 10000 else: assert images.shape[0] == labels.shape[0], ( "images.shape: %s labels.shape: %s" % (images.shape, labels.shape)) self._num_examples = images.shape[0] # Convert shape from [num examples, rows, columns, depth] # to [num examples, rows*columns] (assuming depth == 1) assert images.shape[3] == 1 images = images.reshape(images.shape[0], images.shape[1] * images.shape[2]) # Convert from [0, 255] -&gt; [0.0, 1.0]. images = images.astype(numpy.float32) images = numpy.multiply(images, 1.0 / 255.0) self._images = images self._labels = labels self._epochs_completed = 0 self._index_in_epoch = 0 @property def images(self): return self._images @property def labels(self): return self._labels @property def num_examples(self): return self._num_examples @property def epochs_completed(self): return self._epochs_completed def next_batch(self, batch_size, fake_data=False): """Return the next `batch_size` examples from this data set.""" if fake_data: fake_image = [1.0 for _ in xrange(784)] fake_label = 0 return [fake_image for _ in xrange(batch_size)], [ fake_label for _ in xrange(batch_size)] start = self._index_in_epoch self._index_in_epoch += batch_size if self._index_in_epoch &gt; self._num_examples: # Finished epoch self._epochs_completed += 1 # Shuffle the data perm = numpy.arange(self._num_examples) numpy.random.shuffle(perm) self._images = self._images[perm] self._labels = self._labels[perm] # Start next epoch start = 0 self._index_in_epoch = batch_size assert batch_size &lt;= self._num_examples end = self._index_in_epoch return self._images[start:end], self._labels[start:end]def read_data_sets(train_dir, fake_data=False, one_hot=False): class DataSets(object): pass data_sets = DataSets() if fake_data: data_sets.train = DataSet([], [], fake_data=True) data_sets.validation = DataSet([], [], fake_data=True) data_sets.test = DataSet([], [], fake_data=True) return data_sets TRAIN_IMAGES = 'train-images-idx3-ubyte.gz' TRAIN_LABELS = 'train-labels-idx1-ubyte.gz' TEST_IMAGES = 't10k-images-idx3-ubyte.gz' TEST_LABELS = 't10k-labels-idx1-ubyte.gz' VALIDATION_SIZE = 5000 local_file = maybe_download(TRAIN_IMAGES, train_dir) train_images = extract_images(local_file) local_file = maybe_download(TRAIN_LABELS, train_dir) train_labels = extract_labels(local_file, one_hot=one_hot) local_file = maybe_download(TEST_IMAGES, train_dir) test_images = extract_images(local_file) local_file = maybe_download(TEST_LABELS, train_dir) test_labels = extract_labels(local_file, one_hot=one_hot) validation_images = train_images[:VALIDATION_SIZE] validation_labels = train_labels[:VALIDATION_SIZE] train_images = train_images[VALIDATION_SIZE:] train_labels = train_labels[VALIDATION_SIZE:] data_sets.train = DataSet(train_images, train_labels) data_sets.validation = DataSet(validation_images, validation_labels) data_sets.test = DataSet(test_images, test_labels) return data_sets 使用时，将下面的代码导入到你的项目里面（直接复制粘贴到你的代码文件里面）代码里就可以使用： 12import input_data_google mnist = input_data_google.read_data_sets("MNIST_data/", one_hot=True) 注意，要将 input_data_google.py 文件放置到你调用代码的同目录下，让其可以 import 到。 4 –&gt; 使用注意： 数据集下载后，上述的脚本文件会自动下载并转化 MNIST 数据集中的数据格式，将数据从原始数据包格式解析成训练和测试神经网络时的数据格式，并通过 read_data_sets() 函数返回 MNIST 数据集对象： 数据集 目的 data_sets.train 55000组 图片和标签, 用于训练。 data_sets.validation 5000组 图片和标签, 用于迭代验证训练的准确性。 data_sets.test 10000组 图片和标签, 用于最终测试训练的准确性。 其中，read_data_sets() 函数默认下还会自动将 MNIST 数据集划分为 train（55000）、test（10000） 以及 validation（5000）三个数据集。 DataSet.next_batch(batch_size) 函数是用于获取以 batch_size 为大小的一个元组，其中包含了一组图片和标签，该元组会被用于当前的 TensorFlow 运算会话中。 MNIST 数据集MNIST 官方数据集分成两部分：60000 行的训练数据集（mnist.train）和 10000 行的测试数据集（mnist.test）。每一个 MNIST 数据单元（数据对象）有两部分组成：一张包含手写数字的图片和一个对应的标签。以训练数据集部分为例：比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。 每一张图片都代表了 0~9 中的一个数字，图片大小为 28 像素 X 28 像素。我们可以用一个像素矩阵来表示这张数字图片。下图展示了一张数字图片以及它所对于的像素矩阵： 1 –&gt; 以训练集为例： mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于 0 和 1 之间。 mnist.train.labels 是一个形状为 [60000, 10] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的分类标签（one-hot vectors）。在此张量里的每一个元素，都表示某张图片对应分类的 one-hot vectors 向量（one-hot vectors 向量除了某一位的数字是 1 以外其余各维度数字都是 0） 。比如，标签0将表示成([1,0,0,0,0,0,0,0,0,0,0])。因此，mnist.train.labels是一个形状为 [60000, 10] 的数字矩阵： HOW TO USE只要下载好数据集之后，就可以使用下面的代码进行数据集读取了。测试代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849''' Code For ：MNIST 手写数字图像数据集使用'''import tensorflow as tf# 从模块 tensorflow.examples.tutorials.mnist 中导入用于下载和解析 MNIST 数据集的 python 源文件：见 input_data.pyfrom tensorflow.examples.tutorials.mnist import input_datadef main(arg=None): ###################### Functions for downloading and reading MNIST data. ###################### ''' ## 初始化：下载或读取用于训练、测试以及验证的 MNIST 手写数字图片（28px * 28px）数据集 ## MNIST 数据集分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。每一个 MNIST 数据单 元（数据对象）有两部分组成：一张包含手写数字的图片和一个对应的标签。比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。 mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。 mnist.train.labels 是一个形状为 [60000, 10] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的分类标签（one-hot vectors）。在此张量里的每一个元素，都表示某张图片对应分类的 one-hot vectors 向量，数值只有0和1 。 实际，read_data_sets 类会将数据从原始数据包格式解析成训练和测试神经网络时的数据格式 。read_data_sets会自动将 MNIST 数据集 划分为 train（55000）、test（10000） 以及 validation（5000）三个数据集。 ''' # 指定 MNIST 数据集的下载和读取的路径： MNIST_data_Path = "./MNIST_data/" mnist = input_data.read_data_sets(MNIST_data_Path, one_hot=True) # print mnist.train dataSet size : print("Training data size : ", mnist.train.num_examples) # print mnist.validation dataSet size : print("Validating data size : ", mnist.validation.num_examples) # print mnist.test dataSet size : print("Testing data size : ", mnist.test.num_examples) # print mnist.train.images[0] / mnist.train.labels[0] Format # print("Example training data : ", "\n", mnist.train.images[0]) # print("Example training data lable : ", mnist.train.labels[0]) ### Next：可以使用了 #### TensorFlow 提供的一个主程序入口，tf.app.run 会调用上面定义的 main 函数：if __name__ == '__main__': tf.app.run()]]></content>
      <categories>
        <category>DataSet</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>MNIST</tag>
        <tag>DataSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Image PixMatrix]]></title>
    <url>%2FDataSet%2FImage-PixMatrix%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 学过线性代数肯定对矩阵并不陌生。一般来说，图像是一个标准的矩形，有着宽度（width）和高度（height）。而矩阵有着行（row）和列（column），矩阵的操作在数学和计算机中的处理都很常见且成熟，于是很自然的就想到把图像作为一个矩阵，把对图像的操作转换成对矩阵的操作，实际上所有的图像处理工具都是这么做的。 灰度图像像素矩阵我们看看下面这张灰度图像： 1 –&gt; 读取图像 用 Pyhton 图像库 OpenCV 读取： 1234import cv2mountain = cv2.imread('img/mountain.png', 0) # 读取为灰度图print ("shape:", mountain.shape)print ("type:", type(mountain)) 输出信息如下： 1233 注意：(Row, Column) =&gt; (Height, Width)shape: (480, 640)type: &lt;class 'numpy.ndarray'&gt; 可以看出，图像在读取后，存放到了一个 ndarray 中，这是 numpy 中的矩阵类型。 源图像的宽度（Width）是 640，高度（Height）是 480，我们一般习惯上依次用高度、宽度的顺序。在矩阵中，高度对应的是多少行（row），宽度对应的是多少列（column），而矩阵的顺序是（row，column），所以输出的 shape 是（480, 640）。 2 –&gt; 一些图像基本操作 类似于矩阵，把图像作为矩阵可以很方便的进行一些操作，例如取图像中某个区域的值，也就是所谓的 crop（裁剪）操作： 1print (mountain[9:12, 9:12]) 输出以下子矩阵： 123[[244 244 244] [244 236 244] [244 244 236]] 我们来重新截取，并且显示一下截取到的图像： 12345import cv2# 取大一些的区域，并显示出来。cv2.imshow("crop", mountain[200:400, 200:600])print(mountain[200:400, 200:600].shape)cv2.waitKey() 输出如下： 1(200, 400) 裁剪到的子图像如下： 最后来查看一下裁剪到的子图像所对应的矩阵： 123456789101112131415import cv2mountain_crop = cv2.imread('img/crop_image.png', 0)print ('Shape:', mountain_crop.shape)print ('Image Pix Matrix:','\n', mountain_crop)# 输出如下Shape: (234, 403)Pix Matrix: [[127 127 127 ... 127 127 127] [127 255 255 ... 255 255 127] [127 255 255 ... 255 255 127] ... [127 128 132 ... 4 28 127] [127 127 127 ... 127 127 127] [222 217 214 ... 205 208 212]] 彩色图像像素矩阵下面来看彩色图像的像素矩阵： 1 –&gt; 读取图像 用 Pyhton 图像库 OpenCV 读取： 12345import cv2person = cv2.imread('img/color_image.jpg')print ("type:", type(person))print ("shape:", person.shape)print (person[100, 100]) 输出信息如下： 123type: &lt;class 'numpy.ndarray'&gt;shape: (689, 650, 3)[250 255 254] 可以看出，彩色图像同样是一个矩阵，只是矩阵中的每一个点不是一个值，而是包含 3 个值的数组，这 3 个值对应的是图像的三通道 RGB 值。 2 –&gt; 3 Channels 图像（RGB ） 注意，Python OpenCV 的读取彩色图像对应的矩阵中顺序是 BGR。下面我们抽离出各通道矩阵来查看其图像： 1234567891011121314151617181920212223242526import matplotlib.pyplot as plt%matplotlib inlineimport numpy as npimport cv2original = cv2.imread('img/color_image.jpg')original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)# 各通道的图像时一张灰度图，这里我们将其转化为三通道彩色图:red = np.zeros_like(original)red[..., 0] = original[..., 0]green = np.zeros_like(original)green[..., 1] = original[..., 1]blue = np.zeros_like(original)blue[..., 2] = original[..., 2]pixMatrix_all = [original, red, green, blue]channels = ["RGB", "red", "green", "blue"]for i in range(4): plt.subplot(2, 2, i + 1) plt.imshow(pixMatrix_all[i]) plt.title(channels[i])plt.show()#for index in pixMatrix_all:# print(index.shape) 输出如下： 可以看出，每一个 RGB 通道都是一个矩阵。这 3 个 RGB 通道叠在一起形成了彩色图像。]]></content>
      <categories>
        <category>DataSet</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Image PixMatrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 基本工作原理]]></title>
    <url>%2FTensorFlow%2FTensorFlow-%E5%9F%BA%E6%9C%AC%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 前面的章节我们已经完成了 基于TensorFlow的深度学习框架环境的搭建 ，并且在已经安装好的 TensorFlow 环境中成功运行了一个简单的向量加法实例。本章节我们将通过两部分内容详细介绍 TensorFlow 的基本概念： 第一部分：分别通过 TensorFlow 的计算模型（Graph：计算图）、数据模型（Tensor：张量）以及运行模型（Session：会话），帮助我们对 TensorFlow 的工作原理有一个基本的了解； 第二部分：我们将简单介绍神经网络的基本概念，主要计算流程，以及如何通过 TensorFlow 来实现神经网络计算过程。 1. TensorFlow 基本工作原理TensorFlow 见名知义：Tensor（张量） 和 Flow（流），表达了它最重要的两个概念： 在 TensorFlow 中，第一个词 Tensor，就是张量（属于数学或物理中的概念，这里不强调其本身的含义），可以被简单理解为多维数组，表达了 TensorFlow 的数据模型；第二个词 Flow，就是流（数据的流动或转化）它直观的表达了数据（张量）之间通过计算相互转换的过程；而运行模型是用来执行 TensorFlow 中定义好的运算的。 1.1 TensorFlow 计算模型（Graph：计算图）计算图（Graph）是 TensorFlow 中最基本的一个概念。Tensorflow 是一个通过计算图的形式表述计算的编程系统，也就是说，所有的 TensorFlow 程序都可以通过计算图的形式来表示。 更深入的理解是，TensorFlow 程序中的所有计算都会被转化为计算图上的节点，计算图中的每一个节点就表示一个计算。计算图中节点之间的边描述了计算之间的依赖关系。 基于上述，下图展示了通过 TensorFlow 可视化工具 TensorBoard 画出的两个向量相加程序样例的计算图： 如上计算图所示：图中的每一个节点都代表了一个计算，如 a 节点、b 节点 (TensorFlow 会将常量转化成一种永远输出固定值的运算)以及 add 节点（加法运算）；节点之间的边表示了计算之间的依赖关系，如 add 运算的输入依赖于 a 和 b 运算的输出，而 a 和 b 两个常量不依赖于任何计算。 1.1.1 计算图的使用TensorFlow 程序一般分为两个阶段：第一个阶段需要定义计算图中的所有需要执行的运算；第二阶段为执行定义好的运算（Session）。 下面，首先来看如何定义 TensorFlow 程序（计算图上）中的所有计算： 这里，给出一个 TensorFlow 向量加法程序定义计算图中计算节点（a，b，result）的样例： import tensorflow as tf a = tf.constant([1.0, 2.0], name=&apos;a&apos;) b = tf.constant([2.0, 3.0], name=&apos;b&apos;) result = a + b 导入 TensorFlow 模块：import tensorflow as tf； (1) 默认计算图 TensorFlow 程序中，系统会自动为其维护一个默认的计算图，TensorFlow 会自动将定义好的计算转化为计算图上的节点。如上述向量加法样例中 a、b、以及 result 节点所属计算图即为默认计算图。 如何获取 TensorFlow 程序默认的计算图以及如何查看运算所属的计算图？ 通过 a.graph 可查看张量所属的计算图 通过 tf.get_default_graph() 函数获取当前默认的计算图 样例： print (a.graph) print (tf.get_default_graph()) print (a.graph is tf.get_default_graph()) 样例语句输出: &lt;tensorflow.python.framework.ops.Graph object at 0x00000204B01969E8&gt; &lt;tensorflow.python.framework.ops.Graph object at 0x00000204B01969E8&gt; True 2）生成计算图 除了使用 TensorFlow 默认计算图， TensorFlow 还支持通过 tf.Graph() 函数来生成新的计算图。 1234import tensorflow as tfg = tf.Graph()print(g is tf.get_default_graph()) 1.1.2 计算图的作用1）Graph 隔离张量和计算 在不同的计算图上的张量和运算不会共享，故 Graph 可以用来隔离张量和计算。样例程序如下所示： import tensorflow as tf ### 1. 定义计算图中的所有需要执行的运算 ### g1 = tf.Graph() with g1.as_default(): # 在计算图 g1 中定义变量 &apos;v&apos;，并初始化为 0 ： # Old Version Error: # v = tf.get_variable( # &quot;v&quot;, initializer=tf.zeros_initializer(shape=[1])) # New Version: v = tf.get_variable( &quot;v&quot;, initializer=tf.zeros_initializer()(shape=[1])) g2 = tf.Graph() with g2.as_default(): # 在计算图 g2 中定义变量 &apos;v&apos;，并初始化为 1 ： v = tf.get_variable( &quot;v&quot;, initializer=tf.ones_initializer()(shape=[1])) ### 2. 执行定义好的运算 ### # 在计算图 g1 中读取变量 ‘v’的取值： with tf.Session(graph=g1) as sess: # Old Version Error: # AttributeError: module &apos;tensorflow&apos; has no attribute &apos;initializer_all_variables&apos; # tf.initializer_all_variables().run() # New Version: tf.global_variables_initializer().run() with tf.variable_scope(&quot;&quot;, reuse=True): # 在计算图 g1 中，变量 &apos;v&apos; 的取值应该为 0，所以这里输出: [0.] print (sess.run(tf.get_variable(&apos;v&apos;))) # 在计算图 g2 中读取变量 ‘v’的取值： with tf.Session(graph=g2) as sess: tf.global_variables_initializer().run() with tf.variable_scope(&quot;&quot;, reuse=True): # 在计算图 g2 中，变量 &apos;v&apos; 的取值应该为 0，所以这里输出: [1.] print (sess.run(tf.get_variable(&apos;v&apos;))) 样例程序执行结果如下： [0.] [1.] 注意 g = tf.Graph() g.as_default() 和 tf.Session(graph=g) 的用法。 2）Graph 管理运算以及资源 TensorFlow 中的 Graph 不仅仅可以用来隔离张量和计算，还提供了管理张量和计算的机制。 –&gt; 管理张量和计算：例如，计算图可以通过 tf.Graph.device() 函数来指定执行运算的设备： # TensorFlow 还提供了对 GPU 的支持，来加速计算。 # 具体使用 GPU 的办法随后章节会介绍，这里我们知道 TensorFlow 提供了 GPU 加速的机制即可。 g = tf.Graph() with g.as_default(): a = tf.constant([1.0, 2.0], name=&apos;a&apos;) b = tf.constant([2.0, 3.0], name=&apos;b&apos;) with g.device(&apos;/gpu:0&apos;): result = a + b with tf.Session(graph=g) as sess: print (sess.run(result)) 注意，tf.Graph().device() 就是 图.device()。 –&gt; 除了管理计算之外，Graph 还能有效管理 TensorFlow 程序中的资源（资源可以是张量、变量或者程序运行时的队列资源等）。 例如：在一个计算图中，可以通过集合（collection）来管理不同类别的资源。比如：通过 tf.add_to_collection() 函数将资源加入到一个或多个集合中； 通过 tf.get_collection() 函数来获取一个集合中的所有资源。 为了使用方便，TensorFlow 自动管理了一些常用的集合，如下图所示的几个 TensorFlow 自动维护的集合： 集合名称 集合内容 使用场景 tf.GraphKeys.VARIABLES 所有变量 持久化 TensorFlow 模型 tf.GraphKeys.TRAINABLE_VARIABLES 可学习的变量(一般指神经网络中的参数) 模型训练、生成模型可视化内容 tf.GraphKeys.SUMMARIES 日志生成相关的张量 TensorFlow 计算可视化 tf.GraphKeys.QUEUE_RUNNERS 处理输入的 QueueRunner 输入处理 tf.GraphKeys.MOVING_AVERAGE_VARIABLES 所有计算了滑动平均值的变量 计算变量的滑动平均值 关于上述集合的具体使用，在后续相关内容介绍部分会进行说明。 1.2 TensorFlow 数据模型（Tensor：张量）这一小节，我们来介绍 TensorFlow 中另一个重要的基本概念：Tensor。张量是 TensorFlow 管理数据的形式，在 TensorFlow 程序中，所有的数据都通过张量的形式表示。 从功能角度来看，张量可以被简单理解为多维数组：零阶张量表示标量（scalar），也就是一个数；第一阶张量表示向量（vector），也就是一个一维数组；第 n 阶张量可以理解为一个 n 维数组。 但是我们要明白，“可以被理解为” 并不是 “实际上就是”！实际上，张量在 TensorFlow 中的实现并不是直接采用数组的形式，它只是一个对 TensorFlow 运算结果的引用，张量中并没有存储真正的数值，它保存的一个运算的过程。 如何理解？来看下面的代码，运行后并不会得到加法的结果，而是对结果的一个引用： import tensorflow as tf # tf.constant()是一个计算，计算的结果是一个张量，存储在变量 a 中： a = tf.constant([1.0, 2.0], name=&apos;a&apos;) b = tf.constant([1.0, 2.0], name=&apos;b&apos;) result = tf.add(a, b, name=&apos;add&apos;) print (result) 样例程序执行结果如下： Tensor(&quot;add_2:0&quot;, shape=(2,), dtype=float32) 上从面的测试可以看出：TensorFlow 中的张量和 Numpy 中的数组是不同的，不是一个数组，不存储数值，而是一个张量结构。根据上面样例输出的张量，接下来我们来看张量结构： 1.2.1 张量结构从上述结果中可以看出，一个张量结构主要保存了三个属性：名字（name）、维度（shape）以及 类型（dtype）。 1）名字（name） 第一个属性：名字，不仅是张量的唯一标识，也给出了张量是如何计算出来的。 我们知道：TensorFlow 程序都可以通过计算图模型来建立，而计算图中的每一个节点都代表的是一个个的计算，计算结果的引用就存储在张量中，所有张量和计算图中的节点是对应的。 这样张量的命名就可以通过 node_name:src_output 的形式给出。（node_name: 表示当前张量对应节点的名称；src_output: 表示当前张量来至对应节点的第几个输出；） 例如：add_2:0 表示 result 这个张量是节点 add_2 输出的第一个结果（编号从 0 开始）。 2）维度（shape） 第二个属性：维度，描述了一个张量的维度信息。维度是张量的一个重要属性，围绕维度 TensorFlow 给出了很多有用的运算，后面我们会涉及到部分相关运算。 例如：shape=(2,) 表示 result 这个张量是一个一维数组，数组的长度为 2。 3）类型（dtype） 第三个属性：类型，每个张量会有一个 唯一 的类型，TenosorFlow 会对所有参与运算的张量进行类型检查。一旦发现类型不匹配时会报错，如下代码： import tensorflow as tf a = tf.constant([1.0, 2.0], name=&apos;a&apos;) b = tf.constant([1, 2], name=&apos;b&apos;) # 去掉数值后的小数点，会使 b 的类型变为整型 result = a + b 执行上述代码报错： TypeError: Input &#39;y&#39; of &#39;Add&#39; Op has type int32 that does not match type float32 of argument &#39;x&#39; 。 如果将 b = tf.constant([1, 2], name=&#39;b&#39;) 改为： b = tf.constant([1, 2], name=&#39;b&#39;， dtype=tf.float32) 或者 b = tf.constant([1.0, 2.0], name=&#39;b&#39;) 就不会报错了。 |————————————————————————— 类型属性补充说明： TensorFlow 中，如果不指定类型，TensorFlow 会给出默认的类型： 不带小数点的数会被默认为 int32； 带小数点的数会被默认为 float32； 由于使用默认类型可能导致潜在的类型不匹配问题，所以 一般建议通过 dtype 属性来明确指出变量或常量类型。 TensorFlow 支持 14 中不同的类型： 实数型：tf.float32、tf.float64 ；整数型：tf.int8、tf.int16、tf.int32、tf.int64、tf.unit8 ；布尔型：tf.bool ；复数型：tf.complex64、tf.complex128。 —————————————————————————| 1.2.2 张量的使用张量的使用主要有两类： 1）第一类：对中间结果的引用 直接计算向量和，可读性较差： result2 = tf.constant([1.0, 2.0], name=&apos;a&apos;) + tf.constant([1.0, 2.0], name=&apos;b&apos;) 使用张量记录中间结果，增强代码可读性： a = tf.constant([1.0, 2.0], name=&apos;a&apos;) b = tf.constant([1.0, 2.0], name=&apos;b&apos;) result1 = a + b 除了提高代码可读性，这还使得我们的计算更加方便与灵活： # 如卷积神经网络中，我们卷积层和池化层都有可能改变张量维度，通过中间结果的引用，我们可以随时查看维度的变化: print (result.shape) print (result.get_shape()) % 上述语句输出： (2,) (2,) 张量还可以通过自身属性字段查看属性值： print (result.name) print (result.dtype) % 上述语句输出： add_2:0 &lt;dtype: &apos;float32&apos;&gt; 2）第二类：用来获取计算结果 根据张量基本概念的介绍可知，虽然张量本身没有存储具体数值（计算结果对我们来说是重要的），但它是对计算结果的引用。我们可以通过 sess = tf.Session() ; sess.run(result) 来取得张量所对应的计算结果。 1.3 TensorFlow 运行模型（Session：会话）在计算图部分我们提到过，TensorFlow 程序可以分为两个阶段：I 定义计算图中所有的计算；II 执行定义好的计算（Session）。正如前面两节介绍了 TensorFlow 如何组织数据和运算。这里我们来看 TensorFlow 中的 会话（Session） 是如何来执行定义好的运算的： 会话用来执行计算图中定义好的运算； 会话拥有并管理 TensorFlow 程序运行时的所有资源； 计算完成后需要关闭会话来帮助系统回收资源，否则会出现资源泄漏问题； 由于会话的使用可能导致资源泄漏问题的出现，这里衍生出了会话的两种使用模式： 1.3.1 会话的两种使用模式–&gt; 第一种模式：明确调用会话生成函数和关闭函数 # 创建一个会话，用于执行运算： sess = tf.Session() # 使用创建好的会话，得到我们关心的运算结果 result ： sess.run( result ) # 计算完成后，关闭会话回收系统资源，防止资源泄露： sess.close() 尽管我们可以明确调用关闭函数来释放资源占用，但这种模式仍然是不安全的！ 风险场景：当所有计算完成之后，我们需要程序明确调用 tf.Session.close() 来关闭会话并释放资源。然而，当程序因为异常而退出，导致关闭会话函数不会被执行而导致资源泄露。基于此，TensorFlow 支持通过 Python 上下文管理器来使用会话： –&gt; 第二种模式：通过 Python 上下文管理器使用会话 # 创建一个会话，并通过 Python 上下文管理器管理这个会话： with tf.Session() as sess: # 使用创建好的会话，得到我们关心的运算结果 result ： sess.run( result ) # 不需要再明确调用 Session.close() 函数来关闭会话了。当上下文退出时会自动关闭会话和完成资源释放 通过 Python 上下文管理器机制，我们只需要将需要执行的运算放在 with 内部就可以。不用担心因为忘记关闭会话或程序异常退出导致的资源泄露问题。 1.3.2 默认会话机制在计算图的使用部分，我们提到过 TensorFlow 会自动生成一个默认的计算图，如果没有特殊指定，运算会被自动加入到默认的计算图。TensorFlow 会话也有类似的机制，但 TensorFlow 不会自动生成默认的会话，需要我们去手动指定（想想这也是合理的）。当会话被指定被指定为默认会话后，我们可以使用默认会话的一些相关函数方法： # 当默认会话被指定后，可以通过 tf.Tensor.eval() 函数来直接获得计算结果： sess = tf.Session() with sess.as_default(): print (result.eval()) # 是不是很方便 sess.close() % 程序输出： [2. 4.] 以下代码也可完成相同功能： sess = tf.Session() # 下面两个指令功能相同： print (sess.run(result)) print (result.eval(session=sess)) % 程序输出： [2. 4.] [2. 4.] 为了交互式测试环境下更方便的使用默认会话，TensorFlow 提供了一种 交互式 下直接构建默认会话的函数：tf.InteractiveSession() ,它会自动生成会话并注册为默认会话： sess = tf.InteractiveSession() print (result.eval()) # 注意，明确创建默认会话后需要关闭： sess.close() 1.3.3 会话的配置在执行会话时，我们还可以通过 ConfigProto Protocol Buffer 来配置需要生成的会话。通过 tf.ConfigProto() 函数可以配置类似并行的线程数、GPU 分配策略、运算超时等参数，最常使用的有两个： config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True) # 不管使用什么方式创建会话都可以进行配置： sess1 = tf.InteractiveSession(config=config) sess2 = tf.Session(config=config) |————————————————————————— 相关参数说明： [1] allow_soft_placement: 布尔型参数 当 allow_soft_placement=True 时，在以下任意一个条件成立时，GPU 运算可以放到 CPU 上进行： 运算无法在 GPU 上执行（GPU 上不支持该类型数值运算） 没有 GPU 资源（比如运算被指定在第二个 GPU 上运行，当机器只有第一个 GPU资源） 运算输入包含对 CPU 计算结果的引用 allow_soft_placement 参数默认为 False，但为了使得代码的移植性更强（可以同时适应 GPU 和 CPU 环境），一般会将其设置为 True。并且不同 GPU 驱动版本可能对计算的支持有略微差别，当某些运算无法被当前 GPU 支持时，可以自动调整到 CPU，而不是报错。 [2] log_device_placement：布尔型参数 当 log_device_placement=True 时，日志中会记录每个节点被安排在哪个设备上以便调试。而在生产环境将其设置为 False 可以减少日志量。 —————————————————————————| 2. 初识神经网络这一部分我们将简单介绍神经网络的基本概念，主要计算流程，以及如何通过 TensorFlow 来实现神经网络计算。 待补充基础？….. 2.1 TensorFlow 游乐场以及神经网络介绍这一小节我们将通过 TensorFlow 游乐场来快速介绍神经网络的工作流程。TensorFlow 游乐场，是一个通过网页浏览器就可以训练的简单神经网络并实现了可视化训练过程的工具。下图给出了 TensorFlow 游乐场默认设置图： 详细操作教程见网络 TensorFlow 游乐场教程，由于篇幅原因，这里不做介绍。 通过 TensorFlow 游乐场的使用，我们给出使用神经网络解决分类问题的主要流程： 从给出的原始数据提取实体的特征向量作为神经网络的输入； 定义神经网络结构，并定义神经网络的前向传播算法（从输入到输出）； 定义损失函数以及反向传播优化算法，并通过训练优化神经网络参数； 使用训练好的神经网络模型来预测未知数据类型； 2.2 神经网络结构与前向传播算法简单地说，定义神经网络结构，并定义神经网络的前向传播（Forward-Propagation），这个过程就是神经网络的前向传播算法(从输入如何得到输出的过程)。不同结构的神经网络前向传播的方式是不相同的。 这一小节我们仅以最简单的全连接神经网络的前向传播算法为例，展示 TensorFlow 如何实现这个过程。 在解读全连接神经网络的前向传播算法过程之前，我们先来看神经网络结构组成： 2.2.1 神经网络结构1）神经元（nerve cell）神经元是构成神经网络的最小单元，也可以称之为 神经网络节点。下面给出一个简单的神经元单元结构图： 从上图看出，一个神经元有多个输入和一个输出。每个神经元的输入既可以是其他神经元的输出，也可以是整个神经网络的输入（非神经元节点）。 严格来说，神经网络中除了输入层之外的所有节点都代表了一个神经元结构。很多文档会将输入节点也看作是神经元，所以输入层也看作一层神经网络层（这也是很多时候将一个只有一层隐藏层和输出层的神经网络称为三层神经网络的原因，严格来说，应该是两层神经网络）。 2）神经网络结构（Neural Network）所谓的神经网络结构：是指不同神经元之间的连接结构。这里我们采用的是全连接神经网络结构（相邻两层之间任意两个神经元节点之间都有连接），后面章节我们还会介绍卷积神经网络（CNN）、循环神经网络（RNN）、残差神经网络等等网络结构。 根据 1）中神经元结构可知，一个简单神经元的输出就是所有输入的加权和以及偏置项通过一个激活函数得到，而不同的输入权重以及神经元节点的偏置就是神经元的参数。神经网络的优化 过程就是优化神经元中参数的过程。 2.2.2 全连接神经网络前向传播算法这里我们会以一个简单的三层全连接神经网络来介绍其前向传播过程，如图所示： 由上图可知：神经网络前向传播算法结果需要三部分信息(上标表示神经网络层数)： 神经网络节点输入（从实体中提取到的特征向量）； 神经网络连接结构（全连接）； 神经元参数（这里指权重，不包含偏置项，不使用激活函数） 根据神经网络前向传播算法结果需要三部分信息，这里我们给出了神经元节点 $a_{11}$、$a_{12}$、$a_{13}$ 以及 $Y$ 的输出结果如何计算（即神经网络如何进行前向传播？）： 实际上，我们可以把同属一个网络层的所有节点的前向传播计算过程表示为矩阵运算。如上图，假设我们要求隐藏层所有节点的值 $ a^{(1)}=[a_{11},a_{12},a_{13}] $ 以及输出层节点的值 $ Y=[y] $，前向传播计算过程如下： 首先表示输入特征向量（矩阵）：$ x = [x_1, x_2] $ 权重矩阵为：$ W^{(1)} = \left[ \begin{array} {cccc}W_{1,1}^{(1)} &amp; W_{1,2}^{(1)} &amp; W_{1,3}^{(1)}\\W_{2,1}^{(1)} &amp; W_{2,2}^{(1)} &amp; W_{2,3}^{(1)}\\\end{array} \right] $ 矩阵运算过程如下，展示了得到节点 $ a^{(1)} $ 整个前传播计算过程： $$ a^{(1)} = [a_{11}, a_{12}, a_{13}] = xW^{(1)} = [x_1, x_2]\left[ \begin{array} {cccc}W_{1,1}^{(1)} &amp; W_{1,2}^{(1)} &amp; W_{1,3}^{(1)}\\W_{2,1}^{(1)} &amp; W_{2,2}^{(1)} &amp; W_{2,3}^{(1)}\\\end{array} \right] \\ = [W_{1,1}^{(1)}x_1+W_{2,1}^{(1)}x_2, W_{1,2}^{(1)}x_1+W_{2,2}^{(1)}x_2, W_{1,3}^{(1)}x_1+W_{2,3}^{(1)}x_2] $$ 类似的，输出层计算可以表示为： $$ Y = [y] = a^{(1)}W^{(2)} = [a_{11}, a_{12}, a_{13}]\left[ \begin{array} {cccc} W_{1,1}^{(2)}\\ W_{2,1}^{(2)}\\ W_{3,1}^{(2)}\\\end{array} \right] = [W_{1,1}^{(2)}a_{11} + W_{2,1}^{(2)}a_{12} + W_{3,1}^{(2)}a_{13}] $$ 这样就将前向传播算法通过矩阵乘法的方式给出了。TensorFlow 中矩阵乘法是很容易实现的，我们通过 TensorFlow 来表示上述过程（前向传播计算过程）： $$ a^{(1)} = tf.matmul(x, W^{(1)}) $$ $$ y = tf.matmul(a^{(1)}, W^{(2)}) $$ 在之后的章节将会继续介绍增加了偏置项（bias）、激活函数（activation-function）等更加复杂的神经元结构，以及更加复杂的神经网络结构（RNN、CNN、Resnet）的前向传播过程的实现。 2.3 神经网络参数与 TensorFlow 变量上文我们介绍了全连接神经网络前向传播算法的实现原理（矩阵乘法）。这一小节，我们首先来看在 TensorFlow 中如何 组织以及存储 神经网络中的参数，以给出一个前向传播算法的具体 TensorFlow 实现。 2.3.1 TensorFlow 变量以及其初始化TensorFlow 中通过变量 (tf.Variable) 来保存和更新神经网络中的参数。 同样，和某些其它编程语言类似，TensorFlow 中的变量在声明之后也需要指定初始值，对变量进行初始化。TensorFlow 中变量的初始值可以设置为 随机数、常数 或者 通过其它变量的的初始值 计算得到。下面我们将会分别介绍上述几种初始化方法： 1）TensorFlow 支持的几种随机数生成器 首先，我们先给出一个样例来简单说明变量的声明以及初始化：在神经网络中，给参数赋予随机初始值最为常见，所以一般使用随机数给 TensorFlow 中的变量进行初始化。 下面给出一种在 TensorFlow 中声明一个 [2 * 3] 的矩阵变量的方法： weights = tf.Variable(tf.random_normal([2, 3], stddev=2)) 上述代码调用了 TensorFlow 变量的声明函数 tf.Variable()。并且在声明函数中给出了初始化这个变量的随机生成函数 tf.random_normal()。 tf.random_normal([2, 3], stddev=2) 会产生一个 [2 * 3] 的矩阵，矩阵中的元素是满足正态分布，均值为 0，标准差为 2 的随机数（参数 mean 用来指定均值，默认为 0: tf.random_normal([2, 3], stddev=2， mean=0），当然 random_normal() 中也可以设置随机种子 seed。 下面我们来看 TensorFlow 支持的几种随机数初始化函数： 函数名称 随机分布 主要参数 tf.random_normal 正态分布 平均值、标准差、取值类型 tf.truncated_normal 截断正态分布 平均值、标准差、取值类型 tf.random_uniform 均匀分布 最小、最大取值，取值类型 tf.random_gamma Gamma分布 形状参数 alpha、尺度参数 beta、取值类型 其中，截断正态分布表示（比较常用）：如果随机出来的值偏离平均值超过 2 个标准差，会重写随机数。 使用习惯： 随机数初始化通常用来给神经网络的权重参数进行初始化！ 2）常量（Constants） 正如向量加法样例中给出的，TensorFlow 中也支持通过常数来初始化变量，下面我们来看 TensorFlow 常用的几种常数初始化函数： 函数名称 随机分布 样例 tf.zeros 产生全为 0 的数组 tf.zeros([2,3], int32) -&gt; [[0,0,0],[0,0,0]] tf.ones 产生全为 1 的数组 tf.ones([2,3], int32) -&gt; [[1,1,1],[1,1,1]] tf.fill 产生一个全部为给定数字的数组 tf.fill([2,3],9) -&gt; [[9,9,9],[9,9,9]]) tf.constant 产生一个给定值的常量 tf.constant([1,2,3]) -&gt; [1,2,3] 使用习惯： 神经网络中，偏置项（bias）通常会使用常数来初始化： biases = tf.Variable(tf.zeros([3])) print (biases) 样例结果输出如下： &lt;tf.Variable &apos;Variable_1:0&apos; shape=(3,) dtype=float32_ref&gt; 3）其它变量 TensorFlow 也支持通过其它变量的初始值来初始化新变量，如下： weight1 = tf.Variable(tf.random_normal([2,3], stddev=2, dtype=tf.float32)) weight2 = tf.Variable(weight1.initialized_value()) 注意，这种方法不太常用！ 2.3.2 TensorFlow 变量的使用 &amp;&amp; 前向传播算法的实现上面我们已经了解了 TensorFlow 变量如何声明以及初始化(事实上并没有真正被执行)。注意，在 TensorFlow 中，一个变量的值在被使用之前，这个变量初始化的过程必须被明确调用才可以使用！ 这里我们将结合上文神经网络前向传播算法原理在 TensorFlow 中的实现来说明变量的使用。 import tensorflow as tf # 定义一个变量用于作为神经网络输入，暂时将输入特征向量(即一个样本的特征向量)定义为一个常量(1 * 2 的矩阵)： input_x = tf.constant([[0.7,0.9]]) # 声明两个权重变量：w1、w2（这里还通过 seed 设置了随机种子，可以保证每次运行得到的结果一样） w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1)) w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1)) # 通过前向传播算法原理获得神经网络的输出 Y： a = tf.matmul(input_x, w1) y = tf.matmul(a, w2) # 创建会话来执行定义好的运算： sess = tf.Session() # 使用变量 w1、w2 之前,需要明确调用变量的初始化才可以使用： sess.run(w1.initializer) sess.run(w2.initializer) # 执行运算，获取最终结果： print (sess.run(y)) sess.close() 样例结果输出如下： [[3.957578]] 引发的一个问题： 上面的样例中，我们在使用变量 w1、w2 之前需要明确调用其初始化，完成最终的初始化。虽然这看上去是一个可行的方案，但你有没有想过：当我们的模神经网络的变量数目增多（通常会有几万，甚至几十、几百万的参数），或者变量之间存在依赖关系时，你还会去一个个的为每个变量做明确初始化调用么？当然不会！太麻烦了。 TensorFlow 提供了一种更加便捷的方法来一步完成所有变量的初始化调用。如下所示： # Old Version(新版本下会报错)： init_op = tf.initializer_all_variables().run() --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-75-2501d5753001&gt; in &lt;module&gt;() 1 # Old Version(新版本下会报错)： ----&gt; 2 init_op = tf.initializer_all_variables().run() AttributeError: module &apos;tensorflow&apos; has no attribute &apos;initializer_all_variables&apos; # New Version: init_op = tf.global_variables_initializer().run() 2.3.3 TensorFlow 变量属性类似张量，维度（shape）和类型（dtype）也是变量最重要的两个属性。 1）dtype 和大部分程序语言类似，变量类型不可以改变。一个变量一旦构建之后，类型就不能再改变。如上面给出的前向传播样例中，w1类型为 tf.random_normal 函数结果的默认类型 tf.float32，那么它就不能被赋予其它类型的值，如下代码所示： w1 = tf.Variable(tf.random_normal([2,3], stddev=1, name=&quot;w1&quot;)) w2 = tf.Variable(tf.random_normal([2,3], dtype=tf.float64, stddev=1, name=&quot;w2&quot;)) # w1 赋值给 w2： w1.assign(w2) 执行上述程序语句将报错：TypeError: Input &#39;value&#39; of &#39;Assign&#39; Op has type float64 that does not match type float32 of argument &#39;ref&#39;。 这类似于张量，TensorFlow 会自动对变量的类型进行类型检查！ 2）shape 维度是变量另外一个重要的属性。和类型不大一致，维度在 TensorFlow 程序中是可变的，但需要通过 validate_shape=False 设置。如下样例： w1 = tf.Variable(tf.random_normal([2,3], stddev=1, name=&quot;w1&quot;)) w2 = tf.Variable(tf.random_normal([2,2], stddev=1, name=&quot;w2&quot;)) # tf.assigh(w1,w2) # ValueError: Shape (2,3) and (2,2) are not compatible # 这样才可以执行： tf.assign(w1,w2, validate_shape=False) 样例结果输出如下： &lt;tf.Tensor &apos;Assign_1:0&apos; shape=(2, 2) dtype=float32_ref&gt; 当然，TensorFlow 支持改变变量维度的用法在实践中比较罕见。 2.3.4 TensorFlow 变量和张量1）变量和张量的区别 从前面的章节，我们提到：TensorFlow 中所有的数据都是通过 Tensor 来组织的，这一小节我们又介绍了 TensorFlow 变量，那么张量和变量是什么关系呢？TensorFlow 中，变量的声明函数是一个运算，而运算结果的引用就是一个张量，所以可以看出，这个张量就是我们这一小节所说的变量，也就是说 变量是一种特殊的张量。 2）关于变量使用的补充 在计算图的使用小节中，我们提到 TensorFLow 中可以通过集合（collection）来管理运行时的各种资源，它自动维护一些默认集合。 例如，所有的变量都会被自动加入到 tf.GraphKeys.VARIABLES 集合。通过 tf.all_variables() 函数可以拿到当前计算图上所有的变量。拿到计算图上的所有变量有助于 TensorFlow 持久化整个计算图的运行状态。 另外，当构建机器学习模型时，我们需要不断训练参数以获得最佳的模型，可以通过变量声明函数中的 trainable 属性来区分需要优化的参数（神经网络中的参数）和其他参数（迭代轮数）： 如果声明变量时参数 trainable 为 True（默认为：False），那么这个变量会被加入到 tf.GraphKeys.TRAINABLE_VARIABLES 集合。 在 TensorFlow 中可以通过 tf.trainable_variables() 函数得到所有需要优化的参数。并且 TensorFlow 中提供的神经网络算法会将 tf.GraphKeys.TRAINABLE_VARIABLES 集合中的变量作为默认的优化对象。 2.4 TensorFlow 训练神经网络模型（优化参数）上面的小节，我们给出了一个样例来完成全连接神经网络的前向传播过程。但是，这个样例中所有变量（参数）的初始取值都是随机的。事实上，在使用神经网络来解决实际的分类或回归问题时，我们需要通过训练神经网络模型不断调整参数获取到更好的参数取值，以获取最佳的神经网络模型。 这一小节我们将介绍如何使用监督学习的方式结合训练算法（反向传播）来更合理的设置参数取值，并且给出了一个 TensorFlow 实现这一过程的样例。优化神经网络参数的过程就是神经网络的训练过程，只有经过有效训练的神经网络模型才可以真正解决分类或回归问题。 监督学习最重要的思想 就是：在已知答案的标注数据集上，模型给出的预测结果要尽可能接近真实标记。通过 BP 算法调整神经网络中的参数对训练数据的拟合，可以使得模型对未知样本提供预测能力。 2.4.1 神经网络训练（优化）算法在神经网络优化算法中，最常用的方法就是反向传播算法。这里，我们先简单了解一下反向传播算法（Backpropagation，BP）的概念，后续会做深入介绍。BP 是训练神经网络的核心算法，它可以根据定义好的损失函数来优化神经网络中参数的取值，从而使得神经网络模型在训练数据集上的损失函数达到一个较小值。下面我们给出一个使用了反向传播算法训练神经网络模型的流程图： 从上图看出，反向传播算法本质是实现了一个迭代的过程。每次迭代开始，首先需要读取一部分训练数据（来源于训练数据集），这一小部分数据称为一个 batch。然后这个 batch 的数据通过前向传播算法得到其在神经网络模型的预测结果。 因为训练数据都是有正确答案标注的，所有可以计算出当前神经网络模型的预测答案和正确答案的差距（通过损失函数来定义）。最后，反向传播算法会根据这个差距更新神经网络的参数，使得预测结果要尽可能接近真实标记。 –&gt; 如何实现反向传播算法？？？ 1）TensorFlow 表达 Batch 首先，我们来看如何从训练数据集读取一个 batch 的数据，在 TensorFlow 中进行表达。我们在实现全连接神经网络前向传播算法样例中，曾经使用过用常量来表达一个样本数据： 1input_x = tf.constant([[0.7,0.9]]) 引发一个问题： 如果每次迭代中选取的数据都要通过常量来创建，那么 TensorFlow 的计算图将会太大。因为每生成一个常量，TensorFlow 都会在计算图中增加一个计算节点。一般来说，一个神经网络的训练过程会经过几百万轮甚至几亿轮数的迭代，这样计算图就会非常大，而且利用率很低。 为了避免这个问题，TensorFlow 提供了 placeholder 机制用于提供输入数据。placeholder 相当于定义了一个位置（占位符），这个位置中的数据在程序运行时再指定（必须），这样只需要将读入的数据通过 placeholder 传入 TensorFlow 计算图即可。 也就是说，我们通过 placeholder 告诉 TensorFlow 程序，这里有一个“空间”，我们会在执行程序时再给定这个“空间”的取值以供计算图使用。 placeholder : placeholder 在定义时，这个位置上的数据类型是需要指定的，而且和其它张量一样，类型是不可更改的。placeholder 中数据的维度信息可以根据提供的数据自动推导得出，所以不一定要给出。下面我们将 placeholder 的使用引入全连接神经网络的前向传播算法实现中： import tensorflow as tf # placeholder 的使用： input_x = tf.placeholder(tf.float32, shape=(1,2), name=&quot;input_x&quot;) # 声明两个权重变量：w1、w2（这里还通过 seed 设置了随机种子，可以保证每次运行得到的结果一样） w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1)) w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1)) # 通过前向传播算法原理获得神经网络的输出 Y： a = tf.matmul(input_x, w1) y = tf.matmul(a, w2) # 创建会话来执行定义好的运算： sess = tf.Session() # 明确调用变量 init_op = tf.global_variables_initializer().run() # 执行运算会报错： # print (sess.run(y)) sess.close() 我们发现，直接执行 sess.run(y) 会发生报错： InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor &#39;input_x_1&#39; with dtype float and shape [1,2]。 WTF ？？？ 神奇的 feed_dict : 上面我们仅仅是在计算图中创建了一个占位符，但是运行时我们并没有给 placeholder 传入数据。TensorFlow 中使用 feed_dict（类型：字典）来为 placeholder 提供样本数据，feed_dict 字典中需要给出每个用到的 placeholder 取值（一个 batch 的数据）。如下给出实现代码： import tensorflow as tf # placeholder 的使用： input_x = tf.placeholder(tf.float32, shape=(3,2), name=&quot;input&quot;) # 声明两个权重变量：w1、w2（这里还通过 seed 设置了随机种子，可以保证每次运行得到的结果一样） w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1)) w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1)) # 通过前向传播算法原理获得神经网络的输出 Y： a = tf.matmul(input_x, w1) y = tf.matmul(a, w2) # 创建会话来执行定义好的运算： sess = tf.Session() # 明确调用变量 init_op = sess.run(tf.global_variables_initializer()) # 执行运算会报错： # print (sess.run(y)) # batch 中含有 3 个样本： # 当然可以包含多个 print (sess.run(y, feed_dict={input_x: [[0.7,0.9], [0.1,0.4], [0.5,0.8]]})) sess.close() 样例运行结果如下（每一行都是一个样本的前向传播结果。）： [[3.957578 ] [1.1537654] [3.1674924]] 2）损失函数（Loss Function） &amp;&amp; 优化器（Optimizer） 前面说过，我们需要定义一个损失函数来刻画当前预测值和真实标注之间的差距，然后通过反向传播算法来调整神经网络参数取值使得差距可以被缩小。下面我们给出一个简单的反向传播算法模型的定义： # 定义损失函数： cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0))) # 定义反向传播的优化方法： train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy) 除了定义损失函数外，我们还需要根据实际问题采用合理的反向传播优化方法（优化器）以更新参数（更新参数对我们来说是关键的）。下面我们来看 TensorFlow 支持的三种常用优化器：tf.train.GradientDescentOptimizer()、tf.train.AdamOptimier()、tf.train.MomentumOptimizer()。 在定义了反向传播算法之后，通过运行 sess.run(train_op) 就可以对所有在 tf.GraphKeys_TRAINABLE_VARIABLES() 集合中的变量进行自动优化，使得神经网络模型在当前 batch 的损失函数更小。 2.4.2 完整神经网络样例综上所述，这一小节我们将在一个模拟数据集上训练全连接神经网络模型来解决 TensorFlow 游乐场中提到过的二分类问题： import tensorflow as tf from numpy.random import RandomState # 定义训练数据 batch 的大小 batch_size = 8 # 待输入的样本特征向量以及标注的占位符： # None 方便自适应不同的 batch 大小 input_x = tf.placeholder(tf.float32, shape=(None,2), name=&quot;input_x&quot;) input_y = tf.placeholder(tf.float32, shape=(None,1), name=&quot;input_y&quot;) # 定义全连接神经网络的参数 w1 = tf.Variable(tf.random_normal([2,3], stddev=1, seed=1)) w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1)) # 定义神经网络的前向传播： a = tf.matmul(input_x, w1) y = tf.matmul(a, w2) # 定义神经网络模型损失函数以及反向传播算法： # 定义损失函数： cross_entropy = -tf.reduce_mean(input_y * tf.log(tf.clip_by_value(y, 1e-10, 1.0))) learning_rate = 0.001 # 定义反向传播的优化方法： train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy) # 通过随机数生成一个模拟数据集： rdm = RandomState(1) dataset_size = 128 # 训练数据集样本数目 X = rdm.rand(dataset_size, 2) # 定义规则给出样本的标签（x1+x2&lt;1 认为是正样本）： Y = [ [int(x1+x2 &lt; 1)] for (x1, x2) in X ] # 下面创建一个会话来运行程序： with tf.Session() as sess: init_op = sess.run(tf.global_variables_initializer()) # 打印训练之前的神经网络参数： print (sess.run(w1)) print (sess.run(w2)) # 开始训练： # 定义训练轮数： STEPS = 5000 for i in range(STEPS): # 每次选取一个 batch 的数据进行训练： start = (i * batch_size) % dataset_size end = min(start + batch_size, dataset_size) data_feed = feed_dict={input_x: X[start:end], input_y: Y[start:end]} # 训练神经网络参数 sess.run(train_op, data_feed) if i % 1000 == 0: # 每迭代 1000 次输出一次在所有数据上的交叉熵损失： total_cross_entropy = sess.run(cross_entropy, feed_dict={input_x: X, input_y:Y}) print (&quot;After %d training step(s), cross entropy on all data is %g&quot; % (i, total_cross_entropy)) # 打印训练之后的神经网络参数： print (sess.run(w1)) print (sess.run(w2)) 样例程序输出日志信息如下： [[-0.8113182 1.4845988 0.06532937] [-2.4427042 0.0992484 0.5912243 ]] [[-0.8113182 ] [ 1.4845988 ] [ 0.06532937]] After 0 training step(s), cross entropy on all data is 0.0674925 After 1000 training step(s), cross entropy on all data is 0.0163385 After 2000 training step(s), cross entropy on all data is 0.00907547 After 3000 training step(s), cross entropy on all data is 0.00714436 After 4000 training step(s), cross entropy on all data is 0.00578471 [[-1.9618274 2.582354 1.6820377] [-3.4681718 1.0698233 2.11789 ]] [[-1.8247149] [ 2.6854665] [ 1.418195 ]]]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>ANN</tag>
        <tag>Artificial Neural Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jpype 安装以及使用指南]]></title>
    <url>%2FJpype%2FJpype-%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： 如何在 Python 中优雅的调用 Java？！ Environment： 1. Anaconda3 2. Python3.5 3. Linux（Centos | Ubuntu）/Windows 1. Jpype1.1 初识 Jpype1）JPype 是什么？ JPype 是一个能够让 Python 很方便地调用 Java 代码并获取其返回值的工具，可以将其看作是 Python 中使用 JVM 的第三方库。Jpype 的引入，弥补了 Python 自身某些性能方面的不足。 2）JPype VS. Jython ( JPython 后继者) 运行环境：Jython 运行在 JVM 上，而 JPype 的实际运行环境仍然是 PythonRuntime，只是在运行期间启动了一个嵌入的 JVM； 使用者：像很多人说，Jython 是给 Java 程序玩的，JPype 是给 Python 程序员玩的。 1.2 Jpype 安装这里我们先给出 Jpype 官方下载源介绍： Get JPype from the github or from PyPi. If you are using Anaconda Python stack, you can install pre-compiled binaries from conda-forge for Linux, OSX and Windows. 下面我们提供三种安装方式： 1）Binary Install By Anaconda-conda For Windows / Linux (推荐) 1. System requirements: * Ensure you have installed Anaconda * 32- or 64-bit computer. * Python 2.7, 3.4, 3.5 or 3.6. 2. Install from the conda-forge software channel: $ conda install -c conda-forge jpype1 2）Install JPype from PyPi For Linux PyPi（ Python Package Index ）是 Python 官方的第三方库的仓库，所有人都可以免费下载第三方库或上传自己开发的库到 PyPI。这里给出 PyPi 官方地址。 1. Download JPype1-0.6.3.tar.gz 进入 PyPI 官网直接搜索 JPype1 即可找到其下载源。 2. Decompression JPype1-0.6.3.tar.gz $ tar -zxvf JPype1-0.6.3.tar.gz $ cd JPype1-0.6.3 $ python setup.py install 3）Install by PIP[$ pip install jpype1] For Linux $ pip install jpype1 # Centos 可能发生报错：error gcc -pthread -fno-strict-aliasing -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fPIC -I/usr/lib/jvm/java-1.5.0-sun-1.5.0.08/include -I/usr/lib/jvm/java-1.5.0-sun-1.5.0.08/include/linux -Isrc/native/common/include -Isrc/native/python/include -I/usr/include/python2.4 -c src/native/common/jp_proxy.cpp -o build/temp.linux-x86_64-2.4/src/native/common/jp_proxy.o gcc: error trying to exec &apos;cc1plus&apos;: execvp: No such file or directory error: command &apos;gcc&apos; failed with exit status 1 # solution: $ yum install gcc-c++ $ pip install jpype1 1.3 Jpype 基础语法1.3.1 Startup and shutdown the JVM1）如何启动 JVMstartJVM(jvm, *args) 1—1）jpype.startJVM() 参数说明： —- 参数 1 —-： jvm 描述的是系统中 jvm.dll 文件所在的路径，如 “C:\Program Files\Java\jdk1.8.0_144\jre\bin\jvm.dll。 找不见？！没关系，我们可以直接通过通过调用 jpype.getDefaultJVMPath() 得到默认的 JVM 路径（可以参见后续给出的使用实例）。 —- 参数 2 —-： args 为可选参数，会被 JPype 直接传递给 JVM 作为 Java 虚拟机的启动参数。此处适合所有合法的 JVM 启动参数，例如： -agentlib:libname[=options] -classpath classpath -verbose -Xint 1—2）jpype.startJVM() 使用实例： ############# JpypeStartJVM at PythonRuntime ############# # Start the JAVA Virtual Machine：[ startJVM(jvmpath, *args) ]; getDefaultJVMPath():Gets the default JVM path. startJVM(getDefaultJVMPath(),jvmArgs) 1—3）判断 JVM 是否启动： JPype 提供的 jpype.isJVMStarted() 函数的作用用于判断当前时刻 JVM 是否已启动。jpype.isJVMStarted() 的返回值为 True 表示 JVM 已经启动，返回值为 False 表示 JVM 还未启动。 import jpype jvmPath = jpype.getDefaultJVMPath() jvmArg = &quot; -Xint &quot; if not jpype.isJVMStarted(): # Start the JAVA Virtual Machine：[ startJVM(jvmpath, *args) ]; getDefaultJVMPath():Gets the default JVM path. jpype.startJVM(jvmPath，jvmArg) 注意：为了避免非预期程序逻辑将 JVM 自动关闭掉，最好在使用 jpype.startJVM() 时与 jpype.isJVMStarted() 配合使用。 2）如何关闭 JVM2—1）当使用完 JVM 后，可以通过 jpype.shutdownJVM() 来关闭 JVM 该函数没有输入参数。当 python 程序退出时，JVM 会自动关闭。 shutdownJVM() 2—2）jpype.shutdownJVM() 使用实例： # Shut down JVM shutdownJVM() 1.3.2 How To Calls Java1）访问 JAVA 的系统属性很多时候 Java 应用需要设置或者获取 JVM 中的系统属性。假设：你要设置的属性名为 yourProperty，属性值为 yourValue 。 1-1）设置 JVM 中的系统属性： JVM 启动时设置系统变量示例 –&gt; import jpype jvmPath = jpype.getDefaultJVMPath() jvmArg = &quot; -DyourProperty=yourValue &quot; if not jpype.isJVMStarted(): jpype.startJVM(jvmPath，jvmArg) 或：在程序中设置系统变量示例 –&gt; import jpype prop= &quot; yourProperty &quot; value = &quot; yourValue &quot; system = jpype.JClass(&apos;java.lang.System&apos;) system.setProperty(str(prop),str(value)) 1-2）获取系统变量示例 import jpype prop = “ yourProperty ” system = jpype.JClass(&apos;java.lang.System&apos;) value = system.getProperty(str(prop)) 2）直接调用 JAVA APIfrom jpype import * import os.path startJVM(&quot;C:/Java/jdk1.6.0_10/jre/bin/client/jvm.dll&quot;,&quot;-ea&quot;) java.lang.System.out.println(&quot;helloWorld&quot;) shutdownJVM() 3）调用 JAVA Class 和 JAVA 第三方扩展 JAR 包3-1）调用第三方 Java 扩展包示例 –&gt; ############ Jpype local Variable Define ############ import jpype # Define the directory where the jar package is located (define the lib) jarpath = os.path.join(os.path.abspath(&apos;.&apos;), &apos;/root/guojie/&apos;) # Define the jar package name jarname = &apos;TextVectExpression-0.0.1-SNAPSHOT.jar&apos; # Define the directory where the jvm.dll file is located jvmPath = getDefaultJVMPath() # Start the JAVA Virtual Machine：[ startJVM(jvmpath, *args) ]; getDefaultJVMPath():Gets the default JVM path. if not jpype.isJVMStarted(): startJVM(getDefaultJVMPath(),&quot;-ea&quot;, &quot;-Djava.class.path=%s&quot; % (jarpath + jarname)) 3-2) 使用 JAVA 类 Jpype 中调用的 JAVA CLASS 可以是普通类文件，也可以是第三方扩展包中的类。 # 假设要调用 Java 类定义如下（普通类文件/第三方扩展包中的类） package src.com.sts.javaproject; public class JavaClass { public String value = &quot;&quot;; /** * Creates a new JavaClass object. * * @param value */ public JavaClass(String value) { this.value = value; } public String getValue() { return this.value; } public void setValue(String val) { this.value = val; } } 清单1：获取对应 Java 类 javaClass = jpype.JClass(&apos;src.com.sts.javaproject.JavaClass&apos;) 清单2：调用类的构造函数生成实例 actual_parameter = &quot; oldvalue &quot; javaInstance = javaClass(actual_parameter) 清单3：调用 CLASS 中的方法(可获取到返回值) javaInstance.setValue( “ newvalue ” ) print javaInstance.getValue() 1.3 Jpype 调用 JAVA JAR 测试实例&apos;&apos;&apos; ### JpypeDemo:以文本向量表达为例 ### ## Jpype Function: Start a JVM at python runtime to run Java jar &apos;&apos;&apos; from jpype import * import os.path import sys ############################ Jpype local Variable Define ########################### # Define the directory where the jar package is located (define the lib) jarpath = os.path.join(os.path.abspath(&apos;.&apos;), &apos;/root/PackageJar/&apos;) # Define the jar package name jarname = &apos;TextVectExpression-0.0.1-SNAPSHOT.jar&apos; # Define the class file path classpath = &apos;com.project.sts.textVector.TextVectExpression&apos; # Define preparticiple text text = &apos; JPype是一个能够让 Python 代码方便地调用 Java 代码的工具，从而克服了 Python 在某些领域（如服务器端编程）中的不足。&apos; ################################ Jpype Java API ############################### # JpypeJavaApi Function: Calling TextVectExpression-0.0.1-SNAPSHOT.jar through jpype, return javaInstance. def jpype_java_api(classpath, jarpath, jarname): if not isJVMStarted(): # Start the JAVA Virtual Machine：[ startJVM(jvmpath, *args) ]; getDefaultJVMPath():Gets the default JVM path. startJVM(getDefaultJVMPath(),&quot;-ea&quot;, &quot;-Djava.class.path=%s&quot; % (jarpath + jarname)) # Java println() object: Use to println(log) == [jprint(log)] #jprint = java.lang.System.out.println #JDClass = JClass(&quot;com.neusoft.sts.Test.App&quot;) JDClass = JClass(classpath) # Call java constructor javaInstance = JDClass() #javaInstance = JPackage(&quot;jpype&quot;).JpypeDemo() # 两种创建实例 javaInstance 的方法 return javaInstance try: # [1]: TextVectExpression-0.0.1-SNAPSHOT.jar result = javaInstance.run(text) for index in result: jprint(&apos;shape[i]: &apos; + str(index)) except Exception as e: print (&apos;Caught exception : &apos;, JavaException.message() ) print (JavaException.stackTrace()) except: print (&quot;Unknown Error&quot;) finally: # Shut down JVM shutdownJVM() 1.4 Java 类型到 python 类型的转换# 表 ：Java 类型到 python 类型的转换 Java 类型 转换成的 python 类型 byte, short and int --&gt; int long --&gt; long float and double --&gt; float boolean --&gt; int of value 1 or 0 char --&gt; unicode of length 1 String --&gt; unicode arrays --&gt; JArray other Java object --&gt; JavaObject Class --&gt; JavaClass array Class --&gt; JavaArrayClass 示例： # Jpype 调用 JAVA 类的 run() 方法的返回值是一个 array[float] ，Python中使用 result 接收，成为JArray.float[]。 result = javaInstance.run(text) # 还可以通过 list(),将其转化为 list]]></content>
      <categories>
        <category>Jpype</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Jpype</tag>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Shell 脚本自动化部署]]></title>
    <url>%2FAutomated-Deployment%2FLinux-Shell-%E8%84%9A%E6%9C%AC%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[在项目开发中，我们经常会遇到这样的需求：多台服务器设备要做相同的部署工作时，一般来说，我们需要分别远程登陆 Server 列表中的所有设备，然后按照应用程序官方给出的安装流程依次对所有设备进行安装。那么随之而来将会产生一个问题： 当服务器集群中有大量 Server 节点，并且部署大量的应用程序时，你还会考虑逐台登录然后安装配置么？ 答案很显然是不会！如：开发环境需要我们在一个 Server 节点数为 10 的集群中安装配置 JDK (很简单的需求)。我们需要分别远程登陆到这 10 台的 Server 上完成 10 次 JDK 的安装以及配置。那么，当节点数增加到成百上千呢！？这无疑是最笨拙的方法… 为了避免上述需求中重复性的应用程序部署，就需要用到自动化部署脚本。简单来说，自动化部署就是批量的在目标机器上安装应用程序。本文会结合一些简单应用来说明软件自动化部署的实现思路。 1. 设计思路在集群一台 Server（主服务器）运行脚本，集群中其它 Server 节点会完成相同的安装以及配置； 1）环境准备： 集群节点需要实现 SSH 免密码登陆 或者 脚本中包含 SSH 免密码登陆相关实现； 软件自动化部署脚本：放置于主服务器，会被分发到集群中各 Server 节点，用于应用程序的自动化安装； 自动化部署引导脚本：放置于主服务器上用于引导集群中各节点完成软件的自动化安装。 2）脚本执行分为两个过程： 1. 如果未配置 SSH 免密登陆先完成配置，然后使用自动化部署引导脚本 `autodeply.sh` 登陆节点服务器，发送一个软件自动部署脚本 `install.sh` 脚本到各个 Server 上； 2. 由软件自动部署脚本 `install.sh` 实现软件的自动化部署操作。 上述执行过程又可以细分为：SSH 登陆远程节点发送自动化脚本、下载安装包、安装以及配置环境变量。 2. JDK 自动化部署实例2.1 实验环境1）Servers 三台 Centos Server：一台作为主服务器，其它两台为其它服务器节点。 BASE_SERVER（主服务器）IP：192.168.2.2 NODE1 IP：192.168.2.3 NODE2 IP：192.168.2.4 2）待安装应用程序 JDK 版本号：jdk1.8.0_201 3）Reference 1. 集群所有节点服务器安装配置 SSH 下面我们会首先介绍在一台 Server 节点中，如何实现应用程序的自动部署，即：应用程序自动部署脚本 install.sh 的具体实现。 然后说明如何通过 自动化部署引导脚本 autodeply.sh 在集群中实现多台 Server 节点的自动化快捷部署。 2.2 实现自动化部署脚本软件自动部署脚本 install.sh 脚本用于在当前集群节点设备上完成应用程序的自动化下载，安装以及环境变量配置： 1）相关声明 首先我们来指定一些应用程序下载、安装相关的变量。例如：JDK 安装版本、JDK 安装目录，以及 JDK 下载 URL 地址等： #!/bin/bash #this shell to use installing jdk1.8.121 echo &quot;------------------start to install jdk----------------------&quot; # 是否切换到 root 权限 #sudo su ##### ------ part 1: HyperParameters Definition ------ ##### # 指定主服务器地址： #BASE_SERVER=192.168.2.2 # 指定 JDK Version Name： JDK_VERSION=jdk1.8.0_201 # 指定 JDK 下载源（URL）： JDK_URL=https://download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-linux-x64.tar.gz # 指定 JDK Install Package Name： JDK_PACKAGE=jdk-8u201-linux-x64.tar.gz # 指定 JDK 安装目录： JDK_DOWNLOAD_DIR=/usr/local/java # Create the installation directory make_directory(){ if [ ! -d $JDK_DOWNLOAD_DIR ];then mkdir $JDK_DOWNLOAD_DIR cd $JDK_DOWNLOAD_DIR else echo &quot;Add another JAVA version: $JDK_VERSION&quot; cd $JDK_DOWNLOAD_DIR fi } make_directory 注意，指定 JDK 下载源（URL）： JDK_URL 来源于 JDK 官网下载链接，你可以根据需要指定相应版本下载链接，然后将 JDK Version 、JDK Install Package 修改为相应版本即可。 |——————————————– 我们知道，上述指定的 JDK_URL 源来至于 JDK 官网，也就是说集群中所有节点设备都需要到 JDK 官网下载安装包。一般，为了避免后续安装包下载受网络带宽的，我们可以提前将需要安装的应用程序安装包（例如：jdk-8u201-linux-x64.tar.gz）下载好一份并放置于 主服务器 上。当其它节点服务器需要时，只需要走局域网络从主服务器下载即可。 这里，假设我们下载好了 jdk1.8.0_201 安装包：jdk-8u201-linux-x64.tar.gz，并放置于主服务器下的 /root/Download 目录中。那么，JDK_URL 也可以设置为如下： JDK_URL= 192.168.2.2/root/Download/jdk-8u201-linux-x64.tar.gz |——————————————– 2）下载以及安装 JDK 下面代码会从我们在 1）中定义 JDK_URL 源中下载 JDK 安装包，并解压完成安装： # Download jdk1.8.tar.gz wget --no-check-certificate --no-cookies -c --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; $JDK_URL # tar &amp;&amp; install tar -zxvf $JDK_PACKAGE rm $JDK_PACKAGE 注意，wget 通过 --no-check-certificate --no-cookies -c --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; 可以实现 免检测证书、免用户验证 下载（如果不设置的话，可能无法从 JDK 官网下载链接下载 JDK 安装包的）。 3）配置环境变量 这里我们将 JDK 环境变量配置到 ~/.bashrc 文件中，如下： #set environment echo &quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Configuring the JDK environment variable &gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot; export JAVA_HOME=&quot;/usr/local/java/jdk1.8.0_201&quot; if ! grep &quot;JAVA_HOME=/usr/local/java/jdk1.8.0_201&quot; ~/.bashrc then echo &quot;JAVA_HOME=/usr/local/java/jdk1.8.0_201&quot; | sudo tee -a ~/.bashrc echo &quot;export JAVA_HOME&quot; | sudo tee -a ~/.bashrc echo &quot;JRE_HOME=${JAVA_HOME}/jre&quot; | sudo tee -a ~/.bashrc echo &quot;export JRE_HOME&quot; | sudo tee -a ~/.bashrc echo &quot;CLASSPATH=.:\${JAVA_HOME}/lib/dt.jar:\${JAVA_HOME}/lib/tools.jar&quot; | sudo tee -a ~/.bashrc echo &quot;export CLASSPATH&quot; | sudo tee -a ~/.bashrc echo &quot;PATH=${JAVA_HOME}/bin:\$PATH&quot; | sudo tee -a ~/.bashrc echo &quot;$PATH&quot; fi # update environment #source ~/.bashrc echo &quot;--------------- JDK 1.8.0_171 has installed ! ---------------&quot; 下面我们给出软件自动部署脚本 install.sh 的完整代码： #!/bin/bash #this shell to use installing jdk1.8.121 echo &quot;------------------start to install jdk----------------------&quot; # 是否切换到 root 权限 #sudo su ##### ------ part 1: HyperParameters Definition ------ ##### # 指定主服务器地址： #BASE_SERVER=192.168.2.2 # 指定 JDK Version Name： JDK_VERSION=jdk1.8.0_201 # 指定 JDK 下载源（URL）： JDK_URL=https://download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-linux-x64.tar.gz # 指定 JDK Install Package Name： JDK_PACKAGE=jdk-8u201-linux-x64.tar.gz # 指定 JDK 安装目录： JDK_DOWNLOAD_DIR=/usr/local/java # Create the installation directory make_directory(){ if [ ! -d $JDK_DOWNLOAD_DIR ];then mkdir $JDK_DOWNLOAD_DIR cd $JDK_DOWNLOAD_DIR else echo &quot;Add another JAVA version: $JDK_VERSION&quot; cd $JDK_DOWNLOAD_DIR fi } make_directory ##### ------ part 2: jdk1.8.tar.gz Setup ------ ##### # Download jdk1.8.tar.gz # yum install -y wget wget --no-check-certificate --no-cookies -c --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; $JDK_URL # tar &amp;&amp; install tar -zxvf $JDK_PACKAGE rm $JDK_PACKAGE ##### ------ part 3: Environment Configuration ------ ##### #set environment echo &quot;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Configuring the JDK environment variable &gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot; export JAVA_HOME=&quot;/usr/local/java/jdk1.8.0_201&quot; if ! grep &quot;JAVA_HOME=/usr/local/java/jdk1.8.0_201&quot; ~/.bashrc then echo &quot;JAVA_HOME=/usr/local/java/jdk1.8.0_201&quot; | sudo tee -a ~/.bashrc echo &quot;export JAVA_HOME&quot; | sudo tee -a ~/.bashrc echo &quot;JRE_HOME=${JAVA_HOME}/jre&quot; | sudo tee -a ~/.bashrc echo &quot;export JRE_HOME&quot; | sudo tee -a ~/.bashrc echo &quot;CLASSPATH=.:\${JAVA_HOME}/lib/dt.jar:\${JAVA_HOME}/lib/tools.jar&quot; | sudo tee -a ~/.bashrc echo &quot;export CLASSPATH&quot; | sudo tee -a ~/.bashrc echo &quot;PATH=${JAVA_HOME}/bin:\$PATH&quot; | sudo tee -a ~/.bashrc echo &quot;$PATH&quot; fi # update environment #source ~/.bashrc echo &quot;--------------- JDK 1.8.0_171 has installed ! ---------------&quot; 2.3 实现自动化部署引导脚本这一小节我们来看自动化部署引导脚本 autodeply.sh 是如何管理集群中所有节点服务器完成应用程序安装的： 1）相关声明 和 2.2 实现自动化部署脚本 一样，首先我们需要指定集群节点服务器相关配置项。例如 集群节点服务器 IP、登陆用户密码等： #!/bin/bash # 指定主服务器 IP： BASE_SERVER=192.168.2.2 # 集群其它节点服务器 IP： SERVERS=&quot;192.168.2.3 192.168.2.4&quot; # 主服务器远程登陆集群其它节点服务器用户密码（建议集群各节点密码最好一致）： PASSWORD=123456789 2）SSH 免密码登陆 auto_ssh_copy_id() { expect -c &quot;set timeout -1; spawn ssh-copy-id $1; expect { *(yes/no)* {send -- yes\r;exp_continue;} *assword:* {send -- $2\r;exp_continue;} eof {exit 0;} }&quot;; } ssh_copy_id_to_all() { for SERVER in $SERVERS do auto_ssh_copy_id $SERVER $PASSWORD done } ssh_copy_id_to_all 3）发送自动化部署脚本 for SERVER in $SERVERS do scp install.sh root@$SERVER:/root ssh root@$SERVER /root/install.sh done 下面我们给出自动化部署引导脚本 autodeply.sh 的完整代码： #!/bin/bash # 指定主服务器 IP： BASE_SERVER=192.168.2.2 # 集群其它节点服务器 IP： SERVERS=&quot;192.168.2.3 192.168.2.4&quot; # 主服务器远程登陆集群其它节点服务器用户密码（建议集群各节点密码最好一致）： PASSWORD=123456789 auto_ssh_copy_id() { expect -c &quot;set timeout -1; spawn ssh-copy-id $1; expect { *(yes/no)* {send -- yes\r;exp_continue;} *assword:* {send -- $2\r;exp_continue;} eof {exit 0;} }&quot;; } ssh_copy_id_to_all() { for SERVER in $SERVERS do auto_ssh_copy_id $SERVER $PASSWORD done } ssh_copy_id_to_all for SERVER in $SERVERS do scp install.sh root@$SERVER:/root ssh root@$SERVER /root/install.sh done 2.4 开始部署1）增加自动化部署脚本执行权限： root@ubuntu-Lenovo-Product:~# chmod +x autodeply.sh install.sh 2）主服务器必须配置有 SSH 公秘钥： 如果没有，可以直接执行如下命令，一路 Enter 即可： root@ubuntu-Lenovo-Product:~# ssh-keygen 3）自动化部署： 执行如下命令即可开始集群节点应用程序的自动化部署： root@ubuntu-Lenovo-Product:~# ./autodeply.sh # 或 root@ubuntu-Lenovo-Product:~# sh autodeply.sh]]></content>
      <categories>
        <category>Automated Deployment</category>
      </categories>
      <tags>
        <tag>Linux Shell</tag>
        <tag>Automated Deployment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 + Nvidia GTX 970 + CUDA8.0 + CUDNN5.1 + Anaconda3 + Tensorflow1.2]]></title>
    <url>%2FTensorFlow%2FUbuntu16-04-Nvidia-GTX-970-CUDA8-0-CUDNN5-1-Anaconda3-Tensorflow1-2%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 关于搭建 TensorFlow [GPU Support] 环境： 搭建基于 GPU 显卡加速运算的 TensorFlow 深度学习环境前，建议先明确当前服务器独立显卡（GPU ）详细信息（包括显卡型号、显存、计算性能等等）。这是因为，TensorFlow只支持某些类型的显卡（N卡：Nvidia），有些 TensorFlow 模型需要较大的 GPU 内存，或更多的 GPU 计算核心（即更强的计算能力，加速模型运算），了解服务器中 GPU 的详细信息可以更好的帮助我们进行计算加速。 然后依据附录中的 Tensorflow&amp;&amp;NVIDIA 版本信息对照表，选择安装相互兼容的显卡并行计算包（CUDA）、深度神经网络 GPU 加速包（CUDNN）以及相应版本的 Tensorflow 即可。 Environmental introduction搭建环境: Ubuntu16.04 + Nvidia GeForce GTX 970 + CUDA8.0 + CUDNN5.1 + Anaconda3 + Tensorflow1.2 硬件环境介绍: 搭载 Nvidia GeForce GTX 970 独立显卡联想 ThinkCentre 商业主机； 搭载 3TB 硬盘用于存储数据以及 32GB（4 * 8GB）内存条； 四核心 Inter CORE I7 CPU； ThinkCentre 服务器安装 Ubuntu16.04。 联想 ThinkCentre 商业主机搭载 Nvidia GTX 960 独立显卡时问题： 电源提供接口不够，购买电源转接线电源口 IDE 大 4D 芯一分二（4 pin公转母），显卡 6pin 转双大 4D。电源接线不够时，建议升级较大功率电源。 机箱主板空间较小无法安装索泰（ZOTAC）Nvidia GTX 970 独立显卡，购买 PCI-E 16X 显卡延长线。 购买 DVI 转 VGA24+5 Pin（显卡显示器高清视频转换接头）。 CUDA &amp;&amp; CUDNN 再来了解一下什么是 CUDA 和 CUDNN： CUDA 是显卡厂商 NVIDIA 推出的通用并行计算架构（平台），它使得 GPU 能够解决并行的、复杂的计算问题。 CUDNN 其实就是 cuDNN（CUDA DNN），它是深度神经网络的 GPU 加速库。想要在 CUDA 上运行深度神经网络，就要安装 cuDNN。 1. Nvidia GTX 970 安装显卡驱动搭建 TensorFlow [GPU Support] 环境第一步就是要解决显卡驱动问题，具体步骤如下： 1.1 查询 GPU 详细信息搭载 GPU 独显的服务器安装 Ubuntu16.04 后（接 GPU 显示输出）： 1）查询本机的显卡型号 因显卡一般是 PCI 接口，可以通过 lspci 查询显卡相关信息。一般我们可以查看到两种类型显卡：一块时集显；一块是独显。 1234567891011121314$ sudo lspci -vnn |grep VGA -A 1201:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM206 [GeForce GTX 960] [10de:1401] (rev a1) (prog-if 00 [VGA controller]) Subsystem: ZOTAC International (MCO) Ltd. GM206 [GeForce GTX 960] [19da:1379] Flags: bus master, fast devsel, latency 0, IRQ 124 Memory at f6000000 (32-bit, non-prefetchable) [size=16M] Memory at e0000000 (64-bit, prefetchable) [size=256M] Memory at f0000000 (64-bit, prefetchable) [size=32M] I/O ports at e000 [size=128] Expansion ROM at 000c0000 [disabled] [size=128K] Capabilities: [60] Power Management version 3 Capabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+ Capabilities: [78] Express Legacy Endpoint, MSI 00 Capabilities: [100] Virtual Channel Capabilities: [258] L1 PM Substates =============================================================== 显卡信息： 独立显卡: 硬件厂商 NAVIDA（N卡）；型号名称 GM206（GeForce GTX 960）。 集成显卡: 这里由于直接是搭载 GPU 的服务器安装系统，所以集显驱动未安装，无法看到集显信息。 =============================================================== 2）确认本机显卡驱动是否正常加载 1234567891011121314$ sudo lshw -C display$ sudo lshw -C display *-display description: VGA compatible controller product: GM206 [GeForce GTX 960] vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:01:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress vga_controller bus_master cap_list rom configuration: driver=nouveau latency=0 resources: irq:124 memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:c0000-dffff 输出信息 configuration 字段中，如果 driver=“驱动名称” 不为空，说明系统支持该显卡的驱动； 我们可以看出，Ubuntu 系统支持 GTX 970 显卡且自动安装有一个默认的显卡驱动：nouveau（Linux 开源的显卡驱动），但 nouveau 驱动开发不是很完善。 事实上，我们需要重新安装适合显卡的（这里是 GTX970） Nvidia 显卡驱动才可以正常使用深度学习显卡加速。下面我们来看如何安装合适的 Nvidia 显卡驱动： 1.2 查询适合的 Nvidia 驱动1）首先打开以及登陆 Nvidia Driver 官网 根据独立显卡型号查询适合自己显卡的驱动： 2）显卡驱动信息查询 1.3 安装 Nvidia 驱动查询到适用的 Nvidia 驱动版本后，开始安装 Nvidia 驱动 390.25： 1）使用 PPA 安装 1$ sudo add-apt-repository ppa:graphics-drivers/ppa 第一次运行会出现如下的警告： 123456789101112131415161718Fresh drivers from upstream, currently shipping Nvidia.## Current StatusWe currently recommend: `nvidia-361`, Nvidia's current long lived branch.For GeForce 8 and 9 series GPUs use `nvidia-340`For GeForce 6 and 7 series GPUs use `nvidia-304`## What we're working on right now:- Normal driver updates- Investigating how to bring this goodness to distro on a cadence.- ## WARNINGS:This PPA is currently in testing, you should be experienced with packaging before you dive in here. Give us a few days to sort out the kinks.Volunteers welcome! See also: https://github.com/mamarley/nvidia-graphics-drivers/http://www.ubuntu.com/download/desktop/contribute更多信息： https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa按回车继续或者 Ctrl+c 取消添加 –&gt; Enter 后继续： 12345678910# Enter 后显示如下信息表示添加成功：gpg: keyring `/tmp/tmp7b1tsfut/secring.gpg' createdgpg: keyring `/tmp/tmp7b1tsfut/pubring.gpg' createdgpg: requesting key 1118213C from hkp server keyserver.ubuntu.comgpg: /tmp/tmp7b1tsfut/trustdb.gpg: trustdb createdgpg: key 1118213C: public key "Launchpad PPA for Graphics Drivers Team" importedgpg: no ultimately trusted keys foundgpg: Total number processed: 1gpg: imported: 1 (RSA: 1)OK 如果添加 PPA 仓库报错的话，可以先 remove 掉，然后重新尝试： 12$ sudo add-apt-repository --remove ppa:graphics-drivers/ppasudo add-apt-repository ppa:graphics-drivers/ppa 添加完 PPA 仓库，需要更新一下本地 apt-get 源： 1$ sudo apt-get update 添加 PPA 仓库并且更新源后，先来识别显卡模型和查看源中推荐的驱动程序： 12345678910111213$ ubuntu-drivers devices== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==modalias : pci:v000010DEd00001401sv000019DAsd00001379bc03sc00i00model : GM206 [GeForce GTX 960]vendor : NVIDIA Corporationdriver : nvidia-415 - third-party freedriver : nvidia-384 - distro non-freedriver : nvidia-396 - third-party freedriver : nvidia-418 - third-party freedriver : nvidia-410 - third-party freedriver : xserver-xorg-video-nouveau - distro free builtindriver : nvidia-390 - third-party freedriver : nvidia-430 - third-party free recommended –&gt; 开始安装： 首先你可以看到，PAA 仓库中提供有我们之前查询到的 390 版本的 Nvidia 驱动 。 当然，你可根据源中推荐的 Nvidia 版本进行安装（recommended）。这里我选择了 nvidia-384，它是支持 GTX 970 的一个稳定版本。 12345$ sudo apt-get install nvidia-384# 安装一些可能的依赖插件：$ sudo apt-get install mesa-common-dev$ sudo apt-get install freeglut3-dev 结论： 推荐使用 PPA 仓库安装 Nvidia 驱动，这是最简单的驱动安装方式。关于使用官方的驱动进行手动安装这里不介绍了。 2) 重启系统让 GTX970 显卡驱动生效 1$ sudo reboot 3) 驱动安装测试 123456789101112131415161718192021222324$ watch -n -1 nvidia-smi% 显示如下信息表示显卡驱动安装成功（Ctrl + c 可退出查看状态）： Sat Jun 2 17:03:40 2018 +-----------------------------------------------------------------------------+| NVIDIA-SMI 384.130 Driver Version: 384.130 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 960 Off | 00000000:01:00.0 On | N/A || 36% 45C P8 11W / 120W | 3863MiB / 4036MiB | 3% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| 0 1143 G /usr/lib/xorg/Xorg 94MiB || 0 2070 G compiz 42MiB || 0 18635 G /usr/lib/firefox/firefox 1MiB || 0 29337 C ...naconda3/envs/tensorflow-3.5/bin/python 3710MiB |+-----------------------------------------------------------------------------+ 4）确认重新安装的显卡驱动是否正常加载 12345678910111213$ sudo lshw -C display *-display description: VGA compatible controller product: GM206 [GeForce GTX 960] vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:01:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress vga_controller bus_master cap_list rom configuration: driver=nvidia latency=0 resources: irq:127 memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:c0000-dffff 可以发现，driver=nvidia 表明安装成功。 1.4 Nvidia 驱动卸载对于已经安装了 Nvidia 显卡驱动的服务器，可能由于其驱动版本过低，无法正常使用新版本的 CUDA。故，我们一般会将 Nvidia 显卡驱动更新到一个较新的版本。 前面我们已经知道如何安装全新的 Nvidia 显卡驱动，这里来看如何卸载服务器原有的显卡驱动程序： –&gt; 卸载低版本 Nvidia 显卡驱动 1$ sudo apt-get remove nvidia* 上述我们已经卸载了系统中的 Nvidia 显卡驱动，接下来我们需要根据 1.3 安装 Nvidia 驱动 完成新的 Nvidia 显卡驱动的安装（注意此时千万不能重启，重新电脑可能会导致无法进入系统，安装好新驱动后再重启）。 1.5 Want to know more好奇么？驱动安装不是都已经结束了，怎么还有一小节？？？黑脸 前面我们是基于搭载 GPU 独显的服务器安装 Ubuntu16.04 后开始驱动安装的。其实我们还可以先把 GPU 拿掉，然后在只有集显的服务器上先安装 Ubuntu16.04 ，接着按照上面的 （1） 过程安装 Nvidia 驱动，然后 shutdown 关机安装上 GPU 重启即可。 最后通过 （3）、（4）过程进行测试，会发现也可以。 这种方法可以避免驱动安装中的一些麻烦。 在开始 CUDA 和 cuDNN 的安装之前，通过需要查看 Tensorflow &amp;&amp; NVIDIA 版本信息对照表，选择和TensorFlow [GPU Support] 版本兼容的 CUDA &amp;&amp; cuDNN 安装版本。 2. 下载和安装 CUDA 8.0在安装 CUDA 之前，Google 了一下，发现在 Ubuntu 下安装 CUDA8.0 非常常见，支持 GTX 970（其它版本 CUDA 安装类似），下面我们将以 CUDA8.0 的安装为样例： 注意，安装 CUDA8.0 之前请先确认系统中是否已安装有默认的 CUDA 版本？执行如下命令查看系统 CUDA 版本： 1234567891011$ nvcc -V# 输出如下信息表示当前系统已安装有 CUDA，安装目录见：/usr/local/nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2016 NVIDIA CorporationBuilt on Tue_Jan_10_13:22:03_CST_2017Cuda compilation tools, release 8.0, V8.0.61# 没有的话输出如下：The program 'nvcc' is currently not installed. You can install it by typing:sudo apt install nvidia-cuda-toolkit 如果有，请先跳转至 2.4 CUDA 卸载，先卸载掉系统中已安装的 CUDA 版本，再开始下面的步骤。 2.1 CUDA 下载下载 CUDA 需要注册和登陆 NVIDIA 开发者账号，CUDA8 下载页面提供了很详细的系统选择和安装说明。这里选择了 Ubuntu16.04 系统 Runfile 安装方案，千万不要选择 deb 方案，前方无数坑： 这里，我们首先提供 CUDA 8.0 下载地址，并且选择做如下平台设置（其它版本戳 这里）: 配置好平台设置后，进入下载界面。如下： 下载好的 cuda_8.0.27_linux.run 有 1.4G。下面按照 Nivdia 官方给出的方法安装 CUDA8： 2.2 CUDA 安装1$ sudo sh cuda_8.0.27_linux.run --tmpdir=/opt/temp/ 这里加了 --tmpdir 主要是因为直接运行 sudo sh cuda_8.0.27_linux.run 可能会提示空间不足的错误（如下），实际上是全新的电脑主机，硬盘足够大的（当报错后，你知道如何解决即可）。 1234Not enough space on parition mounted at /.Need 5091561472 bytes.Disk space check has failed. Installation cannot continue. –&gt; 执行 sh cuda_8.0.27_linux.run 后会有一系列提示让你确认，非常非常非常非常关键的地方是是否安装 361 这个低版本的驱动： 1Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.62 ? 答案必须是 n，否则之前安装的 GTX970 驱动就白费了，而且后续问题多多。 –&gt; Next（详细安装步骤如下）: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061Logging to /opt/temp//cuda_install_6583.logUsing more to view the EULA.End User License Agreement--------------------------Preface-------The following contains specific license terms and conditionsfor four separate NVIDIA products. By accepting thisagreement, you agree to comply with all the terms andconditions applicable to the specific product(s) includedherein.Do you accept the previously read EULA?accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.62?(y)es/(n)o/(q)uit: nInstall the CUDA 8.0 Toolkit?(y)es/(n)o/(q)uit: yEnter Toolkit Location[ default is /usr/local/cuda-8.0 ]:Do you want to install a symbolic link at /usr/local/cuda?(y)es/(n)o/(q)uit: yInstall the CUDA 8.0 Samples?(y)es/(n)o/(q)uit: yEnter CUDA Samples Location[ default is /home/textminer ]:Driver: Not SelectedInstalling the CUDA Toolkit in /usr/local/cuda-8.0 ...Installing the CUDA Samples in /home/textminer ...Copying samples to /home/textminer/NVIDIA_CUDA-8.0_Samples now...Finished copying samples. ============ Summary ============Driver: Not SelectedToolkit: Installed in /usr/local/cuda-8.0Samples: Installed in /home/textminerPlease make sure that- PATH includes /usr/local/cuda-8.0/bin- LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc/ld.so.conf and run ldconfig as rootTo uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-8.0/binPlease see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-8.0/doc/pdf for detailed information on setting up CUDA.***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 361.00 is required for CUDA 8.0 functionality to work.To install the driver using this installer, run the following command, replacing with the name of this run file:sudo .run -silent -driverLogfile is /opt/temp//cuda_install_6583.log –&gt; 配置环境变量： 安装完毕后，需要再声明一下环境变量，并将其写入到 ~/.bashrc 的尾部: 12export PATH=/usr/local/cuda-8.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH 更新 ~/.bashrc 配置文件： 1$ source ~/.bashrc 注意：如果是已经安装了 NVIDIA 和 CUDA 的云服务器，还需要添加环境变量才可以使用。 如果环境变量设置错误，PATH 值被覆盖了，这会导致 ls、make 等基本命令都用不了，提示 xxx: command not found。后来查阅资料，通过输入以下语句，可还原 PATH 变量值进行恢复（恢复至默认 PATH 值）： 1export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin 至此，CUDA8.0的安装其实已经完成了！！！但是请注意安装过程中这里可能有报错（missing recommended）： 123Driver: Not SelectedToolkit: Installed in /usr/local/cuda-8.0Samples: Installed in /home/zhou, but missing recommended 不注意的话，会导致后续 CUDA 测试（ nbody 样例）： 123$ cd ~/NVIDIA_CUDA-8.0_Samples/5_Simulations/nbody $ make $ ./nbody 即在 CUDA 上运行 nbody 样例，make 部分报错： 1234567891011&gt;&gt;&gt; WARNING - libGLU.so not found, refer to CUDA Getting Started Guide for how to find and install them. &lt;&lt;&lt;&gt;&gt;&gt; WARNING - libX11.so not found, refer to CUDA Getting Started Guide for how to find and install them. &lt;&lt;&lt;&gt;&gt;&gt; WARNING - gl.h not found, refer to CUDA Getting Started Guide for how to find and install them. &lt;&lt;&lt;&gt;&gt;&gt; WARNING - glu.h not found, refer to CUDA Getting Started Guide for how to find and install them. &lt;&lt;&lt;&gt;&gt;&gt; WARNING - Xlib.h not found, refer to CUDA Getting Started Guide for how to find and install them. &lt;&lt;&lt;[@] /usr/local/cuda-8.0/bin/nvcc -ccbin g++ -I../../common/inc -m64 -ftz=true -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o bodysystemcuda.o -c bodysystemcuda.cu[@] /usr/local/cuda-8.0/bin/nvcc -ccbin g++ -I../../common/inc -m64 -ftz=true -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o nbody.o -c nbody.cpp[@] /usr/local/cuda-8.0/bin/nvcc -ccbin g++ -I../../common/inc -m64 -ftz=true -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o render_particles.o -c render_particles.cpp[@] /usr/local/cuda-8.0/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_20,code=sm_20 -gencode arch=compute_30,code=sm_30 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_60,code=compute_60 -o nbody bodysystemcuda.o nbody.o render_particles.o -L/usr/lib/nvidia-367 -lGL -lGLU -lX11 -lglut[@] mkdir -p ../../bin/x86_64/linux/release[@] cp nbody ../../bin/x86_64/linux/release –&gt; 解决方案：从 make 报错，我们知道缺少一些编译库，下面我们来安装这些库文件： 123$ sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev libglfw3-dev libgles2-mesa-dev$ GLPATH=/usr/lib make –&gt; 成功之后会显示： 12345678910&gt; Windowed mode&gt; Simulation data stored in video memory&gt; Single precision floating point simulation&gt; 1 Devices used for simulationgpuDeviceInit() CUDA Device [0]: "GeForce GTX 1080&gt; Compute 6.1 CUDA device: [GeForce GTX 1080]number of bodies = 256000256000 bodies, total time for 10 iterations: 2291.469 ms= 286.000 billion interactions per second= 5719.998 single-precision GFLOP/s at 20 flops per interaction 2.3 CUDA 测试1）NVCC $ nvcc –V 输出信息显示如下： 2）显示 GPU 信息 –&gt; 进入 cd NVIDIA_CUDA-8.0_Sample/1_Utilities/deviceQuery 目录： –&gt; 执行：make； –&gt; 执行：./deviceQuery，结果如下： 3）nbody 测试见 2.2 CUDA 安装。 恭喜你！至此，CUDA 安装已经完成！！！ 2.4 CUDA 卸载通过前面的介绍，我们了解了如何在服务器上安装 CUDA 应用。事实上，很多时候我们的服务器本身已经默认安装了某个版本的 CUDA，而搭建 TensorFlow GPU Support 环境要求安装特定版本的 CUDA。故这一小节我们来看如何卸载服务器中已安装的 CUDA。 分别针对 .deb 和 .run 两种不同的安装方式（卸载方法不同），这里提供两种方式来卸载系统原有的 CUDA： 1）.run 方法卸载 执行如下命令进行自动卸载（以 cuda-8.0 卸载为例）： $ sudo /usr/local/cuda-8.0/bin/uninstall_cuda-8.0.pl 有上述卸载文件 uninstall_cuda-8.0.pl 就说明是之前是用 .run 文件安装的，没有则是用 .deb 文件安装的，可以使用第二种方法进行卸载： 2）.deb 方法卸载 12345$ sudo apt-get remove cuda$ sudo apt-get autoclean$ sudo apt-get remove cuda* 3）清空残留文件 最后，将当前目录切换至 /usr/local/ 下，查看是否还残留有未删除干净的 CUDA 相关文件： 12345$ cd /usr/local/$ ls% 存在残留文件则删除：sudo rm -rf cuda-8.0 下面让我们来看 CUDNN 的安装： 3. CUDNN 5.1 安装3.1 CUDNN 下载CUDNN 全称 CUDA Deep Neural Network library，是 NVIDIA 专门针对深度神经网络设计的一套 GPU 计算加速库，被广泛用于各种深度学习框架，例如 Caffe, TensorFlow, Theano, Torch, CNTK等。 从 Nvidia 官方 下载链接选择一个版本，不过下载 cuDNN 前同样需要登录甚至填写一个简单的调查问卷，链接界面如下： 3.2 CUDNN 安装安装 CUDNN 比较简单，解压后把相应的文件拷贝到对应的 CUDA 目录下即可： 123456789101112$ tar -zxvf cudnn-8.0-linux-x64-v5.1.tgzcuda/include/cudnn.hcuda/lib64/libcudnn.socuda/lib64/libcudnn.so.5cuda/lib64/libcudnn.so.5.0.5cuda/lib64/libcudnn_static.a$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/$ sudo chmod a+r /usr/local/cuda/include/cudnn.h$ sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 可以发现，cuDNN 中的 5 个文件（全部），在 /usr/local/cuda/include/ &amp;&amp; /usr/local/cuda/lib64/ 中找不到相同文件： 12$ ls /usr/local/cuda/lib64/ |grep libcudnn$ ls /usr/local/cuda/include/ |grep cudnn cuDNN 其实就是 CUDA 的扩展计算库，把 cuDNN 相关库文件添加 CUDA 里，不会对CUDA造成其他影响，即所谓的插入式设计。这保证了当前系统环境中可以存在多个版本的 cuDNN。 3.3 CUDNN 升级这里，假设一下：上面我们已经完成了 cuDNN v5.1 for CUDA8.0 的安装。事实上，我们需要的是 cuDNN v6.0 for CUDA8.0 计算加速包，那么我们如何将 cuDNN 版本升级到 cuDNN v6.0 呢？ 其实很简单。下载 cuDNN v6.0 安装包 cudnn-8.0-linux-x64-v6.0.tgz、解压以及使用 cuDNN v6.0 库文件覆盖 CUDA 中的 cuDNN v5.1 库文件即可。详细步骤如下： $ tar -zxvf cudnn-8.0-linux-x64-v6.0.tgz cuda/include/cudnn.h cuda/lib64/libcudnn.so cuda/lib64/libcudnn.so.6 cuda/lib64/libcudnn.so.6.0.6 cuda/lib64/libcudnn_static.a $ rm /usr/local/cuda/include/cudnn.h $ rm /usr/local/cuda/lib64/libcudnn* $ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ $ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ $ sudo chmod a+r /usr/local/cuda/include/cudnn.h $ sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 至此，CUDA 8.0 以及 CUDNN 5.1 安装完成！接下来就可以开始 Tensorflow GPU Enabled 的安装。 4. Anaconda + Tensorflow-GPU 安装这里我们推荐使用 Anconda 来安装 Tensorflow-GPU。 4.1 Conda Envs install TensorFlow通过 Anaconda 安装 Tensorflow 详细过程见博文 Centos6（7）/Ubuntu 在线搭建 Anaconda &amp; Tensorflow 开发环境中【2.4 使用 Anaconda 安装】小节（Ubuntu14.04/16.04 环境下已测试），篇幅原因不赘述。 对于其它方式 TensorFlow 的安装可以参见博文中的其它章节。 4.2 TensorFlow 导入测试正常情况，如果安装的 Python、TensorFlow 版本正确，import tensorflow as tf 时，无报错。 import tensorflow as tf 时，可能产生报错： libcudnn.so.6:cannot open sharedobjectfile:No such file or directory 问题解释： 根据错误代码，应该是找不到 libcudnn.so.6。到指定文件夹下发现只有 5.0 和 8.0 的版本，没有 6.0，查找原因是因为当前已经是 1.3 版本，而 tensorflow-gpu1.3 已经开始去找 cudnn6 了（也就是说是用 cudnn6 编译的）…所以需要换到 tensorflow-gpu1.2 版本，就解决问题了。 解决方法： 先卸载掉之前安装的 tensorflow 环境或 tensorflow 环境中安装的 tensorflow： $ conda remove –n tensorflow-3.5 –all $ conda create –n tensorflow-3.5 python=3.5 $ pip install tensorflow-gpu==1.2 4.3 GPU 运算测试测试代码：gpu_test.py。这里以一个简单的向量加法为例来测试 TensorFlow [GPU Support] 环境： import tensorflow as tf a = tf.constant([1.0,2.0,3.0], shape=[3], name=”a”) b = tf.constant([1.0,2.0,3.0], shape=[3], name=”b”) result = a + b # 通过 log_device_placement 参数来输出运行每一个运算的设备： sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) print (sess.run(result)) 结果输出信息如下： [3.1] 在没有 GPU 的机器上运行上述代码，可以得到如下输出： Device mapping: no known devices add :/job:localhost/replica:0/task:0/cpu:0 a : /job:localhost/replica:0/task:0/cpu:0 b : /job:localhost/replica:0/task:0/cpu:0 [3.2] 配置好 GPU 环境的 TensorFlow 中，没有明确制定运行设备的话，TensorFlow 会优先选择 GPU 设备运行，会得到如下结果： Device mapping: /job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0 2018-02-04 13:01:33.343286: I tensorflow/core/common_runtime/direct_session.cc:265] Device mapping: /job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0 add: (Add): /job:localhost/replica:0/task:0/gpu:0 2018-02-04 13:01:33.343948: I tensorflow/core/common_runtime/simple_placer.cc:847] add: (Add)/job:localhost/replica:0/task:0/gpu:0 b: (Const): /job:localhost/replica:0/task:0/gpu:0 2018-02-04 13:01:33.343980: I tensorflow/core/common_runtime/simple_placer.cc:847] b: (Const)/job:localhost/replica:0/task:0/gpu:0 a: (Const): /job:localhost/replica:0/task:0/gpu:0 2018-02-04 13:01:33.343997: I tensorflow/core/common_runtime/simple_placer.cc:847] a: (Const)/job:localhost/replica:0/task:0/gpu:0 [2. 4. 6.] 自此，Ubuntu16.04 + Nvidia GTX 970 + CUDA8.0 + CUDNN + Anaconda3 + Tensorflow1.2 深度学习环境搭建完成。 5. I Want To Know More5.1 NVIDIA 驱动相关1）nvidia-smi 英伟达系统管理接口（NVIDIA System Management Interface, 简称 nvidia-smi），属于命令行管理组件，旨在帮助管理和监控 NVIDIA GPU 设备。 执行 nvidia-smi 命令可以查看当前系统安装的 NVIDIA 驱动信息以及 GPU 使用情况，显示如下： $ root@Ubuntu:~# nvidia-smi Thu Feb 28 15:50:39 2019 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 390.46 Driver Version: 390.46 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P4 On | 00000000:00:07.0 Off | 0 | | N/A 30C P8 7W / 75W | 0MiB / 7611MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ 可以看出：驱动版本（NVIDIA-SMI 390.46）、显卡型号（Tesla P4）、内存（7611MiB：8G）以及 GPU 使用率（GPU-Util Compute M.： 0% Default）等。 nvidia-smi 配合 watch -n 一起使用，可用于动态查看 GPU 使用情况： $ watch -n -1 nvidia-smi 2）lspci 查看服务器集成显卡信息： $ lspci | grep VGA 00:02.0 VGA compatible controller: Cirrus Logic GD 5446 查看服务器 NVIDIA 显卡信息： $ lspci | grep NVIDIA 00:07.0 3D controller: NVIDIA Corporation Device 1bb3 (rev a1) 3）集显与独显的切换 NVIDIA 还提供了用于切换显卡的命令。例如，查看当前使用的显卡： $ sudo prime-select query 如何切换 nvidia 显卡： $ sudo prime-select nvidia 如何切换 intel 显卡： $ sudo prime-select intel 5.2 CUDA 多版本共存和实时切换事实上，很多情况下我们需要多 CUDA 版本兼容存在于服务器系统中，并且可以很容易地在不同版本之间进行迅速切换。请参见如下场景： 多人共享使用当前服务器，由于使用的 TensorFlow 框架版本不同，所需要的 CUDA 版本也不同。此时，系统需要存在多个版本的 CUDA； 之前搭建 TensorFlow 环境使用的是 CUDA8.0 和 cuDNN5.1，当我们需要在 TensorFlow 环境中兼容其它深度学习环境时（搭建 TensorFlow + Pytorch 环境），Pytroch GPU Support 可能需要更高版本的 CUDA 以及 cuDNN。此时，升级 CUDA 以及 cuDNN 版本以适应 TensorFlow + Pytorch 环境是必要的。 这一部分，我们会以同时安装 cuda-8.0 和 cuda-9.0 版本为例进行说明。 5.2.1 CUDA 多版本共存1）CUDA 的下载与安装方法选择 对于 cuda-8.0 和 cuda-9.0，无论先安装哪个版本，都一样。上面已经安装过 cuda-8.0 了，这里，我们将补充安装 cuda-9.0 版本。 下载 CUDA 需要注册和登陆 NVIDIA 开发者账号。这里，我们首先给出 CUDA 版本库地址 供选择下载目标版本 CUDA 安装包，版本库页面如下： 通过 CUDA 版本库地址 进入 CUDA9.0 下载页面后，可以看到这里提供了很详细的系统选择和安装说明。这里选择了 Ubuntu16.04 系统 Runfile 安装方案，（千万不要选择 deb 方案，否则前方无数坑）,下载界面如下： 下载完成之后，进入到 CUDA9.0 安装包（cuda_9.0.176_384.81_linux.run）所在目录运行如下命令即可开始进行安装，详细安装过程讲解如下： $ sudo sh cuda_9.0.176_384.81_linux.run Using more to view the EULA. End User License Agreement -------------------------- Preface ------- The following contains specific license terms and conditions for four separate NVIDIA products. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the specific product(s) included herein. # 接受安装协议： Do you accept the previously read EULA? accept/decline/quit: accept # 询问是否重新安装显卡驱动，这里一定要选择 `n`，否则最初安装的显卡驱动会被覆盖产生出错： Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81? (y)es/(n)o/(q)uit: n Install the CUDA 9.0 Toolkit? (y)es/(n)o/(q)uit: y # 定义安装目录（一般默认回车即可，即安装在 `/usr/local/cuda-9.0` 目录下）, Enter Toolkit Location [ default is /usr/local/cuda-9.0 ]: # 询问是否将 `cuda-9.0` 安装目录通过软连接的方式 link 到 `/usr/local/cuda`？【 yes or no 】 均可。 # 系统当前启用的 `cuda` 为哪个版本？取决于哪个版本的 CUDA 安装目录被链接到 `/usr/local/cuda` 上。 # 对于首次安装 CUDA，肯定是需要建立软连接的（yes）；对于安装额外版本的 CUDA，【 yes or no 】 需要根据是否启动新版本 CUDA 决定。 Do you want to install a symbolic link at /usr/local/cuda? (y)es/(n)o/(q)uit: n # 询问是否安装样例，可用于后续检测是否安装成功： Install the CUDA 9.0 Samples? (y)es/(n)o/(q)uit: n # 定义样例安装目录： Enter CUDA Samples Location [ default is /root ]: /home/textminer # *** 安装信息显示 **** # Driver: Not Selected Installing the CUDA Toolkit in /usr/local/cuda-9.0 ... Installing the CUDA Samples in /home/textminer ... Copying samples to /home/textminer/NVIDIA_CUDA-9.0_Samples now... Finished copying samples. =========== = Summary = =========== Driver: Not Selected Toolkit: Installed in /usr/local/cuda-9.0 Samples: Installed in /home/textminer Please make sure that - PATH includes /usr/local/cuda-9.0/bin - LD_LIBRARY_PATH includes /usr/local/cuda-9.0/lib64, or, add /usr/local/cuda-9.0/lib64 to /etc/ld.so.conf and run ldconfig as root To uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-9.0/bin Please see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-9.0/doc/pdf for detailed information on setting up CUDA. ***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 361.00 is required for CUDA 9.0 functionality to work. To install the driver using this installer, run the following command, replacing with the name of this run file: sudo .run -silent -driver Logfile is /opt/temp//cuda_install_6583.log 2）配置 CUDA 环境变量 在配置文件 ~/.bashrc 文件末尾添加如下内容： export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export PATH=$PATH:/usr/local/cuda/bin export CUDA_HOME=$CUDA_HOME:/usr/local/cuda 注意，安装过程中是否创建 symbolic link 是关键。首次安装，选 【yes】；安装额外的版本，选 【no】。自此，我们已经成功完成了 cuda-8.0 和 cuda-9.0 的安装。下面我们来看如何迅速完成 CUDA 版本的切换： 5.2.2 CUDA 版本实时切换通过上文可知，当我们安装了多个版本的 CUDA 之后，CUDA 一般会被安装到 /usr/local/ 目录（或你自己的自定义目录）下。查看一下当前目录下的文件： $ cd /usr/local $ ls bin cuda cuda-8.0 cuda-9.0 etc games ...... 上述 cuda-8.0 和 cuda-9.0 是系统中已安装的 CUDA 版本，而 cuda 就是我们上面创建的 symbolic link。cuda 指向系统当前正在启用的 cuda 版本，方便了我们切换 cuda 版本，可以让我们不用每次都去 ~/.bashrc 修改环境变量的值（cuda-8.0 or cuda-9.0）。 我们先来查看当前 cuda 软链接指向的哪个 cuda 版本: $ stat cuda File: &apos;cuda&apos; -&gt; &apos;/usr/local/cuda-8.0&apos; Size: 19 Blocks: 0 IO Block: 4096 symbolic link Device: 807h/2055d Inode: 133904 Links: 1 Access: (0777/lrwxrwxrwx) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2019-02-28 10:44:36.494902702 +0800 Modify: 2018-02-03 10:52:47.543432671 +0800 Change: 2018-02-03 10:52:47.543432671 +0800 Birth: - 可以看到，cuda&#39; -&gt; &#39;/usr/local/cuda-8.0，此时系统当前正在启用的 cuda 版本为 cuda-8.0。 那么，我们如何快速完成 cuda-9.0 的版本呢？其实很简单： # 查看系统当前启用的 CUDA 版本： $ nvcc -V nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Tue_Jan_10_13:22:03_CST_2017 Cuda compilation tools, release 8.0, V8.0.61 # 从 `cuda8.0` 切换到 `cuda9.0`: $ rm -rf /usr/local/cuda $ sudo ln -s /usr/local/cuda-9.0/ /usr/local/cuda/ $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Tue_Jan_10_13:22:03_CST_2018 Cuda compilation tools, release 9.0, V9.0.176 Tensorflow &amp;&amp; NVIDIA 版本信息对照表For Windows官方经过测试的 Tensorflow &amp;&amp; CUDA &amp;&amp; cuDNN 版本对应关系（持续更新）：https://tensorflow.google.cn/install/source_windows#tested_build_configurations 。 Windows 123456789101112131415161718192021222324252627282930TensorFlow 版本： CPU/GPU: Python 版本: 编译器: 构建工具: CUDNN: CUDA:----------------------------------------------------------------------------------------------------------------------tensorflow-1.13.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.13.0 GPU 3.5-3.6 MSVC 2015 update 3 Bazel 0.15.0 7 9tensorflow-1.12.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.12.0 GPU 3.5-3.6 MSVC 2015 update 3 Bazel 0.15.0 7 9tensorflow-1.11.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.11.0 GPU 3.5-3.6 MSVC 2015 update 3 Bazel 0.15.0 7 9tensorflow-1.10.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.10.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.9.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.9.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.8.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.8.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.7.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.7.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.6.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.6.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.5.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.5.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.4.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.4.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 6 8tensorflow-1.3.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.3.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 6 8tensorflow-1.2.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.2.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 5.1 8tensorflow-1.1.0 CPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.1.0 GPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 5.1 8tensorflow-1.0.0 CPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.0.0 GPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 5.1 8 For Linux &amp;&amp; Mac官方经过测试的 Tensorflow &amp;&amp; CUDA &amp;&amp; cuDNN 版本对应关系（持续更新）：https://tensorflow.google.cn/install/source#tested_build_configurations 。 Linux 123456789101112131415161718192021222324252627282930TensorFlow 版本： CPU/GPU: Python 版本: 编译器: 构建工具: CUDNN: CUDA:----------------------------------------------------------------------------------------------------------------------tensorflow-1.13.1 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.19.2 N Ntensorflow_gpu-1.13.1 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7.4 10.0tensorflow-1.12.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 N Ntensorflow_gpu-1.12.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow-1.11.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 N Ntensorflow_gpu-1.11.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow-1.10.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 N Ntensorflow_gpu-1.10.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow-1.9.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.11.0 N Ntensorflow_gpu-1.9.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.11.0 7 9tensorflow-1.8.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 N Ntensorflow_gpu-1.8.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 7 9tensorflow-1.7.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 N Ntensorflow_gpu-1.7.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow-1.6.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 N Ntensorflow_gpu-1.6.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow-1.5.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.8.0 N Ntensorflow_gpu-1.5.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.8.0 7 9tensorflow-1.4.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.5.4 N Ntensorflow_gpu-1.4.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.5.4 6 8tensorflow-1.3.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 N Ntensorflow_gpu-1.3.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 6 8tensorflow-1.2.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 N Ntensorflow_gpu-1.2.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 5.1 8tensorflow-1.1.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 N Ntensorflow_gpu-1.1.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8tensorflow-1.0.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 N Ntensorflow_gpu-1.0.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8 Mac 12345678910111213141516171819TensorFlow 版本： CPU/GPU: Python 版本: 编译器: 构建工具: CUDNN: CUDA:----------------------------------------------------------------------------------------------------------------------tensorflow-1.13.1 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.19.2 N Ntensorflow-1.12.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.15.1 N Ntensorflow-1.11.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.15.1 N Ntensorflow-1.10.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.15.1 N Ntensorflow-1.9.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.11.1 N Ntensorflow-1.8.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.10.1 N Ntensorflow-1.7.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.10.1 N Ntensorflow-1.6.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.8.1 N Ntensorflow-1.5.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.8.1 N Ntensorflow-1.4.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.5.4 N Ntensorflow-1.3.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.5 N Ntensorflow-1.2.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.5 N Ntensorflow-1.1.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 N Ntensorflow_gpu-1.1.0 GPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 5.1 8tensorflow-1.0.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 N Ntensorflow_gpu-1.0.0 GPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 5.1 8]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>GPU</tag>
        <tag>CUDA&amp;&amp;CUDNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[libstdc++.so.6 version 'CXXABI_1.3.X' not found]]></title>
    <url>%2FTensorFlow%2Flibstdc-so-6-version-CXXABI-1-3-X-not-found%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 在基于 Centos6.9 + Anaconda 搭建 TensorFlow 深度学习环境成功后，在导入 import TensorFlow 时,产生如下报错： /usr/lib/libstdc++.so.6:version ‘CXXBAI_1.3.9’not found... 通过查阅相关资料，发现问题是由于升级了 GCC，却没有将升级后的 GCC 的动态库去替换老版本的 GCC 动态库所致，本文我们会给出相关的解决办法。 详细步骤如下: 1）查看动态库 $ strings /usr/lib64/libstdc++.so.6 | grep CXXABI 输出日志信息如下： 1.CXXABI_1.3 2.CXXABI_1.3.1 3.CXXABI_1.3.2 4.CXXABI_1.3.3 这里可以发现，CXXABI 最高版本只有 1.3.3，没有 1.3.9 的。说明出现这些问题，是因为升级 gcc 时，生成的动态库没有替换老版本 gcc 的动态库。 接下来查看 gcc 动态库链接情况： $ ls -l /usr/lib64/libstdc++.so.6 输出日志信息如下： lrwxrwxrwx. 1 rootroot 19 Apr 7 17:57 $ /usr/lib/libstdc++.so.6 -&gt;libstdc++.so.6.0.13 我们发现，libstdc++.so.6 是个软连接，被链接到了 libstdc++.so.6.0.13 上，而 libstdc++.so.6.0.13 是老版本的动态库。所以下面我们要做的就是将 libstdc++.so.6 连接到新版本的动态库上去。 2）查找编译 GCC 时生成的最新动态库 $ sudo find / -name &quot;libstdc++.so.*&quot; 输出日志信息如下： /usr/lib64/libstdc++.so.6.0.13 /usr/lib64/libstdc++.so.6 /root/anaconda2/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6.0.24 /root/anaconda2/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /root/anaconda2/lib/libstdc++.so.6.0.24 /root/anaconda2/lib/libstdc++.so.6 /root/anaconda2/envs/tensorflow/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6.0.24 /root/anaconda2/envs/tensorflow/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /root/anaconda2/envs/tensorflow/lib/libstdc++.so.6.0.24 /root/anaconda2/envs/tensorflow/lib/libstdc++.so.6 /root/anaconda2/pkgs/libstdcxx-ng-7.2.0-hdf63c60_3/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6.0.24 /root/anaconda2/pkgs/libstdcxx-ng-7.2.0-hdf63c60_3/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /root/anaconda2/pkgs/libstdcxx-ng-7.2.0-hdf63c60_3/lib/libstdc++.so.6.0.24 /root/anaconda2/pkgs/libstdcxx-ng-7.2.0-hdf63c60_3/lib/libstdc++.so.6 其中 /root/anaconda2/envs/tensorflow/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 就是我们新版本的动态库。 3）将原生 /usr/lib64/libstdc++.so.6 备份 $ cp /usr/lib64/libstdc++.so.6 /usr/lib64/libstdc++.so.6.bak $ rm /usr/lib64/libstdc++.so.6 4）将 /usr/lib64/libstdc++.so.6 链接到 /root/anaconda2/envs/tensorflow/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 上去 $ ln –s /root/anaconda2/envs/tensorflow/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /usr/lib64/libstdc++.so.6 5）默认动态库升级完成。重新运行以下命令检查动态库 $ strings /usr/lib64/libstdc++.so.6 | grep CXXABI 输出日志信息如下： CXXABI_1.3 CXXABI_1.3.1 CXXABI_1.3.2 CXXABI_1.3.3 CXXABI_1.3.4 CXXABI_1.3.5 CXXABI_1.3.6 CXXABI_1.3.7 CXXABI_1.3.8 CXXABI_1.3.9 CXXABI_1.3.10 CXXABI_1.3.11 CXXABI_TM_1 CXXABI_FLOAT128 可看到确实是链接到新的动态库上去了。完成！ 实际环境详细安装过程如下： [root@localhost lib64]# find / -name &quot;libstdc++.so.*&quot; /root/anaconda3/envs/tensorflow/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6.0.24 /root/anaconda3/envs/tensorflow/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /root/anaconda3/envs/tensorflow/lib/libstdc++.so.6.0.24 /root/anaconda3/envs/tensorflow/lib/libstdc++.so.6 /root/anaconda3/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6.0.24 /root/anaconda3/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /root/anaconda3/lib/libstdc++.so.6.0.24 /root/anaconda3/lib/libstdc++.so.6 /root/anaconda3/pkgs/libstdcxx-ng-7.2.0-h7a57d05_2/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6.0.24 /root/anaconda3/pkgs/libstdcxx-ng-7.2.0-h7a57d05_2/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /root/anaconda3/pkgs/libstdcxx-ng-7.2.0-h7a57d05_2/lib/libstdc++.so.6.0.24 /root/anaconda3/pkgs/libstdcxx-ng-7.2.0-h7a57d05_2/lib/libstdc++.so.6 /usr/lib64/libstdc++.so.6 /usr/lib64/libstdc++.so.6.0.13 /usr/share/gdb/auto-load/usr/lib64/libstdc++.so.6.0.13-gdb.pyo /usr/share/gdb/auto-load/usr/lib64/libstdc++.so.6.0.13-gdb.pyc /usr/share/gdb/auto-load/usr/lib64/libstdc++.so.6.0.13-gdb.py /usr/share/gdb/auto-load/usr/lib/libstdc++.so.6.0.13-gdb.pyo /usr/share/gdb/auto-load/usr/lib/libstdc++.so.6.0.13-gdb.pyc /usr/share/gdb/auto-load/usr/lib/libstdc++.so.6.0.13-gdb.py [root@localhost lib64]# cp /usr/lib64/libstdc++.so.6 /usr/lib64/libstdc++.so.6.bak [root@localhost lib64]# rm /usr/lib64/libstdc++.so.6 rm: remove symbolic link `/usr/lib64/libstdc++.so.6&apos;? y [root@localhost lib64]# ln -s /root/anaconda3/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /usr/lib64/libstdc++.so.6 [root@localhost lib64]# source activate tensorflow (tensorflow) [root@localhost lib64]# python Python 3.5.4 |Anaconda, Inc.| (default, Nov 20 2017, 18:44:38) [GCC 7.2.0] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import tensorflow &gt;&gt;&gt;]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>GCC Error</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GLIBC 升级失败导致系统崩溃解决方法]]></title>
    <url>%2FTensorFlow%2FGLIBC-%E5%8D%87%E7%BA%A7%E5%A4%B1%E8%B4%A5%E5%AF%BC%E8%87%B4%E7%B3%BB%E7%BB%9F%E5%B4%A9%E6%BA%83%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 在较为老旧版本的 Centos 中搭建 TensorFlow 深度学习开发环境时（例如：Centos6.9），由于 GNU GLIBC 库（内核运行库）版本比较低，我们需要对其进行更新以满足 TensorFlow 的安装需求。但请注意：更新有风险，升级需谨慎，一招不慎就容易导致系统崩溃（显示错误信息）！！！ 执行误操作之后只要不断开远程连接还有挽回的余地，但远程异常断开连接之后很多人就没有辙了，准备抢救数据&amp;文件，重装系统。事实上，我们先不要急着重装系统，来试一试是否可以 rescue ！ 1. 场景GNU GLIBC 库（内核运行库）升级失败导致系统崩溃，导致终端下使用命令会显示错误信息。主要是因为当前系统中使用到 Glibc 库的应用程序是用旧版 Glibc 编译的，而在安装新版 Glibc 库时却在安装时出了错（或者库是安装成功了，但不兼容），导致安装没成功，却把旧版的库给替换了（也可能是没有替换只是链接给覆盖掉了），这就进入了死循环了，导致系统所有命令几乎都执行不了。 解决办法：只能是进入到不依赖于当前系统中 Glibc 库的终端中去把 Glibc 库的链接（或文件）还原成原来的备份版，这只能使用光盘镜像的方式实现了。 如果当前没有光盘，就在开机时直接切换到恢复模式进入 Shell（关于如何进入到恢复模式下的 Shell 可见后文），看下面的关键命令是否可用，如果可用就直接解决了： $ rm -rf /lib64/libc.so.6 $ LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so /lib64/libc.so.6 如果不行的话，只能用光盘镜像进行恢复了。下面我们来看如何基于光盘镜像进行 rescue： 2. RescueFirst Step –&gt; Rescue installed system 首先准备好系统安装盘，进入 BIOS，使用系统镜像安装盘进行启动，选择 Rescue installed system。如下： 注意，选择恢复模式之后，可能还需要选择 语言和键盘。保持默认的就好。 Second Step –&gt; Setup Networking 上述操作完成后会询问 是否设置网络，一般来说网络没问题就不用设置了，这里我们选择 No。如下： Third Step –&gt; Rescue 设置网络后，我们需要设置 Rescue 选项。界面如下所示： 选项参数说明： Continue 选项：此选项下，救援模式程序会自动查找系统中已有的文件系统，并把他们挂载到 /mnt/sysimage 目录下； Read-Only 选项：此选项下，表示以只读的方式挂载已有的文件系统； Skip 选项：表示想要手动挂载 ； Advanced 选项：高级选项就不作说明了。 这里，我们选择 Continue 选项。 Fourth Step –&gt; Root for rescue 我们知道，原系统会被挂载到 /mnt/sysimage 路径下。设置好 Rescue 选项之后，会提醒我们如果想获得原系统的 root 环境，通过执行 chroot /mnt/sysimage 即可。如下： Fifth Step –&gt; Start shell 这里，可以执行 fakd 诊断。当然我们选择直接进入恢复模式下的 shell： Sixth Step –&gt; Solution 由于 /usr/lib64/libc-2.12.so &amp; libc.so.6 -&gt; libc-2.12.so 的问题，我们发现执行 chroot /mnt/sysimage 会产生报错： Starting shell... bash-4.1# chroot /mnt/sysimage /bin/sh: error while loading shared libraries: libc.so.6 cannot open ... file: no such file or directory 此时不要害怕，执行： $ cp /lib64/libc-2.12.so /mnt/sysimage/lib64/libc-2.12.so $ cp /lib64/libc.so.6 /mnt/sysimage/lib64/libc.so.6 这是因为在升级 Glibc 添加软连接指向 libc.so.6 时出错所以将软连接删除，更换为以前系统的glibc-2.12 指向的软连接，再次执行: bash-4.1# chroot /mnt/sysimage 发现可以正常切换，没有再出现错误的提示。现在可以重启系统，设置后开机就没问题。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Linux Glibc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos6（7）/Ubuntu 在线搭建 Anaconda & Tensorflow 开发环境【持续更新】]]></title>
    <url>%2FTensorFlow%2FCentos6%EF%BC%887%EF%BC%89-Ubuntu-%E5%9C%A8%E7%BA%BF%E6%90%AD%E5%BB%BA-Anaconda3-Tensorflow-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 工欲善其事，必先利其器。在开始深度学习课程之前，首要的是需要选择一台性能合适的机器来部署深度学习环境以进行学习试验。关于“选择性能合适的机器”，一般意味着“好的CPU”，“大内存”，“大硬盘”，”GPU支持“，DIY 组机参见 深度学习攒机小记。当然考虑到经济问题以及以学习为优先，利用我们手头闲置的配置最好的机器就可以开始配置深度学习环境了（性能低）。 本文我们将详细介绍如何基于 Centos6(7) / Ubuntu 单机或集群服务器搭建 TensorFlow [ CPU/GPU Support ] 深度学习单机或集群分布式学习开发环境。事实上，TensorFlow 集群分布式环境搭建就是多台单机服务器的重复安装过程，可以通过自动化部署完成。 最后，我们将在安装好的环境中运行和测试简单的 TensorFlow 样例程序以验证安装是否成功。 Setup Reference在安装 TensorFlow 之前，我们需要知道的是: TensorFlow 属于很高层的应用。 高层应用的一个比较大的麻烦就是需要依赖的底层相关很多，如果底层依赖没有弄好的话，高层应用是没法玩转的。那么 解决 Tensorflow 应用和其相关底层的依赖关系 对于开发者的安装而言尤为重要。 TensorFlow 教程中，提供了五种 Tensorflow 常用的安装方式：&#39;原生&#39; Pip、Docker、 Virtualenv、 Anaconda 或 源码编译 来安装 TensorFlow。后文会对以上五种方式分别进行介绍，以方便我们根据具体的软硬件以及应用需求采用不同的安装方式。 这里，首先给出一般场景下的 TensorFlow 开发环境搭建来帮助我们明确安装需求： 搭建 TensorFlow [CPU Support] 环境：搭建基于 CPU 运算的 TensorFlow 深度学习环境是最简单常见的，性能较低。 搭建 TensorFlow [GPU Support] 环境：搭建基于 GPU 运算的 TensorFlow 深度学习环境。建议先明确当前服务器独立显卡（GPU）信息（型号、计算性能等），然后参考后文附录 Tensorflow&amp;&amp;NVIDIA版本信息对照表，选择安装相互兼容的显卡驱动（CUDA）、显卡加速包（CUDNN）以及 Tensorflow 版本。 搭建 TensorFlow 集群（既有集群/新建集群）分布式开发环境：在新建的服务器集群上搭建 TensorFlow 深度学习环境可以不断尝试安装，不用担心把服务器系统搞崩。但对于在既有集群搭建 TensorFlow 环境（生产环境），建议先从集群子节点开始一台台安装 TensorFlow，将安装风险降到最低。当然，如果还是担心有风险，可以先通过虚拟机重构伪集群开发环境测试安装过程。熟悉安装过程后，再开始实际安装，将风险降到最低。 初学者一般推荐使用 Anaconda 进行 TensorFlow 学习环境的安装。更多关于 Anaconda 的了解，建议参考文档 –&gt; Anaconda 介绍、安装以及使用教程。 1. TensorFlow 预安装环境准备关于下面给出的 3 点开发环境要求不用深究，后文你会看到相关详细解释。 Linux 系统需要包含 GNU 较高版本的 Glibc 库（Centos7）； 源码安装要求 Java_JDK 1.8 以上版本； Python 2.7 或 Python 3.3 以上版本。 1.1 服务器操作系统选择下面两项注意事项是关于用于搭建 TensorFlow 深度学习开发环境的服务器（Server）的操作系统的选择，选择一个合适的 OS 可以帮我们减少很多的麻烦。 对于初学 TensorFlow 深度学习框架，仅仅想做做简单测试，强烈建议选择 Ubuntu！！！当然，在既有 Server 中搭建 Tensorflow Envs 则需要对当前服务器信息进行近一步的了解。 Centos/Ubuntu Server Select： 如果我们选择的是 Centos Server 来安装 TensorFlow。那么我们的 Centos 版本最好选择 Centos7。Centos6 因为版本较低，其操作系统内置的库无法很好兼容 TensorFlow。虽可以解决，但会为安装带来更多的麻烦(可以通过升级一些运行时库得以解决）。当然对于操作系统为 Centos6 的既有服务器，且不可以更换操作系统的安装需求，后文会对解决方法做详细说明。注意，为了可以进行分布式运算，对于 TensorFlow 要求版本大于 0.8，Tensorflow 0.8 之前版本不支持分布式。 如果我们选择的是 Ubuntu Server 来安装 TensorFlow，那么我们的 Ubuntu 版本最好是 Ubuntu 14.04/16.04。这里只是因为使用 Ubuntu14.04/16.04 的人较多，可以更好的帮我们解决安装以及使用 TensorFlow 过程中遇到的问题。 1.2 关于 TensorFlow 服务器根据实际搭建过程中的现有资源以及实际安装需求，给出两种选择: 【 1. 安装以及配置全新的 Linux Server For TensorFlow 】，还是 【 2. 基于既有服务器环境部署 Tensorflow 开发框架 】。这一部分，我们分别介绍上述两种需求下 Linux Server 的注意事项。 1.2.1 安装以及配置新的 Linux Server For TensorFlow Frame1.2.1_1 &gt; Centos6（7）/Ubuntu Server 安装–&gt; 工具准备 % I. Windows 操作系统电脑一台； % II. 官网或其它下载源下载 Centos6（7）/Ubuntu IOS 镜像文件； % III. Windows 安装两款软件：大白菜 U 盘启动盘制作工具、UltraISO Linux U盘启动盘制作工具；具体使用方法百度，不做赘述。 大白菜 官方下载地址：[ http://dabaicai.csmd4.com/dbcpe/ ] UltraISO 官方下载地址：[ https://cn.ultraiso.net/ ] % IV. U 盘 2 个：一个安装 PE 系统 U 盘启动盘，可以用来格式化 Server 机原有的磁盘分区，并设置 MBR（可防止 Linux 分区时磁盘柱头自动产生 1M 大小的空闲分区，无法引导）；另一个用来制作 Linux 系统 U 盘启动盘； –&gt; 安装 Ubuntu/Centos Server 1# 使用 Linux 系统 U 盘启动盘安装系统：安装方法见网络教程，网上有较为详细的安装教程，这里不做详细介绍。特别提到的是：安装系统时会进行系统分区。推荐手动分区，详细分区教程参考个人磁盘空间情况以及网络分区教程。 1.2.1_2 &gt; Centos6（7）/Ubuntu Server 快速配置–&gt; Server 基础配置 [1] Ubuntu 基础配置参见：Ubuntu Server 快速配置指南。 [2] Centos 基础配置参见：Centos Server 快速配置指南。 1.2.2 基于既有开发环境部署 Tensorflow Frame对于在既有服务器搭建 TensorFlow 环境，需要明确当前服务器操作系统信息以及已部署环境信息： 1）察看 Linux 操作系统版本信息 12345678$ lsb_release -a% 运行输出：LSB Version: :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarchDistributor ID: CentOSDescription: CentOS release 6.9 (Final)Release: 6.9Codename: Final 2）察看 Linux 系统内核版本（包括 GCC 版本信息） 1234$ cat /proc/version% 运行输出：Linux version 2.6.32-696.10.1.el6.x86_64 (mockbuild@c1bl.rdu2.centos.org) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-18) (GCC) ) #1 SMP Tue Aug 22 18:51:35 UTC 2017 2. TensorFlow 正式安装准备好 TensorFlow 预安装环境之后，接下来我们正式来看官方提供的不同安装方式的实现： 2.1 使用原生 Pip 安装我们知道，PIP 是一种包管理工具，用于安装和管理用 Python 的库。同样的，通过 pip 可以安装已经打包好的 TensorFlow 应用以及 TensorFlow 所需要的依赖，可以较为方便地解决 Tensorflow 应用和其相关底层的依赖关系。 注意：Centos6（7）服务器环境下还可能需要如下准备（2.1.1 Centos6/7 环境准备），Ubuntu14.04（16.04） 服务器环境可以直接跳过此处（推荐阅读）。 2.1.1 Centos6/7 环境准备1）Python 更新安装 TensorFlow 环境要求我们，Python 版本必须选择 2.7 以 3.3 以上版本。而 Centos6/7 中原生 Python 版本可能很老，所以需要我们在不影响到系统使用的前提下升级 Python。 12# 察看当前 Centos 中系统 Python 版本：$ python -V 例如我的 Centos6.9 系统，通过查阅版本号发现：Centos 原生 Python 版本是 2.4.3，版本很旧了，升级系统 Python 版本是必须选择的。 Python3.X &amp;&amp; Python2.X 同样, Python3.X 和 Python2.X 的选择也很重要。我们知道 3.X 和 2.X 有很多不同，基于 Python3.X 和 Python2.X 的 Python 脚本不能很好互相兼容。这里我们推荐使用 Python3.X （原因不解释…）。于是，我们的一般想法是更新 Python 版本到 Python3.X，让其可以正常运行 Python3.X 的脚本。 事实上，还有更好的方法 –&gt; Python 多版本并存 一个系统中可以存在多个 Python 版本，也就是说新老版本 Python 是可以共存的。但需要注意的是：很多基本的命令、软件包都要依赖系统默认的老版本 Python 的，比如 yum 等，更新 Python 版本后，相关依赖软件可能无法正常使用（后面会给出解决办法）。故，更新Python千万不要把老版本的删除，可以随时恢复！！！下面让我们来开始 Python 的更新: First Step –&gt; 更新 GCC GCC 版本太旧会导致新版本 Python 包编译不成功，安装 Python 前首先需要更新系统 gcc 版本： 1$ yum -y install gcc Second Step –&gt; 下载 Python 安装包 以 Python-3.3.0 为例 1$ wget http://python.org/ftp/python/3.3.0/Python-3.3.0.tar.bz2 注意：按照上述命令下载的软件包会存放在你当前的工作目录下。链接中的数字对应的是 Python 版本号，你也可以把 3.3.0 换成你需要的版本。 Third Step –&gt; 解压已下载的二进制包并编译安装 1234567$ tar -jxvf Python-3.3.0.tar.bz2 $ cd Python-3.3.0 $ ./configure $ make all $ make install $ make clean $ make distclean Fourth Step –&gt; 设置软链接 建立软连接指向到当前系统默认 Python 命令的 bin 目录，让系统使用新版本 Python： 12345678# 原始 Python 的版本为 2.4，所以为了保留 Python2.4，将原始 /usr/bin/python 改名为 Python2.4$ mv /usr/bin/python /usr/bin/python2.4# 将更新后的 python3.3 指向 /usr/bin/python，替换原始的 python2.4$ ln -s /usr/local/bin/python3.3 /usr/bin/python# 查看当前默认 Python 版本 $ python -V 这样配置之后，默认的 Python 成功指向 Python3.3.0 。yum(依赖 Python2.4 的应用程序) 不能正常使用，还需要修改 yum 的配置文件。 下面我们给出如何通过修改相关配置文件来解决依赖 Python2.4 的应用程序无法正常调用的问题。以修改 yum 配置文件（其它应用程序类似）为例： 1$ vim /usr/bin/yum 把文件头部的 #!/usr/bin/python 改成 #!/usr/bin/python2.4 即可。 最终方案： 当然，很多同学可能回说：“那我怎么能知道都有哪些应用程序的配置文件需要修改呢？”。故为了更安全的使用更新后的 Python 版本，我们可以不覆盖原本的 /usr/bin/python ，而是在 /usr/bin/ 下新建一个 python3.3 链接。如下： 1$ ln -s /usr/local/bin/python3.3 /usr/bin/python3.3 这样，调用系统命令 python 时仍然指向的老版本的 Python2.4。当我们需要使用新版本的 Python 时，只需要使用 python3.3 即可实现调用。 2）GNU GLIBC 库版本更新注意：此处更新有风险，升级需谨慎（一般不要随意升级内核运行库）！！！ 服务器是 Centos6.X 系统，即使通过 pip 安装好了 TensorFlow，在运行的时候我们也会发现仍然会报 glibc 版本过低的错误。这是由于 Centos6 上 glibc 最多到 2.12 版本，而强行使用高版本的 glibc 会导致程序意外崩溃。 12# Error：import tensorflow 时提示 GLIBC_2.14 找不到importError: /lib64/libc.so.6: version `GLIBC_2.14' not found 解决方法如下： First Step –&gt; 安装或更新 GCC 首先察看当前系统的 gcc 版本: gcc –-version，然后 yum update gcc；如果没有 gcc 需要安装 gcc。后续 GLIBC 库升级需要使用 gcc 编译。 注意，由于在 Python 更新时我们安装或更新过 GCC，故可以直接跳过。否则首先需要完成 安装或更新 GCC 。 下面我们开始看如何更新系统中的 GNU GLIBC 库： Second Step –&gt; 查看当前 GLIBC 支持的版本 1234567891011121314151617[root]$ strings /lib64/libc.so.6 |grep GLIBC_GLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_PRIVATE 可以看到，Centos6.9 自带的 Glibc 库最高版本是 2.12 ，所以需要对当前版本进行升级。 Third Step –&gt; 安装 GLIBC_2.17 123456789101112131415$ wget http://ftp.gnu.org/pub/gnu/glibc/glibc-2.17.tar.xz$ tar -zxvf glibc-2.17.tar$ cd glibc-2.17$ mkdir build$ cd build$ ../configure --prefix=/usr --disable-profile --enable-add-ons --with-headers=/usr/include --with-binutils=/usr/bin$ make -j4$ make install% 等待安装成功即可。 Fourth Step –&gt; 察看更新 12345678910111213141516171819202122[root]$ strings /lib64/libc.so.6 |grep GLIBCGLIBC_2.2.5GLIBC_2.2.6GLIBC_2.3GLIBC_2.3.2GLIBC_2.3.3GLIBC_2.3.4GLIBC_2.4GLIBC_2.5GLIBC_2.6GLIBC_2.7GLIBC_2.8GLIBC_2.9GLIBC_2.10GLIBC_2.11GLIBC_2.12GLIBC_2.13GLIBC_2.14GLIBC_2.15GLIBC_2.16GLIBC_2.17GLIBC_PRIVATE 此时，我们发现 Centos6 中 glibc 库版本过低问题得以解决！如果升级不成功可以使用如下语句简单还原: 12$ rm -rf /lib64/libc.so.6$ LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so /lib64/libc.so.6 如果发生系统崩溃，可参见文档 GLIBC 升级失败导致系统崩溃解决方法。 Fifth Step –&gt; libstdc++.so.6: version `CXXABI_1.3.9’ not found 更新 GLIBC 后，在 import tensorflow 测试时可能还会产生一个问题：libstdc++.so.6: version &#39;CXXABI_1.3.9&#39; not found (required by bin/opencv_test_core)? 这是由于 更新和安装 GCC 导致的。需要使用升级后的 gcc 的动态库去替换老版本的 gcc 动态库。 解决方法参见文档：libstdc++.so.6: version `CXXABI_1.3.9’ not found.doc。 2.1.2 Setup1）Install Pip首先检查是否安装有 pip： 1$ pip -V 如果有的话，可以看到 Pip 版本信息。否则，执行如下指令进行安装： 12345678# Ubuntu/Linux 64-bit$ sudo apt-get install python-pip python-dev# CentOS, Fedora, RHEL$ sudo yum install python-pip python-devel# Mac OS X$ sudo easy_install pip 2）Export TensorFlow（CPU/GPU）URL首先，我们给出一个 TensorFlow.whl 安装包 下载地址（清华大学开源镜像站），提供了不同平台下不同安装版本的 TensorFlow 安装包下载 URL，从这里我们可以获取到我们需要的 TensorFlow 版本。 然后，将TensorFlow URL 导入系统环境变量中，用于后续的下载： 1234567891011121314151617181920212223242526272829303132####### 1. 寻找合适的 Tensorflow 安装包下载链接 ######## 环境变量 $TF_BINARY_URL 根据你的实际需求(想要下载的 Tensorflow 版本)进行设置，典型选项如下：# 注意，以下给出的 URL 均属于 Google 官方链接，下载时可能出现网络中断，你可以将其更换为清华大学开源镜像站 URL。# Ubuntu/Linux 64-bit, CPU only, Python 2.7$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl# Ubuntu/Linux 64-bit, GPU enabled, Python 2.7# 需要 CUDA toolkit 8.0（7.5） and CuDNN v5（v4）.$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl# Mac OS X, CPU only, Python 2.7:$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py2-none-any.whl# Mac OS X, GPU enabled, Python 2.7:$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py2-none-any.whl# Ubuntu/Linux 64-bit, CPU only, Python 3.4$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl# Ubuntu/Linux 64-bit, GPU enabled, Python 3.4# 需要 CUDA toolkit 8.0 and CuDNN v5.0 $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34-cp34m-linux_x86_64.whl# Ubuntu/Linux 64-bit, CPU only, Python 3.5$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl# Ubuntu/Linux 64-bit, GPU enabled, Python 3.5# Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see "Installing from sources" below.$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl# Mac OS X, CPU only, Python 3.4 or 3.5:$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py3-none-any.whl# Mac OS X, GPU enabled, Python 3.4 or 3.5: $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl 搭建 TensorFlow GPU 计算环境时请注意： 目前只有在安装了特定（ CUDA 和 CuDNN 版本）版本的环境（如：CUDA toolkit 8.0（7.5） and CuDNN v5.1（v4） 的 64 位 Ubuntu ）下才可以通过 Pip 安装支持 GPU 的 TensorFlow，详情参见 Tensorflow &amp;&amp; NVIDIA 版本信息对照表（已测试）。 前面我们提到过，搭建基于 GPU 的 TensorFlow 深度学习环境前，我们需要部署 CUDA/CuDNN 环境。关于 CUDA/CuDNN 的安装见：Ubuntu16.04 + Nvidia GTX 960+CUDA8.0 + CUDNN+ TensorFlow1.2。 Only CPU 和 GPU enabled 版本区别：当处理数据量较大时，支持 GPU 版本处理速度要比支持 CPU 版本运行速度更快。当数据量较小时，GPU 可能运行更慢。 3）Install TensorFlow在 2）中我们已经选择了合适版本的 TensorFlow URL，运行如下命令可完成 TensorFlow 的安装： 12345# Python 2 环境 $ sudo pip install --upgrade $TF_BINARY_URL# Python 3 环境 $ sudo pip3 install --upgrade $TF_BINARY_URL 这是完成环境搭建的最后一步,过程中会出现 网络连接超时 等问题。如果一次无法成功安装 $TF_BINARY_URL,并且你确定你所配置的网络或网络代理等都正常。那么，多试几次，不厌其烦，如果出现下面的信息，则表示你快要成功了： 123456789101112131415161718192021222324252627282930###### Python 3 成功运行记录如下 ###### You are using pip version 7.1.0, however version 9.0.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. Processing ./tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl Collecting mock&gt;=2.0.0 (from tensorflow-cpu==0.12.1) Using cached mock-2.0.0-py2.py3-none-any.whl Collecting protobuf&gt;=3.1.0 (from tensorflow-cpu==0.12.1) Downloading protobuf-3.1.0.post1-py2.py3-none-any.whl (347kB) 100% |████████████████████████████████| 348kB 164kB/s Collecting numpy&gt;=1.11.0 (from tensorflow-cpu==0.12.1) Downloading numpy-1.11.3.zip (4.7MB) 100% |████████████████████████████████| 4.7MB 34kB/s Collecting wheel (from tensorflow-cpu==0.12.1) Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB) 100% |████████████████████████████████| 69kB 58kB/s Collecting six&gt;=1.10.0 (from tensorflow-cpu==0.12.1) Downloading six-1.10.0-py2.py3-none-any.whl Collecting funcsigs&gt;=1 (from mock&gt;=2.0.0-&gt;tensorflow-cpu==0.12.1) Downloading funcsigs-1.0.2-py2.py3-none-any.whl Collecting pbr&gt;=0.11 (from mock&gt;=2.0.0-&gt;tensorflow-cpu==0.12.1) Downloading pbr-1.10.0-py2.py3-none-any.whl (96kB) 100% |████████████████████████████████| 98kB 93kB/s Requirement already satisfied (use --upgrade to upgrade): setuptools in /usr/lib/python2.7/site-packages (from protobuf&gt;=3.1.0-&gt;tensorflow-cpu==0.12.1) Installing collected packages: funcsigs, six, pbr, mock, protobuf, numpy, wheel, tensorflow-cpu Found existing installation: six 1.9.0 Uninstalling six-1.9.0: Successfully uninstalled six-1.9.0 Running setup.py install for numpy Successfully installed funcsigs-1.0.2 mock-2.0.0 numpy-1.11.3 pbr-1.10.0 protobuf-3.1.0.post1 six-1.10.0 tensorflow-cpu-0.12.1 wheel-0.29.0 如果以上下载过程发生中断，重新执行命令即可。如果你一次成功，那么恭喜。否则此处需要耐性！！！如若上述一直不成功，这里还提供一种解决思路： 123$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0rc0-py2-none-any.whl# 先 wget $TF_BINARY_URL（或浏览器）下载 tensorflow—xxx.whl 文件到本地；# 然后使用 pip install *.whl进行安装. 2.1.3 Pip 安装总结注意，以上安装过程在 Centos6.9(7)、ubuntu14.04/16.04 服务器环境下已测试通过。 优缺点： 优点：应该是几种安装方法中最简单、最快的一种。 缺点：不能灵活定制。操作系统、GPU 硬件、CUDA 版本、cuDNN 版本必须与官方标称一致。由于属于直接安装于本机 Python 环境中，对原生开发环境有较大影响，不推荐使用该方法。 2.2 使用 Docker 镜像安装（未测试）Docker 是新一代的虚拟化技术，让开发者可以打包他们的应用以及相关依赖包到一个可移植的容器中，然后发布到任何流行的 Linux（内核版本高于 3.10） 机器上（Docker 支持大部分的操作系统：Linux[Ubunto / Centos / Debian / Red Hat等]、Windows7 以上以及 Mac OS 等）。 它可以将 TensorFlow 以及 TensorFlow 的所有依赖关系统一到 Docker 镜像中，从而大大简化安装过程。当你通过 Docker 安装和运行 TensorFlow 时，它与你机器上之前已安装的软件包完全隔离。 详细教程可参考网络。后续如有需要再做补充。 2.2.3 Docker 镜像安装总结优缺点： 优点：适合在大量相同环境机器构成的集群上批量部署； 缺点：有墙的孩子像根草，增加了 Docker 学习成本； 2.3 使用源码编译安装新手不推荐使用此方法。详细教程可参考网络。后续如有需要再做补充。 2.4 使用 Anaconda 安装Anaconda 是 Python 的一个发行版，如果把 Python 比作 Linux，那么 Anancoda 就是 CentOS 或者 Ubuntu。它通过 conda 工具解决了 Python 开发者的两大痛点： 提供包管理，功能类似于 pip，Windows 平台安装第三方包经常失败的场景得以解决。 提供虚拟环境管理，功能类似于 virtualenv，解决了多版本 Python 并存问题。 通俗的讲： 使用 Anaconda 安装 TensorFlow 进行开发时，相当于将所有的 TensorFlow 底层依赖细节全部已经打包封装好了！并且，Anaconda 还能创建自己的计算环境，相当于将 TensorFlow 的环境与其他环境做了隔离，这样你就可以将 TensorFlow 随便玩，爱怎么玩怎么玩，也不用担心破坏之前的环境！！！ 如果是玩数据玩 ML 的同学，如果你还不知道 Anaconda，你就 out 啦！ Anaconda 是一个基于 Python 的科学计算平台，这个平台里包含有 Python、R、Scala 等绝大部分主流的用于科学计算的包，这极大的方便了开发人员的使用。 想了解更多关于 Anaconda 的介绍以及使用请参见博文 Anaconda 介绍、安装以及使用教程。 2.4.1 Anaconda 安装以及配置1）Anaconda 的安装指南首先，我们给出 Anaconda 官方网址 用于下载相应的安装包（下载较慢的话可以尝试前往：清华大学开源软件镜像站）。 可以发现，Anaconda 对 Linux、Mac 以及 Windows 等系统均提供支持。下载 Linux Anaconda 安装包时，会发现有两个不同版本的 Anaconda，分别对应 Python 2.7 和 Python 3.6，两个版本其实除了这点区别外其他都一样。后面我们会看到，安装哪个版本并不是本质，因为通过 conda 环境管理，我们可以很方便地切换运行时的 Python 版本以适应不同 Python 版本开发需求（这里我们选择安装 Python3.6 为例）。 下载完成后，Centos/ubuntu系统~/Downloads（下载目录）下会有一个文件：Anaconda3-5.0.1-Linux-x86_64.sh,它就是我们要安装的 Anaconda3 的安装包。 2）Anaconda 安装及配置First Step –&gt; 在 ~/Downloads 路径下执行 $ sudo bash Anaconda3-5.0.1-Linux-x86_64.sh % 可能报错: PREFIX=/home/cdh/anaconda3 Anaconda3-5.0.1-Linux-x86_64.sh: line 335: bunzip2: command not found tar: This does not look like a tar archive tar: Exiting with failure status due to previous errors % 安装 bzip2 即可解决: $ yum install -y bzip2 安装过程中很简单，Enter（/yes） 即可（提前参看：Second Step）。安装完成后，会产生一个 ~/Anaconda3 目录，即安装目录。 Second Step –&gt; Anaconda PATH 设置 在 Linux 安装 Anaconda3 时，安装最后会询问是否自动配置了 PATH，输入 yes 即可完成自动配置。安装完成后，建议察看 ~/.bashrc 文件末尾是否有追加 export PATH，自动配置后会在~/.bashrc 文件末尾是否有追加 export PATH。 如果没有配置环境变量，则执行以下命令也可以完成相同功能： $ echo &apos;export PATH=&quot;~/anaconda3/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrc Third Step –&gt; 更新 bashrc 使其生效 $ source ~/.bashrc 2.4.2 Conda 管理工具使用conda 是 Anaconda 下用于包管理和环境管理的命令行工具，可以看作是 pip 和 vitualenv 的组合。 Anaconda 安装成功后 conda 会默认加入到环境变量中，因此可直接在命令行窗口运行 conda 命令: # 测试安装是否成功 $ conda –V 或 which conda # 假如安装的是 Python 2.7 对应的版本,运行： $ python -V # 可以看到：Python 2.7.12 :: Anaconda 4.3.30 (64-bit)，说明安装成功。 1）conda 多环境管理（实现多版本 Python 环境切换）conda 工具可以隔离出一个和当前系统 Python 环境分离的虚拟环境： ### [1] 创建虚拟环境： # 基于 python3.6 创建一个名 Python3 的环境 $ conda create --name Python3 python=3.5 # 基于 python2.7 创建一个名为 Python2 的环境 $ conda create --name Python2 python=2.7 # 删除环境 conda remove -n Python2 --all ### [2] 切换虚拟环境： # 激活 Python3 环境 $ activate Python3 # windows $ source activate Python3 # linux/mac # 退出 Python3 环境 $ source deactivate # Python3 切换到 python2 activate python2 2）conda 包管理（实现依赖包管理）conda（类似于 pip） 的包管理功能是对 pip 的一种补充。如果当前已经激活了某个 Python 环境，那么就可以在当前环境开始安装第三方包。 对于那些用 pip 无法安装成功的模块你都可以尝试用 conda 来安装，如果用 conda 找不到相应的包，当然你继续选择 pip 来安装包也是没问题的。 3）conda 常用命令# 察看 conda 命令帮助 $ conda -h # 查看某个指定环境的已安装包 $ conda list -n python27 # 查找 package信息 $ conda search numpy # 向某个虚拟环境安装 package $ conda install -n python27 numpy # 更新 package $ conda update -n python27 numpy # 删除 package $ conda remove -n python27 numpy 2.4.3 Pip &amp;&amp; Anaconda 换源1 &gt; 更换 Anaconda 镜像地址类似于 conda install 创建环境失败 或 conda update 更新失败等操作，Anaconda 使用过程中可能产生类似如下错误信息（和 Pip 换源一个道理）： Fetching package metadata ....... CondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https://nanomirrors.tuna.tsinghua.edu.cn/anaconda/cloud/linux-64/rpodata.json&gt; Elapsed: - An HTTP error occurred when trying to retrieve this URL. HTTP errors are often intermittent, and a simple retry will get you on your way. ConnectTimeout(MaxRetryError(&quot;HTTPSConnectionPool(host=&apos;nanomirrors.tuna.tsinghua.edu.cn&apos;, port=443): Max retries exceded with url: /anaconda/cloud/linux-64/repodata.json (Caused by ConnectTimeoutError(&lt;requests.packages.urllib3.connecton.VerifiedHTTPSConnection object at 0x7fb6d340dcc0&gt;, &apos;Connection to nanomirrors.tuna.tsinghua.edu.cn timed out. (connct timeout=9.15)&apos;))&quot;,),) 产生原因以及解决方法如下： 1）Packages Install 如果需要安装很多 Packages，你会发现 conda 下载的速度经常很慢，最后导致崩掉。或创建环境时间太长崩掉。因为 Anaconda.org 的服务器在国外。 所幸的是，清华TUNA 、中科大等国内镜像源有 Anaconda 仓库的镜像。这意味着我们可以将 Anaconda 默认的镜像源换为 清华TUNA 镜像源等： 注意：清华、中科大由于暂时未获取到 Anaconda 授权，宣布将于 2019.5 月底暂停 Anaconda 镜像服务。等吧…. # Linux/Mac：修改或新创建 ~/.condarc 配置文件添加如下内容： $ vim ~/.condarc channes: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults show_channel_urls: true 或通过命令行运行如下命令也可以将Anaconda 默认的镜像源换为 清华TUNA 镜像源： $ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ $ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ $ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ $ conda config --set show_channel_urls yes 命令行设置后如何快速换回默认源： 1conda config --force --remove-key channels 2 &gt; 更换 Pip 源地址pip 安装（本地 .whl 文件）或从源下载软件，经常出现下载后安装出错问题。除此之外，你也可以把 pip 的镜像源地址也换成国内的，豆瓣源速度比较快。 国内常用源（新版 Ubuntu 要求使用 https 源，要注意。）： 清华：https://pypi.tuna.tsinghua.edu.cn/simple 阿里云：http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 华中理工大学：http://pypi.hustunique.com/ 山东理工大学：http://pypi.sdutlinux.org/ 豆瓣：http://pypi.douban.com/simple/ 以设置阿里 pip 源为例，加速 pip 下载速度： $ mkdir ~/.pip $ cat &gt; ~/.pip/pip.conf &lt;&lt; EOF &gt;[global] &gt;trusted-host=mirrors.aliyun.com &gt;index-url=http://mirrors.aliyun.com/pypi/simple/ &gt;EOF pip 换源后,可直接从 pip 下载源下载需要的 tensorflow 版本: $ pip install tensorflow==1.2 2.4.4 Setup TensorFlow熟悉了 Anaconda 环境后，我们来看如何基于 Anaconda 虚拟环境来搭建 TensorFlow 学习环境： 1）CUDA &amp;&amp; CUDNN（GPU Support 版需要执行该步骤，CPU Support 跳过这一部分即可）Tensorflow 同 Caffe 一样，也是支持 CPU 和 GPU 两个版本。故也需要安装 CUDA 和 CUDNN。根据上文版本对照表信息推荐安装 CUDA 版本为 8.0，CUDNN 版本为 5.1。 详细安装教程见文档：Ubuntu16.04+Nvidia GTX 960+CUDA8.0.+CUDNN+TensorFlow1.2。 2）建立名叫 tensorflow 的计算环境Anaconda 的环境准备好了以后，接下来我们建立一个 conda 的计算环境，给这个环境取名叫 tensorflow: # Python 2.7 $ conda create -n tensorflow python=2.7 # Python 3.5 $ conda create -n tensorflow python=3.5 如果发现，无法成功创建，不要灰心，多创建几次会成功的。 3）激活 tensorflow 环境，然后用 pip 安装 TensorFlow先给出TensorFlow.whl安装包下载地址（清华大学开源镜像站），提供了不同平台下不同安装版本的 TensorFlow 安装包下载 URL，从这里，我们可以获取到我们需要的 TensorFlow 版本。 –&gt; [1]：建立了名叫 tensorflow 的计算环境后，先激活 tensorflow 环境。 1[root@localhost ~]# source activate tensorflow 注意，激活 tensorflow 环境后会发现用户名前出现 (tensorflow) 的标识。实际上我们这样切换，是更换了环境变量里不同版本的 pip 和 python ，实现了多环境多版本管理。 –&gt; [2]：切换到 tensorflow 的计算环境中，安装 Tensorflow 应用程序有两种方法（以安装 TensorFlow1.2 为例）： 2.1] 简单安装方式： 12345$ pip install tensorflow==1.2# 或：pip install tensorflow-gpu==1.2 2.2] 虚拟环境中使用类原生 pip 安装 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 和 2.1 中安装方法一样，唯一有区别的是这里我们不是原生 Pip 环境，而是处于 conda 管理的虚拟环境（如下）：(tensorflow)[root@localhost ~]$ export TF_BINARY_URL= https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl(tensorflow)[root@localhost ~]$ pip install --ignore-installed --upgrade $TF_BINARY_URL# 2.2.1] 当然上面的命令对应的是 python2.7，系统为 linux，cpu only。# 注意 TensorFlow 0.8.0 版本不支持分布式计算。想支持分布式计算，TensorFlow 版本 &gt;= 1.0。# 根据 TensorFlow 官方提供的资料，不同的系统与不同的版本命令如下：## ---&gt; Python2.7# Ubuntu/Linux 64-bit, CPU only, Python 2.7$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl# Ubuntu/Linux 64-bit, GPU enabled, Python 2.7# 需要 CUDA toolkit 8.0（7.5） and CuDNN v5（v4）.$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl# Mac OS X, CPU only, Python 2.7:$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py2-none-any.whl# Mac OS X, GPU enabled, Python 2.7:$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py2-none-any.whl## ---&gt; Python3.4/3.5# Ubuntu/Linux 64-bit, CPU only, Python 3.4$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp34-cp34m-linux_x86_64.whl# Ubuntu/Linux 64-bit, GPU enabled, Python 3.4# 需要 CUDA toolkit 8.0 and CuDNN v5.0 $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34-cp34m-linux_x86_64.whl# Ubuntu/Linux 64-bit, CPU only, Python 3.5$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl# Ubuntu/Linux 64-bit, GPU enabled, Python 3.5# Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see "Installing from sources" below.$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl# Mac OS X, CPU only, Python 3.4 or 3.5:$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py3-none-any.whl# Mac OS X, GPU enabled, Python 3.4 or 3.5: $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl# 2.2.2] 命令提交以后，等待 pip 安装成功信息即可。# 2.2.3] 如果以上下载过程发生中断，重新执行命令即可。如果你一次成功，那么恭喜。否则此处需要耐性！！！如若上述一直不成功，这里还提供一种解决思路：$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0rc0-py2-none-any.whl# 先 wget $TF_BINARY_URL 下载 .whl 文件到本地；# 然后使用 pip install *.whl 进行安装。 4）Glibc &amp;&amp; libstdc++.so.6: version `CXXABI_1.3.9’ not foundCentos6/7 下可能出现 Glibc 库升级 与 libstdc++.so.6: version &#39;CXXABI_1.3.9&#39; not found报错。Ubuntu 下安装不会出现该问题。 解决方法见 原生 pip 安装 部分。 2.4.5 Anaconda 多环境多版本 Python 指南借鉴 2.4.2 Conda 管理工具 中 conda 多环境管理 使用方法，解决如何在一个 Anaconda 上装两个不同 python 版本。 参看过博文 Anaconda 介绍、安装以及使用教程 的同学肯定了解过 Anaconda 生态。例如：我们安装的是 Anaconda3。里面默认安装的是 python3 的 Spyder 和 Jupter notebook。那么我们如何在 Anaconda3 里面装上 python2 的 Spyder 和 Jupter notebook 呢？ 方法一：Terminal # 使用 conda 创建一个名为 tensorflow2 的环境，并下载对应版本python2.7 $ conda create --name tensorflow2 python=2.7 # 激活 tensorflow2 环境 $ source activate python2 # 在 tensorflow2 的环境下下载 spyder 和 Jupter notebook $ conda install spyder $ conda install jupyter # 使用 pip 安装 tensorflow（教程见上） 这样，你想用 python3 编写代码时，就打开 python3 的 spyder；你想用 python2 编写代码时，就打开 python2 的 spyder。两种环境的切换非常方便（当然这里是存在问题的，感兴趣的同学可以考虑以下）。 方法二：Desktop Anaconda Navigator（简单、直观、耗时；不推荐） # 1. Terminal下打开 Anaconda Navigator $ anaconda-navigator # 2. 点击界面左侧 environment，可以看到已安装 # 3. 然后选择在你的想要的环境中下载spyder和jupyter notebook。 但是，因为 Anaconda Navigator 的环境不稳定，容易出现屏幕卡死或者闪退现象，不推荐这种方法。 2.4.6 Anaconda 安装总结1.强烈推荐使用Anaconda环境安装，真的不是一般的简单方便。 2.梯子，还是梯子，不解释。 3.激活与退出 tensorflow 计算环境： #激活TensorFlow环境 source activate tensorflow #退出 source deactivate 2.4.7 Anaconda 激活环境困扰1）设置 TensorFlow 永久环境每次运行 tensorflow 环境都需要启动环境，如何设置永久环境： $ printf &apos;. /root/anaconda3/bin/activate tensorflow-3.5&apos; &gt;&gt;~/.bashrc $ source ~/.bashrc 2）TensorFlow 环境快捷启动每次启动 tensorflow 环境时命令太长，如何设置命令别名： ## tensorflow 启动：tensorflow-py3.5 环境 $ printf &apos;\nalias tensorflow=&quot;source /root/anaconda3/bin/activate tensorflow-py3.5&quot;&apos; &gt;&gt;~/.bashrc ## exittensorflow 退出环境 $ printf &apos;\nalias exittensorflow=&quot;source deactivate&quot;&apos; &gt;&gt;~/.bashrc 3）Tensorflow 服务Tensorflow 程序作为项目一部分，嵌入其它开发环境使用，调用该 Python 程序时启动环境问题： % 1. 提供了一个 python 通过调用 shell 命令实现启动 tensorflow 环境并执行目标程序的接口； % 2. 将 Python 写成一个服务（例如：Flask），通过 Http 请求访问。 2.5 使用 VirtualEnv 安装 3. TensorFlow 环境测试3.1 HelloWorld 样例测试(tensorflow-3.5) root@ubuntu-Lenovo-Product:~# python Python 3.5.4 | packaged by conda-forge | (default, Dec 18 2017, 06:30:33) [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; import tensorflow as tf &gt;&gt;&gt; hello = tf.constant(&apos;Hello, TensorFlow!&apos;) &gt;&gt;&gt; sess = tf.Session() &gt;&gt;&gt; print (sess.run(hello)) b&apos;Hello, TensorFlow!&apos; &gt;&gt;&gt; a = tf.constant(10) &gt;&gt;&gt; b = tf.constant(32) &gt;&gt;&gt; print (sess.run(a+b)) 42 &gt;&gt;&gt; 样例结果与上述输出信息一致则表示 TensorFlow 安装成功！ 如果 tf.Session() 时遇到问题： I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use:SSE4.1 SSE4.2 AVX 上述输出是不影响 TensorFlow 使用的，可能你觉得这看起来也太难受了。解决方法如下: # 1. 在代码开始导入： &gt;&gt;&gt;import os &gt;&gt;&gt;os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;]=&apos;2&apos; &gt;&gt;&gt;import tensorflow as tf # 2. 使用 TensorFlow 源码编译安装 TensorFlow Tensorflow &amp;&amp; NVIDIA 版本信息对照表For Windows官方经过测试的 Tensorflow &amp;&amp; CUDA &amp;&amp; cuDNN 版本对应关系（持续更新）：https://tensorflow.google.cn/install/source_windows#tested_build_configurations 。 Windows 123456789101112131415161718192021222324252627282930TensorFlow 版本： CPU/GPU: Python 版本: 编译器: 构建工具: CUDNN: CUDA:----------------------------------------------------------------------------------------------------------------------tensorflow-1.13.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.13.0 GPU 3.5-3.6 MSVC 2015 update 3 Bazel 0.15.0 7 9tensorflow-1.12.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.12.0 GPU 3.5-3.6 MSVC 2015 update 3 Bazel 0.15.0 7 9tensorflow-1.11.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.11.0 GPU 3.5-3.6 MSVC 2015 update 3 Bazel 0.15.0 7 9tensorflow-1.10.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.10.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.9.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.9.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.8.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.8.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.7.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.7.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.6.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.6.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.5.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.5.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 7 9tensorflow-1.4.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.4.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 6 8tensorflow-1.3.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.3.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 6 8tensorflow-1.2.0 CPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.2.0 GPU 3.5-3.6 MSVC 2015 update 3 Cmake v3.6.3 5.1 8tensorflow-1.1.0 CPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.1.0 GPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 5.1 8tensorflow-1.0.0 CPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 N Ntensorflow_gpu-1.0.0 GPU 3.5 MSVC 2015 update 3 Cmake v3.6.3 5.1 8 For Linux &amp;&amp; Mac官方经过测试的 Tensorflow &amp;&amp; CUDA &amp;&amp; cuDNN 版本对应关系（持续更新）：https://tensorflow.google.cn/install/source#tested_build_configurations 。 Linux 123456789101112131415161718192021222324252627282930TensorFlow 版本： CPU/GPU: Python 版本: 编译器: 构建工具: CUDNN: CUDA:----------------------------------------------------------------------------------------------------------------------tensorflow-1.13.1 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.19.2 N Ntensorflow_gpu-1.13.1 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7.4 10.0tensorflow-1.12.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 N Ntensorflow_gpu-1.12.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow-1.11.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 N Ntensorflow_gpu-1.11.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow-1.10.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 N Ntensorflow_gpu-1.10.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow-1.9.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.11.0 N Ntensorflow_gpu-1.9.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.11.0 7 9tensorflow-1.8.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 N Ntensorflow_gpu-1.8.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 7 9tensorflow-1.7.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 N Ntensorflow_gpu-1.7.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow-1.6.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 N Ntensorflow_gpu-1.6.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow-1.5.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.8.0 N Ntensorflow_gpu-1.5.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.8.0 7 9tensorflow-1.4.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.5.4 N Ntensorflow_gpu-1.4.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.5.4 6 8tensorflow-1.3.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 N Ntensorflow_gpu-1.3.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 6 8tensorflow-1.2.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 N Ntensorflow_gpu-1.2.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 5.1 8tensorflow-1.1.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 N Ntensorflow_gpu-1.1.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8tensorflow-1.0.0 CPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 N Ntensorflow_gpu-1.0.0 GPU 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8 Mac 12345678910111213141516171819TensorFlow 版本： CPU/GPU: Python 版本: 编译器: 构建工具: CUDNN: CUDA:----------------------------------------------------------------------------------------------------------------------tensorflow-1.13.1 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.19.2 N Ntensorflow-1.12.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.15.1 N Ntensorflow-1.11.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.15.1 N Ntensorflow-1.10.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.15.1 N Ntensorflow-1.9.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.11.1 N Ntensorflow-1.8.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.10.1 N Ntensorflow-1.7.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.10.1 N Ntensorflow-1.6.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.8.1 N Ntensorflow-1.5.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.8.1 N Ntensorflow-1.4.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.5.4 N Ntensorflow-1.3.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.5 N Ntensorflow-1.2.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.5 N Ntensorflow-1.1.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 N Ntensorflow_gpu-1.1.0 GPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 5.1 8tensorflow-1.0.0 CPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 N Ntensorflow_gpu-1.0.0 GPU 2.7、3.3-3.6 XCode 中的 Clang Bazel 0.4.2 5.1 8]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 之磁盘空间信息查询]]></title>
    <url>%2FLinux-Shell%2FLinux-%E4%B9%8B%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… Linux 中 df 命令可用于查看 linux 服务器的 “文件系统的磁盘空间占用情况”。利用该命令可以帮助我们获取到硬盘被占用了多少空间，目前还剩下多少空间，以及磁盘空间文件系统类型等信息。 du 命令也是查看使用空间的。但是与 df 命令不同的是，du 命令是对 “文件和目录磁盘使用的空间” 的查看。 1 &gt; df : 显示磁盘分区空间信息df 命令用于检查 linux 服务器的 “文件系统的磁盘空间占用情况”，默认显示单位为 1K-Block。下面我们来看 df 命令使用解析： 1.1 语法1）命令语法格式： df -option(选项) args(参数：文件) 2）命令功能描述： 显示指定磁盘文件的可用空间。如果没有文件名被指定，则显示当前所有被挂载的文件系统的可用空间。 默认情况下，磁盘空间将以 1K-Block 为单位进行显示。 1.2 可用选项1）检查全部的文件系统： $ df -a #或 $ df --all 2）以指定的区块大小来显示区块数目： $ df --block-size=&lt;区块大小&gt; [默认值为：1K-blocks] Sample： $ df --block-size=2 3）以可读性较高的方式来显示信息: ### 3.1）-h：以 KB 级之上单位显示（MB、GB、TB） $ df -h #或 $ df --human-readable ### 3.2）-H：以 KB 级之上单位显示（MB、GB、TB） $df -H #或 $ df --si -h 和 -H 功能一样，但请注意 -H 在计算时是以 1000 Bytes 为换算单位而非 -h 计算时的 1024 Bytes。 4）设置文件系统区块大小： ### 4.1）指定区块大小为 1024 字节（1K-Blocks，默认值） $ df -k $或 $ df --kilobytes ### 4.2）指定区块大小为 兆字节（1M-Blocks） $ df -m #或 $ df --megabytes 5）显示文件系统的 inode 信息： $ df -i #或 $ df --inodes 6）仅显示本地端的文件系统： $ df -l #或 $ df --local 7）文件系统类型显示相关： ### 7.1）仅显示 所指定文件系统类型 的磁盘信息：df -t &lt;文件系统类型&gt; 或 df --type=&lt;文件系统类型&gt; $ df -t ext4 ### 7.2）显示文件系统的类型：df -T 或 df --print-type $ df -T ### 7.3）不要显示 所指定文件系统类型 的磁盘信息：df -x &lt;文件系统类型&gt; 或 df --exclude-type=&lt;文件系统类型&gt; $ df -x ext4 8）Reference： --help：显示帮助； --version：显示版本信息。 1.3 实测# 指定挂载文件系统： root@ubuntu-Lenovo-Product:~# df -lh /home Filesystem Size Used Avail Use% Mounted on /dev/sda8 765G 311G 416G 43% /home # 不指定挂载文件系统： root@ubuntu-Lenovo-Product:~# df -lH Filesystem Size Used Avail Use% Mounted on udev 13G 0 13G 0% /dev tmpfs 2.6G 44M 2.5G 2% /run /dev/sda5 50G 36G 11G 77% / /dev/sda7 99G 7.6G 86G 9% /usr /dev/sda1 380M 143M 214M 41% /boot /dev/sda8 821G 334G 446G 43% /home /dev/sdb1 3.0T 1.2T 1.7T 40% /home/cdh/data 2 &gt; df : du : 显示文件和目录磁盘空间信息du 命令也是用来查看使用空间的。但是与上面介绍的 df 命令不同之处在于：du 命令是用于检查 “文件和目录” 磁盘的使用空间。 2.1 语法1）命令语法格式： du -option(选项) args(参数：文件) 2）命令功能描述： 显示每个文件或目录的磁盘使用空间。 2.2 可用选项1）显示 当前目录 下包含的所有文件的大小： $ du -a #或 $ du -all 2）除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和： $ du -c #或 $ du --total 3）仅显示占用总计（只列出所有文件或目录占用总数）： $ du -s 4）设置目录或文件大小显示单位： ### 4.1）显示目录或文件大小时，以 byte 为单位输出 $ du -b #或 $ du --bytes ### 4.2）以 KB(1024 bytes) 为单位输出 $ du -k #或 $ du --kilobytes ### 4.3）# 以 MB 为单位输出 $ du -m #或 $ du --megabytes ### 4.4）以 KB，MB，GB 为单位，提高信息的可读性 $ du -h #或 $ du --human-readable #或（--si 换算单位为：1000） $ du --si 5）Reference $ du --help 2.3 实测# 显示指定文件所占空间： $ du -h example.log # 查看指定目录的所占空间： $ du -h src_directory # 显示多个文件所占空间： $ du -h example1.log example2.log # 只显示总和的大小: $ du -sh src_directory # 输出当前目录下各个子目录所使用的空间 $ du -h --max-depth=1]]></content>
      <categories>
        <category>Linux Shell</category>
      </categories>
      <tags>
        <tag>Linux cmd</tag>
        <tag>df，du</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 系统环境常用属性查询]]></title>
    <url>%2FJava%2FJVM-%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%B8%B8%E7%94%A8%E5%B1%9E%E6%80%A7%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… JAVA 提出的 “ Writer once,run anywhere ” 口号的确让我们领略到了 Java 的神奇，也成为 SUN 宣传 Java 的金字招牌。拂开这层神秘的面纱，我们不难发现其核心是运行在各种操作系统中的 JVM（ Java 虚拟机）在发挥着重要的作用。 JVM 允许我们的纯 Java 类可以达到 “一次编写，到处运行”。每个安装在不同操作系统的 JVM 负责着 Java 程序与操作系统之间的工作，因此每个 Java 虚拟机的系统环境属性是不同的。 很多时候，获知关于当前操作系统的一些基本信息是必要的。 根据官方文档，标准 SDK 中提供了 java.lang.System 类，这个类定义了一个对系统设备（包括系统属性和系统输入输出数据流）与平台无关的接口。调用方法：getProperties() 会返回一个 java.util.Property 对象，对象中存放了 JVM 的系统属性列表，我们可以通过这个列表来得到 Java 虚拟机的一些系统属性。 故，通过访问 Java 虚拟机的系统属性来获知一些关于当前操作系统的一些基本信息，是极其方便的。 下面我们给出最常用的几个系统属性的获取实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.util.ArrayList;import java.util.Hashtable;import java.util.List;import java.util.Properties;public class SystemInfo &#123; // 存放 JVM 获得的系统属性： private static Properties property; // Main function： public static void main(String[] args) &#123; property = getSystemProperty(); Hashtable&lt;String, String&gt; hashKey = new Hashtable&lt;String, String&gt;(); // 将系统信息的关键字和标题放到 hashtable： hashKey.put("java.version", "Java Version Number : "); hashKey.put("java.vendor", "JAVA Provider Name : "); hashKey.put("os.name", "Operating System Name : "); hashKey.put("java.home", "JAR HOME "); hashKey.put("java.compiler", "Java Compiler : "); hashKey.put("java.ext.dirs", "Java Execution Path : " ); hashKey.put("file.separator", "File Delimiter : "); hashKey.put("path.separator", "Path Delimiter : "); hashKey.put("user.name", "Current Active User : "); hashKey.put("user.home", "Current User Directory : "); hashKey.put("user.dir", "Current Path(·) "); List&lt;String&gt; propertyKeySet = new ArrayList&lt;String&gt;(); propertyKeySet.add("os.name"); propertyKeySet.add("user.name"); propertyKeySet.add("java.vendor"); propertyKeySet.add("java.version"); //propertyKeySet.add("java.home"); //propertyKeySet.add("java.compiler"); propertyKeySet.add("java.ext.dirs"); propertyKeySet.add("user.dir"); propertyKeySet.add("user.home"); propertyKeySet.add("file.separator"); propertyKeySet.add("path.separator"); // 打印： for(String propertyKey : propertyKeySet) &#123; System.out.printf("%-25s", (String)hashKey.get(propertyKey)); System.out.printf(property.getProperty(propertyKey) + "\n"); &#125; &#125; /** * 获得系统属性列表 * @return Properties */ private static Properties getSystemProperty() &#123; // TODO Auto-generated method stub property = System.getProperties(); return property;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 的面向对象编程]]></title>
    <url>%2FPython%2FPython-%E7%9A%84%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 写在前面： 前面我们提到过面向对象的编程概述，我们知道：面向过程是具体化的，流程化的，解决一个问题，你需要一步一步的分析，一步一步的实现，这就是面向过程的设计。而面向对象呢？其实，面向对象是模型化的，你只需抽象出一个类，这是一个封闭的盒子，在这里你拥有数据也拥有解决问题的方法。 面向对象的程序设计把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 Python 面向对象编程Python 从设计之初就已经是一门面向对象的语言。在 Python 中，所有数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。 我们以一个例子来说明面向过程和面向对象在程序流程上的不同之处： 假设我们要处理学生的成绩表， –&gt; 面向过程: 为了表示一个学生的成绩，面向过程的程序可以用一个 dict 表示： 12std1 = &#123; 'name': 'Michael', 'score': 98 &#125;std2 = &#123; 'name': 'Bob', 'score': 81 &#125; 而处理学生成绩可以通过函数实现，比如打印学生的成绩： 12def print_score(std): print('%s: %s' % (std['name'], std['score'])) –&gt; 面向对象： 而如果采用面向对象的设计思想，我们首选思考的不是程序的执行流程，而是 Student 这种数据类型应该被视为一个对象，这个对象拥有 name 和 score 这两个属性（Property）。如果要打印一个学生的成绩，首先必须创建出这个学生对应的对象，然后，给对象发一个 print_score 消息，让对象自己把自己的数据打印出来。 12345678class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 给对象发消息实际上就是调用对象对应的关联函数，我们称之为对象的方法（Method）。面向对象的程序写出来就像这样： 1234bart = Student('Bart Simpson', 59)lisa = Student('Lisa Simpson', 87)bart.print_score()lisa.print_score() 上面，我们定义了 Class——Student，是指学生这个概念；而实例（Instance）则是一个个具体的 Student，比如：Bart Simpson 和 Lisa Simpson 是两个具体的 Student。 1. 类和实例面向对象最重要的概念就是类（Class）和实例（Instance）。 必须牢记类是抽象的模板，比如 Student 类，而实例是根据类实例化出来的一个个具体的 “对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 1.1 创建类Python 中，定义类是通过 class关键字，以 Student 类为例： 12345678class Student(object): '类的帮助信息' &lt;statement-1&gt; . . . &lt;statement-N&gt; pass class后面紧接着是类名，即 Student，类名通常是大写开头的单词。紧接着是 (object)，表示该类是从哪个类继承下来的，通常，如果没有合适的继承类，就使用 object 类，这是所有类最终都会继承的类（也称为顶级父类）。 实际上，创建一个类之后，可以就通过类名访问其属性了： 12345678&gt;&gt;&gt; class Student(object):... name = "Google"... score = 88...&gt;&gt;&gt; Student.name'Google'&gt;&gt;&gt; Student.score88 1.2 类的对象类创建好之后，我们可以对其进行实例化，实例化成一个个的对象。 例如，根据 Student 类创建出 Student 的实例，创建实例是通过 类名() 的方式实现的： 12345&gt;&gt;&gt; bart = Student()&gt;&gt;&gt; bart&lt;__main__.Student object at 0x10a67a590&gt;&gt;&gt;&gt; Student&lt;class '__main__.Student'&gt; 可以看到，变量 bart 指向的就是一个 Student 的实例，后面的 0x10a67a590 是内存地址，每个 object 的地址都不一样，而 Student 本身则是一个类。 可以自由地给一个实例变量绑定属性，比如，给实例 bart 绑定一个 name 属性： 12345&gt;&gt;&gt; class Student(object):... pass&gt;&gt;&gt; bart.name = 'Bart Simpson'&gt;&gt;&gt; bart.name'Bart Simpson' 1 –&gt; 构造方法 由于类可以起到模板的作用，因此可以在创建实例的时候，把一些我们认为必须绑定的属性强制填写进去。类有一个名为 __init__() 的特殊方法（类的构造方法），该方法在类实例化时会自动调用。 在创建类是只要将需要绑定的属性写入 __init__()，在创建实例的时候就会自动把 name，score 等属性绑上去： 1234567class Student(object): def __init__(self, name, score): self.name = name self.score = score pass 注意到 __init__ 方法的第一个参数永远是 self，表示实例本身（代表类的实例，而非类）。因此在 __init__ 方法内部，就可以把各种属性绑定到 self，因为 self 就指向创建的实例本身。 12345678910&gt;&gt;&gt; class Student(object):... def __init__(self, name, score):... self.name = name... self.score = score...&gt;&gt;&gt; bart = Student("Google", 80)&gt;&gt;&gt; bart.name'Google'&gt;&gt;&gt; bart.score80 有了 __init__ 方法，在创建实例的时候，就不能传入空的参数了，否组会报错： 1234&gt;&gt;&gt; maven = Student()Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: __init__() missing 2 required positional arguments: &apos;name&apos; and &apos;score&apos; 注意必须传入与 __init__ 方法匹配的参数，但 self 不需要传，Python 解释器自动按照传入顺序把实例变量传进去。 2 –&gt; 类中的函数 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量 self，并且，调用时，不用传递该参数。 除此之外，类的方法和普通函数没有什么区别，所以我们仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 3 –&gt; 数据封装 面向对象编程的一个重要特点就是数据封装。在上面的 Student 类中，每个实例就拥有各自的 name 和 score 这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩： 12345&gt;&gt;&gt; def print_score(std):... print('%s: %s' % (std.name, std.score))...&gt;&gt;&gt; print_score(bart)Bart Simpson: 80 但是，既然Student实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在 Student 类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和 Student 类本身是关联起来的，我们称之为类的方法： 12345678class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print('%s: %s' % (self.name, self.score)) 要定义一个方法，除了第一个参数是 self 外，其他和普通函数一样。要调用一个方法，只需要在实例变量上直接调用，除了 self不用传递，其他参数正常传入： 123&gt;&gt;&gt; bart = Student("Google", 80)&gt;&gt;&gt; bart.print_score()Bart Simpson: 80 这样一来，我们从外部看Student类，就只需要知道，创建实例需要给出 name 和 score，而如何打印，都是在 Student类的内部定义的，这些数据和逻辑被“封装”起来了。调用很容易，但却不用知道内部实现的细节，只要会用就可以了。 封装的另一个好处是可以给 Student类增加新的方法，比如 get_grade： 123456789101112class Student(object): def __init__(self, name, score): self.name = name self.score = score def get_grade(self): if self.score &gt;= 90: return 'A' elif self.score &gt;= 60: return 'B' else: return 'C' 外部调用时，我们只需要知道 Student 类提供了那些方法，直接在实例变量上调用，不需要知道方法内部的实现细节： 1234lisa = Student('Lisa', 99)bart = Student('Bart', 59)print(lisa.name, lisa.get_grade())print(bart.name, bart.get_grade()) 4 –&gt; 注意 和静态语言不同，Python 允许对实例变量绑定任何数据，也就是说，对于两个实例变量，虽然它们都是同一个类的不同实例，但拥有的变量名称都可能不同： 123456789&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; lisa = Student('Lisa Simpson', 87)&gt;&gt;&gt; bart.age = 8&gt;&gt;&gt; bart.age8&gt;&gt;&gt; lisa.ageTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute 'age' 2. 访问限制（封装）在 Class 内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样就隐藏了内部的复杂逻辑。 但是，从前面 Student 类的定义来看，外部代码还是可以自由地修改一个实例的 name、score 属性： 123456&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.score59&gt;&gt;&gt; bart.score = 99&gt;&gt;&gt; bart.score99 2.1 私有变量如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线 __，在 Python 中，实例的变量名如果以 __ 开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把 Student 类改一改： 12345678class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print('%s: %s' % (self.__name, self.__score)) 改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问 实例变量.__name 和 实例变量.__score 了： 12345&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.__nameTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;AttributeError: 'Student' object has no attribute '__name' 这样就确保了外部代码不能随意修改对象内部的状态，这样通过访问限制的保护，使得代码更加健壮。 2.2 Get &amp;&amp; Set 方法但是如果外部代码要获取 name 和 score 怎么办？可以给Student类增加 get_name 和 get_score 这样的方法： 12345678class Student(object): ... def get_name(self): return self.__name def get_score(self): return self.__score 如果又要允许外部代码修改 score 、name怎么办？可以再给 Student 类增加 set_score 、set_name 方法： 12345678class Student(object): ... def set_score(self, score): self.__score = score def set_name(self, name): self.__name = name 你也许会问，原先那种直接通过 bart.score = 99 也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数： 12345678class Student(object): ... def set_score(self, score): if 0 &lt;= score &lt;= 100: self.__score = score else: raise ValueError('bad score') 2.3 私有变量命名问题需要注意的是，在 Python 中，正如我们前面提过的，变量名类似 __xxx__ 的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量。特殊变量是可以直接访问的，不是 private 变量，所以不能用 __name__、__score__ 这样的变量名。 有些时候，你会看到以一个下划线开头的实例变量名，比如 _name，这样的实例变量外部是可以访问的。但是按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问 __name 是因为 Python 解释器对外把 __name 变量改成了 _Student__name，所以，仍然可以通过 _Student__name 来访问 __name 变量： 12&gt;&gt;&gt; bart._Student__name'Bart Simpson' 但是强烈建议你不要这么干，因为不同版本的 Python 解释器可能会把 __name 改成不同的变量名。 最后注意下面的这种错误写法： 123456&gt;&gt;&gt; bart = Student('Bart Simpson', 59)&gt;&gt;&gt; bart.get_name()'Bart Simpson'&gt;&gt;&gt; bart.__name = 'New Name' # 设置__name变量！&gt;&gt;&gt; bart.__name'New Name' 表面上看，外部代码“成功”地设置了 __name 变量，但实际上这个 __name 变量和 class 内部的 __name 变量不是一个变量！内部的 __name 变量已经被 Python 解释器自动改成了 _Student__name，而外部代码给 bart 新增了一个 __name 变量。不信试试： 123456&gt;&gt;&gt; bart.get_name() # get_name()内部返回self.__name'Bart Simpson'&gt;&gt;&gt; bart.__name'New Name'&gt;&gt;&gt; bart._Student__name'Bart Simpson' 3. 继承和多态3.1 继承在 OOP 程序设计中，当我们定义一个 class 的时候，可以从某个现有的 class 继承，新的 class 称为子类（Subclass），而被继承的 class 称为基类、父类或超类（Base class、Super class）。 比如，我们已经编写了一个名为 Animal 的 class，有一个 run() 方法可以直接打印： 123class Animal(object): def run(self): print('Animal is running...') 当我们需要编写 Dog和 Cat 类时，就可以直接从 Animal 类继承： 12345class Dog(Animal): passclass Cat(Animal): pass 对于 Dog来说，Animal 就是它的父类，对于 Animal 来说，Dog 就是它的子类。Cat 和 Dog 类似。 继承有什么好处？最大的好处是子类获得了父类的全部功能（具有父类全部的属性和方法）。由于 Animial 实现了 run() 方法，因此，Dog 和 Cat 作为它的子类，什么事也没干，就自动拥有了 run() 方法： 123456&gt;&gt;&gt; dog = Dog()&gt;&gt;&gt; dog.run()Animal is running...&gt;&gt;&gt; cat = Cat()&gt;&gt;&gt; cat.run()Animal is running... 当然，也可以对子类增加一些属性和方法，比如 Dog 类： 1234567class Dog(Animal): def run(self): print('Dog is running...') def eat(self): print('Eating meat...') 继承的第二个好处需要我们对代码做一点改进。你看到了，无论是 Dog还是 Cat，它们 run() 的时候，显示的都是 Animal is running...，符合逻辑的做法是分别显示 Dog is running... 和 Cat is running...，因此，对 Dog 和 Cat 类改进如下： 123456789class Dog(Animal): def run(self): print('Dog is running...')class Cat(Animal): def run(self): print('Cat is running...') 再次运行，结果如下： 12Dog is running...Cat is running... 3.2 多态上面，当子类和父类都存在相同的 run() 方法时，我们说，子类的 run() 覆盖了父类的 run()，在代码运行的时候，总是会调用子类的 run()。这也就对应继承的另一个好处：多态。 要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个 class 的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和 Python 自带的数据类型，比如 str、list、dict 没什么两样： 123456# a 是 list 类型a = list()# b 是 Animal 类型b = Animal()# c 是 Dog 类型c = Dog() 判断一个变量是否是某个类型可以用 isinstance() 判断： 123456&gt;&gt;&gt; isinstance(a, list)True&gt;&gt;&gt; isinstance(b, Animal)True&gt;&gt;&gt; isinstance(c, Dog)True 应该还记得 type(x) == ? 么？和 isinstance() （是否是实例）有什么区别？不说了… 要理解多态的好处，我们还需要再编写一个函数，这个函数接受一个 Animal 类型的变量： 123def run_twice(animal): animal.run() animal.run() 当我们传入 Animal 的实例时，run_twice() 就打印出： 123&gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running... 当我们传入 Dog 的实例时，run_twice() 就打印出： 123&gt;&gt;&gt; run_twice(Dog())Dog is running...Dog is running... 当我们传入 Cat 的实例时，run_twice() 就打印出： 123&gt;&gt;&gt; run_twice(Cat())Cat is running...Cat is running... 看上去没啥意思，但是仔细想想，现在，如果我们再定义一个 Tortoise 类型，也从 Animal 派生： 123class Tortoise(Animal): def run(self): print('Tortoise is running slowly...') 当我们调用 run_twice() 时，传入 Tortoise 的实例： 123&gt;&gt;&gt; run_twice(Tortoise())Tortoise is running slowly...Tortoise is running slowly... 多态的好处就是，当我们需要传入 Dog、Cat、Tortoise…… 时，我们只需要接收 Animal 类型就可以了，因为 Dog、Cat、Tortoise…… 都是 Animal类型，然后按照 Animal 类型进行操作即可。 由于Animal类型有 run() 方法，因此传入的任意类型，只要是 Animal 类或者子类，就会自动调用实际类型的 run() 方法，这就是多态的意思： 对于一个变量，我们只需要知道它是 Animal 类型，无需确切地知道它的子类型，就可以放心地调用 run() 方法，而具体调用的 run() 方法是作用在 Animal、Dog、Cat 还是 Tortoise 对象上，由运行时该对象的确切类型决定，这就是多态真正的威力： 调用方只管调用，不管细节，而当我们新增一种 Animal 的子类时，只要确保 run() 方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则： 对扩展开放：允许新增 Animal 子类； 对修改封闭：不需要修改依赖 Animal 类型的 run_twice() 等函数。 3.3 静态语言 vs 动态语言对于静态语言（例如 Java）来说，如果需要传入 Animal 类型，则传入的对象必须是 Animal 类型或者它的子类，否则，将无法调用 run() 方法。 对于 Python 这样的动态语言来说，则不一定需要传入 Animal 类型。我们只需要保证传入的对象有一个 run() 方法就可以了： 123class Timer(object): def run(self): print('Start...') 就像我们前面提到过的，Python 的 “file-like object“ 类型。对真正的文件对象，它有一个 read() 方法，返回其内容。 但是对于许多对象，只要有 read() 方法，都被视为 “file-like object“。许多函数接收的参数就是 “file-like object“，你不一定要传入真正的文件对象，完全可以传入任何实现了 read() 方法的对象。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 之 IO 编程]]></title>
    <url>%2FPython%2FPython-%E4%B9%8B-IO-%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： IO 在计算机中指 Input/Output，也就是输入和输出。由于程序和运行时数据是在内存中驻留，由 CPU 这个超快的计算核心来执行，而涉及到数据交换的地方（通常是磁盘、网络），就需要 IO 接口。这一小节，我们来看 Python 中的 IO 编程。 Python 之 Input/Output 编程1. 认识 IO1.1 IO Stream当我们通过浏览器访问网站（百度搜索首页）时，浏览器就需要通过网络 IO 获取百度的搜索首页。浏览器首先会发送数据给百度服务器，告诉它我想要首页的 HTML，这个动作是往外发数据，叫 Output；随后百度服务器把网页发过来，这个动作是从外面接收数据，叫 Input。所以，通常程序完成 IO 操作会有 Input 和 Output 两个数据流。 IO 编程中，Stream（流）是一个很重要的概念，可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream 就是数据从外面（磁盘、网络）流进内存，Output Stream 就是数据从内存流到外面去。对于浏览网页来说，浏览器和服务器之间至少需要建立两根水管，才可以既能发数据，又能收数据。 1.2 同/异布 IO由于 CPU 和内存的速度远远高于外存的速度，所以，在 IO 编程中，就存在速度严重不匹配的问题。 举个例子来说，比如要把 100M 的数据写入磁盘，CPU 输出 100M 的数据只需要 0.01 秒，可是磁盘要接收这 100M数据可能需要10秒，怎么办呢？有两种办法： 第一种是 CPU 等着，也就是程序暂停执行后续代码，等 100M 的数据在 10 秒后写入磁盘，再接着往下执行，这种模式称为同步 IO； 另一种方法是 CPU 不等待，只是告诉磁盘，“您老慢慢写，不着急，我接着干别的事去了”，于是，后续代码可以立刻接着执行，这种模式称为异步 IO。 同步和异步的区别就在于是否等待 IO 执行的结果。 很明显，使用异步 IO 来编写程序性能会远远高于同步 IO，但是异步 IO 的缺点是编程模型复杂度却远远高于同步 IO。 当然，操作 IO 的能力都是由操作系统提供的，每一种编程语言都会把操作系统提供的低级 C 接口封装起来方便使用，Python 也不例外。 注意，本文后续涉及到的 IO 编程都是同步模式，异步 IO 由于复杂度太高，后续涉及到服务器端程序开发时我们再讨论。 2. 文件读写不仅在 Python 中，程序设计语言最常见的 IO 操作就是：读写文件。Python 内置了读写文件的函数，用法和 C 是兼容的。 读写文件前，我们必须先明确： 在磁盘上读写文件的功能都是由操作系统提供的低级接口实现的，现代操作系统不允许普通的程序直接操作磁盘。 所以，读写文件就是请求操作系统打开一个文件对象（通常称为文件描述符），然后通过操作系统提供的接口从这个文件对象中读取数据（读文件），或者把数据写入这个文件对象（写文件）。 Python 中提供的 open() 方法将会返回一个 file 对象，基本语法格式如下: open(filename, mode) filename：定义你要访问的文件名称； mode：定义了文件的访问模式，以只读，写入，追加等方式访问。这个参数是非强制的，默认文件访问模式为只读(r)。 关于 mode 参数支持的文件访问模式： 123456789101112131415'r'：只读（默认）文件若不存在就报错(IOError)'w'：写入（覆盖），文件若不存在就创建'a'：追加，新的内容将会被写入到已有内容之后，文件若不存在就创建'r+' == r+w（可读可写，文件若不存在就报错(IOError)）'w+' == w+r（可读可写，文件若不存在就创建）'a+' ==a+r（可追加可写，文件若不存在就创建）对应的，如果是二进制文件，就都加一个 b 就好啦：'rb' 'wb' 'ab' 'rb+' 'wb+' 'ab+' 通过open() 获取文件对象（文件描述符）是文件读写的第一步，然后才可以通过文件对象方法操作文件。 2.1 读文件要以读文件的模式打开一个文件对象，传入相应的文件名和模式标示符即可。例如打开 file_io.txt 文本文件： 1file = open("./file_io.txt", 'r') 标示符 &#39;r&#39;表示只读（默认访问模式），这样我们就成功地打开了一个文件。 如果文件不存在，open()函数就会抛出一个 IOError 的错误，并且给出错误码和详细的信息告诉你文件不存在： 123Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;FileNotFoundError: [Errno 2] No such file or directory: './file_io1.txt' 1）file.read() 如果文件打开成功（无异常输出），不干点啥怎么好意思？？？接下来，我们调用 Python 文件对象方法： read() 方法来读取文件内容。read() 方法可以一次性读取文件的全部内容（把内容读到内存），返回一个 str对象表示： 123&gt;&gt;&gt; text = file.read()&gt;&gt;&gt; text'Hello,Python developer.\nThis is a read file test.' 2）file.close() 最后一步我们还需要明确调用文件对象方法 close() 来关闭文件。文件使用完毕后必须关闭，这是因为文件对象会占用操作系统的资源，并且操作系统同一时间能打开的文件数量也是有限的： 1&gt;&gt;&gt; file.close() 由于文件读写时都有可能产生 IOError，一旦出错，后面的 f.close() 就不会调用,这是不安全的。所以为了保证无论是否出错都能正确地关闭文件，我们可以使用 try ... finally 来实现： 12345678try: file = open("./file_io.txt", 'r') content = file.read() print(type(content)) print(content)finally: if file: file.close() 但是每次都这么写实在太繁琐，我们还可以通过引入 Python 上下文管理器 with 语句来自动帮我们调用close()方法： 1234with open("./file_io.txt", 'r') as file: content = file.read() print(type(content)) print(content) 代码更佳简洁，并且不必明确调用 file.close() 方法来回收资源。 2.1.1 read(size) &amp;&amp; readline() &amp;&amp; readlines()除了上面我们介绍过的 read() 文件对象读取方法外，Python 文件对象还提供了 file.readline() 方法和 file.readlines() 方法用来读取文件，三者用法略有不同。 1）read(size) 首先考虑一个问题：我们知道，read() 方法会一次性读取文件的全部内容到内存中，如果我们要读取处理的文件有 10G，那么很大可能会导致内存直接就爆了，MemoryError 。 注意，read() 方法还提供了一个可选的数字类型的参数：size。当 size 被忽略（默认）或者为负时，该文件的所有内容都将被读取并且返回。故，如果待处理文件较大（几个 G），为了保险起见，我们可以通过反复调用 read(size) 方法，每次最多读取 size 个字节的内容。 实际使用： read() 方法会生成文件内容最直接的字符串表示，但对于连续的面向行的处理，它却是不必要的。对于大文件处理，需要使用 size 参数来控制每次读取的字节数。 2）readlines() 相较于 read(size) 方法，readlines() 也是一次性读取整个文件，但它会自动将文件内容分析成一个行的列表，适用于面向行的文件处理。实现方式如下： 123456789101112131415try: file = open("./file_io.txt", 'r') text = file.readlines() print("type(text): ", type(text)) print(text)finally: if file: file.close()# -------------------------------------- with open("./file_io.txt", 'r') as file: text = file.readlines() print("type(text): ", type(text)) for line in text: print(text) 结果输出如下： 12345678type(text): &lt;class 'list'&gt;['Hello,Python developer.\n', 'This is a read file test.']# -------------------------------------- type(text): &lt;class 'list'&gt;Hello,Python developer.This is a read file test. 实际使用： readlines() 方法会生成文件内容的行列表表示，适用于面向行的文件处理，一次性读取意味着大文件不适用。故，多用于简单配置文件配置文件读取。 3）readline() readline() 每次只读取一行，返回的是一个为当前行内容的字符串对象。同样适用于面向行的文件处理，但比 readlines() 慢得多。返回的字符串对象为空时，表示文件已读取到最后一行。实现方式如下： 1234567891011121314151617181920try: file = open("./file_io.txt", 'r') whilt True: line = file.readline() if line: print (line) else: breakfinally: if file: file.close() # -------------------------------------- with open("./file_io.txt", 'r') as file: whilt True: line = file.readline() if line: print (line) else: break 结果输出如下： 12345678Hello,Python developer.This is a read file test.# -------------------------------------- Hello,Python developer.This is a read file test. 实际使用： 当没有足够内存可以一次读取整个文件（文件较大）时，采用 readlines() 方法进行文件处理。 注意：可以看出，上面的三种方法都是把每行末尾的换行符（&#39;\n&#39;）也读进来了，它并不会默认的把 &#39;\n&#39; 去掉，需要我们手动去掉。就像我们在使用 read() 方法读取 ./file_io.txt 文件时： 1'Hello,Python developer.\nThis is a read file test.' 2.1.2 读取大文件数据这里我们给出两种 Python 常用的大文件读取方法： 1）分批读取 处理大文件是很容易想到的就是将大文件分割成若干小文件处理，处理完每个小文件后释放该部分内存。这里用了iter &amp; yield： 1234567891011121314def read_in_chunks(filePath, chunk_size=1024*1024): """ Lazy function (generator) to read a file piece by piece. Default chunk size: 1M You can set your own chunk size """ file = open(filePath) while True: chunk_data = file.read(chunk_size) if not chunk_data: break yield chunk_data if __name__='__main__': filePath = './path/filename' for chunk in read_in_chunks(filePath): process(chunk) # &lt;do something with chunk&gt; size 设置成 1024*1024 的意思就是：1bit*1024*1024 = 1M，也就是说每隔 1M 读取一次数据。 2）Using with open() open() 方法返回的不仅仅是一个文件对象，也是一个可迭代对象。对可迭代对象 f，进行迭代遍历：for line in f 会自动地使用缓冲 IO（buffered IO）以及内存管理，而不必担心任何大文件的问题。 123with open(filename, 'r') as f: for line in f: &lt;do something with the line&gt; 2.1.3 读文件相关1）file-like Object 像 open() 函数返回的这种有个 read() 方法的对象，在 Python 中统称为 file-like Object。除了 file 外，还可以是内存的字节流，网络流，自定义流等等。file-like Object 不要求从特定类继承，只要写个 read() 方法就行。 StringIO 就是在内存中创建的 file-like Object，常用作临时缓冲。后续章节会介绍到。 2）二进制文件 前面讲的默认都是读取文本文件，并且是 UTF-8 编码的文本文件。要读取二进制文件，比如图片、视频等等，用 &#39;rb&#39; 模式打开文件即可： 123&gt;&gt;&gt; f = open('/Users/michael/test.jpg', 'rb')&gt;&gt;&gt; f.read()b'\xff\xd8\xff\xe1\x00\x18Exif\x00\x00...' # 十六进制表示的字节 3）字符编码问题 要读取非 UTF-8 编码的文本文件，需要给 open() 函数传入 encoding 参数，例如，读取 GBK 编码的文件： 123&gt;&gt;&gt; f = open('.\gbk.txt', 'r', encoding='gbk')&gt;&gt;&gt; f.read()'测试' 遇到有些编码不规范的文件，你可能会遇到 UnicodeDecodeError，因为在文本文件中可能夹杂了一些非法编码的字符。遇到这种情况，open() 函数还接收一个 errors 参数，表示如果遇到编码错误后如何处理。最简单的方式是直接忽略： 1&gt;&gt;&gt; f = open('/Users/michael/gbk.txt', 'r', encoding='gbk', errors='ignore') 2.2 写文件写文件和读文件是一样的，首先要进行的肯定是使用 open()函数获取文件对象，唯一区别是调用 open()函数时，传入标识符 &#39;w&#39; 或者 &#39;wb&#39; 表示写文本文件或写二进制文件： 1file = open("./file_io.txt", 'w') 注意，这里我们的文件访问模式是 &#39;w&#39;，即覆盖式写入。以 &#39;w&#39;模式写入文件时，如果文件已存在，会直接覆盖（相当于删掉后新写入一个文件）。如果我们希望追加到文件末尾怎么办？可以传入 &#39;a&#39;以追加（append）模式写入。 打开文件对象（文件描述符），然后才是调用相应的文件对象方法 write() 进行写入： 12f.write('Hello, this is write file test!')f.close() 可以反复调用 write()来写入文件，但是务必要调用 f.close() 来关闭文件： 当我们写文件时，操作系统往往不会立刻把数据写入磁盘，而是放到内存缓存起来，空闲的时候再慢慢写入。只有明确调用 close() 方法时，操作系统才保证把没有写入的数据全部写入磁盘。忘记调用 close() 的后果是数据可能只写了一部分到磁盘，剩下的丢失了。所以，还是用 with 语句来得保险： 12with open("./file_io.txt", 'w') as f: f.write("Hello, this is write file test!") Python 文件对象提供了两个“写”方法： write() 和 writelines()。 1）write() write() 方法和 read()、readline() 方法对应，是将字符串写入到文件中。 2）writelines() writelines() 方法和 readlines() 方法对应，也是针对 列表 的操作。它接收一个 字符串列表 作为参数，将它们写入到文件中，换行符不会自动的加入，因此，需要显式的加入换行符。 12345f1 = open('./file_io.txt', 'w')f1.writelines(["1", "2", "3"])f1 = open('./file_io.txt', 'w')f1.writelines(["1\n", "2\n", "3\n"]) 同样的，要写入特定编码的文本文件，请给 open() 函数传入 encoding 参数，将字符串自动转换成指定编码。 2.3 文件定位文件对象中有一个很重要的概念：文件指针，它表示了文件对象读写文件时的位置信息。 就像前面我们说的文件追加式写入 a 和 覆盖式写入 w 的区别：a 打开的文件中的文件指针位于文件末尾，而 w 打开的文件文件指针位于文件开头，后续内容会被删除。 文件对象提供了 tell() 以及 seek(offset [,from]) 方法操作文件指针： 1）file.tell() tell() 方法告诉你文件内文件指针的当前位置。换句话说，就是文件下一次的读写会发生在文件开头这么多字节之后（就是一个相对于文件开头的偏移量）。 12345678910file = open("./file_io.txt", 'r')str1 = file.read(10)print("读取到的字符串为: ", str)position = file.tell()print("当前文件指针位置: ", position)if file: file.close() 输出结果如下： 123$ python HelloWorld.py读取到的字符串为: Hello,Pyth当前文件指针位置: 10 此时如果我们再次读取的话： 12345678910111213file = open(&quot;./file_io.txt&quot;, &apos;r&apos;)str1 = file.read(10)print(&quot;读取到的字符串为: &quot;, str1)position = file.tell()print(&quot;当前文件指针位置: &quot;, position)str2 = file.read()print(&quot;再次读取到的字符串为: &quot;, str2)if file: file.close() 输出结果如下： 12345&gt;python HelloWorld.py读取到的字符串为: Hello,Pyth当前文件指针位置: 10再次读取到的字符串为: on developer.This is a read file test. 2）file.seek(offset [, whence]) seek(offset [, whence]) 方法用于移动文件读写指针到指定位置。 有两个参数，第一个 offset 表示偏移量，需要移动的字节数。第二个 whence 表示: 可选值，默认为0，表示文件开头；1表示相对于当前的位置；2表示文件末尾 例如，seek(0, 0) 表示以文件开头作为参考位置，偏移 0 个字节，说明此时文件内的文件指针处于文件开头。 正如我们上面再次读取文件字符串时，只显示文件指针在 10 之后的字符串。那么如何来读取文件中的完整字符串呢？ 12345678910111213141516171819202122232425file = open("./file_io.txt", 'r')str1 = file.read(10)print("读取到的字符串为: ", str1)position1 = file.tell()print("当前文件指针位置: ", position1)str2 = file.read()print("再次读取到的字符串为: ", str2)position2 = file.tell()print("当前文件指针位置: ", position2)str3 = file.read()print("此时读取到的字符串为: ", str3)# seek(0, 0)position3 = file.seek(0,0)print("当前文件指针位置: ", position3)str4 = file.read()print("此时读取到的字符串为: ", str4)if file: file.close() 输出结果如下： 12345678910&gt;python HelloWorld.py读取到的字符串为: Hello,Pyth当前文件指针位置: 10再次读取到的字符串为: on developer.This is a read file test.当前文件指针位置: 50此时读取到的字符串为:当前文件指针位置: 0此时读取到的字符串为: Hello,Python developer.This is a read file test. 注意，读取文件时要时刻关注文件指针的位置。一旦文件指针处于文件尾部，如果不移动文件指针位置的话，再进行读取的话，会获取到空串。 3. 内存中的数据读写上一小节我们提到过 Python 中的 file-like object，如内存的字节流 StringIO、BytesIO 等等。比如，当我们需要对获取到的数据进行操作，但是并不想把数据写到本地硬盘上，这时我们可以使用内存的字节流来处理数据。 不管是后续介绍到的 StringIO（字符串流，操作字符串），还是 BytesIO（字节流，操作二进制数据），在使用前必须从 Python 中导入相应的内置模块： 12from io import StringIOfrom io import BytesIO 3.1 StringIO很多时候，数据读写不一定是文件，也可以在内存中读写。StringIO 顾名思义就是在内存中读写 str。 类似于文件，在开始操作内存中的字符串流之前，首先来看如何创建一个 StringIO 对象： 12345678910&gt;&gt;&gt; from io import StringIO&gt;&gt;&gt; &gt;&gt;&gt; str_stream1 = StringIO()# 或者直接在创建时赋一个初始 str：&gt;&gt;&gt; str_stream1 = StringIO("This is a StringIO test.")&gt;&gt;&gt; type(str_stream1)&lt;class '_io.StringIO'&gt;&gt;&gt;&gt; str_stream2&lt;_io.StringIO object at 0x000001110192FDC8&gt; 掌握了如何创建 StringIO 对象后，我们就可以调用相应的 file-like object 方法（help(StringIO ) 可查看 StringIO 支持的方法）操作字符串流了。除了 file-like object 读取方法，StringIO 对象还提供了 getvalue() 方法用于获取内存字符串流对象中的 str。 下面我们将通过一个样例来看其使用： 1234567891011121314151617181920#! /usr/bin/envs python3'a StringIO tese'from io import StringIOdef outputString(): return "string \nfrom \noutputString \nfunction"# 模拟待处理的字符串数据：s = outputString()# 将函数返回的字符串数据读到内存中：sio = StringIO(s)# 用 StringIO 本身提供的方法读取 str：print(sio.getvalue())# 当然也可以用 file-like object 的方法读取：for element in sio.readlines(): print(element.strip()) 再来看一下 getvalue() 和 file-like object 读取方法（例如，file.readlines()）的使用差异（同样的，StringIO 中也支持 tell()、seek() 方法）： 12345678910111213141516171819202122232425#! /usr/bin/envs python3'a StringIO tese'from io import StringIOdef outputString(): return "string \nfrom \noutputString \nfunction"# 模拟待处理的字符串数据：s = outputString()# 内存中创建字节流对象：sio = StringIO()# 向内存中写入（指针已移动到末尾）：sio.write(s)# 用 StringIO 本身提供的方法读取 str：print(sio.getvalue())print("当前指针位置：", sio.tell())# 此时，如果用 file-like object 的方法查看的时候，会发现数据为空：for ele in sio.readlines(): print(ele) 结果输出如下： 123456$ python HelloWorld.pystringfromoutputStringfunction当前指针位置： 36 可以发现，file-like object 方法会导致 StringIO 内存指针位置的变化。 3.2 BytesIOStringIO 操作的只能是 str，如果要操作二进制数据，就需要使用 BytesIO。 下面我们来看 BytesIO 的使用： 123456789101112s = "This is a BytesIO test"# 内存中创建字节流对象：bio = BytesIO()# 写入bio.write(s.encode('utf-8'))print(bio.getvalue())bio.seek(0,0)print(bio.tell())for i in bio.readlines(): print(i.strip()) 结果输出如下: 1234$ python HelloWorld.pyb'This is a BytesIO test'0b'This is a BytesIO test' 同样，和 StringIO 类似，可以用一个 bytes 对象初始化 BytesIO，然后，像读文件一样读取： 1234&gt;&gt;&gt; from io import BytesIO&gt;&gt;&gt; f = BytesIO(b'\xe4\xb8\xad\xe6\x96\x87')&gt;&gt;&gt; f.read()b'\xe4\xb8\xad\xe6\x96\x87' 4 OS Module上面我们知道，文件读写是 Python 程序很常见，且很重要的操作，而文件读写不可避免的需要涉及到系统中文件以及目录操作，这是必要的。 如果我们想要操作系统中的文件、目录，可以在命令行（Terminal）下输入操作系统提供的各种命令来完成。比如创建目录 mkdir、复制文件 cp 、删除文件 rm 等等。 如果要在 Python 程序中执行这些目录和文件的操作怎么办？？？ 事实上，操作系统提供的命令只是简单地调用了操作系统提供的接口函数，而 Python 内置的 os 模块也可以直接调用操作系统提供的接口函数完成相同的功能。Python 中的 os 模块提供了非常丰富的方法用来处理文件和目录。 1import os 4.1 查看系统基本属性1）os.name os.name 可用于查看当前操作系统类型。 如果值是 posix，说明系统是 Linux、Unix 或 Mac OS X；如果是 nt，就是 Windows 系统。 1234567# Linux、Unix 或 Mac OS X 平台下：&gt;&gt;&gt; os.name'posix'# Windows 平台下：&gt;&gt;&gt; os.name'nt' 2）os.uname() 如果想要获取更加详细的系统信息的话，可以调用 uname()函数： 12&gt;&gt;&gt; os.uname()posix.uname_result(sysname='Linux', nodename='ThinkCentre-M910s-N000', release='4.15.0-45-generic', version='#48~16.04.1-Ubuntu SMP Mon Mar 23 11:59:05 PDT 2019', machine='x86_64') 注意 uname() 函数在 Windows 上不支持。 3）os.environ Python 中的变量 os.environ 保存了当前操作系统中定义的全部环境变量，可以直接查看一下： 12&gt;&gt;&gt; os.environenviron(&#123;'VERSIONER_PYTHON_PREFER_32_BIT': 'no', 'TERM_PROGRAM_VERSION': '326', 'LOGNAME': 'deeplearning', 'USER': 'deeplearning', 'PATH': '/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin', ...&#125;) 如果我们想要获取某个环境变量的值，比如常用的 PATH ，可以调用 os.environ.get(&#39;key&#39;)： 12&gt;&gt;&gt; os.environ.get('PATH')'/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/opt/X11/bin:/usr/local/mysql/bin' 4.2 Operate File &amp;&amp; Dir我们知道，Python 操作文件和目录的函数主要存放于 os 模块中，os 模块还包含了一个用于获取文件属性的 os.path 子模块。 4.2.1 OS 模块常用方法1）os.access(path, mode) os.access(path, mode) 方法可用于检查当前访问路径的权限。如果允许访问返回 True , 否则返回 False。： 参数 path：用于检测的访问路径； 参数 mode：用于检验的访问权限： os.F_OK: 包含在 access() 的 mode 参数中，测试 path 是否存在; os.R_OK: 包含在 access() 的 mode 参数中 ，测试 path 是否可读； os.W_OK: 包含在 access() 的 mode 参数中 ，测试 path 是否可写； os.X_OK: 包含在 access() 的 mode 参数中 ，测试 path 是否可执行。 演示样例： 123456789101112131415import os# 假定 ./test.py 文件存在，并有读写、可执行权限ret = os.access("./test.py", os.F_OK)print ("F_OK - 返回值 %s"% ret)ret = os.access("./test.py", os.R_OK)print ("R_OK - 返回值 %s"% ret)ret = os.access("./test.py", os.W_OK)print ("W_OK - 返回值 %s"% ret)ret = os.access("./test.py", os.X_OK)print ("X_OK - 返回值 %s"% ret) 样例输出结果为： 12345$ python test.pyF_OK - 返回值 TrueR_OK - 返回值 TrueW_OK - 返回值 TrueX_OK - 返回值 True 2）os.getcwd() os.getcwd() 方法可用于返回当前工作目录： 1234import os# 打印当前目录print ("当前工作目录 : %s" % os.getcwd()) 输出结果为： 12$ python test.py当前工作目录 : C:\Users\alienware\Desktop\Python 3）os.listdir(path) os.listdir() 方法用于返回指定目录中包含的文件或目录的名称列表（不包含下一级目录），以字母顺序显示。 1234567import oscurrent_path = os.getcwd()dirs = os.listdir(current_path)for dir in dirs: print(dir) 样例输出结果如下： 1234$ python test.pyclientservertest.py 4）创建以及删除 os.mkdir(path) &amp;&amp; os.makedirs(path) os.mkdir(path) 和 os.makedirs(path) 方法都可用于创建目录。区别在于当父目录不存在的时候os.mkdir(path) 不会创建，os.makedirs(path) 则会连同父目录一起创建。 123456import ospath = "./test"os.mkdir(path)os.makedirs("./test1/test2/test3") os.remove(path) &amp;&amp; os.rmdir(path) &amp;&amp; os.removedirs(path) os.remove() 方法用于删除指定路径的文件。如果指定的路径是一个目录，将抛出 OSError: 12345678910import os# 列出当前目录下所有文件：print ("目录为: %s" %os.listdir(os.getcwd()))# 移除 test.py 文件os.remove("test.py")# 移除后列出目录print ("移除后 : %s" %os.listdir(os.getcwd())) 样例输出结果为： 123$ python test.py目录为: ['client', 'server', 'test.py']移除后 : ['client', 'server'] os.rmdir() 方法用于删除指定路径的目录。仅当这文件夹是空的才可以, 否则, 抛出 OSError。 12345678910import os, sys# 列出目录print ("目录为: %s"%os.listdir(os.getcwd()))# 删除路径os.rmdir("./client")# 列出重命名后的目录print ("目录为: %s" %os.listdir(os.getcwd())) os.removedirs() 方法用于递归删除目录（会判断其父目录是否为空，为空的话也会删除掉）。但只可以删除一个空目录，否组会报错，这决定了它很少用。 想一下：如何删除某一目录下的所有文件以及目录？？？ 这里我们先给出一种遍历删除法： 1234567def del_file(path): for file in os.listdir(path): file_path = os.path.join(path, file) if os.path.isfile(file_path): os.remove(file_path) else: del_file(file_path) 5）os.renames(src, dst) &amp;&amp; os.renames(src, dst) os.rename() 方法用于命名文件或目录，从 src 到 dst。如果 dst 是一个存在的目录, 将抛出 OSError。 12345678910import os, sys# 列出目录print ("目录为: %s"%os.listdir(os.getcwd()))# 重命名os.rename("test.py","test_new.py")# 列出重命名后的目录print ("目录为: %s" %os.listdir(os.getcwd())) os.renames() 方法用于递归重命名目录或文件。 1234567891011import os, sysprint ("当前目录为: %s" %os.getcwd())# 列出目录print ("目录为: %s"%os.listdir(os.getcwd()))# 重命名 "aa1.txt"os.renames("test_new.py","newdir/test.py")# 列出重命名的文件 "aa1.txt"print ("目录为: %s" %os.listdir(os.getcwd())) 4.2.2 os.path Module1）os.path.abspath(path) 12&gt;&gt;&gt; os.path.abspath('.')'C:\\Users\\alienware\\Desktop\\Python' 2）os.path.basename(path) &amp;&amp; os.path.dirname(path) 返回路径文件名： 12&gt;&gt;&gt; os.path.basename(os.getcwd())'Python' 返回路径目录名： 1234&gt;&gt;&gt; os.path.dirname('./server/test/client')'./server/test'&gt;&gt;&gt; os.path.dirname('./server/README.txt')'./server' 3）os.path.exists(path) 判断路径 path 是否存在。路径存在则返回True,路径损坏返回False 1234&gt;&gt;&gt; os.path.exists("./server/README.txt")True&gt;&gt;&gt; os.path.exists("./server/app")True 4）os.path.isfile(path) &amp;&amp; os.path.isdir(path) os.path.isfile(path) 方法可用于判断路径是否为文件，而 os.path.isdir(path) 可以判断路径是否为目录。 12345678&gt;&gt;&gt; os.path.isfile("./server/README.txt")True&gt;&gt;&gt; os.path.isdir("./server/README.txt")False&gt;&gt;&gt; os.path.isdir("./server/app")True&gt;&gt;&gt; os.path.isfile("./server/app")False 5）os.path.getsize(path) 用于返回文件大小，如果文件不存在就返回错误。 12&gt;&gt;&gt; os.path.getsize("./server/README.txt")0 6）os.path.join(path1[, path2[, …]]) 可把目录和文件名合成一个路径： 1234&gt;&gt;&gt; os.path.join("/home", "test", "server.py")'home\\test\\server.py'# 注意，os.sep 可获取当前系统分隔符 7）os.path.split(path) &amp;&amp; os.path.splitext(path) os.path.split(path) 方法可以把路径分割成 dirname 和 basename，返回一个元组： 123456&gt;&gt;&gt; os.path.basename("/home/test.py")'test.py'&gt;&gt;&gt; os.path.dirname("/home/test.py")'/home'&gt;&gt;&gt; os.path.split("/home/test.py")('/home', 'test.py') os.path.splitext(path) 方法把路径分割成路径名和文件扩展名的元组： 12&gt;&gt;&gt; os.path.splitext(&quot;/home/test.py&quot;)(&apos;/home/test&apos;, &apos;.py&apos;) 4.2.3 常用功能模块这一部分将结合 4.2.1 和 4.2.2 中模块方法给出开发中常用的功能模块方法： 1）创建以及删除目录 这里我们使用 Python 另一个内置模块 shutil 来删除目录中文件，可以将其看作是 os 模块的一个补充。 12345678910111213141516import os, shutildef mkdir(path): path = path.strip() IsExists = os.path.exists(path) if not IsExists: # 目录不存在，则创建 os.makedirs(path) else: # 如果存在，则删除重新创建： shutil.rmtree(path) os.makedirs(path) print(path + "：had been created.") 2）删除目录下所有文件 1234567891011121314151617181920import os, shutildef rmfiles(path): path = path.strip() IsExists = os.path.exists(path) if IsExists: filelist=os.listdir(path) for file in filelist: file_path = os.path.join(path, file) if os.path.isfile(file_path): os.remove(filepath) elif os.path.isdir(file_path): shutil.rmtree(filepath) else: print("Please check your: " + path + " exists?") print("The files had been deleted in : " + path) 3）文件过滤 例如列出当前目录下的所有目录： 1234&gt;&gt;&gt; os.listdir(".")['server', 'test.py', 'test_new.py']&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isdir(x)]['server'] 列出当前目录下的所有 Python 脚本文件： 12&gt;&gt;&gt; [x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']['test.py', 'test_new.py'] 5. Python 序列化]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Input/Output</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 模块]]></title>
    <url>%2FPython%2FPython-%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： Python 中模块就是一个包含所有你定义的变量、函数和类的文件（实现特定功能），其后缀名是 .py。在 Python 中，一个 .py 文件（脚本）就称之为一个模块（Module）。这一小节我们来看 Python 中模块的使用。 1. Python 模块模块使用的最大好处就是大大提高了代码的可维护性。当一个模块编写完毕，就可以被其他地方引用，以使用该模块中的函数等功能。 例如我们在编写程序的时候，也经常引用其他模块，包括 Python 内置的模块和来自第三方的模块。 1.1 认识模块（Module）除了 Python 标准库，Python 一个很大的特点就是具有丰富的第三方库。这些库文件里面包含了很多非常有用的模块以方便我们调用编程。对于 Python 自身的标准库，只要安装好 Python 就可以使用其丰富的内置模块了。而对于第三方库需要先单独安装然后使用。 下面我们先来给出一个使用 Python 内建的 sys 模块的样例脚本（test.py）： 12345678910111213141516171819#!/usr/bin/env python3# -*- coding: utf-8 -*-' a test module '__author__ = 'TheNightIsYoung'import sysdef module_test(): print('输入的命令行参数如下:') count = 1 for arg in sys.argv: print("No.%d arg is [ %s ]" % (count,arg)) count += 1if __name__=='__main__': module_test() print("\nPython 模块搜索路径：",sys.path) 上面给出的 test.py 脚本其实就是一个模块，功能是用于打印程序执行时的命令行参数。以上就是一个 Python 模块定义的标准模板，我们编写其它功能模块时可以按这样的结构来。 执行结果如下： 12345678$ python HelloWorld.py arg_1 arg_2 arg_3输入的命令行参数如下:No.1 arg is [ HelloWorld.py ]No.2 arg is [ arg_1 ]No.3 arg is [ arg_2 ]No.4 arg is [ arg_3 ]Python 模块搜索路径： ['C:\\Users\\xxxx\\Desktop\\Python', 'E:\\Python\\Python37\\python37.zip', 'E:\\Python\\Python37\\DLLs', 'E:\\Python\\Python37\\lib', 'E:\\Python\\Python37', 'E:\\Python\\Python37\\lib\\site-packages'] 上面我们说 test.py 模块可以看作是一个 Python 模块定义的标准模板，下面我们来看其各部分作用： 1）import sys test.py 模块中使用了 Python 内置模块 sys 。可以看出，使用 sys 模块的第一步，就是导入该模块： 1import sys import sys 引入了 Python 标准库中的 sys.py 模块，这是引入某一模块的方法。导入 sys模块后，我们就有了变量 sys 指向该模块，利用 sys 这个变量，就可以访问 sys模块的所有功能。 2）sys.argv module_test() 函数是 test.py 模块实现其功能的主要部分，主要用到了 sys.argv。 sys模块有一个 argv 变量，sys.argv 是一个包含命令行参数的列表。argv 至少有一个元素，因为第一个参数永远是该 .py 文件的名称（No.1 arg is [ HelloWorld.py ]）。sys.argv 返回列表信息如下： 1["HelloWorld.py", arg_1, arg_2, arg_3] 3）__name__==’__main__’ 注意最后两行代码，我们称之为 if 测试： 1234# if 测试:if __name__=='__main__': module_test() print("\nPython 模块搜索路径：",sys.path) 每个模块都有一个 __name__ 属性，它对于两个取值：1. 当模块是被 import 映入调用时，取值为模块的名字；2. 当模块是直接执行的，则该变量取值为 __main__。 所以很好理解，当我们在命令行运行 test 模块文件时，Python 解释器获取到的特殊变量 __name__ 值为 __main__，运行 if 测试部分；而如果在其它源程序中导入该 test 模块时，if 测试将不会执行。 因此，这种 if 测试可以让该程序块仅在该模块自身运行时执行。最常见的用法就是对模块运行功能测试。 1.1.1 import 语句想要使用上面我们定义的源程序（test 模块）只需在另一个源文件里执行 import 语句，语法格式如下： 1import module1[, module2[,... moduleN] 并且注意 一个模块只会被导入一次，不管你执行了多少次 import，这样可以防止导入模块被一遍又一遍地执行。 我们来再写一个 hello 模块来看如何在其它程序中使用： 123456789101112#!/usr/bin/env python3# -*- coding: utf-8 -*-' a test module '__author__ = 'TheNightIsYoung'def welcomeSb(par): print("Hello,", par)if __name__=='__main__': welcomeSb("TheNightIsYoung") 直接运行上述程序可以完成 hello 模块功能测试。而在 test.py 程序中引入的话如下： 123456#!/usr/bin/env python3# -*- coding: utf-8 -*-import hellohello.welcomeSb("TheNightIsYoung") 以上实例输出如下： 12$ python test.pyHello, TheNightIsYoung 1.1.2 sys.path 搜索路径有些人可能会想：当我们使用 import 语句的时候，Python 解释器是怎样找到对应的模块文件的呢？ 这就涉及到我们前面说到过的 Python 的搜索路径（sys.path），搜索路径是由一系列目录名组成的列表。 import 时，Python 解释器会依次从这些目录中去寻找所引入的模块。是不是看起来很像环境变量？？？ 搜索路径被存储在 sys 模块中的 path 变量，测试一下： 123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path['', 'E:\\Python\\Python37\\python37.zip', 'E:\\Python\\Python37\\DLLs', 'E:\\Python\\Python37\\lib', 'E:\\Python\\Python37', 'E:\\Python\\Python37\\lib\\site-packages'] sys.path 输出是一个列表，其中第一项是空串，代表当前目录（若是从一个脚本中打印出来的话，可以更清楚地看出是脚本位于哪个目录），亦即我们执行 Python 解释器的目录（对于脚本的话就是运行的脚本所在的目录）。正如我们在 1.1 中样例那样： 1'C:\\Users\\xxxx\\Desktop\\Python' 1.1.3 Module Use1）模块使用 这一小节我们来看如何在其它源程序或模块中使用映入的模块属性。在解释器的当前目录或者 sys.path 中的一个目录里面来创建一个 fibo.py 的文件，代码如下： 12345678910111213141516171819202122232425#!/usr/bin/env python3# -*- coding: utf-8 -*-' 斐波那契（Fibonacci）数列模块 '__author__ = 'TheNightIsYoung'def print_Fibo(max): # 打印 max 以内的斐波那契数列 a, b = 0, 1 while b &lt;= max: print(b, end=" ") a, b = b, a + b print()def get_Fibo(max): # 获取 max 以内的斐波那契数列 result = [] a, b = 0, 1 while b &lt;= max: result.append(b) a, b = b, a + b return resultif __name__=='__main__': print_Fibo(1000) print("Fibonacci :", print_Fibo(1000)) 完成功能测试后，然后进入 Python 解释器，使用下面的命令导入这个模块： 1&gt;&gt;&gt; import fibo 这样做并没有把直接定义在 fibo 模块中的函数名称（print_Fibo、get_Fibo）写入到当前符号表里，只是把模块 fibo 的名字写到了那里。 此时，可以使用模块名称来访问函数： 1234567&gt;&gt;&gt; fibo.__name__'fibo'&gt;&gt;&gt; fibo.print_Fibo(100)1 1 2 3 5 8 13 21 34 55 89&gt;&gt;&gt; fibo.get_Fibo(100)[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89]&gt;&gt;&gt; 如果我们打算多处使用其中一个函数，你还可以把它赋给一个本地的名称： 123&gt;&gt;&gt; get_Fibo = fibo.get_Fibo&gt;&gt;&gt; get_Fibo(100)[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89] 上面我们说了 import somemodule 的导入方式，没有把直接定义在 fibo 模块中的函数名称（print_Fibo、get_Fibo）写入到当前符号表里，只是把模块 fibo 的名字写到了那里。Python 的 from 语句让我们从模块中导入一个指定的部分到当前命名空间中，所以用法如下： 将整个模块(somemodule)导入，格式为： import somemodule 为方便模块使用，简写格式为：import somemodule as xxx 从某个模块中导入某个函数，格式为： from somemodule import somefunction 从某个模块中导入多个函数，格式为： from somemodule import firstfunc, secondfunc, thirdfunc 将某个模块中的全部函数、变量导入，格式为： from somemodule * 2）作用域在一个定义好的模块中可能会定义很多函数和变量，但有的函数和变量我们希望给别人使用，有的函数和变量我们希望仅仅在模块内部使用。在 Python 中，是通过 _ 前缀来实现的。 还记得我们在标识符（Identifier）中说过的，以下划线开头的标识符是有特殊意义的。 正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等； __xxx__ 命名的变量或方法 类似 __xxx__ 这样的变量是特殊变量，可以被直接引用，但是有特殊用途。比如上面的__author__，__name__ 就是特殊变量，fibo.py 模块定义的文档注释也可以用特殊变量 __doc__ 访问，我们自己的变量一般不要用这种变量名。 类似 __xxx__ 这样的函数代表 Python 里特殊方法专用的标识，如 __init__() 代表类的构造函数。 _xxx 和 __xxx 命名的变量或方法 类似 _xxx和 __xxx 这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等。 之所以我们说，private 函数和变量 “不应该” 被直接引用，而不是 “不能” 被直接引用，是因为 Python 并没有一种方法可以完全限制访问 private 函数或变量。但是，从编程习惯上不应该引用 private 函数或变量。 private 函数或变量不应该被别人引用，那它们有什么用呢？请看例子： 1234567891011def _private_1(name): return 'Hello, %s' % namedef _private_2(name): return 'Hi, %s' % namedef greeting(name): if len(name) &gt; 3: return _private_1(name) else: return _private_2(name) 公开 greeting()函数，而把内部逻辑用 private 函数隐藏起来了，这样，调用 greeting() 函数不用关心内部的 private 函数细节，这也是一种非常有用的代码封装和抽象的方法，即： 外部不需要引用的函数全部定义成 private，只有外部需要引用的函数才定义为 public。 1.2 深入模块上面我们介绍了模块的基础用法，这里我们来概述一下模块的用法。 模块除了方法定义（或 import、if 测试块），还可以包括可执行的代码。这些代码一般用来初始化这个模块。这些代码只有在第一次被导入时才会被执行。 每个模块有各自独立的符号表，在模块内部为所有的函数当作全局符号表来使用。 模块是可以导入其他模块的。在一个模块（或者脚本，或者其他地方）的最前面使用 import 来导入一个模块，当然这只是一个惯例，而不是强制的。被导入的模块的名称将被放入当前操作的模块的符号表中。 还有一种导入的方法，可以使用 import 直接把模块内（函数，变量的）名称导入到当前操作模块。这种导入的方法不会把被导入的模块的名称放在当前的字符表中，somemodule 是没有定义的： 1from somemodule import firstfunc, secondfunc, thirdfunc 这还有一种方法，可以一次性的把模块中的所有（函数，变量）名称都导入到当前模块的字符表: 1from somemodule * 这将把所有的名字都导入进来，但是那些由单一下划线（_）开头的名字不在此例。大多数情况，Python 程序员不使用这种方法，因为引入的其它来源的命名，很可能覆盖了已有的定义。 1.2.1 dir() 函数Python 内置的函数 dir() 可以找到模块内定义的所有名称。以一个字符串列表的形式返回: 12345&gt;&gt;&gt; import fibo, sys&gt;&gt;&gt; dir(fibo)['__author__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'get_Fibo', 'print_Fibo']&gt;&gt;&gt; dir(sys)['__breakpointhook__', '__displayhook__', '__doc__', '__excepthook__', '__interactivehook__', '__loader__', '__name__', '__package__', '__spec__', '__stderr__', '__stdin__', '__stdout__', '_clear_type_cache', '_current_frames', '_debugmallocstats', '_enablelegacywindowsfsencoding', '_framework', '_getframe', '_git', '_home', '_xoptions', 'api_version', 'argv', 'base_exec_prefix', 'base_prefix', 'breakpointhook', 'builtin_module_names', 'byteorder', 'call_tracing', 'callstats', 'copyright', 'displayhook', 'dllhandle', 'dont_write_bytecode', 'exc_info', 'excepthook', 'exec_prefix', 'executable', 'exit', 'flags', 'float_info', 'float_repr_style', 'get_asyncgen_hooks', 'get_coroutine_origin_tracking_depth', 'get_coroutine_wrapper', 'getallocatedblocks', 'getcheckinterval', 'getdefaultencoding', 'getfilesystemencodeerrors', 'getfilesystemencoding', 'getprofile', 'getrecursionlimit', 'getrefcount', 'getsizeof', 'getswitchinterval', 'gettrace', 'getwindowsversion', 'hash_info', 'hexversion', 'implementation', 'int_info', 'intern', 'is_finalizing', 'last_traceback', 'last_type', 'last_value', 'maxsize', 'maxunicode', 'meta_path', 'modules', 'path', 'path_hooks', 'path_importer_cache', 'platform', 'prefix', 'ps1', 'ps2', 'set_asyncgen_hooks', 'set_coroutine_origin_tracking_depth', 'set_coroutine_wrapper', 'setcheckinterval', 'setprofile', 'setrecursionlimit', 'setswitchinterval', 'settrace', 'stderr', 'stdin', 'stdout', 'thread_info', 'version', 'version_info', 'warnoptions', 'winver'] 1.2.2 注意事项模块是一组 Python 代码的集合，可以使用其他模块，也可以被其他模块使用。 创建自己的模块时，要注意： 模块名要遵循Python变量命名规范，不要使用中文、特殊字符； 模块名不要和系统模块名冲突，最好先查看系统是否已存在该模块检查方法是在 Python 交互环境执行import abc，若成功则说明系统存在此模块。 1.3 包的引入我们可能会想到，如果不同的人编写的模块名相同怎么办？为了避免模块名冲突，引入了 “包”。 1.3.1 何为包？包是一种以目录来组织模块从而管理 Python 模块命名空间的形式，采用”点模块名称”。 比如一个模块的名称是 A.B， 那么他表示一个包 A 中的子模块 B 。 引入包以后，就好像使用模块的时候，你不用担心不同模块之间的全局变量相互影响一样，采用包的管理形式也不用担心不同库之间的模块重名的情况。 举个例子，一个 abc.py 的文件就是一个名字叫 abc 的模块，一个 xyz.py 的文件就是一个名字叫 xyz 的模块。 现在，假设我们的 abc 和 xyz 这两个模块名字与其他模块冲突了，于是我们可以通过包来组织模块，避免冲突。方法是选择一个顶层包名，比如 mytest，按照如下目录存放： 1234mytest├─ __init__.py├─ abc.py└─ xyz.py 引入了包以后，只要顶层的包名不与别人冲突，那所有模块都不会与别人冲突。现在，abc.py 模块的名字就变成了 mytest.abc，类似的，xyz.py的模块名变成了 mytest.xyz。 在导入 mytest 包时，Python 会根据 sys.path 中的目录来寻找这个包中包含的子目录。 但请注意，目录只有包含一个叫做 __init__.py 的文件才会被认作是一个包。否则，Python 就把这个目录当成普通目录，而不是一个包。当然这个文件中也可以包含一些初始化代码或者为 __all __ 变量赋值（将在后面介绍的）。 1.3.2 包的使用类似的，可以有多级目录，组成多级层次的包结构。比如如下的目录结构： 123456789101112mytest ├─ web │ ├─ __init__.py │ ├─ utils.py │ └─ www.py ├─ server │ ├─ __init__.py │ └─ deal.py ├─ __init__.py ├─ abc.py └─ xyz.py └─ utils.py 文件 www.py 的模块名就是 mytest.web.www，两个文件utils.py的模块名分别是 mytest.utils 和 mytest.web.utils 。 类似的， 用户可以每次只导入一个包里面的特定模块，比如: 1import mytest.web.utils 这将会导入子模块: mytest.web.utils。 此时，必须使用全名去访问: 1mytest.web.utils.func(arg_1,arg_2) 还有一种导入子模块的方法是: 1from mytest.web import utils 这同样会导入子模块: utils，并且不需要那些冗长的前缀，所以可以这样使用: 1utils.func(arg_1,arg_2) 注意当使用 from package import item 这种形式的时候，对应的 item 既可以是包里面的子模块（子包），或者包里面定义的其他名称，比如函数，类或者变量。 import 语法会首先把 item 当作一个包定义的名称，如果没找到，再试图按照一个模块去导入。如果还没找到，一个: exc:ImportError 异常被抛出了。 反之，如果使用形如 import item.subitem.subsubitem 这种导入形式，除了最后一项，都必须是包。而最后一项则可以是模块或者是包，但是不可以是类，函数或者变量的名字。 1.3.3 从一个包中导入 *嫌麻烦最好不要用，毕竟这种方式不是非必须的。 设想一下，如果我们使用 from mytest.web import * 会发生什么？ Python 会进入文件系统，找到这个包里面所有的子模块，一个一个的把它们都导入进来。 但是很不幸，这个方法在 Windows 平台上工作的就不是非常好，因为 Windows 是一个大小写不区分的系统。在这类平台上，没有人敢担保一个叫做 ECHO.py 的文件导入为模块 echo 还是 Echo 甚至 ECHO。 为了解决这个问题，只能烦劳包作者提供一个精确的包的索引了。导入语句遵循如下规则： 如果包定义文件 __init__.py 存在一个叫做 __all__ 的列表变量，那么在使用 from package import * 的时候就把这个列表中的所有名字作为包内容导入。 作为包的作者，可别忘了在更新包之后保证 __all__ 也更新了啊。例如，mytest/web/__init__.py 1__all__ = [&quot;echo&quot;, &quot;surround&quot;, &quot;reverse&quot;] 这表示当你使用 from mytest.web import * 这种用法时，你只会导入包里面这三个子模块。 通常我们并不主张使用 (*) 这种方法来导入模块，因为这种方法经常会导致代码的可读性降低。记住，使用 from Package import specific_submodule 这种方法永远不会有错。事实上，这也是推荐的方法。除非是你要导入的子模块有可能和其他包的子模块重名。 如果在结构中包是一个子包（比如这个例子中对于包 web 来说），而你又想导入兄弟包（同级别的包）你就得使用导入绝对的路径来导入。比如，如果模块 mytest.web.util 要使用包 mytest.server 中的模块 deal，你就要写成 from mytest.server import deal。 还有一些相对路径用法: 123from . import echofrom .. import formatsfrom ..filters import equalizer 1.4 第三方模块前面我们说过，Python 一个很大的特点就是具有丰富的第三方库，包含了很多有用的模块。和 Python 内置以及用户自定义模块的使用不同的是，第三方模块往往需要先下载安装然后才可以导入使用。 1.4.1 Pip在 Python 中，安装第三方模块，是通过包管理工具 pip 完成的。 例如，我们要安装一个非常常用的第三方库——Python Imaging Library，这是 Python 下非常强大的处理图像的工具库。不过，PIL 目前只支持到 Python 2.7，并且有年头没有更新了。因此，基于 PIL 的 Pillow 项目开发非常活跃，并且支持最新的 Python 3。 使用 pip 的安装命令是： 1pip install pillow 类似的，一般来说，第三方库都会在 Python 官方的 pypi.python.org （可以从这里下载所需的第三方库）网站注册，要安装一个第三方库，可以在 Pypi 上搜索下载其安装文件。 1.4.2 AnacondaAnaconda 的介绍和使用可以参看 Python 简介与开发环境搭建 中第四部分（4. Anaconda）。 1.4.3 Use安装成功后，就可以正常导入使用了。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Module</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 之函数式编程]]></title>
    <url>%2FPython%2FPython-%E4%B9%8B%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 我们知道函数是 Python 内建支持的一种封装，通过一层一层的函数调用，就可以把复杂任务分解成简单的任务，这种分解可以称之为面向过程的程序设计。函数就是面向过程的程序设计的基本单元。 而函数式编程（请注意多了一个“式”字）——Functional Programming，虽然也可以归结到面向过程的程序设计，但其思想更接近数学计算。 函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！ Python 之函数式编程1. 高阶函数高阶函数英文叫 Higher-order function。什么是高阶函数？我们以实际代码为例子，一步一步深入概念。 1.1 理解高阶函数1.1.1 变量可以指向函数以 Python 内置的求绝对值的函数 abs() 为例，调用该函数用以下代码： 12&gt;&gt;&gt; abs(-10)10 但是，如果只写 abs 呢？ 12&gt;&gt;&gt; abs&lt;built-in function abs&gt; 可见，abs(-10) 是函数调用，而 abs 是函数本身。 我们知道，要获得函数调用结果，我们可以把结果赋值给变量： 123&gt;&gt;&gt; x = abs(-10)&gt;&gt;&gt; x10 但是，如果把函数本身赋值给变量呢？ 123&gt;&gt;&gt; f = abs&gt;&gt;&gt; f&lt;built-in function abs&gt; 结论：函数本身也可以赋值给变量，即：变量可以指向函数。 如果一个变量指向了一个函数，那么，可否通过该变量来调用这个函数？用代码验证一下： 123&gt;&gt;&gt; f = abs&gt;&gt;&gt; f(-10)10 成功！说明变量 f 现在已经指向了 abs函数本身。直接调用 abs() 函数和调用变量 f() 完全相同。 1.1.2 函数名也是变量那么函数名是什么呢？函数名其实就是指向函数的变量！对于 abs()这个函数，完全可以把函数名 abs 看成变量，它指向一个可以计算绝对值的函数！ 如果把 abs指向其他对象，会有什么情况发生？ 12345&gt;&gt;&gt; abs = "test"&gt;&gt;&gt; abs(-10)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'str' object is not callable 把 abs 指向 10 后，就无法通过 abs(-10)调用该函数了！因为 abs 这个变量已经不指向求绝对值函数而是指向一个整数10！ 当然实际代码绝对不能这么写，这里是为了说明函数名也是变量。要恢复 abs函数，请重启 Python 交互环境。 1.1.3 传入函数既然变量可以指向函数，而函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。 一个最简单的高阶函数： 12def add(x, y, f): return f(x) + f(y) 当我们调用add(-5, 6, abs)时，参数x，y和f分别接收-5，6和abs，根据函数定义，我们可以推导计算过程为： 12&gt;&gt;&gt; add(-5,6,abs)11 1.2 map/reducePython 内建了 map() 和 reduce() 函数。 1.2.1 map()我们先看 map。map() 函数接收两个参数：一个是函数，一个是 Iterable。map 将传入的函数依次作用到序列的每个元素，并把结果作为新的 Iterator 返回。 举例说明，比如我们有一个函数 f(x)=x2，要把这个函数作用在一个 list [1, 2, 3, 4, 5, 6, 7, 8, 9] 上，就可以用 map() 实现如下： 我们来看一下 Python 实现： 123456&gt;&gt;&gt; def f(x):... return x * x...&gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; list(r)[1, 4, 9, 16, 25, 36, 49, 64, 81] map()传入的第一个参数是 f，即函数对象本身。由于结果 r 是一个 Iterator，Iterator 是惰性序列，因此通过 list() 函数让它把整个序列都计算出来并返回一个 list。 你可能会想，不需要 map()函数，写一个循环，也可以计算出结果： 的确可以，但是，从上面的循环代码，能一眼看明白 “把f(x)作用在list的每一个元素并把结果生成一个新的list” 吗？ 所以，map()作为高阶函数，事实上它把运算规则抽象了。 1.2.2 reduce()再看 reduce 的用法。reduce 把一个函数作用在一个序列 [x1, x2, x3, ...] 上，这个函数必须接收两个参数。reduce 把结果继续和序列的下一个元素做累积计算，其效果就是： reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4) 比方说对一个序列求和，就可以用 reduce 实现： 123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def add(x, y):... return x + y...&gt;&gt;&gt; reduce(add, [1, 3, 5, 7, 9])25 当然求和运算可以直接用 Python 内建函数 sum()，没必要动用 reduce。 但是如果要把序列 [1, 3, 5, 7, 9] 变换成整数13579，reduce 就可以派上用场： 123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fn(x, y):... return x * 10 + y...&gt;&gt;&gt; reduce(fn, [1, 3, 5, 7, 9])13579 这个例子本身没多大用处，但是，如果考虑到字符串 str也是一个序列，对上面的例子稍加改动，配合map()，我们就可以写出把 str 转换为 int 的函数： 12345678910from functools import reduceDIGITS = &#123;'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9&#125;def str2int(s): def fn(x, y): return x * 10 + y def char2num(s): return DIGITS[s] return reduce(fn, map(char2num, s)) 还可以用 lambda 函数进一步简化成： 123456789from functools import reduceDIGITS = &#123;'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9&#125;def char2num(s): return DIGITS[s]def str2int(s): return reduce(lambda x, y: x * 10 + y, map(char2num, s)) 也就是说，假设Python没有提供 int() 函数，你完全可以自己写一个把字符串转化为整数的函数，而且只需要几行代码！ 1.3 filterPython内建的 filter() 函数可用于过滤序列。 和 map() 类似，filter() 也接收一个函数和一个序列。和 map() 不同的是，filter() 把传入的函数依次作用于每个元素，然后根据返回值是 True 还是 False 决定保留还是丢弃该元素。 例如，在一个 list 中，删掉偶数，只保留奇数，可以这么写： 12345&gt;&gt;&gt; def is_odd(n):... return n % 2 == 1...&gt;&gt;&gt; list(filter(is_odd, [0,1,2,3,4,5,6,7,8,9]))[1, 3, 5, 7, 9] 把一个序列中的空字符串删掉，可以这么写： 12345&gt;&gt;&gt; def not_empty(s):... return s and s.strip()...&gt;&gt;&gt; list(filter(not_empty, ['A', '', 'B', None, 'C', ' ']))['A', 'B', 'C'] 可见用 filter()这个高阶函数，关键在于正确实现一个“筛选”函数。 注意到 filter() 函数返回的是一个 Iterator，也就是一个惰性序列，所以要强迫 filter() 完成计算结果，需要用 list() 函数获得所有结果并返回 list。 1.3.1 filter 计算素数计算素数的一个方法是埃氏筛法，它的算法理解起来非常简单： 首先，列出从2开始的所有自然数，构造一个序列： 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 取序列的第一个数2，它一定是素数，然后用2把序列的2的倍数筛掉： 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 取新序列的第一个数3，它一定是素数，然后用3把序列的3的倍数筛掉： 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 取新序列的第一个数5，然后用5把序列的5的倍数筛掉： 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, … 不断筛下去，就可以得到所有的素数。 用 Python 来实现这个算法，可以先构造一个从 3 开始的奇数序列（注意这是一个生成器，并且是一个无限序列）： 12345def _odd_iter(): n = 1 while True: n = n + 2 yield n 然后定义一个筛选函数： 12def _not_divisible(n): return lambda x: x % n &gt; 0 最后，定义一个生成器，不断返回下一个素数： 1234567def primes(): yield 2 it = _odd_iter() # 初始序列 while True: n = next(it) # 返回序列的第一个数 yield n it = filter(_not_divisible(n), it) # 构造新序列 这个生成器先返回第一个素数2，然后，利用 filter() 不断产生筛选后的新的序列。 由于 primes()也是一个无限序列，所以调用时需要设置一个退出循环的条件： 123456# 打印1000以内的素数:for n in primes(): if n &lt; 1000: print(n) else: break 1.4 sorted排序也是在程序中经常用到的算法。无论使用冒泡排序还是快速排序，排序的核心是比较两个元素的大小。 如果是数字，我们可以直接比较，但如果是字符串或者两个 dict 呢？直接比较数学上的大小是没有意义的，因此，比较的过程必须通过函数抽象出来。 1）list(int) 我们前面提到过的，Python内置的 sorted() 函数就可以对 list 进行排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21])[-21, -12, 5, 9, 36] 此外，sorted() 函数也是一个高阶函数，它还可以接收一个 key 函数来实现自定义的排序，例如按绝对值大小排序： 12&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)[5, 9, -12, -21, 36] key 指定的函数将作用于 list 的每一个元素上，并根据 key 函数返回的结果进行排序。对比原始的 list 和经过 key=abs 处理过的 list： 123list = [36, 5, -12, 9, -21]keys = [36, 5, 12, 9, 21] 然后 sorted()函数按照 keys 进行排序，并按照对应关系返回 list 相应的元素。 2）list(str) 我们再看一个字符串排序的例子： 12&gt;&gt;&gt; sorted(['bob', 'about', 'Zoo', 'Credit'])['Credit', 'Zoo', 'about', 'bob'] 默认情况下，对字符串排序，是按照ASCII的大小比较的，由于&#39;Z&#39; &lt; &#39;a&#39;，结果，大写字母Z会排在小写字母a的前面。 现在，我们提出排序应该忽略大小写，按照字母序排序。要实现这个算法，不必对现有代码大加改动，只要我们能用一个key函数把字符串映射为忽略大小写排序即可。忽略大小写来比较两个字符串，实际上就是先把字符串都变成大写（或者都变成小写），再比较。 这样，我们给 sorted 传入key函数，即可实现忽略大小写的排序： 12&gt;&gt;&gt; sorted(['bob', 'about', 'Zoo', 'Credit'], key=str.lower)['about', 'bob', 'Credit', 'Zoo'] 要进行反向排序，不必改动 key 函数，可以传入第三个参数 reverse=True： 12&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower, reverse=True)[&apos;Zoo&apos;, &apos;Credit&apos;, &apos;bob&apos;, &apos;about&apos;] 从上述例子可以看出，高阶函数的抽象能力是非常强大的，而且，核心代码可以保持得非常简洁。 用 sorted() 排序的关键在于实现一个映射函数。 2. 返回函数2.1 函数作为返回值高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回。由于变量可以指向函数，想想这也是合理的。 我们来实现一个不定长参数的求和。通常情况下，求和的函数是这样定义的： 12345def clac_sum(*args): sum = 0 for arg in args: sum += args return sum 但是，如果不需要立刻求和，而是在后面的代码中，根据需要再计算怎么办？可以不返回求和的结果，而是返回求和的函数： 12345678&gt;&gt;&gt; def lazy_sum(*args):... def calc_sum():... sum = 0... for arg in args:... sum += arg... return sum... return calc_sum... 当我们调用 lazy_sum() 时，返回的并不是求和结果，而是求和函数： 123&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt; 调用函数 f时，才真正计算求和的结果： 12&gt;&gt;&gt; f()25 2.1.1 闭包（Closure）在这个例子中，我们在函数 lazy_sum 中又定义了函数 sum，并且，内部函数 sum 可以引用外部函数lazy_sum的参数和局部变量，当 lazy_sum 返回函数 sum 时，相关参数和变量都保存在返回的函数中，这种称为“闭包（Closure）”的程序结构拥有极大的威力。 请再注意一点，当我们调用 lazy_sum() 时，每次调用都会返回一个新的函数，即使传入相同的参数： 1234&gt;&gt;&gt; f1 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f2 = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f1==f2False f1()和f2()的调用结果互不影响。可以看作每次执行函数调用都会新开辟一块内存空间。 我们注意到，返回的函数在其定义内部引用了局部变量args。所以，当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易。 1）闭包中引入循环变量 另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了 f() 才执行。我们来看一个例子： 12345678910def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) print(fs) return fsf1, f2, f3 = count() 在上面的例子中，每次循环，都创建了一个新的函数，然后，把创建的 3 个函数都返回了。 你可能认为调用f1()，f2()和f3()结果应该是1，4，9，但实际结果是： 123456&gt;&gt;&gt; f1()9&gt;&gt;&gt; f2()9&gt;&gt;&gt; f3()9 全部都是 9！原因就在于返回的函数引用了变量 i，但它并非立刻执行。等到 3 个函数都返回时，它们所引用的变量 i 已经变成了 3，因此最终结果为 9。 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变： 12345678910&gt;&gt;&gt; def count():... def f(j):... def g():... return j*j... return g... fs = []... for i in range(1, 4):... fs.append(f(i)) # f(i)立刻被执行，因此i的当前值被传入f()... return fs... 再看看结果： 1234567891011121314&gt;&gt;&gt; f1, f2, f3 = count()&gt;&gt;&gt; f1()1&gt;&gt;&gt; f2()4&gt;&gt;&gt; f3()9&gt;&gt;&gt; f4 = count()&gt;&gt;&gt; type(f4)&lt;class 'list'&gt;&gt;&gt;&gt; f4[&lt;function count.&lt;locals&gt;.f.&lt;locals&gt;.g at 0x000001D616AD79D8&gt;, &lt;function count.&lt;locals&gt;.f.&lt;locals&gt;.g at 0x000001D616AD7620&gt;, &lt;function count.&lt;locals&gt;.f.&lt;locals&gt;.g at 0x000001D616AD7BF8&gt;]&gt;&gt;&gt; f4[0]()1 缺点是代码较长，可利用 lambda 函数缩短代码。 3. 匿名函数当我们在传入函数时，有些时候，不需要显式地定义函数，直接传入匿名函数更方便。 在 Python 中，对匿名函数提供了有限支持。还是以 map() 函数为例，计算 f(x)=x2 时，除了定义一个 f(x) 的函数外，还可以直接传入匿名函数： 12&gt;&gt;&gt; list(map(lambda x: x * x,[1,2,3,4,5,6]))[1, 4, 9, 16, 25, 36] 可以发现，匿名函数 lambda x: x * x实际上就是： 12def f(x): return x * x 关键字 lambda表示匿名函数，冒号前面的 x表示函数参数。匿名函数有个限制，就是只能有一个表达式，不用写 return，返回值就是该表达式的结果。 用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数： 123&gt;&gt;&gt; f = lambda x: x * x&gt;&gt;&gt; f(5)25 同样，也可以把匿名函数作为返回值返回，比如： 12def build(x, y): return lambda: x * x + y * y 下面我们给出一个样例来看如何使用匿名函数： 1234def is_odd(n): return n % 2 == 1L = list(filter(is_odd, range(1, 20))) 等价于： 123&gt;&gt;&gt; L = list(filter(lambda n: n % 2 == 1, range(1, 20)))&gt;&gt;&gt; L[1, 3, 5, 7, 9, 11, 13, 15, 17, 19] 4. 装饰器前面我们提到过，函数也是一个对象，而且函数对象可以被赋值给变量，所以通过变量也能调用该函数。 123456&gt;&gt;&gt; def now():... print("2019-4-22")...&gt;&gt;&gt; f = now&gt;&gt;&gt; f()2019-4-22 在函数对象中，有一个 __name__属性，可以拿到函数的名字： 1234&gt;&gt;&gt; f.__name__'now'&gt;&gt;&gt; now.__name__'now' 现在，假设我们要增强 now() 函数的功能，比如在函数调用前后自动打印日志，但又不希望修改 now() 函数的定义，这种在代码运行期间动态增加功能的方式，称之为 “装饰器”（Decorator）。 4.1 Decorator 定义本质上，decorator 就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的 decorator，可以定义如下： 12345def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 观察上面的 log，因为它是一个 decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助 Python 的 @ 语法，把 decorator 置于函数的定义处： 123@logdef now(): print("2019-4-22") 调用 now() 函数，不仅会运行 now() 函数本身，还会在运行 now() 函数前打印一行日志： 123&gt;&gt;&gt; now()call now():2019-4-22 把 @log 放到 now() 函数的定义处，相当于执行了语句： 1now = log(now) 可以看出，由于 log()是一个 decorator，返回一个函数。所以，原来的 now() 函数仍然存在，只是现在同名的 now 变量指向了新的函数，于是调用 now() 将执行新函数，即在 log() 函数中返回的 wrapper() 函数。 4.1.1 带参 decorator我们知道，wrapper() 函数的参数定义是 (*args, **kw)，因此，wrapper() 函数可以接受任意参数的调用。在 wrapper() 函数内，首先打印日志，再紧接着调用原始函数。 如果 decorator 本身需要传入参数，那就需要编写一个返回 decorator 的高阶函数，写出来会更复杂。比如，要自定义 log 的文本： 1234567def log(text): def decorator(func): def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 这个 3 层嵌套的 decorator 用法如下： 123@log('execute')def now(): print('2019-04-22') 执行结果如下： 123&gt;&gt;&gt; now()execute now():2019-04-22 和两层嵌套的 decorator 相比，3 层嵌套的效果是这样的： 1&gt;&gt;&gt; now = log(&apos;execute&apos;)(now) 我们来剖析上面的语句，首先执行 log(&#39;execute&#39;)，返回的是 decorator 函数，再调用返回的函数，参数是 now函数，返回值最终是 wrapper 函数。 12&gt;&gt;&gt; now.__name__'wrapper' 4.1.2 完整装饰器出现上述情况也不意外，因为返回的那个 wrapper() 函数名字就是 &#39;wrapper&#39;。所以需要把原始函数的 __name__等属性复制到 wrapper() 函数中，否则有些依赖函数签名的代码执行就会出错。 不需要编写 wrapper.__name__ = func.__name__ 这样的代码，Python内置的 functools.wraps 就是干这个事的，所以，一个完整的 decorator 的写法如下： 12345678import functoolsdef log(func): @functools.wraps(func) def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 或者针对带参数的 decorator： 12345678910import functoolsdef log(text): def decorator(func): @functools.wraps(func) def wrapper(*args, **kw): print('%s %s():' % (text, func.__name__)) return func(*args, **kw) return wrapper return decorator 5. 偏函数Python 的 functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。要注意，这里的偏函数和数学意义上的偏函数不一样。 在介绍函数参数的时候，我们讲到，通过设定参数的默认值，可以降低函数调用的难度。而偏函数也可以做到这一点。举例如下： int()函数可以把数值型字符串转换为整数，当仅传入字符串时，int()函数默认按十进制转换： 12&gt;&gt;&gt; int('12345')12345 但 int() 函数还提供额外的 base 参数，默认值为 10。如果传入 base 参数，就可以做 N 进制的转换： 1234&gt;&gt;&gt; int('12345', base=8)5349&gt;&gt;&gt; int('12345', 16)74565 假设要转换大量的二进制字符串，每次都传入 int(x, base=2) 非常麻烦，于是，我们想到，可以定义一个 int2() 的函数，默认把 base=2 传进去： 12def int2(x, base=2): return int(x, base) 这样，我们转换二进制就非常方便了： 1234&gt;&gt;&gt; int2('1000000')64&gt;&gt;&gt; int2('1010101')85 functools.partial 就是帮助我们创建一个偏函数的，不需要我们自己定义 int2()，可以直接使用下面的代码创建一个新的函数 int2： 123456&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2('1000000')64&gt;&gt;&gt; int2('1010101')85 所以，简单总结 functools.partial 的作用就是，把一个函数的某些参数给固定住（也就是设置默认值），返回一个新的函数，调用这个新函数会更简单。 注意到上面的新的 int2 函数，仅仅是把 base 参数重新设定默认值为 2，但也可以在函数调用时传入其他值： 12&gt;&gt;&gt; int2('1000000', base=10)1000000 最后，创建偏函数时，实际上可以接收函数对象、*args 和 **kw 这 3 个参数，当传入： 1int2 = functools.partial(int, base=2) 实际上固定了 int() 函数的关键字参数 base，也就是： 1int2('10010') 相当于： 12kw = &#123; 'base': 2 &#125;int('10010', **kw) 当传入： 1max2 = functools.partial(max, 10) 实际上会把10作为 *args 的一部分自动加到左边，也就是： 1max2(5, 6, 7) 相当于： 12args = (10, 5, 6, 7)max(*args) 结果为10。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Functional Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 语法之高级特性]]></title>
    <url>%2FPython%2FPython-%E8%AF%AD%E6%B3%95%E4%B9%8B%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 前面我们已经学过了 Python 的基本数据类型、语句和函数，基本上就可以编写出很多有用的程序了。但是在 Python 中，代码不是越多越好，而是越少越好。代码不是越复杂越好，而是越简单越好。 基于这一思想，我们来介绍 Python 中非常有用的高级特性，1 行代码能实现的功能，决不写 5 行代码。请始终牢记，代码越少，开发效率越高。 Python 语法之高级特性在高级语法特性中，我们将分别介绍 Python 中的切片、迭代、列表生成式、生成器以及迭代器，用于提升开发效率。 1. 切片（Slice）取一个 list 或 tuple 的部分元素是非常常见的操作，我们在介绍 list 或 tuple 数据类型时也介绍了各自切片的简单用法。切片是序列对象最基础的操作，这一小节，我们详细来看切片的不同用法。 1.1 List比如，一个 list 如下： 1&gt;&gt;&gt; list1 = ["Google", "Baidu", "360", "Opera", "Sogo"] 如果我们想获取到前 3 个元素，应该怎么做？下面给出使用索引进行获取的方式（笨办法）： 12&gt;&gt;&gt; [list1[0], list1[1], list1[2]]['Google', 'Baidu', '360'] 之所以是笨办法是因为扩展一下，取前 N 个元素就没辙了。 取前 N 个元素，也就是索引为 0~(N-1) 的元素，一般可以想到用循环： 1234567&gt;&gt;&gt; result = []&gt;&gt;&gt; n = 3&gt;&gt;&gt; for index in range(n):... result.append(list1[index])...&gt;&gt;&gt; result['Google', 'Baidu', '360'] 这时，Python 提供了切片（Slice）操作符，能大大简化这种操作。 对于上面的问题，取前 3 个元素，用一行代码就可以完成切片： 12&gt;&gt;&gt; list1[0:3]['Google', 'Baidu', '360'] 如果第一个索引是0，还可以省略： 12&gt;&gt;&gt; list1[:3]['Google', 'Baidu', '360'] 类似的，既然Python支持 list1[-1] 取倒数第一个元素，那么它同样支持倒数切片，试试： 12&gt;&gt;&gt; list1[-2:]['Opera', 'Sogo'] 注意，对于切片来说是 前闭后开 的： 123456789101112&gt;&gt;&gt; list2 = list(range(100))# 取前 10 个数&gt;&gt;&gt; list2[:10][0, 1, 2, 3, 4, 5, 6, 7, 8, 9]# 取后 10 个数&gt;&gt;&gt; list2[-10:][90, 91, 92, 93, 94, 95, 96, 97, 98, 99]# 前 11-20 个数：&gt;&gt;&gt; list2[10:20][10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 我们之前说过，切片还支持设置步长： 123456# 前 10 个数，每两个取一个：&gt;&gt;&gt; list2[:10:2][0, 2, 4, 6, 8]# 所有数，每 5 个取一个：&gt;&gt;&gt; list2[::5][0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] 甚至什么都不写，只写 [:] 就可以原样复制一个 list： 123456&gt;&gt;&gt; list1 = ["Google", "Baidu", "360", "Opera", "Sogo"]&gt;&gt;&gt; L = list1[:]&gt;&gt;&gt; id(list1)2395406623432&gt;&gt;&gt; id(L)2395407112584 1.2 Tuple &amp;&amp; String我们知道元组（tuple）和字符串（str）也是序列的一种，上述的切片操作也应该同样适用于 tuple 和 str。 1）Tuple tuple 也是一种 list，唯一区别是 tuple 不可变。因此，tuple 也可以用切片操作，只是操作的结果仍是 tuple： 12&gt;&gt;&gt; (0, 1, 2, 3, 4, 5)[:3](0, 1, 2) 2）String 字符串 &#39;xxx&#39; 也可以看成是一种 list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串： 1234&gt;&gt;&gt; 'ABCDEFG'[:3]'ABC'&gt;&gt;&gt; 'ABCDEFG'[::2]'ACEG' 在很多编程语言中，针对字符串提供了很多各种截取函数（例如：substring ），其实目的就是对字符串切片。Python 没有针对字符串的截取函数，只需要切片一个操作就可以完成，非常简单。 2. 迭代（Iteration）我们知道，如果给定一个 list 或 tuple，我们可以通过 for 循环来遍历这个 list 或 tuple，这种遍历我们称为迭代（Iteration）。 123456&gt;&gt;&gt; for element in ["Google", "Baidu", "Opera"]:... print(element)...GoogleBaiduOpera 2.1 for … in我们知道，Python for 循环可以遍历任何序列的项目，如列表、字符串、元组等。 Python 中，迭代是通过 for ... in 来完成的，而很多语言比如 C 语言，迭代 list 是通过下标完成的，比如 Java 代码： 123for (i=0; i&lt;list.length; i++) &#123; n = list[i];&#125; 2.1.1 可迭代对象可以看出，Python 的 for 循环抽象程度要高于 C 的 for 循环，这是因为 Python 的 for 循环不仅可以用在 list 或 tuple 等 序列 上，还可以作用在其他 可迭代对象 上。 1）Dictionary list 这种数据类型虽然有下标，但很多其他数据类型是没有下标的。但只要是可迭代对象，无论有无下标，都可以迭代，比如 dict 就可以迭代： 1234567&gt;&gt;&gt; adict = &#123;"Google":1, "Opera":2, "Baidu":3&#125;&gt;&gt;&gt; for key in adict:... print(key)...GoogleOperaBaidu 由于 dict 的存储不是按照 list 的方式顺序排列（无序集合），所以迭代出的结果顺序很可能不一样。 通过前面字典的学习，我们知道，默认情况下，dict 迭代的是 key。如果要迭代 value，可以用for value in d.values()；如果要同时迭代 key 和 value，可以用for k, v in d.items()。 2）String 由于字符串既是一个序列，也是可迭代对象。因此，也可以作用于 for 循环： 123456&gt;&gt;&gt; for ch in 'ABC':... print(ch)...ABC 所以，当我们使用 for 循环时，只要作用于一个可迭代对象，for 循环就可以正常运行，而我们不太关心该对象究竟是 list 还是其他数据类型。 那么，如何判断一个对象是可迭代对象呢？方法是通过 collections.abc 模块的 Iterable 类型判断： 123456789101112131415161718192021222324&gt;&gt;&gt; from collections.abc import Iterable# dict 是否为可迭代对象&gt;&gt;&gt; isinstance(&#123;"G":1,"B":2&#125;,Iterable)# list 是否为可迭代对象&gt;&gt;&gt; isinstance([1,2,3],Iterable)True# tuple 是否为可迭代对象&gt;&gt;&gt; isinstance((1,2,3),Iterable)True# set 是否为可迭代对象&gt;&gt;&gt; isinstance(&#123;1,2,3&#125;,Iterable)True# str 是否为可迭代对象&gt;&gt;&gt; isinstance("Google",Iterable)True# int 是否为可迭代对象&gt;&gt;&gt; isinstance(123456,Iterable)False 2.1.2 下标循环如果要对 list 或 tuple 实现类似 Java 那样的下标循环怎么办？Python 内置的 enumerate 函数可以把一个 list （tuple）变成 index-element（索引-元素）对，这样就可以在for循环中同时迭代索引和元素本身： 123456&gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']):... print(i, value)...0 A1 B2 C 上面的 for 循环里，同时引用了两个变量，在 Python 里是很常见的，比如下面的代码： 123456&gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print(x, y)...1 12 43 9 通过查看 enumerate 帮助，我们可以发现，enumerate 接收的是可迭代对象。这也就是说，任何可迭代对象都可以使用 enumerate 方法。 3. 列表生成式（List Comprehensions）列表生成式即 List Comprehensions，是 Python 内置的非常简单却强大的可以用来创建 list 的生成式。这里的生成式也就是我们在介绍数据类型时提过的推导式。 3.1 [ [表达式] for … in ]举个例子，要生成 list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 可以用 list(range(1, 11))： 12&gt;&gt;&gt; list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 但如果要生成 [1x1, 2x2, 3x3, ..., 10x10] 怎么做？方法一是循环： 123456&gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)...&gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的 list： 12&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把 list 创建出来，十分有用。 1） [ [表达式] for … in … if …] for 循环后面还可以加上 if 判断，这样我们就可以筛选出仅偶数的平方： 12&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： 12&gt;&gt;&gt; [m + n for m in 'ABC' for n in 'XYZ']['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'] 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现： 123&gt;&gt;&gt; import os&gt;&gt;&gt; [d for d in os.listdir(".")]['.ipynb_checkpoints', '360安全卫士.lnk', 'Microsoft Edge.lnk', 'Opera 浏览器.lnk', 'Python', 'WPS 2019.lnk', '百度网盘.lnk', '谷歌浏览器.lnk'] 4. 生成器我们知道，通过列表生成式，我们可以直接创建一个列表。 但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含 100 万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的 list，从而节省大量的空间。在 Python 中，这种一边循环一边计算的机制，称为生成器：generator。 4.1 生成器创建要创建一个generator，有很多种方法：[] --&gt;、yield。 4.1.1 [] --&gt;一个很简单的方法就是，把一个列表生成式的 [] 改成 ()，就创建了一个 generator： 123456&gt;&gt;&gt; list1 = [x for x in range(10)]&gt;&gt;&gt; list1[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; gen = (x for x in range(10))&gt;&gt;&gt; gen&lt;generator object &lt;genexpr&gt; at 0x0000013E74760DE0&gt; 创建 list1 和 gen 的区别仅在于最外层的[]和()，list1 是一个list，而 gen 是一个 generator。 1）next() 我们可以直接打印出 list 的每一个元素，但我们怎么打印出 generator 的每一个元素呢？ 如果要一个一个打印出来，可以通过 next() 函数获得 generator 的下一个返回值： 123456789101112131415161718192021222324&gt;&gt;&gt; next(gen)0&gt;&gt;&gt; next(gen)1&gt;&gt;&gt; next(gen)2&gt;&gt;&gt; next(gen)3&gt;&gt;&gt; next(gen)4&gt;&gt;&gt; next(gen)5&gt;&gt;&gt; next(gen)6&gt;&gt;&gt; next(gen)7&gt;&gt;&gt; next(gen)8&gt;&gt;&gt; next(gen)9&gt;&gt;&gt; next(gen)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 我们讲过，generator 保存的是算法，每次调用 next(gen)，就计算出 gen 的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出 StopIteration 的错误。 2）for … in 当然，上面这种不断调用next(gen)实在是太变态了，正确的方法是使用 for 循环，因为 generator 也是 可迭代对象： 1234567891011121314&gt;&gt;&gt; gen = (x for x in range(10))&gt;&gt;&gt; for element in gen:... print(element)...0123456789 所以，我们创建了一个 generator 后，基本上永远不会调用 next()，而是通过 for 循环来迭代它，并且不需要关心 StopIteration 的错误。 4.1.2 yield如果推算的算法比较复杂，用类似列表生成式的 for 循环无法实现的时候，还可以用函数来实现。 比如，著名的斐波拉契数列（Fibonacci），除第一个和第二个数外，任意一个数都可由前两个数相加得到： 1, 1, 2, 3, 5, 8, 13, 21, 34, ... 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： 12345678910111213141516&gt;&gt;&gt; def fib(max):... index = 0... a,b = 0,1... while index &lt; max:... print(b)... a,b = b, a + b... index = index + 1... return "done"...# 上面的函数可以输出斐波那契数列的前 N 个数：&gt;&gt;&gt; fib(3)112'done' 仔细观察，可以看出，fib 函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似 generator。 也就是说，上面的函数和generator仅一步之遥。要把 fib函数变成 generator，只需要把 print(b) 改为 yield b 就可以了： 1234567891011&gt;&gt;&gt; def fib(max):... index = 0... a,b = 0,1... while index &lt; max:... yield b... a,b = b, a + b... index = index + 1... return "done"...&gt;&gt;&gt; fib(6)&lt;generator object fib at 0x0000013E74760DE0&gt; 这就是定义 generator 的另一种方法。如果一个函数定义中包含 yield 关键字，那么这个函数就不再是一个普通函数，而是一个 generator。 1）执行方式 这里，最难理解的就是 generator 和函数的执行流程不一样。函数是顺序执行，遇到 return 语句或者最后一行函数语句就返回。而变成 generator 的函数，在每次调用 next() 的时候执行，遇到 yield 语句返回，再次执行时从上次返回的 yield 语句处继续执行。 举个简单的例子，定义一个 generator，依次返回数字1，3，5： 1234567def odd(): print('step 1') yield 1 print('step 2') yield(3) print('step 3') yield(5) 调用该 generator 时，首先要生成一个 generator 对象，然后用 next() 函数不断获得下一个返回值： 1234567891011121314&gt;&gt;&gt; o = odd()&gt;&gt;&gt; next(o)step 11&gt;&gt;&gt; next(o)step 23&gt;&gt;&gt; next(o)step 35&gt;&gt;&gt; next(o)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 可以看到，odd 不是普通函数，而是 generator，在执行过程中，遇到 yield 就中断，下次又继续执行。执行3次 yield 后，已经没有 yield 可以执行了，所以，第4次调用 next(o) 就报错。 同样的，把函数改成 generator 后，我们基本上从来不会用 next() 来获取下一个返回值，而是直接使用 for 循环来迭代： 123456789&gt;&gt;&gt; for i in fib(6):... print(i)...112358 但是用 for 循环调用 generator 时，发现拿不到 generator 的 return 语句的返回值。如果想要拿到返回值，必须捕获 StopIteration 错误，返回值包含在 StopIteration 的 value 中： 12345678910111213141516&gt;&gt;&gt; g = fib(6)&gt;&gt;&gt; while True:... try:... x = next(g)... print('g:', x)... except StopIteration as e:... print('Generator return value:', e.value)... break...g: 1g: 1g: 2g: 3g: 5g: 8Generator return value: done 5. 迭代器我们已经知道，可以直接作用于 for 循环的数据类型有以下几种： 一类是集合（或序列）数据类型，如list、tuple、dict、set、str等； 一类是 generator，包括生成器和带 yield 的 generator function。 这些可以直接作用于 for 循环的对象统称为可迭代对象：Iterable。 前面我们已经知道，可以使用 isinstance() 判断一个对象是否是 Iterable 对象： 1234567891011&gt;&gt;&gt; from collections.abc import Iterable&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance(&#123;&#125;, Iterable)True&gt;&gt;&gt; isinstance('abc', Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)True&gt;&gt;&gt; isinstance(100, Iterable)False 而生成器不但可以作用于 for 循环，还可以被 next() 函数不断调用并返回下一个值，直到最后抛出 StopIteration 错误表示无法继续返回下一个值了。 5.1 何为迭代器？1）next() 可以被 next() 函数调用并不断返回下一个值的对象称为迭代器：Iterator。 迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。 如何判断一个对象是否属于 Iterator： 123456789&gt;&gt;&gt; from collections.abc import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance('abc', Iterator)False 所以，我们知道生成器都是 Iterator 对象。但 list、dict、str虽然是 Iterable，却不是 Iterator（不存在 next() 函数）。 注意，Python 的 for 循环本质上就是通过不断调用 next() 函数实现的，例如： 1234567891011for x in [1, 2, 3, 4, 5]: pass# 等价于：it = iter([1, 2, 3, 4, 5])while True: try: x = next(it) except StopIteration: # 遇到 StopIteration 就退出循环 break 2）iter() 但是 Python 提供了一个 iter() 函数，可以将 list、dict、str 等 Iterable 变为 Iterator，从而创建迭代器： 123456&gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter('abc'), Iterator)True&gt;&gt;&gt; isinstance(iter(&#123;&#125;), Iterator)True 5.2 Iterator 数据流你可能会问，为什么list、dict、str等数据类型不是Iterator？ 这是因为 Python 的 Iterator 对象表示的是一个数据流，Iterator 对象可以被 next() 函数调用并不断返回下一个数据，直到没有数据时抛出 StopIteration 错误。 惰性的计算序列： 可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。 Iterator 甚至可以表示一个无限大的数据流，例如全体自然数。而使用 list 是永远不可能存储全体自然数的。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Slice</tag>
        <tag>Iteration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的函数]]></title>
    <url>%2FPython%2FPython-%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 程序语言中的，函数能提高应用的模块性，和代码的重复利用率。这一章节我们详细来看 Python 中函数的使用。 Python 中的函数前面在介绍 Python 基本数据结构时，我们已经知道 Python 提供了许多内建函数。当然，根据我们的任务需要快速定制我们自己的函数（自定义函数）也是必要的。 那么，Python 中如何定义一个函数呢？ 1. 定义函数当我们需要定制一个目标功能的函数时，需要依据以下规则： 函数定义以def 关键词开头，后接 函数标识符名称 、圆括号 () 和冒号:，表示函数头；函数内容以冒号 : 起始，使用缩进，表示函数体； 圆括号 () 之间可以用于定义参数：任何传入参数和自变量必须放在圆括号中间； 函数的第一行语句可以选择性地使用文档字符串用于函数功能说明，非必须； return [表达式]：函数一旦执行到return时，就执行完毕，并将表达式结果返回。如果没有return语句或不带表达式的 return 结果相都当于返回 None。 语法格式如下： 12def 函数名 (参数列表): 函数体 我们来看一个面积函数定义实例： 12345&gt;&gt;&gt; def area(width, height):... print("width: %d, height: %d" % (width, height))... return width * height...&gt;&gt;&gt; 有时想定义一个什么事也不做的空函数，或还没想具体的逻辑实现，可以用我们前面提到过的pass语句： 12def nop(): pass pass 语句什么都不做，那有什么用？事实上，pass语句可以保证代码结构完整性，让代码能运行起来（如果不加会报错）： 1234567&gt;&gt;&gt; def nop():...... File &quot;&lt;stdin&gt;&quot;, line 3 ^IndentationError: expected an indented block 1.1 函数调用不管是 Python 中提供的内建函数，亦或是上面我们自定义的函数，都是为了开发人员调用使用的，调用方法都一样。要想调用一个函数，需要知道函数的名称和参数。 对于 Python 支持的内建函数，可以直接从 Python 的官方网站查看更详细的文档说明： https://docs.python.org/zh-cn/3.7/library/functions.html 下面我们将以上面自定义的面积函数来说明 Python 中的函数调用： 1234567891011&gt;&gt;&gt; graph_width = 10&gt;&gt;&gt; graph_height = 5&gt;&gt;&gt; graph_area = area(graph_width, graph_height)width: 10, height: 5&gt;&gt;&gt; print(&quot;The area of graph is &quot;,graph_area)The area of graph is 50&gt;&gt;&gt; area1 = area(10,50)width: 10, height: 50&gt;&gt;&gt; area1500 调用函数时，默认情况下，传入参数值（实参）和参数名称（形参）是按函数声明中定义的顺序匹配起来的。 函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个“别名”： 1234&gt;&gt;&gt; computer_area = area&gt;&gt;&gt; computer_area(2,3)width: 2, height: 36 1.1.1 参数检查1）参数个数检查 调用函数时，如果参数个数不对，Python 解释器会自动检查出来，并抛出TypeError： 1234&gt;&gt;&gt; area(2,3,5)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: area() takes 2 positional arguments but 3 were given 2）参数类型检查 但是如果参数类型不对，Python 解释器就无法帮我们检查。试试 area() 函数和 Python 内置的函数 abs 的差别： 123456789&gt;&gt;&gt; area("Google", 12)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 2, in areaTypeError: %d format: a number is required, not str&gt;&gt;&gt; abs("A")Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: bad operand type for abs(): 'str' 当传入了不恰当的参数时，内置函数 abs 会自动检查出参数错误，而我们定义的 area() 没有进行参数检查，会导致出错信息和 abs 不一样。所以，这个函数定义不够完善。 故在我们在定义函数时对参数类型做检查，只允许整数的参数。数据类型检查可以用内置函数 isinstance() 实现： 1234567&gt;&gt;&gt; def area(width, height):... if not isinstance(width, int) or not isinstance(height,int):... raise TypeError("bad operand type for area(int，int)")... print("width: %d, height: %d" % (width, height))... return width * height...&gt;&gt;&gt; 添加了参数检查后，如果传入错误的参数类型，函数就可以抛出一个错误： 12345&gt;&gt;&gt; area(12.0,10)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 3, in areaTypeError: bad operand type for area(int，int) 关于错误和异常处理将在后续讲到，这里不用深究。 1.1.2 参数传递这一部分我们来看函数调用时的参数传递。在学习基本数据类型时我们提过，Python 中，变量是没有类型的，类型属于对象（变量所指向的内存数据）。例如： 12test = [1,2,3]test = "HelloPython" 以上代码中，[1,2,3] 是 List 类型，&quot;HelloPython&quot; 是 String 类型，而变量 test 是没有类型，她仅仅是一个对象的引用（一个指针），可以是指向 List 类型对象，也可以是指向 String 类型对象。 1）可变与不可变对象 我们知道，Python中数据类型分为可变类型（list、dict、set）和不可变类型（number、str、tuple）。 不可变类型：*变量赋值 a=5 后再赋值 a=10，这里实际是新生成一个 int 值对象 10，再让 a 指向它，而 5 被丢弃，不是改变 a 的值，相当于新生成了 a。 可变类型：变量赋值 alist=[1,2,3,4] 后再赋值 alist[2]=5 则是将 list alist 的第三个元素值更改，本身 alist 没有动，只是其内部的一部分值被修改了。 2）Python 函数的参数传递： 对于不可变类型参数：类似 c++ 的值传递，如 整数、字符串、元组。如 fun（a），传递的只是 a 的值，没有影响 a 对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象（一个浅拷贝），不会影响 a 本身。 对于可变类型参数参数 :类似 c++ 的引用传递，如 列表，字典。如 fun（alist），则是将 alist 真正的传过去，修改后 fun 外部的 alist 也会受影响。 3）测试 看下面实例进行区别印证： Python 传递不可变对象实例 1234567&gt;&gt;&gt; def ChangeInt(test):... test = 10...&gt;&gt;&gt; test = 2&gt;&gt;&gt; ChangeInt(test)&gt;&gt;&gt; print(test)2 Python 传递可变对象实例 对于可变对象在函数里修改了参数，那么在调用这个函数的函数里，原始的参数也被改变了。例如： 1234567891011&gt;&gt;&gt; def changeme( mylist ):... "修改传入的列表"... mylist.append([1,2,3,4])... print ("函数内取值: ", mylist)... return...&gt;&gt;&gt; mylist = [10,20,30]&gt;&gt;&gt; changeme( mylist )函数内取值: [10, 20, 30, [1, 2, 3, 4]]&gt;&gt;&gt; print ("函数外取值: ", mylist)函数外取值: [10, 20, 30, [1, 2, 3, 4]] 1.2 return [表达式]前面我们知道，当程序执行到 return [表达式] 时标志函数执行完成，并将表达式结果进行返回。如果函数中没有 return [表达式] 语句或者 return 语句（无表达式），则获得的是一个 None。 思考，函数可以返回多个值吗？ 答案是肯定的。 比如在游戏中经常需要从一个点移动到另一个点，给出坐标、位移，就可以计算出新的新的坐标： 12345&gt;&gt;&gt; def move(start_x, start_y, step):... end_x = start_x + step... end_y = start_y + step... return end_x, end_y... 然后，我们就可以同时获得两个返回值： 12&gt;&gt;&gt; print(current_x,current_y)12 14 但其实这只是一种假象，Python 函数返回的仍然是单一值： 12345&gt;&gt;&gt; current_location = move(2,4,10)&gt;&gt;&gt; print(current_location)(12, -6)&gt;&gt;&gt; type(current_location)&lt;class 'tuple'&gt; 我们发现，原来 return 的返回值是一个 tuple！但是，在语法上，返回一个可以省略括号的 tuple ，而多个变量可以同时接收一个 tuple，按位置赋给对应的值。 2. 函数中的参数Python 的函数定义非常简单，但灵活度却非常大。除了正常定义的 必选参数 外，还可以使用默认参数、不定长参数和 关键字参数，使得函数定义出来的接口，不但能处理复杂的参数，还可以简化调用者的代码。 2.1 必选参数（位置参数）必需参数须以正确的顺序（位置）传入函数。调用时的数量必须和声明时的一样。就像我们前面定义的 move 函数： 12345&gt;&gt;&gt; def move(start_x, start_y, step):... end_x = start_x + step... end_y = start_y + step... return end_x, end_y... move(start_x, start_y, step) 函数中有三个参数：start_x、start_y 以及 step，这三个参数都是位置参数。调用函数时，传入的三个值按照位置顺序依次赋给参数start_x、start_y以及step`。 不知道你有没有这种感觉，按顺序给参数传入数值总是刚觉不太靠谱，一旦疏忽搞错怎么办？事实上，Python 解释器能够用参数名匹配参数值： 12&gt;&gt;&gt; move(start_x=2, start_y=3, step=2)(4, 5) 2.2 默认参数调用函数时，如果没有传递参数，则会使用默认参数。以下实例中如果没有传入 step 参数，则使用默认值： 12345&gt;&gt;&gt; def move(start_x, start_y, step=1):... end_x = start_x + step... end_y = start_y + step... return end_x, end_y... 这样，当我们调用 move(2, 3) 时，相当于调用了 move(2, 3，1)。 12&gt;&gt;&gt; move(2,3)(3, 4) 而对于其他情况，就必须明确地传入 step，比如power(5, 3，3)。 12&gt;&gt;&gt; move(5,3,3)(8, 6) 从上面的例子可以看出，默认参数可以简化函数的调用。设置默认参数时，有几点要注意： 必选参数在前，默认参数在后，否则Python的解释器会报错（思考一下为什么默认参数不能放在必选参数前面）； 如何设置默认参数：当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。 我们来看一个样例，写个一年级小学生注册的函数，需要传入name和gender两个参数： 123def enroll(name, gender): print('name:', name) print('gender:', gender) 这样，调用 enroll() 函数只需要传入两个参数： 123&gt;&gt;&gt; enroll('Sarah', 'F')name: Sarahgender: F 如果要继续传入年龄、城市等信息怎么办？这样会使得调用函数的复杂度大大增加。此时，我们可以把年龄和城市设为默认参数： 12345def enroll(name, gender, age=8, city='Beijing'): print('name:', name) print('gender:', gender) print('age:', age) print('city:', city) 这样，大多数学生注册时不需要提供年龄和城市，只提供必须的两个参数： 12345&gt;&gt;&gt; enroll('Sarah', 'F')name: Sarahgender: Fage: 8city: Beijing 只有与默认参数不符的学生才需要提供额外的信息： 12enroll('Bob', 'M', 7)enroll('Adam', 'M', city='Tianjin') 可见，默认参数降低了函数调用的难度，而一旦需要更复杂的调用时，又可以传递更多的参数来实现。无论是简单调用还是复杂调用，函数只需要定义一个。 有多个默认参数时，调用的时候，既可以按顺序提供默认参数，比如调用enroll(&#39;Bob&#39;, &#39;M&#39;, 7)，意思是，除了name，gender这两个参数外，最后1个参数应用在参数age上，city参数由于没有提供，仍然使用默认值。 也可以不按顺序提供部分默认参数。当不按顺序提供部分默认参数时，需要把参数名写上。比如调用enroll(&#39;Adam&#39;, &#39;M&#39;, city=&#39;Tianjin&#39;)，意思是，city参数用传进去的值，其他默认参数继续使用默认值。 1）默认参数必须指向不变对象 我们知道，默认参数很有用。但使用不当，也会掉坑里。默认参数有个最大的坑，演示如下： 先定义一个函数，传入一个 list，添加一个END再返回： 123def add_end(L=[]): L.append('END') return L 当你正常调用时，结果似乎不错： 1234&gt;&gt;&gt; add_end([1, 2, 3])[1, 2, 3, 'END']&gt;&gt;&gt; add_end(['x', 'y', 'z'])['x', 'y', 'z', 'END'] 当你使用默认参数调用时，一开始结果也是对的： 12&gt;&gt;&gt; add_end()['END'] 但是，再次调用add_end()时，结果就不对了： 1234&gt;&gt;&gt; add_end()[&apos;END&apos;, &apos;END&apos;]&gt;&gt;&gt; add_end()[&apos;END&apos;, &apos;END&apos;, &apos;END&apos;] 很多初学者很疑惑，默认参数是[]，但是函数似乎每次都“记住了”上次添加了&#39;END&#39;后的 list。 原因解释：还记得我们前面在参数传递中说过的可变类型对象的传递么？原因正在这里。 定义默认参数要牢记一点：默认参数必须指向不变对象！ 要修改上面的例子，我们可以用 None 这个不变对象来实现： 12345def add_end(L=None): if L is None: L = [] L.append('END') return L 现在，无论调用多少次，都不会有问题： 1234&gt;&gt;&gt; add_end()['END']&gt;&gt;&gt; add_end()['END'] 2.3 不定长参数有些时候，你可能需要一个函数能处理比当初声明时更多的参数。在 Python 函数中，还可以定义不定长参数（可变参数）。顾名思义，不定长参数就是传入的参数个数是可变的，可以是 1 个、2 个到任意个，当然还可以是 0 个。 我们以一个计算 a2 + b2 + c2 + ……。为例的样例来说明： 要定义出这个函数，我们必须确定输入的参数。由于参数个数不确定，我们首先想到可以把 a，b，c…… 作为一个list 或 tuple 传进来，这样，函数可以定义如下： 12345def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 但是调用的时候，需要先组装出一个 list 或 tuple： 1234&gt;&gt;&gt; calc([1, 2, 3])14&gt;&gt;&gt; calc((1, 3, 5, 7))84 如果利用可变参数，调用函数的方式可以简化成这样： 1234&gt;&gt;&gt; calc(1, 2, 3)14&gt;&gt;&gt; calc(1, 3, 5, 7)84 2.3.1 *[args]这一小节，我们来引入不定长参数来解决上述问题。加了星号 * 的参数会以元组（tuple）的形式导入，存放所有未命名的变量参数。 123456def calc(arg1, *numbers): sum = 0 for n in numbers: sum = sum + n * n print(arg1) return sum 定义]不定长参数和上面定义一个 list 或 tuple 参数相比，仅仅在参数前面加了一个*号。但在函数内部，参数numbers接收到的是一个 tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括 0 个参数： 123456&gt;&gt;&gt; calc("The result is ", 1, 2, 3, 4)The result is30&gt;&gt;&gt; calc("The result is ")The result is0 如果已经有一个 list 或者 tuple，要调用一个可变参数怎么办？可以这样做： 1234&gt;&gt;&gt; operator = [1,2,3,4,5,6,7,8]&gt;&gt;&gt; calc("The result is ", *operator)The result is204 是不是很方便！！！*operator 表示把operator这个 list（tuple） 的所有元素作为可变参数传进去。这种写法相当有用，而且很常见。 2.4 关键字参数不定长参数允许你传入 0 个或任意个参数，这些可变参数在函数调用时自动组装为一个 tuple。而关键字参数允许你传入 0 个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个 dict。请看示例： 1234&gt;&gt;&gt; def person(name, age, **vardict):... print('name:', name, 'age:', age, 'other:', vardict)...&gt;&gt;&gt; 函数 person 除了必选参数 name 和 age 外，还接受关键字参数 vardict。在调用该函数时，可以只传入必选参数： 12&gt;&gt;&gt; person("Opera",35)name: Opera age: 35 other: &#123;&#125; 也可以传入任意个数的关键字参数： 12&gt;&gt;&gt; person("Google",23,city="Beijing",job="IT")name: Google age: 23 other: &#123;'city': 'Beijing', 'job': 'IT'&#125; 2.4.1 关键字参数作用关键字参数最大的作用就是可以我们扩展函数的功能。比如，在 person 函数里，我们保证能接收到 name 和 age 这两个参数。但是，如果调用者愿意提供更多的参数，我们也能收到。试想你正在做一个用户注册的功能，除了用户名和年龄是必填项外，其他都是可选项，利用关键字参数来定义这个函数就能满足注册的需求。 和不定长参数类似，我们也可以先组装出一个 dict，然后，把该 dict 转换为关键字参数传进去： 123&gt;&gt;&gt; extra = &#123;'city': 'Beijing', 'job': 'Engineer'&#125;&gt;&gt;&gt; person('Jack', 24, **extra)name: Jack age: 24 other: &#123;'city': 'Beijing', 'job': 'Engineer'&#125; **extra 表示把extra这个 dict 的所有 key-value 用关键字参数传入到函数的 **vardict 参数，vardict 将获得一个 dict，注意 vardict 获得的 dict 是 extra 的一份拷贝，对 vardict 的改动不会影响到函数外的extra。 2.4.2 命名关键字参数对于上面说到的关键字参数，函数的调用者可以传入任意不受限制的关键字参数。如果要限制关键字参数的名字，就可以用命名关键字参数。 例如，对于上面的 person 只接收 city 和 job 作为关键字参数。这种方式定义的函数如下： 12def person(name, age, *, city, job): print(name, age, city, job) 和关键字参数 **vardict 不同，命名关键字参数需要一个特殊分隔符 *，* 后面的参数被视为命名关键字参数（进行了限制的参数命名）。 1234567&gt;&gt;&gt; person("Google",23,city="Beijing",job="IT")Google 23 Beijing IT&gt;&gt;&gt; person("Google",23,"Beijing","IT")Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: person() takes 2 positional arguments but 4 were given 如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了： 12def person(name, age, *args, city, job): print(name, age, args, city, job) 命名关键字参数必须传入参数名，这和位置参数不同。如果没有传入参数名，调用将报错： 12345678# 由于调用时缺少参数名 city 和 job，Python 解释器把这 4 个参数均视为位置参数，但 person() 函数仅接受 2 个位置参数，故报错。&gt;&gt;&gt; person("Google",23,"Beijing","IT")Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: person() missing 2 required keyword-only arguments: 'city' and 'job'&gt;&gt;&gt; person("Google",23,"Beijing","IT",city="B",job="IT")Google 23 ('Beijing', 'IT') B IT 同样，命名关键字参数也可以有缺省值，从而简化调用： 12345&gt;&gt;&gt; def person(name, age, *, city="Dalian", job):... print(name, age, city, job)...&gt;&gt;&gt; person("Google", 32, job="IT")Google 32 Dalian IT 2.5 参数组合在 Python 中定义函数，可以用必选参数、默认参数、不定长参数、关键字参数和命名关键字参数，并且这 5 种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 比如定义一个函数，包含上述若干种参数： 12345def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw) 在函数调用的时候，Python 解释器会自动按照参数位置和参数名把对应的参数传进去： 12345678910&gt;&gt;&gt; f1(1, 2)a = 1 b = 2 c = 0 args = () kw = &#123;&#125;&gt;&gt;&gt; f1(1, 2, c=3)a = 1 b = 2 c = 3 args = () kw = &#123;&#125;&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b')a = 1 b = 2 c = 3 args = ('a', 'b') kw = &#123;&#125;&gt;&gt;&gt; f1(1, 2, 3, 'a', 'b', x=99)a = 1 b = 2 c = 3 args = ('a', 'b') kw = &#123;'x': 99&#125;&gt;&gt;&gt; f2(1, 2, d=99, ext=None)a = 1 b = 2 c = 0 d = 99 kw = &#123;'ext': None&#125; 最神奇的是通过一个 tuple 和 dict，你也可以调用上述函数： 12345678&gt;&gt;&gt; args = (1, 2, 3, 4)&gt;&gt;&gt; kw = &#123;'d': 99, 'x': '#'&#125;&gt;&gt;&gt; f1(*args, **kw)a = 1 b = 2 c = 3 args = (4,) kw = &#123;'d': 99, 'x': '#'&#125;&gt;&gt;&gt; args = (1, 2, 3)&gt;&gt;&gt; kw = &#123;'d': 88, 'x': '#'&#125;&gt;&gt;&gt; f2(*args, **kw)a = 1 b = 2 c = 3 d = 88 kw = &#123;'x': '#'&#125; 所以，对于任意函数，都可以通过类似 func(*args, **kw) 的形式调用它，无论它的参数是如何定义的。 虽然可以组合多达 5 种参数，但不要同时使用太多的组合，否则函数接口的可理解性很差。 3. 递归函数在函数内部，可以调用其他函数。如果一个函数在内部调用自身本身，那么这个函数就是递归函数。 来看一个实例，们来计算阶乘 n! = 1 x 2 x 3 x ... x n，用函数 fact(n) 表示，可以看出： fact(n) = n! = 1 x 2 x 3 x ... x (n-1) x n = (n-1)! x n = fact(n-1) x n 所以，fact(n) 可以表示为 n x fact(n-1)，只有 n=1 时需要特殊处理。 于是，fact(n) 用递归的方式写出来就是： 1234567891011&gt;&gt;&gt; def fact(n):... if n==1:... return 1... return n * fact(n - 1)...&gt;&gt;&gt; fact(1)1&gt;&gt;&gt; fact(2)2&gt;&gt;&gt; fact(5)120 如果我们计算fact(5)，可以根据函数定义看到计算过程如下： 12345678910===&gt; fact(5)===&gt; 5 * fact(4)===&gt; 5 * (4 * fact(3))===&gt; 5 * (4 * (3 * fact(2)))===&gt; 5 * (4 * (3 * (2 * fact(1))))===&gt; 5 * (4 * (3 * (2 * 1)))===&gt; 5 * (4 * (3 * 2))===&gt; 5 * (4 * 6)===&gt; 5 * 24===&gt; 120 递归函数的优点是定义简单，逻辑清晰。理论上，所有的递归函数都可以写成循环的方式，但循环的逻辑不如递归清晰。 3.1 栈溢出使用递归函数需要注意防止栈溢出。在计算机中，函数调用是通过栈（stack）这种数据结构实现的，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。 由于栈的大小不是无限的，所以递归调用的次数过多，会导致栈溢出。可以试试fact(1000)： 123456789&gt;&gt;&gt; fact(1000)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 4, in fact File "&lt;stdin&gt;", line 4, in fact File "&lt;stdin&gt;", line 4, in fact [Previous line repeated 995 more times] File "&lt;stdin&gt;", line 2, in factRecursionError: maximum recursion depth exceeded in comparison 1）尾递归 解决递归调用栈溢出的方法是通过尾递归优化，事实上尾递归和循环的效果是一样的，所以，把循环看成是一种特殊的尾递归函数也是可以的。 尾递归是指：在函数返回的时候，调用自身本身，并且 return 语句不能包含表达式。这样编译器或者解释器就可以把尾递归做优化，使递归本身无论调用多少次，都只占用一个栈帧，不会出现栈溢出的情况。 上面的 fact(n) 函数由于 return n * fact(n - 1) 引入了乘法表达式，所以就不是尾递归了。要改成尾递归方式，需要多一点代码，主要是要把每一步的乘积传入到递归函数中： 1234567def fact(n): return fact_iter(n, 1)def fact_iter(num, product): if num == 1: return product return fact_iter(num - 1, num * product) 可以看到，return fact_iter(num - 1, num * product) 仅返回递归函数本身，num - 1 和 num * product 在函数调用前就会被计算，不影响函数调用。fact(5) 对应的 fact_iter(5, 1) 的调用如下： 123456===&gt; fact_iter(5, 1)===&gt; fact_iter(4, 5)===&gt; fact_iter(3, 20)===&gt; fact_iter(2, 60)===&gt; fact_iter(1, 120)===&gt; 120 尾递归调用时，如果做了优化，栈不会增长，因此，无论多少次调用也不会导致栈溢出。 遗憾的是，大多数编程语言没有针对尾递归做优化，Python 解释器也没有做优化，所以，即使把上面的 fact(n) 函数改成尾递归方式，也会导致栈溢出。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的运算符以及流程控制]]></title>
    <url>%2FPython%2FPython-%E4%B8%AD%E7%9A%84%E8%BF%90%E7%AE%97%E7%AC%A6%E4%BB%A5%E5%8F%8A%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 前面我们已经学习了 Python 基本数据结构，本文我们来看 Python 运算符以及程序流程控制结构，以加深了解。 Python 中的运算符以及流程控制1. Python 中的运算符我们先来看个简单的例子： 12&gt;&gt;&gt; 4 + 59 其中 4，5被称为操作数，+ 被称之为运算符。 Python 语言支持以下类型的运算符: 算术运算符 赋值运算符 比较（关系）运算符 逻辑运算符 位运算符 成员运算符 身份运算符 针对上述运算符分类，进行分别学习： 1.1 运算符详解1.1.1 算术运算符在介绍 Number（数字）数据类型时，我们已经接触过算术运算了。这里我们来看如何使用算术运算符进行算术运算： 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; aNumber = 21&gt;&gt;&gt; bNumber = 10&gt;&gt;&gt; cNumber = 0# 加法：&gt;&gt;&gt; cNumber = aNumber + bNumber&gt;&gt;&gt; print("The value of cNumber:",cNumber)The value of cNumber: 31# 减法：&gt;&gt;&gt; cNumber = aNumber - bNumber&gt;&gt;&gt; print("The value of cNumber:",cNumber)The value of cNumber: 11# 乘法：&gt;&gt;&gt; cNumber = aNumber * bNumber&gt;&gt;&gt; print("The value of cNumber:",cNumber)The value of cNumber: 210# 除法：&gt;&gt;&gt; cNumber = aNumber / bNumber&gt;&gt;&gt; print("The value of cNumber:",cNumber)The value of cNumber: 2.1# 整除：&gt;&gt;&gt; cNumber = aNumber // bNumber&gt;&gt;&gt; print("The value of cNumber:",cNumber)The value of cNumber: 2# 取模（取余）：&gt;&gt;&gt; cNumber = aNumber % bNumber&gt;&gt;&gt; print("The value of cNumber:",cNumber)The value of cNumber: 1# 幂运算：&gt;&gt;&gt; cNumber = aNumber ** bNumber&gt;&gt;&gt; print("The value of cNumber:",cNumber)The value of cNumber: 16679880978201 1.1.2 赋值运算符 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c = a 等效于 c = c a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c **= a 等效于 c = c ** a //= 取整除赋值运算符 c //= a 等效于 c = c // a 1.1.3 比较（关系）运算符 运算符 描述 实例 == 等于 - 比较对象是否相等 (a == b) 返回 False。 != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True。 &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。 &lt; 小于 - 返回 x 是否小于 y。所有比较运算符返回 1 表示真，返回 0 表示假。这分别与特殊的变量 True 和 False 等价。注意，这些变量名的大写。 (a &lt; b) 返回 True。 &gt;= 大于等于 - 返回x是否大于等于y。 (a &gt;= b) 返回 False。 &lt;= 小于等于 - 返回x是否小于等于y。 (a &lt;= b) 返回 True。 1.1.4 位运算符按位运算符是把数字看作二进制来进行计算的。Python 中的按位运算法则如下： &amp;：按位与运算符：参与运算的两个值,如果两个相应位都为 1,则该位的结果为1,否则为 0; |：按位或运算符：只要对应的二个二进位有一个为 1 时，结果位就为 1； ^：按位异或运算符：当两对应的二进位相异时，结果为 1 ； ~：按位取反运算符：对数据的每个二进制位取反,即把 1 变为 0，把 0 变为 1； &lt;&lt;：左移动运算符：运算数的各二进位全部左移若干位，由 “&lt;&lt;” 右边的数指定移动的位数，高位丢弃，低位补 0； &gt;&gt;：右移动运算符：把 “&gt;&gt;” 左边的运算数的各二进位全部右移若干位，”&gt;&gt;” 右边的数指定移动的位数 。 123456789101112131415161718192021222324252627&gt;&gt;&gt; aNumber = 60 # 60 = 0011 1100&gt;&gt;&gt; bNumber = 13 # 13 = 0000 1101&gt;&gt;&gt; cNumber = 0&gt;&gt;&gt; cNumber = aNumber &amp; bNumber; # 12 = 0000 1100&gt;&gt;&gt; print ("1.cNumber 的值为：", cNumber)1.cNumber 的值为： 12&gt;&gt;&gt; cNumber = aNumber | bNumber; # 61 = 0011 1101&gt;&gt;&gt; print ("2.cNumber 的值为：", cNumber)2.cNumber 的值为： 61&gt;&gt;&gt; cNumber = aNumber ^ bNumber; # 49 = 0011 0001&gt;&gt;&gt; print ("3.cNumber 的值为：", cNumber)3.cNumber 的值为： 49&gt;&gt;&gt; cNumber = ~aNumber; # -61 = 1100 0011&gt;&gt;&gt; print ("4.cNumber 的值为：", cNumber)4.cNumber 的值为： -61&gt;&gt;&gt; cNumber = aNumber &lt;&lt; 2; # 240 = 1111 0000&gt;&gt;&gt; print ("5.cNumber 的值为：", cNumber)5.cNumber 的值为： 240&gt;&gt;&gt; cNumber = aNumber &gt;&gt; 2; # 15 = 0000 1111&gt;&gt;&gt; print ("6.cNumber 的值为：", c)6.cNumber 的值为： 15 1.1.5 逻辑运算符Python 语言支持的逻辑运算符如下： x and y：布尔 “与” — 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。； x or y：布尔 “或” — 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值； not x：布尔 “非” — 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt;&gt; aNumber = 10&gt;&gt;&gt; bNumber = 20 &gt;&gt;&gt; if ( aNumber and bNumber ):... print ("1 - 变量 aNumber 和 bNumber 都为 true")... else:... print ("1 - 变量 aNumber 和 bNumber 有一个不为 true")...1 - 变量 aNumber 和 bNumber 都为 true &gt;&gt;&gt; if ( aNumber or bNumber ):... print ("2 - 变量 aNumber 和 bNumber 都为 true，或其中一个变量为 true")... else:... print ("2 - 变量 aNumber 和 bNumber 都不为 true")...2 - 变量 aNumber 和 bNumber 都为 true，或其中一个变量为 true # 修改变量 a 的值&gt;&gt;&gt; aNumber = 0&gt;&gt;&gt; if ( aNumber and bNumber ):... print ("3 - 变量 aNumber 和 bNumber 都为 true")... else:... print ("3 - 变量 aNumber 和 bNumber 有一个不为 true")...3 - 变量 aNumber 和 bNumber 有一个不为 true &gt;&gt;&gt; if ( aNumber or bNumber ):... print ("4 - 变量 aNumber 和 bNumber 都为 true，或其中一个变量为 true")... else:... print ("4 - 变量 aNumber 和 bNumber 都不为 true")...4 - 变量 aNumber 和 bNumber 都为 true，或其中一个变量为 true &gt;&gt;&gt; if not( aNumber and bNumber ):... print ("5 - 变量 aNumber 和 bNumber 都为 false，或其中一个变量为 false")... else:... print ("5 - 变量 aNumber 和 bNumber 都为 true")...5 - 变量 aNumber 和 bNumber 都为 false，或其中一个变量为 false 1.1.6 成员运算符除了以上的一些运算符之外，Python 还支持成员运算符。正如我们在字符串（str），列表（list）或元组（tuple）、字典（dict）、集合（set）中进行的成员检查。 in：如果在指定的序列中找到值返回 True，否则返回 False； not in：如果在指定的序列中没有找到值返回 True，否则返回 False。 关于成员运算符的使用请参考前面字符串（str），列表（list）或元组（tuple）、字典（dict）、集合（set）部分。 1.1.7 身份运算符身份运算符用于比较两个对象的存储单元： is：is 是判断两个标识符是不是引用自一个对象。x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False； is not：is not 是判断两个标识符是不是引用自不同对象。x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 注意，id([object])函数用于获取对象的内存地址。 下面我们来看身份运算符如何使用： 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; aNumber = 20&gt;&gt;&gt; bNumber = 20&gt;&gt;&gt; if ( aNumber is bNumber ):... print ("1 - aNumber 和 bNumber 有相同的标识")... else:... print ("1 - aNumber 和 bNumber 没有相同的标识")...1 - aNumber 和 bNumber 有相同的标识&gt;&gt;&gt; if ( id(aNumber) == id(bNumber) ):... print ("2 - aNumber 和 bNumber 有相同的标识")... else:... print ("2 - aNumber 和 bNumber 没有相同的标识")...2 - aNumber 和 bNumber 有相同的标识 # 修改变量 b 的值&gt;&gt;&gt; bNumber = 30&gt;&gt;&gt; if ( aNumber is bNumber ):... print ("3 - aNumber 和 bNumber 有相同的标识")... else:... print ("3 - aNumber 和 bNumber 没有相同的标识")...3 - aNumber 和 bNumber 没有相同的标识&gt;&gt;&gt; if ( aNumber is not bNumber ):... print ("4 - aNumber 和 bNumber 没有相同的标识")... else:... print ("4 - aNumber 和 bNumber 有相同的标识")...4 - aNumber 和 bNumber 没有相同的标识 1.2 运算符优先级以下给出出了从最高到最低优先级的所有运算符： 【**：指数 (最高优先级)】–&gt; 【~ + -：按位取反, 一元加号和减号 (表示正负号)】–&gt; 【* / % //：乘，除，取模和取整除】–&gt; 【+ -：加法减法】–&gt; 【&gt;&gt; &lt;&lt;：右移，左移运算符】– &gt; 【&amp; --&gt; ^ |：位与，异或，位或】–&gt; 【&lt;= &lt; &gt; &gt;=：关系】–&gt; 【== !=：等于运算符】–&gt; 【= %= /= //= -= += *= **=：赋值运算符】–&gt; 【is is not：身份运算符】–&gt; 【in not in：成员运算符】–&gt; 【and or not：逻辑运算符】。 2. Python 中的流程控制结构这一小节我们来看 Python 中的流程控制结构：条件控制、循环结构。 2.1 条件控制我们先来给出条件控制结构流程图以及简单 Python 条件语句： 1234567age = 20if age &gt;= 18: print('your age is', age) print('adult')else: print('your age is', age) print('teenager') 根据 Python 的缩进规则，如果if语句判断是True，就把 if 缩进下的两行 print 语句执行了。否则执行 else 下的缩进代码块（注意，else 非必须）。 事实上，上面的判断是很粗略的，完全可以用elif实现更复杂判断。下面我们给出 Python if 语句的一般形式: 123456if condition_1: statement_block_1elif condition_2: statement_block_2else: statement_block_3 如果 “condition_1” 为 True 将执行 “statement_block_1” 块语句 如果 “condition_1” 为False，将判断 “condition_2” 如果”condition_2” 为 True 将执行 “statement_block_2” 块语句 如果 “condition_2” 为False，将执行”statement_block_3”块语句 Python 中用 elif 代替了 else if，所以 if 语句的关键字为：if – elif – else。从上往下判断，如果在某个判断上是True，把该判断对应的语句执行后，就忽略掉剩下的elif和else 注意，Python 中不支持 switch – case 语句。 2.1.1 if 嵌套if 中还支持嵌套，可以把 if...elif...else 结构放在另外一个 if...elif...else 结构中。语法如下： 123456789101112if 表达式1: 语句 if 表达式2: 语句 elif 表达式3: 语句 else: 语句elif 表达式4: 语句else: 语句 2.2 循环结构Python 中有两种循环结构支持：for ... in 和 while。 2.2.1 for 语句1）for … in Python for 循环可以遍历任何序列的项目，如列表、字符串、元组等。其一般格式如下： 12for &lt;variable&gt; in &lt;sequence&gt;: &lt;statements&gt; 循环实例如下： 12345678&gt;&gt;&gt; languages = ["C", "C++", "Perl", "Python"]&gt;&gt;&gt; for x in languages:... print (x)...CC++PerlPython 2）for … in … else for ... in 语句还可以和 else，配合使用。如下： 12345678910111213&gt;&gt;&gt; languages = ["C", "C++", "Perl", "Python"]&gt;&gt;&gt; for x in languages:... print(x)... else:... print("No elements!!!")...CC++PerlPythonNo elements!!!&gt;&gt;&gt; print("循环完毕")循环完毕 3）range() 如果需要遍历数字序列，可以使用内置 range() 函数。它会生成数列，例如: 12345678&gt;&gt;&gt; for index in range(5):... print(index)...01234 也可以使用 range 指定区间的值（甚至是步长）： 12345678&gt;&gt;&gt; for index in range(0,10,2):... print(index)...02468 我们来看如何使用 len()，range()以通过序列索引遍历序列： 1234567&gt;&gt;&gt; website = ["Google", "Baidu", "Opera"]&gt;&gt;&gt; for index in range(len(website)):... print(index, website[index])...0 Google1 Baidu2 Opera 2.2.2 while循环1）while 语法 首先给出 Python 中 while 语句的一般形式： 12while 判断条件： 语句 注意，Python 中没有 do..while 循环。 循环实例（1~100求和）如下： 123456789&gt;&gt;&gt; n = 100&gt;&gt;&gt; sum = 0&gt;&gt;&gt; counter = 1&gt;&gt;&gt; while counter &lt;= n:... sum += counter... counter += 1...&gt;&gt;&gt; print("The sum is :",sum)The sum is : 5050 2）while … else 和 for ... in 语句一样，while ... else 也可以和 else，配合使用。while … else 在条件语句为 false 时执行 else 的语句块： 123456789101112&gt;&gt;&gt; n = 100&gt;&gt;&gt; sum = 0&gt;&gt;&gt; counter = 1&gt;&gt;&gt; while counter &lt;= n:... sum += counter... counter += 1... else:... print("n &gt; 100")...n &gt; 100&gt;&gt;&gt; print("The sum is :",sum)The sum is : 5050 注意，循环也是可以嵌套。 2.3 Break &amp;&amp; Continue2.3.1 breakbreak 语句可以跳出 for 和 while 的循环体。如果你从 for 或 while 循环中终止，任何对应的循环 else 块将不执行。实例如下： 1234567891011121314151617181920&gt;&gt;&gt; Astr = "Google"&gt;&gt;&gt; flags = 5&gt;&gt;&gt; while flags &gt; 0:... print("Current number:",flags)... for char in Astr:... if char == "o":... break... print("Current char:",char)... flags -= 1...Current number: 5Current char: GCurrent number: 4Current char: GCurrent number: 3Current char: GCurrent number: 2Current char: GCurrent number: 1Current char: G 可以发现，break 只能跳出当前循环体（内循环体），而外部循环仍在进行。 2.3.2 continuecontinue 语句被用来告诉 Python 解释器跳过当前循环块中的剩余语句，然后继续进行下一轮循环。 123456789&gt;&gt;&gt; flags = 5&gt;&gt;&gt; while flags &gt;= 3:... flags -= 1... if flags == 3:... continue... print(flags)...42 注意，我们知道循环语句可以有 else 子句，它在穷尽列表(以 for 循环)或条件变为 false (以 while 循环)导致循环终止时被执行，但循环被 break 终止时不执行。 2.4 passPython 中 pass 是空语句，是为了保持程序结构的完整性，让程序能运行起来。 pass 不做任何事情，一般用做占位语句，如下实例: 12345678class MyEmptyClass: pass def nop(): passif age &gt;= 18: pass]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Operate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 数据结构之 Dict && Set]]></title>
    <url>%2FPython%2FPython-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-Dict-Set%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 关于 Python 基本数据类型，我们已经介绍了数字（Number）、字符串（String）、元组（Tuple）、列表（List）。这一小节我们来看 Python 中的字典（Dict）和集合（Set）。 Python 数据结构之 Dict &amp;&amp; SetPython 中的字典（Dict）和集合（Set）容器模型： 1. 字典（Dict）Python 中另一个非常有用的内置数据类型是字典（Dictionary），相当于其他语言中的 map。 前面我们提过，列表（List）是有序的对象集合，而字典（Dict）是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的。 1.0 构建字典1）字典的创建也很简单，用 { } 标识，它是一个无序的 键(key) : 值(value) 对的集合，可存储任意类型对象。格式如下所示： 1d = &#123;key1 : value1, key2 : value2 &#125; 2）除了上述的定义方式构造字典（Dict）外，还可以使用构造函数dict()的方法直接从键值对序列中构建字典。 语法规则如下: class dict(**kwarg) # 传入关键字 class dict(mapping, **kwarg) # 映射函数方式来构造字典 class dict(iterable, **kwarg) # 可迭代对象方式来构造字典 123456789101112# 可迭代对象方式来构造字典&gt;&gt;&gt; dict([('Google', 1), ('Baidu', 2), ('Opera', 3)])&#123;'Google': 1, 'Baidu': 2, 'Opera': 3&#125;# 传入关键字&gt;&gt;&gt; dict(Google=1, Baidu=2, Opera=3)&#123;'Runoob': 1, 'Google': 2, 'Taobao': 3&#125;# 映射函数方式来构造字典&gt;&gt;&gt; dict1 = dict(zip(["First","Second","Third"],[1,2,3]))&gt;&gt;&gt; print(dict1)&#123;'First': 1, 'Second': 2, 'Third': 3&#125; 3）除此之外，我们还可以使用推导式的方法创建字典： 12&gt;&gt;&gt; &#123;x:x**2 for x in (2,4,6,8)&#125;&#123;2: 4, 4: 16, 6: 36, 8: 64&#125; 如何创建一个空字典： 123&gt;&gt;&gt; dict1 = &#123;&#125;&gt;&gt;&gt; type(dict1)&lt;class 'dict'&gt; 1.1 Key-Value 对从 key:value 对可以看出 Dict 是一种映射类型。 键（key）必须使用不可变类型，且必须唯一，值则可以是任意类型对象。 1）检索速度 并且，相较于 List 的使用，字典具有极快的查找速度。 举个例子，假设要根据公司员工的名字查找对应的薪资，如果用 list 实现，需要两个 list： 12&gt;&gt;&gt; names = ['Google', 'Baidu', '360', 'Opera']&gt;&gt;&gt; salary = [5888, 3888, 2888, 4300] 给定一个名字，要查找对应的薪资，就先要在 names 中找到对应的位置（索引），再从 salary 取出对应的成绩，list 越长，耗时越长。 123&gt;&gt;&gt; indx = names.index("Opera")&gt;&gt;&gt; salary[indx]4300 如果使用 Dict 实现，只需要一个“名字”:“薪资”的对照表，直接根据名字查找薪资，无论这个表有多大，查找速度都不会变慢。用 Python 写一个 dict 如下： 123&gt;&gt;&gt; staff = &#123;'Google':5888, 'Baidu':3888, '360':2888, 'Opera':4300&#125;&gt;&gt;&gt; staff["Opera"]4300 为什么 dict 查找速度这么快？因为 dict 的实现原理和查字典是一样的。假设字典包含了 1 万个汉字，我们要查某一个字： 一个办法是把字典从第一页往后翻，直到找到我们想要的字为止，这种方法就是在 list 中查找元素的方法，list 越大，查找越慢。 第二种方法是先在字典的索引表里（比如部首表）查这个字对应的页码，然后直接翻到该页，找到这个字。无论找哪个字，这种查找速度都非常快，不会随着字典大小的增加而变慢。 dict 就是第二种实现方式，给定一个名字，比如&#39;Opera&#39;，dict 在内部就可以直接计算出Opera对应的存放成绩的“页码”，也就是4300这个数字存放的内存地址，直接取出来，所以速度非常快。 我们可以猜到，这种 key-value 存储方式，在放进去的时候，必须根据 key 算出 value 的存放位置，这样，取的时候才能根据 key 直接拿到 value。 2）dict &amp;&amp; list 和 list 比较，dict 有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 而 list 刚好相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。 所以，dict 是用空间来换取时间的一种方法，用在需要高速查找的很多地方。 正确使用 dict 非常重要，需要牢记的第一条就是 dict 的 key 必须是不可变对象。 这是因为 dict 根据 key 来计算 value 的存储位置，如果每次计算相同的 key 得出的结果不同，那 dict 内部就完全混乱了。这个通过 key 计算位置的算法称为哈希算法（Hash）。要保证 hash 的正确性，作为 key 的对象就不能变。 1.2 访问字典里的值这一模块我们来看如何访问字典中定义好的 key:value 对。 如何查看字典、字典中的 key 值、value 值？ 12345&gt;&gt;&gt; staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300, 'Sogo': 3200&#125;&gt;&gt;&gt; staff.keys()dict_keys(['Google', 'Baidu', '360', 'Opera', 'Sogo'])&gt;&gt;&gt; staff.values()dict_values([5888, 3888, 2888, 4300, 3200]) 前面我们已经知道，字典（Dict）中必须根据 key 算出 value 的存放位置，也就是说需要根据 key 来获取 value 取值： 12&gt;&gt;&gt; print("dict["Google"]:",staff["Google"])dict["Google"]: 5888 如果使用字典里没有的键访问数据时，会输出如下错误： 1234&gt;&gt;&gt; staff["www"]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;KeyError: 'www' 1.3 获取字典长度（容量）len(dict)方法计算字典元素个数，即键值对的总数。 123&gt;&gt;&gt; staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300, 'Sogo': 3200&#125;&gt;&gt;&gt; len(staff)5 1.4 更新字典在刚开始学习基本数据结构时，我们提到过字典（Dict）也是一种可变数据类型。所以我们可以单独对字典中的数据项（key:value 对）进行修改、增加、以及删除等操作。 1）补充 Key-Value 对 把数据放入 dict 的方法，除了初始化时指定外，还可以通过 key 放入： 123&gt;&gt;&gt; staff["Sogo"] = 3000&gt;&gt;&gt; staff&#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300, 'Sogo': 3000&#125; 注意，由于一个 key 只能对应一个 value，多次对一个 key 放入 value，后面的值会把前面的值冲掉。 123&gt;&gt;&gt; staff[&quot;Sogo&quot;] = 3200&gt;&gt;&gt; staff&#123;&apos;Google&apos;: 5888, &apos;Baidu&apos;: 3888, &apos;360&apos;: 2888, &apos;Opera&apos;: 4300, &apos;Sogo&apos;: 3200&#125; 2）修改 Key-Value 对 1234&gt;&gt;&gt; staff = &#123;&apos;Google&apos;: 5888, &apos;Baidu&apos;: 3888, &apos;360&apos;: 2888, &apos;Opera&apos;: 4300, &apos;Sogo&apos;: 3200&#125;&gt;&gt;&gt; staff[&quot;Baidu&quot;] = 4000&gt;&gt;&gt; staff&#123;&apos;Google&apos;: 5888, &apos;Baidu&apos;: 4000, &apos;360&apos;: 2888, &apos;Opera&apos;: 4300, &apos;Sogo&apos;: 3200&#125; 3）删除操作 123456789101112131415161718&gt;&gt;&gt; staff = &#123;&apos;Google&apos;: 5888, &apos;Baidu&apos;: 3888, &apos;360&apos;: 2888, &apos;Opera&apos;: 4300, &apos;Sogo&apos;: 3200&#125;# 删除键值对：&gt;&gt;&gt; del staff[&quot;Sogo&quot;]&gt;&gt;&gt; print(staff)&#123;&apos;Google&apos;: 5888, &apos;Baidu&apos;: 4000, &apos;360&apos;: 2888, &apos;Opera&apos;: 4300&#125;# 清空字典&gt;&gt;&gt; staff.clear()&gt;&gt;&gt; staff&#123;&#125;# 删除字典&gt;&gt;&gt; del staff&gt;&gt;&gt; staffTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;NameError: name &apos;staff&apos; is not defined 1.5 常用字典内置方法1）dict.get(key, default=None) get() 函数返回指定键的值，如果键值不在字典中返回默认值（None）。 12345&gt;&gt;&gt; staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300&#125;&gt;&gt;&gt; print ("Opera 值为 : %d" % staff.get('Opera'))Opera 值为 : 4300&gt;&gt;&gt; print ("Sogo 值为 : %s" % staff.get('Sogo', "NA"))Sogo 值为 : NA 注意，dict.get(&quot;xxx&quot;) 和 dict[&quot;xxx&quot;] 的区别在于返回值。 2）key (not) in dict in 操作符用于判断键是否存在于字典中，如果键在字典 dict 里返回 true，否则返回 false。 而 not in 操作符刚好相反，如果键在字典 dict 里返回 false，否则返回 true。 1234567891011121314&gt;&gt;&gt; staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300&#125;&gt;&gt;&gt; if 'Google' in staff:... print("Key of Google exist")... else:... print("Key of Google not exist")...Key of Google exist&gt;&gt;&gt; if 'Sogo' in staff:... print("Key of Sogo exist")... else:... print("Key of Sogo not exist")...Key of Sogo not exist 注意，key (not) in dict 常应用于判断某一 key 是否在字典中有定义。 3）dict.items() items() 方法以列表返回可供遍历的(键, 值) 元组数组。 1234567891011&gt;&gt;&gt; staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300&#125;&gt;&gt;&gt; print("key-value:",staff.items())key-value: dict_items([('Google', 5888), ('Baidu', 3888), ('360', 2888), ('Opera', 4300)])&gt;&gt;&gt; for value in staff.items():... print(value,end=';')...('Google', 5888);('Baidu', 3888);('360', 2888);('Opera', 4300);&gt;&gt;&gt; for key,value in staff.items():... print(key,":",value,end=';')...Google : 5888;Baidu : 3888;360 : 2888;Opera : 4300; 注意，items() 方法常用于便利字典。 4）dict.keys() &amp;&amp; dict.values() 前面我们已经介绍过 dict.keys() &amp;&amp; dict.values()方法（返回的是一个可迭代对象，可使用 list() 将其转化为列表）的使用，这里我们来看如何通过字典（dict）的键值来实现字典遍历： 12345678910111213&gt;&gt;&gt; staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300&#125;&gt;&gt;&gt; staff.keys()dict_keys(['Google', 'Baidu', '360', 'Opera'])&gt;&gt;&gt; list(staff.keys())['Google', 'Baidu', '360', 'Opera']&gt;&gt;&gt; for key in list(staff.keys()):... print(key,end=';')...Google;Baidu;360;Opera;&gt;&gt;&gt; for key in staff.keys():... print(key,end=';')...Google;Baidu;360;Opera; 5）pop(key[default]) pop() 方法删除字典给定键 key 所对应的值，返回值为被删除的值。 1234567&gt;&gt;&gt; staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300&#125;&gt;&gt;&gt; pop_site = staff.pop("360")&gt;&gt;&gt; print(pop_site)2888&gt;&gt;&gt; staff&#123;'Google': 5888, 'Baidu': 3888, 'Opera': 4300&#125;&gt;&gt;&gt; 6）保证字典键值有效性 12345678staff = &#123;'Google': 5888, 'Baidu': 3888, '360': 2888, 'Opera': 4300&#125;state = staff.get（'4396'， None）if not state: .... # 等价于：if '4396' not in staff or staff['4396'] is None or not staff['4396']: ... 成立有三种情况： dict中不存在 dict中存在，但值是：None dict中存在而且也不是 None，但是是一个等同于 False 的值，比如说空字符串或者空列表。 2. 集合（Set）集合（set）是一个无序的不重复元素序列（结合数学中的集合概念进行理解）。我们可以将其和 dict 进行比较，是一组 key 的集合，但不存储value。所以在 set 中，也没有重复的 key。 2.1 集合构建类似与 dict, Python 提供了使用大括号 {} 以及构造函数 set() 的方法创建集合，创建格式如下： 123parame = &#123;value01,value02,...&#125;或者set(value) 创建实例如下： 123456789101112131415161718&gt;&gt;&gt; set1 = &#123;"Google", "Baidu", "Opera"&#125;&gt;&gt;&gt; print(set1)&#123;'Opera', 'Google', 'Baidu'&#125;&gt;&gt;&gt; set2 = &#123;"Welcom", 32, 123.321&#125;&gt;&gt;&gt; print(set2)&#123;32, 123.321, 'Welcom'&#125;# 还可以使用 set() 方法将字符串，列表，元组等自动去重转化成集合：&gt;&gt;&gt; set3 = set("abcdefg")&gt;&gt;&gt; set3&#123;'d', 'c', 'g', 'f', 'e', 'b', 'a'&#125;&gt;&gt;&gt; set4 = set(["Google", "Baidu", "Opera"])&gt;&gt;&gt; print(set4)&#123;'Opera', 'Google', 'Baidu'&#125;&gt;&gt;&gt; set5 = set(("Google", "Baidu", "Opera", "Google"))&gt;&gt;&gt; print(set5)&#123;'Opera', 'Google', 'Baidu'&#125; 类似列表、字典推导式，同样，集合也支持集合推导式: 123&gt;&gt;&gt; setA = &#123;x for x in 'abcdr' if x not in 'abc'&#125;&gt;&gt;&gt; setA&#123;'d', 'r'&#125; 如何创建一个空集合？ 创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典的（已被占用）。 1234&gt;&gt;&gt; dict1 = &#123;&#125;&gt;&gt;&gt; set1 = set()&gt;&gt;&gt; print(type(dict1),type(set1))&lt;class 'dict'&gt; &lt;class 'set'&gt; 2.2 集合间运算结合数学集合概念，我们来看两个集合间的运算： 1234567891011121314151617181920212223# 集合中元素自动去重&gt;&gt;&gt; setA = set("abcdefghabc")&gt;&gt;&gt; print(setA)&#123;'d', 'c', 'h', 'g', 'f', 'e', 'b', 'a'&#125;&gt;&gt;&gt; setB = set("fghijklmjklm")&gt;&gt;&gt; print(setB)&#123;'h', 'm', 'g', 'i', 'j', 'f', 'k', 'l'&#125;# 差集：集合 setA 中包含而集合 setB 中不包含的元素&gt;&gt;&gt; setA - setB&#123;'d', 'c', 'e', 'b', 'a'&#125;# 并集：集合 setA 或 setB 中包含的所有元素&gt;&gt;&gt; setA | setB&#123;'d', 'c', 'h', 'g', 'm', 'i', 'j', 'f', 'e', 'k', 'b', 'l', 'a'&#125;# 交集：集合 setA 和 setB 中都包含了的元素&gt;&gt;&gt; setA &amp; setB&#123;'h', 'f', 'g'&#125;# 互异：不同时包含于 setA 和 setB 的元素&gt;&gt;&gt; setA ^ setB&#123;'m', 'd', 'i', 'c', 'a', 'j', 'e', 'k', 'l', 'b'&#125; 2.3 集合的基本操作2.3.1 获取集合元素个数len(set) 方法可以来计算集合 set 元素个数。 123&gt;&gt;&gt; setA = &#123;"Google", "Baidu", "Opera"&#125;&gt;&gt;&gt; len(setA)3 2.3.2 成员检查x in set方法可用于判断元素 x 是否在集合 set 中，存在返回 True，不存在返回 False。 123&gt;&gt;&gt; setA = &#123;"Google", "Baidu", "Opera"&#125;&gt;&gt;&gt; "Google" in setATrue 常用于实现集合遍历： 1234&gt;&gt;&gt; for element in setA:... print(element,end=";")...Google;Baidu;Opera; 2.3.3 集合更新我们知道，和列表（List）、字典（Dict）一样，集合（Set）属于可变数据类型。所以我们可以对集合元素（数据项）进行增加和删除。 1）添加元素 set.add( x )方法将元素 x 添加到集合 set 中，如果元素已存在，则不进行任何操作。 1234&gt;&gt;&gt; setA = &#123;"Google", "Baidu", "Opera"&#125;&gt;&gt;&gt; setA.add("Sogo")&gt;&gt;&gt; print(setA)&#123;'Opera', 'Google', 'Baidu', 'Sogo'&#125;tA) 除此之外，set.update( x )方法也可以添加元素，且参数可以是字符串，列表，元组，字典，集合等： 12345678910111213141516171819202122&gt;&gt;&gt; setA = &#123;"Google", "Baidu"&#125;&gt;&gt;&gt; setA.update("Opera")&gt;&gt;&gt; setA&#123;'Baidu', 'r', 'p', 'Google', 'e', 'O', 'a'&#125;&gt;&gt;&gt; setA.update([1,2],[3,4])&gt;&gt;&gt; setA&#123;1, 2, 3, 4, 'Baidu', 'r', 'p', 'Google', 'e', 'O', 'a'&#125;&gt;&gt;&gt; setA.update(("Opera","360"))&gt;&gt;&gt; setA&#123;1, 2, 3, 4, '360', 'Baidu', 'r', 'Opera', 'p', 'Google', 'e', 'O', 'a'&#125;# 并集：合并集合&gt;&gt;&gt; setA.update(&#123;"Google", "Sogo"&#125;)&gt;&gt;&gt; setA&#123;1, 2, 3, 4, '360', 'Baidu', 'r', 'Opera', 'p', 'Google', 'Sogo', 'e', 'O', 'a'&#125;&gt;&gt;&gt; setA.update(&#123;"jinshan":36&#125;)&gt;&gt;&gt; setA&#123;1, 2, 3, 4, '360', 'Baidu', 'r', 'Opera', 'p', 'Google', 'Sogo', 'e', 'O', 'a', 'jinshan'&#125; 2）删除元素 set.remove( x )方法将元素 x 从集合 set 中移除，如果元素不存在，则会发生错误。 123456789&gt;&gt;&gt; setA = &#123;1, 2, 3, 4, '360', 'Baidu', 'r', 'Opera', 'p', 'Google', 'Sogo', 'e', 'O', 'a', 'jinshan'&#125;&gt;&gt;&gt; setA.remove('jinshan')&gt;&gt;&gt; setA&#123;1, 2, 3, 4, '360', 'Baidu', 'r', 'Opera', 'p', 'Google', 'Sogo', 'e', 'O', 'a'&#125;&gt;&gt;&gt; setA.remove(5)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;KeyError: 5 此外还有一个方法 set.discard( x ) 也是移除集合中的元素，且如果元素不存在，不会发生错误。 1234567&gt;&gt;&gt; setA = &#123;"Google", "Baidu", "Opera"&#125;&gt;&gt;&gt; setA.discard("Baidu")&gt;&gt;&gt; setA&#123;'Google', 'Opera'&#125;&gt;&gt;&gt; setA.discard("360")&gt;&gt;&gt; setA&#123;'Google', 'Opera'&#125; 我们还可以使用 set.pop() 方法随机删除集合中的一个元素： 12345&gt;&gt;&gt; setA = &#123;"Google", "Baidu", "Opera", "360", "Sogo"&#125;&gt;&gt;&gt; setA.pop()'Opera'&gt;&gt;&gt; setA.pop()'360' 2.3.4 常用内置方法1）set.clear() clear() 方法用于移除集合中的所有元素。 1234&gt;&gt;&gt; setA = &#123;"Google", "Baidu", "Opera"&#125;&gt;&gt;&gt; setA.clear()&gt;&gt;&gt; setAset() 2）set.difference(set) difference() 方法用于返回集合的差集，即返回的集合元素包含在第一个集合中，但不包含在第二个集合(方法的参数)中。 12345678&gt;&gt;&gt; setA = set("abcdefghabc")&gt;&gt;&gt; setB = set("fghijklmjklm")&gt;&gt;&gt; setA.difference(setB)&#123;'b', 'd', 'c', 'a', 'e'&#125;# 等价于 setA - setB&gt;&gt;&gt; setA - setB&#123;'b', 'd', 'c', 'a', 'e'&#125; 3）set.intersection(set1, set2 … etc) intersection() 方法用于返回两个或更多集合中都包含的元素，即交集。 12345&gt;&gt;&gt; x = &#123;"a", "b", "c"&#125;&gt;&gt;&gt; y = &#123;"c", "d", "e"&#125;&gt;&gt;&gt; z = &#123;"f", "g", "c"&#125;&gt;&gt;&gt; x.intersection(y,z)&#123;'c'&#125; 4）set.issubset(set) &amp;&amp; set.issuperset(set) issubset() 方法用于判断集合的所有元素是否都包含在指定集合中，如果是则返回 True，否则返回 False。即判断是否为子集关系。 12345&gt;&gt;&gt; x = &#123;"a", "b", "c"&#125;&gt;&gt;&gt; y = &#123;"f", "e", "d", "c", "b", "a"&#125;&gt;&gt;&gt; z = x.issubset(y)&gt;&gt;&gt; print(z)True issuperset() 方法用于判断指定集合的所有元素是否都包含在原始的集合中，如果是则返回 True，否则返回 False。 12345&gt;&gt;&gt; x = &#123;"f", "e", "d", "c", "b", "a"&#125;&gt;&gt;&gt; y = &#123;"a", "b", "c"&#125;&gt;&gt;&gt; z = x.issubset(y)&gt;&gt;&gt; print(z)True 5）set.union(set1, set2…) union() 方法返回两个集合的并集，即包含了所有集合的元素，重复的元素只会出现一次。 123456&gt;&gt;&gt; x = &#123;"a", "b", "c"&#125;&gt;&gt;&gt; y = &#123;"f", "d", "a"&#125;&gt;&gt;&gt; z = &#123;"c", "d", "e"&#125;&gt;&gt;&gt; result = x.union(y, z)&gt;&gt;&gt; result&#123;'b', 'c', 'f', 'a', 'd', 'e'&#125; 至此，关于 Python 基本数据类型的介绍已经完成，下面我们简单补充说明一下 Python 中的空值和不可变对象。 3. 空值空值是 Python 里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 None 是一个对象，其数据类型为 NoneType，其对应的 bool 值为 false。好比 0 是一个对象，其类型为 int，其 bool 值为 false。 4. 不可变对象前面我们说过，str 是不变对象，而 list 是可变对象。 对于可变对象，比如 list，对 list 进行操作，list 内部的内容是会变化的，比如： 1234&gt;&gt;&gt; list1 = ['c', 'b', 'a']&gt;&gt;&gt; list1.sort()&gt;&gt;&gt; list1['a', 'b', 'c'] 而对于不可变对象，比如 str，对 str 进行操作呢： 12345&gt;&gt;&gt; str1 = 'abc'&gt;&gt;&gt; str1.replace('a', 'A')'Abc'&gt;&gt;&gt; str1'abc' 虽然字符串有个replace()方法，也确实变出了&#39;Abc&#39;，但变量str1最后仍是&#39;abc&#39;，应该怎么理解呢？ 再来看： 123456&gt;&gt;&gt; str1 = 'abc'&gt;&gt;&gt; str2 = str1.replace('a', 'A')&gt;&gt;&gt; str1'abc'&gt;&gt;&gt; str2'Abc' 事实上，str1是变量，而&#39;abc&#39;才是字符串对象！有些时候，我们经常说，对象str1的内容是&#39;abc&#39;，但其实是指，str1本身是一个变量，它指向的对象的内容才是&#39;abc&#39;。 当我们调用str1.replace(&#39;a&#39;, &#39;A&#39;)时，实际上调用方法replace是作用在字符串对象&#39;abc&#39;上的，而这个方法虽然名字叫replace，但却没有改变字符串&#39;abc&#39;的内容。相反，replace方法创建了一个新字符串&#39;Abc&#39;并返回，如果我们用变量str2指向该新字符串，就容易理解了，变量str1仍指向原有的字符串&#39;abc&#39;，但变量str2却指向新字符串&#39;Abc&#39;了。 所以，对于不变对象来说，调用对象自身的任意方法，也不会改变该对象自身的内容。相反，这些方法会创建新的对象并返回，这样，就保证了不可变对象本身永远是不可变的。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Dictionary</tag>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 数据结构之 List && Tuple]]></title>
    <url>%2FPython%2FPython-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-List-Tuple%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇： 关于 Python 基本数据类型，我们已经介绍了数字（Number）、字符串（String）。这一小节我们来看 Python 中的元组（tuple）和列表（list）。 Python 数据结构之 List &amp;&amp; Tuple元组和列表都属于序列（Sequence），在开始介绍元组（tuple）和列表（list）之前我们先简单了解一下序列。 1. Python 中的序列序列是是 Python 中最基本的数据结构。序列中的每个元素都分配一个 数字（指元素的位置，或索引），第一个索引是 0，第二个索引是 1，依此类推。每个索引对应一个元素。 Python 包含 6 中内建的序列，包括列表、元组、字符串、Unicode字符串、buffer 对象 和 xrange 对象。 对于序列，都可以使用以下操作： 索引 切片 加 乘 成员检查 计算序列的长度 取序列中的最大、最小值 你可以和我们前面介绍过的字符串（String）类型进行一一验证（都介绍过）。 2. 列表（List）List（列表） 是 Python 中使用最频繁的数据类型。如何创建一个列表？只要把逗号分隔的不同的数据项使用方括号括起来即可（是一个有序集合）： 123&gt;&gt;&gt; classmates = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy'] 但请注意，列表的数据项可以具有不同的数据类型（支持数字，字符串甚至可以包含列表（所谓嵌套）等）： 123&gt;&gt;&gt; list1 = ['string test', 123.321, [1,2], &#123;"name":1,"age":2&#125;, (1,2,"test")]&gt;&gt;&gt; list1['string test', 123.321, [1, 2], &#123;'name': 1, 'age': 2&#125;, (1, 2, 'test')] 除此之外，我们还可以使用推导式的方法创建列表： 123&gt;&gt;&gt; list2 = [x for x in "abcdefg"]&gt;&gt;&gt; list2['a', 'b', 'c', 'd', 'e', 'f', 'g'] 如何创建一个空列表： 123&gt;&gt;&gt; list1 = []&gt;&gt;&gt; list1[] 2.1 List 可执行操作由于列表（List）属于 Python 的内建序列，故序列（seq）中包含的基本操作 List 也具有。例如：1.索引、2.切片、3.加、4.乘、5.成员检查、6.获取序列的长度、7.取序列中的最大、最小值等。 2.1.1 获取序列长度len(list)方法可返回列表元素（数据项）个数。 123&gt;&gt;&gt; list1 = [1, 2.0, "welcome", [1, 2, 3]]&gt;&gt;&gt; print(len(list1))4 2.1.2 列表索引和切片1）访问列表元素 与字符串的索引一样，用索引来访问 list 中每一个位置的元素。索引是从0开始的（-1 为从末尾的开始位置）： 1234567891011&gt;&gt;&gt; classmates = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; len(classmates)3&gt;&gt;&gt; classmates[0]'Michael'&gt;&gt;&gt; classmates[1]'Bob'&gt;&gt;&gt; classmates[-1]'Tracy'&gt;&gt;&gt; classmates[-2]'Bob' 对于嵌套列表索引： 1234567&gt;&gt;&gt; list1 = ['str1', 'str2', ['list_element1', 'list_element2']]&gt;&gt;&gt; list1[0]'str1'&gt;&gt;&gt; list1[2][0]'list_element1'&gt;&gt;&gt; list1[2][1]'list_element2' 注意，当索引超出了 list 范围时，Python 会报一个IndexError错误，所以，要确保索引不要越界。 1234&gt;&gt;&gt; classmates[3]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: list index out of range 2）切片 与字符串的分割（切片）一样，我们可以通过[] 和 :对列表进行切片操作： 123456&gt;&gt;&gt; classmates[0:len(classmates)]['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates[0:len(classmates):1]['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates[0:len(classmates):2]['Michael', 'Tracy'] 和字符串一样，我们甚至可以指定切片的步长： 12345&gt;&gt;&gt; classmates[0:len(classmates):2]['Michael', 'Tracy']&gt;&gt;&gt; str = "weqrqewrewq"&gt;&gt;&gt; str[0:len(str):2]'wqqweq' 2.1.3 列表拼接（+）和重复（*）123456&gt;&gt;&gt; list1 = [1.0, 'welcome']&gt;&gt;&gt; list2 = ["python", "world"]&gt;&gt;&gt; print(list1 * 3)[1.0, 'welcome', 1.0, 'welcome', 1.0, 'welcome']&gt;&gt;&gt; print(list1 + list2)[1.0, 'welcome', 'python', 'world'] 2.1.4 成员检查可用于判断数据元素（数据项）是否存在于列表中： 123456&gt;&gt;&gt; list1 = [1.0, 'welcome']&gt;&gt;&gt; list2 = ["python", "world"]&gt;&gt;&gt; 'welcome' in list1True&gt;&gt;&gt; 'welcome' not in list2True 2.1.5 max(list) &amp;&amp; min(list)max(list)方法用于返回列表元素中的最大值，min(list)方法用于返回列表元素中的最小值。 12345&gt;&gt;&gt; list1, list2 = [&apos;Google&apos;, &apos;Baidu&apos;, &apos;360&apos;], [12, 100, 200]&gt;&gt;&gt; print (&quot;list1 最大元素值 : &quot;, max(list1))list1 最大元素值 : Google&gt;&gt;&gt; print (&quot;list2 最小元素值 : &quot;, min(list2))list2 最小元素值 : 12 注意，在获取列表最大或最小值时，必须保证列表元素类型一致。 1234567&gt;&gt;&gt; list1, list2 = ['Google', 'Baidu', '360'], [12, 100, "200"]&gt;&gt;&gt; print ("list1 最大元素值 : ", max(list1))list1 最大元素值 : Google&gt;&gt;&gt; print ("list2 最小元素值 : ", min(list2))Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: '&lt;' not supported between instances of 'str' and 'int' 2.1.6 列表类型转换list( seq )方法用于将元组或字符串转换为列表。 注：元组与列表是非常类似的，区别在于元组的元素值不能修改，元组是放在括号中，列表是放于方括号中。 12345678&gt;&gt;&gt; aTuple = (123.321, 'Google', 'Baidu', '360')&gt;&gt;&gt; list1 = list(aTuple)&gt;&gt;&gt; print ("列表元素 : ", list1)列表元素 : [123.321, 'Google', 'Baidu', '360']&gt;&gt;&gt; str="Hello World"&gt;&gt;&gt; list2=list(str)&gt;&gt;&gt; print ("列表元素 : ", list2)列表元素 : ['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd'] 2.1.7 列表修改与更新列表和字符串均为序列，我们发现二者有很多相似的操作和用法。但和字符串不一样的是，List 是一个可变的有序表，我们可以对列表的数据项进行修改或更新。 1）根据索引更新列表 123456&gt;&gt;&gt; list = ['Google', 'Baidu', '360', 999]&gt;&gt;&gt; print ("第三个元素为 : ", list[2])第三个元素为 : 360&gt;&gt;&gt; list[3] = 1999&gt;&gt;&gt; print ("更新后的第三个元素为 : ", list[2])更新后的第三个元素为 : 360 2）删除列表元素 我们还使用 del 语句来删除列表的的元素，如下实例： 1234&gt;&gt;&gt; list = ['Google', 'Baidu', '360', 999]&gt;&gt;&gt; del list[3]&gt;&gt;&gt; print(list)['Google', 'Baidu', '360'] 3）list.append(obj) &amp;&amp; list.extend(seq) append() 方法用于在列表末尾添加新的数据项（对象）。 1234&gt;&gt;&gt; list = ['Google', 'Baidu', '360', "Google"]&gt;&gt;&gt; list.append("Opera")&gt;&gt;&gt; print ("更新后的列表 : ", list)更新后的列表 : ['Google', 'Baidu', '360', 'Opera'] extend(seq)函数用于在列表末尾一次性追加另一个序列中的多个值（即用新列表扩展原来的列表）。 这里 seq 对应元素列表，可以是列表、元组、集合、字典，若为字典,则仅会将键（key）作为元素依次添加至原列表的末尾。 12345&gt;&gt;&gt; list1 = ['Google', 'Baidu', '360']&gt;&gt;&gt; list2 = list(range(5))&gt;&gt;&gt; list1.extend(list2)&gt;&gt;&gt; print("扩展后的列表:",list1)扩展后的列表: ['Google', 'Baidu', '360', 0, 1, 2, 3, 4] 4）list.insert(index, obj) insert() 函数用于将指定对象插入列表的指定索引位置。 obj – 值要插入列表中的对象，index – 为对象 obj 需要插入的索引位置。 1234&gt;&gt;&gt; list = [&apos;Google&apos;, &apos;Baidu&apos;, &apos;360&apos;]&gt;&gt;&gt; list.insert(list.index(&quot;Baidu&quot;),&quot;Opera&quot;)&gt;&gt;&gt; print(&quot;列表插入元素后为：&quot;,list)列表插入元素后为： [&apos;Google&apos;, &apos;Opera&apos;, &apos;Baidu&apos;, &apos;360&apos;] 关于 list.index(obj) 的用法可见 [ 2.1.8 常见列表方法 ]。 5）list.pop([index=-1]) pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该移除元素的值。 index – 可选参数，要移除列表元素的索引值，不能超过列表总长度。默认为 index=-1，即删除最后一个列表值。 123456789&gt;&gt;&gt; list = ['Google', 'Baidu', '360']&gt;&gt;&gt; list.pop()'360'&gt;&gt;&gt; print ("列表现在为 : ", list)列表现在为 : ['Google', 'Baidu']&gt;&gt;&gt; list.pop(1)'Baidu'&gt;&gt;&gt; print ("列表现在为 : ", list)列表现在为 : ['Google'] 6）list.remove(obj) remove() 函数用于移除列表中某个值的第一个匹配项。 1234&gt;&gt;&gt; list = ['Google', 'Baidu', '360', "Google"]&gt;&gt;&gt; list.remove("Google")&gt;&gt;&gt; print ("列表现在为 : ", list)列表现在为 : ['Baidu', '360', 'Google'] 2.1.8 常用列表方法1）list.count(obj) count()方法用于统计某个元素在列表中出现的次数。 1234list = ['Google', 'Baidu', '360', "Google"]list.count("Google")&gt;&gt;&gt; list = ['Google', 'Baidu', '360', "Google"]&gt;&gt;&gt; list.count("Google")2 2）list.index(obj) 除了 in &amp;&amp; not in外，list.index() 函数可以用于从列表中找出某个值第一个匹配项的索引位置，如果没有找到对象则抛出异常。 1234567&gt;&gt;&gt; list = ['Google', 'Baidu', '360']&gt;&gt;&gt; list.index("Baidu")1&gt;&gt;&gt; list.index("Opera")Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ValueError: 'Opera' is not in list 3）list.reverse() reverse() 函数用于反向列表中元素。没有返回值，但是会对列表的元素进行反向排序。 1234&gt;&gt;&gt; list = ['Google', 'Baidu', '360']&gt;&gt;&gt; list.reverse()&gt;&gt;&gt; print ("列表反转后: ", list)列表反转后: ['360', 'Baidu', 'Google'] 4）list.sort( key=None, reverse=False) sort() 函数用于对原列表进行排序。该方法没有返回值，但是会对列表的对象进行排序。 key – 指定用来进行比较的元素。参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。 reverse – 定义排序规则，reverse = True 降序， reverse = False 升序（默认）。 12345678910111213141516171819&gt;&gt;&gt; list = ['Google', 'Baidu', '360']&gt;&gt;&gt; list.sort()&gt;&gt;&gt; print ( "List : ", list)List : ['360', 'Baidu', 'Google']&gt;&gt;&gt; list.sort(reverse=True)&gt;&gt;&gt; print ( "List : ", list)List : ['Google', 'Baidu', '360']# 获取列表的第二个元素&gt;&gt;&gt; def takeSecond(elem):... return elem[1]...&gt;&gt;&gt; # 列表&gt;&gt;&gt; random = [(2, 2), (3, 4), (4, 1), (1, 3)]# 指定第二个元素排序&gt;&gt;&gt; random.sort(key=takeSecond)# 输出类别&gt;&gt;&gt; print ('排序列表：', random)排序列表： [(4, 1), (2, 2), (1, 3), (3, 4)] 5）list.clear() clear() 函数用于清空列表，类似于 del a[:]。 1234&gt;&gt;&gt; list = ['Google', 'Baidu', '360']&gt;&gt;&gt; list.clear()&gt;&gt;&gt; print("清空后的列表：",list)清空后的列表： [] 6）list.copy() copy() 函数用于复制列表，返回复制后的新列表。 1234&gt;&gt;&gt; list1 = ['Google', 'Baidu', '360']&gt;&gt;&gt; list2 = list1.copy()&gt;&gt;&gt; print("List2：",list2)List2： ['Google', 'Baidu', '360'] 3. 元组（Tuple）前面我们提到过：元组与列表是非常类似的，区别在于元组的元素值不能修改（不可变数据类型），元组是放在括号中，列表是放于方括号中。 故，元组创建也很简单，只需要在小括号中添加元素（数据项），并使用逗号隔开即可。 12345&gt;&gt;&gt; tup1 = ('Google', 'Baidu', '360')&gt;&gt;&gt; tup2 = ('welcome', 'python', 'world', 1996, 123.321)&gt;&gt;&gt; tup3 = 'welcome', 'python', 'world', 1996, 123.321;&gt;&gt;&gt; type(tup3)&lt;class 'tuple'&gt; 注意，元组中只包含一个元素时，需要在元素后面添加逗号，否则括号会被当作运算符使用： 1234567&gt;&gt;&gt;tup1 = (50)&gt;&gt;&gt; type(tup1) # 不加逗号，类型为整型&lt;class 'int'&gt; &gt;&gt;&gt; tup1 = (50,)&gt;&gt;&gt; type(tup1) # 加上逗号，类型为元组&lt;class 'tuple'&gt; 同理，元组中的元素类型也可以不相同： 123&gt;&gt;&gt; tuple1 = ('tuple test', 123.321, [1,2], &#123;"name":1,"age":2&#125;, (1,2,"test"))&gt;&gt;&gt; print(type(tuple1), tuple1)&lt;class 'tuple'&gt; ('tuple test', 123.321, [1, 2], &#123;'name': 1, 'age': 2&#125;, (1, 2, 'test')) 如何创建一个空元组： 123&gt;&gt;&gt; tup1 = ()&gt;&gt;&gt; type(tup1)&lt;class &apos;tuple&apos;&gt; 3.1 Tuple 可执行操作我们知道，元组（tuple）属于 Python 的内建序列，故序列（seq）中包含的基本操作 tuple也具有。例如：1.索引、2.切片、3.加、4.乘、5.成员检查、6.获取序列的长度、7.取序列中的最大、最小值等。 相比较起来，字符串（str）可执行操作要更接近于元组（tuple），这是由于两者都是属于不可变数据类型导致。但是我们要注意，尽管 tuple 元素不可改变，但它可以包含可变对象，比如 list列表等。 3.1.1 获取序列长度len(tuple)方法可返回元组元素（数据项）个数。 123&gt;&gt;&gt; tup1 = ('Google', 'Baidu', '360')&gt;&gt;&gt; print(len(tup1))3 3.1.2 元组索引和切片1）访问列表元素 与字符串的索引一样，用索引来访问 tuple 中每一个位置的元素。索引是从0开始的（-1 为从末尾索引的开始位置）： 1234567891011&gt;&gt;&gt; website = ('Google', 'Baidu', '360')&gt;&gt;&gt; len(website)3&gt;&gt;&gt; website[0]'Google'&gt;&gt;&gt; website[1]'Baidu'&gt;&gt;&gt; website[-1]'360'&gt;&gt;&gt; website[-2]'Baidu' 对于嵌套元组索引： 1234567&gt;&gt;&gt; tup1 = ('str1', 'str2', ['list_element1', 'list_element2'])&gt;&gt;&gt; tup1[0]'str1'&gt;&gt;&gt; tup1[2][0]'list_element1'&gt;&gt;&gt; tup1[2][1]'list_element2' 注意，当索引超出了 tuple 范围时，Python 会报一个IndexError错误，所以，要确保索引不要越界。 1234&gt;&gt;&gt; website[3]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: tuple index out of range 2）切片 与字符串的分割（切片）一样，我们同样可以通过[] 和 :对元组进行切片操作： 12&gt;&gt;&gt; website[0:len(classmates)]('Google', 'Baidu', '360') 和字符串一样，我们甚至可以指定切片的步长： 1234&gt;&gt;&gt; website[0:len(classmates):1]('Google', 'Baidu', '360')&gt;&gt;&gt; website[0:len(classmates):2]('Google', '360') 3.1.3 元组拼接（+）和重复（*）123456&gt;&gt;&gt; tup1 = (1.0, 'welcome')&gt;&gt;&gt; tup2 = ("python", "world")&gt;&gt;&gt; print(tup1 * 3)(1.0, 'welcome', 1.0, 'welcome', 1.0, 'welcome')&gt;&gt;&gt; print(tup1 + tup2)(1.0, 'welcome', 'python', 'world') 3.1.4 成员检查可用于判断数据元素（数据项）是否存在于列表中： 123456&gt;&gt;&gt; tup1 = (1.0, 'welcome')&gt;&gt;&gt; tup2 = ("python", "world")&gt;&gt;&gt; 'welcome' in tup1True&gt;&gt;&gt; 'welcome' not in tup2True 3.1.5 max(tuple) &amp;&amp; min(tuple)max(tuple)方法用于返回列表元素中的最大值，min(tuple)方法用于返回列表元素中的最小值。 12345&gt;&gt;&gt; tup1, tup2 = (&apos;Google&apos;, &apos;Baidu&apos;, &apos;360&apos;), (12, 100, 200)&gt;&gt;&gt; print (&quot;tup1 最大元素值 : &quot;, max(tup1))tup1 最大元素值 : Google&gt;&gt;&gt; print (&quot;tup2 最小元素值 : &quot;, min(tup2))tup2 最小元素值 : 12 注意，在获取列表最大或最小值时，必须保证列表元素类型一致。 1234567&gt;&gt;&gt; tup1, tup2 = ('Google', 'Baidu', '360'), (12, 100, '200')&gt;&gt;&gt; print ("tup1 最大元素值 : ", max(tup1))tup1 最大元素值 : Google&gt;&gt;&gt; print ("tup2 最小元素值 : ", min(tup2))Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: '&lt;' not supported between instances of 'str' and 'int' 3.1.6 元组类型转换tuple( seq )方法用于将列表或字符串转换为元组。 12345678&gt;&gt;&gt; list1 = [123.321, 'Google', 'Baidu', '360']&gt;&gt;&gt; tup1 = tuple(list1)&gt;&gt;&gt; print ("元组元素 : ", tup1)元组元素 : (123.321, 'Google', 'Baidu', '360')&gt;&gt;&gt; str="Hello World"&gt;&gt;&gt; tup2=tuple(str)&gt;&gt;&gt; print ("元组元素 : ", tup2)元组元素 : ('H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd') 3.1.7 元组的更新与删除正如我们之前说的，tuple 是不可变数据类型，其元素值是不允许修改的。 所以，和字符串中一样，修改元组元素操作是非法的： 12345&gt;&gt;&gt; website = ('Google', 'Baidu', '360')&gt;&gt;&gt; website[2] = "Opera"Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'tuple' object does not support item assignment 同理，元组中的元素值是不允许删除的，但我们可以使用 del 语句来删除整个元组 1234567891011&gt;&gt;&gt; website = ('Google', 'Baidu', '360')&gt;&gt;&gt; del website[2]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'tuple' object doesn't support item deletion&gt;&gt;&gt; del website&gt;&gt;&gt; print(website)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;NameError: name 'website' is not defined “可变的”tuple ??? 正如我们前面说的：但是我们要注意，尽管 tuple 元素不可改变，但它可以包含可变对象，比如 list列表等。 先来给出一个样例： 12345&gt;&gt;&gt; tup1 = ('a', 'b', ['A', 'B'])&gt;&gt;&gt; tup1[2][0] = 'E'&gt;&gt;&gt; tup1[2][1] = 'F'&gt;&gt;&gt; print(tup1)('a', 'b', ['E', 'F']) 哎？之前不是说 tuple一旦定义后就不可变了吗？怎么后来又变了？ 表面上看，tuple 的元素确实变了，但其实变的不是 tuple 的元素，而是 list 的元素。tuple 一开始指向的 list 并没有改成别的 list。所以，tuple 所谓的“不变”是说，tuple 的每个元素，指向永远不变。即指向&#39;a&#39;，就不能改成指向&#39;b&#39;，指向一个 list，就不能改成指向其他对象，但指向的这个 list 本身是可变的！]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>List</tag>
        <tag>Tuple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 字符串之 Unicode 编码]]></title>
    <url>%2FPython%2FPython-%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B9%8B-Unicode-%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： 我们已经讲过了，字符串也是一种数据类型，但是字符串比较特殊的是还有一个编码问题。Python 中的字符编码是个老生常谈的话题，这一小节我们围绕字符串来看关于 Python 中的字符编码问题。 Python 字符串之 Unicode 编码1 字符编码1.0 字符与字节字符（chars）是人类能够识别的符号，而这些符号要保存到计算机的存储中或被计算机处理就需要用计算机能够识别处理的字节（byte）来表示。例如：计算机要处理文本，就必须先把文本转换为字节才能处理。 计算机在设计时采用 8 个比特（bit）作为一个字节（byte），也就是 8 位二进制数。所以，一个字节能表示的最大的整数就是255（\b11111111 = \d255，代表最多可映射 255 个符号），如果要表示更大的整数（更多的字符映射），就必须用更多的字节。比如两个字节可以表示的最大整数是65535，4个字节可以表示的最大整数是4294967295等等。 1.1 Unicode 编码的出现何为字符编码？：就是将人类可识别的字符转换为机器可识别的字节码，以及反向过程（解码）。 换句话说，字符编码就是一套字符和数字（字节码）的映射规则。这里的数字（字节码）也被称为代码点（code point），实际上就是十六进制的数字。 而根据字符集以及字符编码字节数的不同，产生很多不同的字符编码： ASCII 编码 由于最初的计算机是美国人发明的，因此，最早只有 127 个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为 ASCII 编码，比如大写字母 A的编码是 65，小写字母 z 的编码是 122。 GB2312 编码 但是要处理中文字符显然一个字节是不够的，至少需要两个字节，而且还不能和 ASCII 编码冲突。所以，中国制定了GB2312编码，用来把中文编码进去。 产生一个问题：你可以想得到的是，全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc-kr里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。 Unicode 编码 基于上述问题，Unicode 应运而生。Unicode 把所有语言都统一到一套编码规则里，这样就不会再有乱码问题了。 Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要 4 个字节）。现代操作系统和大多数编程语言都直接支持 Unicode。 现在，捋一捋 ASCII 编码和 Unicode 编码的区别：ASCII 编码是 1 个字节，而 Unicode 编码通常是 2 个字节。 字母A用ASCII编码是十进制的65，二进制的01000001； 字符0用ASCII编码是十进制的48，二进制的00110000，注意字符&#39;0&#39;和整数0是不同的； 汉字中已经超出了 ASCII 编码的范围，用 Unicode 编码是十进制的20013，二进制的01001110 00101101。 你可以猜测，如果把 ASCII 编码的A用Unicode编码，只需要在前面补 0 就可以，因此，A的 Unicode 编码是00000000 01000001。 综上所述，一个字符通常会有多种采用不用字节数编码的 字符编码。例如，字母 A-Z 都可以用 ASCII 码表示（占用一个字节），也可以用 UNICODE 表示（占两个字节），还可以用后续说到的 UTF-8 表示（占用一个字节）。 1.2 UTF-8 可变长编码新的问题又出现了：如果统一成 Unicode 编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用 Unicode 编码比 ASCII 编码需要多一倍的存储空间，在存储和传输上就十分不划算。 所以，本着节约磁盘空间的目标，又出现了把 Unicode 编码转化为“可变长编码”的UTF-8编码。UTF-8 编码把一个 Unicode 字符根据不同的数字大小编码成 1-6 个字节，常用的英文字母被编码成 1 个字节，汉字通常是 3 个字节，只有很生僻的字符才会被编码成 4-6 个字节。如果你要传输的文本包含大量英文字符，用 UTF-8 编码就能节省空间。如下： 字符 ASCII Unicode UTF-8 A 01000001 00000000 01000001 01000001 中 x 01001110 00101101 11100100 10111000 10101101 从上面的表格还可以发现，UTF-8 编码有一个额外的好处，就是 ASCII 编码实际上可以被看成是 UTF-8 编码的一部分，所以，大量只支持 ASCII 编码的历史遗留软件可以在 UTF-8 编码下继续工作。 1.3 字符编码工作模式搞清楚了ASCII、Unicode 和 UTF-8 的关系，我们就可以总结一下现在计算机系统通用的字符编码工作方式： 在计算机内存中，统一使用 Unicode 编码，当需要保存到硬盘或者需要传输的时候，就转换为 UTF-8 编码。 用记事本编辑的时候，从文件读取的 UTF-8 字符被转换为 Unicode 字符到内存里，编辑完成后，保存的时候再把 Unicode 转换为 UTF-8 保存到文件： 浏览网页的时候，服务器会把动态生成的 Unicode 内容转换为 UTF-8 再传输到浏览器： 所以你看到很多网页的源码上会有类似&lt;meta charset=&quot;UTF-8&quot; /&gt;的信息，表示该网页正是用的 UTF-8 编码。 1.4 编码和解码编码(encode)：将 Unicode 字符串（中的代码点）转换特定字符编码对应的字节串的过程和规则。 解码(decode)：将特定字符编码的字节串转换为对应的 Unicode 字符串（中的代码点）的过程和规则。 2 Python 字符串编码2.1 Python 源代码文件的执行过程我们知道，磁盘上的文件都是以二进制字节码格式存放的，其中文本文件都是以某种特定编码的字节形式存放的。对于程序源代码文件的字符编码是由编辑器指定的，比如我们使用 Pycharm 来编写 Python 程序时会指定工程编码和文件编码为 UTF-8（Windows Eclipse 中编写会被编码为：GBK），那么 Python 代码被保存到磁盘时就会被转换为 UTF-8 编码对应的字节（encode 过程）后写入磁盘。 当执行 Python 代码文件中的代码时，Python 解释器在读取 Python 代码文件中的字节串之后，需要将其转换为 UNICODE 字符串（decode 过程）之后才执行后续操作。 2.2 默认编码如果我们没有在代码文件开始的部分指定字符编码，Python 解释器就会使用哪种字符编码把从代码文件中读取到的字节转换为 UNICODE 代码点呢？ Python 解释器提供了 “默认编码（default encoding）” 机制，来处理字节码。 Python2 的 默认编码 是 ASCII，故不能识别中文字符，需要显式指定字符编码；Python3 的 默认编码 为 Unicode，可以识别中文字符。 12345678910# 通过 sys.getdefaultencoding() 来获取 Python 解释器中默认编码：&gt;&gt;&gt; # Python2&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()'ascii' &gt;&gt;&gt; # Python3&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()'utf-8' 1）UnicodeEncodeError 广为人知的 Python 中文字符问题可以总结为一句话：当无法通过默认的字符编码对字节进行转换时，就会出现解码错误(UnicodeEncodeError)。 对于 Python2 来讲，Python 解释器在读取到中文字符的字节码尝试解码操作时，会先查看当前代码文件头部是否有指明当前代码文件中保存的字节码对应的字符编码是什么。如果没有指定则使用默认字符编码 “ASCII” 进行解码导致解码失败，导致如下错误： 12SyntaxError: Non-ASCII character '\xc4' in file xxx.py on line 11, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details 对于 Python3 来讲，执行过程是一样的，只是 Python3 的解释器以 “UTF-8” 作为默认编码，但是这并不表示可以完全兼容中文问题。比如我们在 Windows 上进行开发时，经常遇到 Python工程及代码文件使用 GBK 编码的情况，也就是说 Python 代码文件是被转换成 GBK 格式的字节码保存到磁盘中的。Python3 的解释器执行该代码文件时，试图用 UTF-8 进行解码操作时，同样会解码失败，导致如下错误： 12SyntaxError: Non-UTF-8 code starting with '\xc4' in file xxx.py on line 11, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details 2）Solution 创建一个 Python 工程之后先确认该工程的字符编码是否已经设置为 UTF-8； 为了兼容 Python2 和 Python3，在代码头部声明字符编码：-*- coding:utf-8 -*-。 2.3 字符串和字节串 UNICDOE 才是真正的字符串，而用 ASCII、UTF-8、GBK 等字符编码表示的是字节串。 我们知道，字符是以字节的形式保存在文件中的。因此当我们在文件中定义的字符串被当做字节串也是可以理解的。但是，我们需要的是字符串，而不是字节串（是给计算机用来进行存储、处理和传输）。一个优秀的编程语言，应该严格区分两者的关系。 遗憾的是，很多编程语言试图混淆 “字符串” 和 “字节串”，他们把字节串当做字符串来使用，PHP、Python2 就属于这种编程语言。而在 Python3 中可能已经意识到之前的错误，重新进行了字符串的实现，所以开始明确的区分字符串与字节。 最能说明这个问题的操作就是 取一个包含中文字符的字符串的长度： 1）Python 2 1234567891011121314# 字节串，长度为字节个数 = len('Hello,')+len('中国') = 6+2*2 = 10&gt;&gt;&gt; str1 = 'Hello,中国'# 字符串，长度为字符个数 = len('Hello,') + len('中国') = 6+2 = 8&gt;&gt;&gt; str2 = u'Hello,中国'# 其实 str2 的定义方式是 str3 定义方式的简写，都是将一个 GBK 编码的字节串解码（decode）为一个 Uniocde 字符串&gt;&gt;&gt; str3 = unicode(str1, 'gbk')&gt;&gt;&gt; &gt;&gt;&gt; print(type(str1), len(str1))(&lt;type 'str'&gt;, 10)&gt;&gt;&gt; print(type(str2), len(str2))(&lt;type 'unicode'&gt;, 8)&gt;&gt;&gt; print(type(str3), len(str3))(&lt;type 'unicode'&gt;, 8)&gt;&gt;&gt; 可以发现，str类型指代的是字节串，而经过unicode编码之后的才是字符串。所以要想 Python2 &amp;&amp; Python3 兼容必须指定其编码方式为# -*- coding:utf-8 -*-。 2）Python3 1234567891011&gt;&gt;&gt; str1 = &apos;Hello,中国&apos;&gt;&gt;&gt; str2 = u&apos;Hello,中国&apos;&gt;&gt;&gt; str3 = str1.encode(&apos;gbk&apos;)&gt;&gt;&gt; &gt;&gt;&gt; print(type(str1), len(str1))&lt;class &apos;str&apos;&gt; 8&gt;&gt;&gt; print(type(str2), len(str2))&lt;class &apos;str&apos;&gt; 8&gt;&gt;&gt; print(type(str3), len(str3))&lt;class &apos;bytes&apos;&gt; 10&gt;&gt;&gt; 可以发现，已经可以明确字符串和字节串了。 2.4 字符串编码转换结合上面我们提到过的解码编码，这一模块我们来看 Python 中的编码转换。 UNICODE 字符串可以与任意字符编码的字节进行相互转换，如图： 1）Python2 首先，我们来简单看一下 Python 中的字符串进行字符编码转换过程是： [字节串] –&gt; [decode(‘原来的字符编码’)] –&gt; [Unicode 字符串] –&gt; [encode(‘新的字符编码’)] –&gt; [字节串] 12345678# -*- coding:utf-8 -*- utf_8_str = '我要学中文'gbk_str = utf_8_str.decode('utf-8').encode('gbk')print(gbk_str.decode('gbk')) # 输出结果：我要学中文 2）Python3 而 Python3 中定义的字符串默认就是 unicode，因此不需要先解码，可以直接编码成新的字符编码： 1234567891011121314151617&gt;&gt;&gt; # -*- coding:utf-8 -*-&gt;&gt;&gt; utf_8_str = '我要学中文'&gt;&gt;&gt; gbk_str = utf_8_str.encode('gbk')&gt;&gt;&gt; print(gbk_str.decode('gbk'))# 输出结果：&gt;&gt;&gt; 我要学中文&gt;&gt;&gt; 'ABC'.encode('ascii')b'ABC'&gt;&gt;&gt; '中文'.encode('utf-8')b'\xe4\xb8\xad\xe6\x96\x87'# 中文编码的范围超过了 ASCII 编码的范围，报错&gt;&gt;&gt; '中文'.encode('ascii')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128) 对于单个字符的编码，Python 提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符： 12345678&gt;&gt;&gt; ord('A')65&gt;&gt;&gt; ord('中')20013&gt;&gt;&gt; chr(66)'B'&gt;&gt;&gt; chr(25991)'文' 如何获取字符串中字符的整数编码： 12345&gt;&gt;&gt; str = "中国"&gt;&gt;&gt; str.encode("unicode_escape")b'\\u4e2d\\u56fd'&gt;&gt;&gt; b'\\u4e2d\\u56fd'.decode("unicode_escape")'中国' 知道了字符串中字符的整数编码，这里我们给出一个等价写法： 123456&gt;&gt;&gt; str1 = "中文"&gt;&gt;&gt; str2 = "\u4e2d\u6587"&gt;&gt;&gt; print(str1 == str2)True&gt;&gt;&gt; print(type(str1), type(str2))&lt;class 'str'&gt; &lt;class 'str'&gt; 我们再来看如何解码： 1234&gt;&gt;&gt; b'ABC'.decode('ascii')'ABC'&gt;&gt;&gt; b'\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')'中文' 如果bytes（字节串）中包含无法解码的字节，decode()方法会报错： 1234&gt;&gt;&gt; b'\xe4\xb8\xad\xff'.decode('utf-8')Traceback (most recent call last): ...UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 3: invalid start byte 如果bytes（字节串）中只有一小部分无效的字节，可以传入errors=&#39;ignore&#39;忽略错误的字节： 12&gt;&gt;&gt; b'\xe4\xb8\xad\xff'.decode('utf-8', errors='ignore')'中']]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Character Encoding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 字符串之格式化输出]]></title>
    <url>%2FPython%2FPython-%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B9%8B%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： Python 的字符串格式化传统有两种方式：％-formatting 方式，format 方式。除此之外，从 Python 3.6 开始映入了一种新的格式化方法：f-string，与传统格式化方法相比，f-string 不仅更易读，更简洁，不易出错，而且速度更快！本文将详细介绍三种格式化方法的用法。 Python 字符串格式化1. % 格式符1.1 语法规则% 格式符语法规则如下： %[(name)][flags][width].[precision]typecode 1.2 参数详解1）(name) (name) 为命名参数，可选。用于选择指定的 key，即用于映射变量(参数为字典) 。 1234&gt;&gt;&gt; name = "python"&gt;&gt;&gt; age = 20&gt;&gt;&gt; print("%(name)s:%(age)d" % &#123;'name':name, 'age':age&#125;)python:20 2）flags flags 为对齐方式以及数值符号设置参数，可选。可供选择的值有：+，-，空格或 0。 + ：右对齐；正数前加正好，负数前加负号； -：左对齐；正数前无符号，负数前加负号； 空格：右对齐；正数前加空格，负数前加负号； 0：右对齐；正数前无符号，负数前加负号；用 0 填充空白处。 3）width width 为显示宽度设置参数，可选。 4）.precision .precision 为小数点后精度设置参数，可选。 5）typecode typecode 为类型码参数，必选。常用类型码如下： s：字符串 类型码 (采用 str() 的显示 )； r：字符串 类型码 (采用 repr() 的显示 )； d：十进制整数 类型码，将整数、浮点数转换成 十 进制表示； i：十进制整数 类型码，将整数、浮点数转换成 十 进制表示； o：八进制整数 类型码，将整数转换成 八 进制表示; x：十六进制整数 类型码，将整数转换成 十六 进制表示; e：指数 类型码(基底写为 e)，将整数、浮点数转换成科学计数法； E：指数 类型码(基底写为 E)，将整数、浮点数转换成科学计数法； f：浮点数 类型码，将整数、浮点数转换成浮点数表示； F：浮点数 类型码，将整数、浮点数转换成浮点数表示； g：指数或浮点数类型码，自动调整将整数、浮点数转换成浮点型或科学计数法表示，超过 6 位数用基底为 e 科学计数法； G：指数或浮点数类型码，自动调整将整数、浮点数转换成浮点型或科学计数法表示，超过 6 位数用基底为 E 科学计数法； %%：字符 “%” 类型码。 1.3 测试样例1）flags &amp;&amp; width 12345678910&gt;&gt;&gt; name = 'python'&gt;&gt;&gt; test = 1000&gt;&gt;&gt; print("%(name)s:%(number)+10d" % &#123;'name':name, 'number':test&#125;)python: +1000&gt;&gt;&gt; print("%(name)s:%(number)-10d" % &#123;'name':name, 'number':test&#125;)python:1000&gt;&gt;&gt; print("%(name)s:%(number) 10d" % &#123;'name':name, 'number':test&#125;)python: 1000&gt;&gt;&gt; print("%(name)s:%(number)010d" % &#123;'name':name, 'number':test&#125;)python:0000001000 2）.precision 12345&gt;&gt;&gt; f = 123.321&gt;&gt;&gt; print("precision test: %(p)f" % &#123;'p':f&#125;)precision test: 123.321000&gt;&gt;&gt; print("precision test: %(p).2f" % &#123;'p':f&#125;)precision test: 123.32 3）typecode 12345678910&gt;&gt;&gt; a,b,c = 'TheMusicIsLoud', 20, 12.3&gt;&gt;&gt; print("my name is: %s; age: %d; level: %+10.4f" % (a,b,c))my name is: TheMusicIsLoud; age: 20; level: +12.3000&gt;&gt;&gt; print("my name is: %s; age: %d; level: %-10.4f" % (a,b,c))my name is: TheMusicIsLoud; age: 20; level: 12.3000&gt;&gt;&gt; print("%05x" % -12)-000c&gt;&gt;&gt; print("%05o" % -12)-0014 1.4 缺点我们可以发现，一旦我们开始使用多个参数和更长的字符串，代码将很快变得不太容易阅读。事情已经开始显得有点凌乱： 1234567first_name = "Eric"last_name = "Idle"age = 27profession = "Developer"affiliation = "Easy Python"print(("Hello, %s %s. You are %s. You are a %s. You were a member of %s." %(first_name, last_name, age, profession, affiliation))) 冗长，不能正确显示元组或字典都是它的缺点。 2. Format 方法Python 中除了上述我们介绍过的 % 格式化方法，还可以通过 {} 和 : 来代替以前的 % 完成格式化输出，增强了字符串格式化的功能。 2.1 语法规则format 格式符语法规则如下： [fill][align][sign][#][width][,][.precision][type] 2.2 参数详解1）fill fill为空白处可填充字符设置参数，可选。 2）align align为对齐方式设置参数，可选，需要配和 width 参数使用。 &lt;：内容左对齐； &gt;：内容右对齐(默认)； =：内容右对齐，将符号放置在填充字符的左侧，且只对数字类型有效。即，符号+填充物+数字。 ^：内容居中。 3）sign sign为有无符号数字设置参数，可选。 +：正号加正，负号加负； -：正号不变，负号加负； 空格：正号空格，负号加负。 4）# #为是否显示进制前缀设置参数，可选。对于二进制、八进制、十六进制，如果加上 #，会显示 0b/0o/0x，否则不显示。 5）, ,为数字分割符设置参数，可选。如：1,000,000。 6）width width 为显示宽度设置参数，可选。 7）.precision .precision 为小数点后精度设置参数，可选。 8）typecode typecode 为类型码参数，可选（注意这里变为可选参数）。常用类型码如下： 空白：未指定类型，则默认是None，同 s； s：字符串 类型码 (采用 str() 的显示 )； r：字符串 类型码 (采用 repr() 的显示 )； d：十进制整数 类型码，将整数、浮点数转换成 十 进制表示； i：十进制整数 类型码，将整数、浮点数转换成 十 进制表示； o：八进制整数 类型码，将整数转换成 八 进制表示; x：十六进制整数 类型码，将整数转换成 十六 进制表示; e：指数 类型码(基底写为 e)，将整数、浮点数转换成科学计数法； E：指数 类型码(基底写为 E)，将整数、浮点数转换成科学计数法； f：浮点数 类型码，将整数、浮点数转换成浮点数表示； F：浮点数 类型码，将整数、浮点数转换成浮点数表示； g：指数或浮点数类型码，自动调整将整数、浮点数转换成浮点型或科学计数法表示，超过 6 位数用基底为 e 科学计数法； G：指数或浮点数类型码，自动调整将整数、浮点数转换成浮点型或科学计数法表示，超过 6 位数用基底为 E 科学计数法； %：显示百分比（默认显示小数点后 6 位） 类型码。 2.3 测试样例1）fill &amp;&amp; align &amp;&amp; width 123456789&gt;&gt;&gt; s1 = "&#123;:*^20&#125;".format('welcome')&gt;&gt;&gt; print(s1)******welcome*******&gt;&gt;&gt; s2 = "&#123;:*&gt;20s&#125;".format('welcome')&gt;&gt;&gt; print(s2)*************welcome&gt;&gt;&gt; s3 = "&#123;:*&lt;20s&#125;".format('welcome')&gt;&gt;&gt; print(s3)welcome************* 3）# 我们还可以通过引用其索引（或变量名称）来以任何顺序引用变量： 123456789&gt;&gt;&gt; a1 = "numbers: &#123;:b&#125;,&#123;:o&#125;,&#123;:d&#125;,&#123;:x&#125;,&#123;:X&#125;, &#123;:%&#125;,&#123;:c&#125;".format(16, 15, 14, 13, 12, 0.87623,99)&gt;&gt;&gt; a2 = "numbers: &#123;0:#b&#125;,&#123;0:#o&#125;,&#123;0:#d&#125;,&#123;0:#x&#125;,&#123;0:#X&#125;, &#123;0:%&#125;,&#123;1:c&#125;".format(15,99)&gt;&gt;&gt; a3 = "numbers: &#123;num:b&#125;,&#123;num:o&#125;,&#123;num:d&#125;,&#123;num:x&#125;,&#123;num:X&#125;, &#123;num:%&#125;,&#123;cc:c&#125;".format(num=15,cc=99)&gt;&gt;&gt; print(a1)numbers: 10000,17,14,d,C, 87.623000%,c&gt;&gt;&gt; print(a2)numbers: 0b1111,0o17,15,0xf,0XF, 1500.000000%,c&gt;&gt;&gt; print(a3)numbers: 1111,17,15,f,F, 1500.000000%,c 4）, &amp;&amp; .precision 123456&gt;&gt;&gt; s1 = '&#123;:,d&#125;'.format(100000000)&gt;&gt;&gt; s2 = '&#123;:.2f&#125;'.format(123.321)&gt;&gt;&gt; print(s1)100,000,000&gt;&gt;&gt; print(s2)123.32 5）常用格式化 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; tp1 = "i am &#123;&#125;, age &#123;&#125;, &#123;&#125;".format("seven", 18, 'alex')&gt;&gt;&gt; tp2 = "i am &#123;&#125;, age &#123;&#125;, &#123;&#125;".format(*["seven", 18, 'alex'])&gt;&gt;&gt; tp3 = "i am &#123;0&#125;, age &#123;1&#125;, really &#123;0&#125;".format("seven", 18)&gt;&gt;&gt; tp4 = "i am &#123;0&#125;, age &#123;1&#125;, really &#123;0&#125;".format(*["seven", 18])&gt;&gt;&gt; tp5 = "i am &#123;name&#125;, age &#123;age&#125;, really &#123;name&#125;".format(name="seven", age=18)&gt;&gt;&gt; tp6 = "i am &#123;name&#125;, age &#123;age&#125;, really &#123;name&#125;".format(**&#123;"name": "seven", "age": 18&#125;)&gt;&gt;&gt; tp7 = "i am &#123;0[0]&#125;, age &#123;0[1]&#125;, really &#123;0[2]&#125;".format([1, 2, 3], [11, 22, 33])&gt;&gt;&gt; tp8 = "i am &#123;:s&#125;, age &#123;:d&#125;, money &#123;:f&#125;".format("seven", 18, 88888.1)&gt;&gt;&gt; tp9 = "i am &#123;:s&#125;, age &#123;:d&#125;".format(*["seven", 18])&gt;&gt;&gt; tp10 = "i am &#123;name:s&#125;, age &#123;age:d&#125;".format(name="seven", age=18)&gt;&gt;&gt; tp11 = "i am &#123;name:s&#125;, age &#123;age:d&#125;".format(**&#123;"name": "seven", "age": 18&#125;)&gt;&gt;&gt; print(tp1)i am seven, age 18, alex&gt;&gt;&gt; print(tp2)i am seven, age 18, alex&gt;&gt;&gt; print(tp3)i am seven, age 18, really seven&gt;&gt;&gt; print(tp4)i am seven, age 18, really seven&gt;&gt;&gt; print(tp5)i am seven, age 18, really seven&gt;&gt;&gt; print(tp6)i am seven, age 18, really seven&gt;&gt;&gt; print(tp7)i am 1, age 2, really 3&gt;&gt;&gt; print(tp8)i am seven, age 18, money 88888.100000&gt;&gt;&gt; print(tp9)i am seven, age 18&gt;&gt;&gt; print(tp10)i am seven, age 18&gt;&gt;&gt; print(tp11)i am seven, age 18 2.4 缺点尽管一定程度上增强了 %-formatting 功能，但当我们开始使用多个参数和更长的字符串，还是具有冗长问题： 1234567first_name = "Eric"last_name = "Idle"age = 27profession = "Developer"affiliation = "Easy Python"print("Hello, &#123;first_name&#125;&#123;last_name&#125;. You are &#123;age&#125;. You are a &#123;profession&#125;. \You were a member of &#123;affiliation&#125;.".format(first_name = first_name, last_name=last_name, age=age, profession=profession, affiliation=affiliation)) 3. f-stringPython 自 Python 3.6 开始在标准库加入一种改进 Python 格式字符串的新方法：f-string。我们可以在 PEP 498 中阅读所有内容。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>String</tag>
        <tag>FormatOutput</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基本数据类型]]></title>
    <url>%2FPython%2FPython-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： 了解了 Python 基础语法规则之后，这一小节我们开始学习 Python 基本数据类型。 Python 基础数据类型Python3 中有六个标准的数据类型： Number（数字） String（字符串） List（列表） Tuple（元组） Set（集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）； 可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 关于数据类型的可变和不可变属性可参考：Python 中的可变对象和不可变对象，等学过 Tuple（元组）、List（列表）之后，可以返回来看这一部分内容，可以帮助我们进一步掌握 6 中标准类型的用法。 在开始学习数据类型之前，我们先来看 Python 中的变量： 1. Python 变量Python 中的变量不需要声明，赋值后可直接使用，变量只有赋值以后该变量才会被创建。 在 Python 中，变量就是变量，它没有类型，我们所说的 “类型” 是变量指向的内存中对象的类型。 1.1 变量赋值数学等号（=）用来给变量赋值。 等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值。例如： 123counter = 100 # 整型变量miles = 1000.0 # 浮点型变量name = "python" # 字符串 1.1.1 多变量赋值Python 允许你同时为多个变量赋值。例如： 1a = b = c = 1 或者： 1a, b, c = 1, 2, "python" # 用法唯一，没有 a, b, c = 1；a, b, c = 1, 2 1.1.2 动态语言特性Python 中可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量，例如： 123test = 100 # 整型变量test = 1000.0 # 浮点型变量test = &quot;python&quot; # 字符串 这种变量本身类型不固定的语言称之为 动态语言，与之对应的是 静态语言。静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错（例如：Java，C 等）。 1.1.3 变量的内存表示当我们定义： 1test = &apos;ABC&apos; 时，Python 解释器干了两件事情： 在内存中创建了一个&#39;ABC&#39;的字符串； 在内存中创建了一个名为test的变量，并把它指向&#39;ABC&#39;。 当然，也可以把一个变量 test赋值给另一个变量 test2，这个操作实际上是把变量 b 指向变量 a 所指向的数据，例如下面的代码： 12345test1 = &apos;ABC&apos;test2 = test1test1 = &apos;XYZ&apos;print(test2) # 输出：ABC 下面我们将开始分别说明上述提到的六种标准数据类型：Number（数字）、String（字符串）、Tuple（元组）、List（列表）、Set（集合）、Dictionary（字典）。 2. Python 数据类型2.1 Number（数字）Number 数据类型用于存储数值，Python 中支持四种不同的数值类型：整数（int）、布尔型（bool）、浮点数（float）和复数（complex）。 2.1.1 数据类型1）整型（int） Python 只有一种整数类型 int，表示为长整型。Python3 整型是没有限制大小的，可以当作 Long 类型使用，所以 Python3 没有 Python2 的 Long 类型。 也就是说 int 可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样，例如：1，100，-8080，0，等等。 计算机由于使用二进制，所以有时候用十六进制、八进制或者二进制表示整数比较方便： 1234567891011&gt;&gt;&gt; number = 0xA0F # 十六进制&gt;&gt;&gt; number2575&gt;&gt;&gt; number=0o37 # 八进制&gt;&gt;&gt; number31&gt;&gt;&gt; number = 0b1111111 # 二进制&gt;&gt;&gt; number127 2）浮点型（float） 浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，比如：$1.23 ×10^9 $ 和 $12.3 × 10^8​$ 是完全相等的。 浮点数可以用数学写法，如1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把 10 用 e 替代，$1.23 × 10^9$ 就是1.23e9，或者12.3e8，0.000012 可以写成1.2e-5，等等。 3）布尔型（bool） Python3 中，布尔值有True、False两种，并把 True 和 False 定义成关键字了（请注意大小写）。并且 bool 类型继承自int， 故它们的值还对应 True == 1 和 False == 0，可以直接和数字相加。 123456&gt;&gt;&gt; print(True==1)True&gt;&gt;&gt; print(False==0)True&gt;&gt;&gt; print(True + 1)2 也可以通过布尔运算（比较运算符：&gt;、&lt;、==）计算出来： 12345678&gt;&gt;&gt; TrueTrue&gt;&gt;&gt; FalseFalse&gt;&gt;&gt; 3 &gt; 2True&gt;&gt;&gt; 3 &gt; 5False 布尔值还可以进行and、or和not逻辑运算： and运算是&quot;与运算&quot;，只有所有都为True，and运算结果才是True` or运算是”或运算”，只要其中有一个为True，or运算结果就是True not运算是”非运算”，它是一个单目运算符，把True变成False，False变成True 1234567891011121314&gt;&gt;&gt; print(True and "test")test&gt;&gt;&gt; print(False and "test")False&gt;&gt;&gt; print(False or "test")test&gt;&gt;&gt; print(True or "test")True&gt;&gt;&gt; print(not "test")False&gt;&gt;&gt; print(not 0)True&gt;&gt;&gt; print(not 1)False 注意，布尔值还常用于条件判断中： 1234567891011121314# 条件判断中以下数值会被认为是 False：# 为零的数：0 or 0.0；# 空字符串：''，""；# 空值：None；# 空集合：()，[]，&#123;&#125;；# 其他的值都认为是 True。&gt;&gt;&gt; if ():... print(True)... else:... print(False)...False&gt;&gt;&gt; 注意：Python2 中是没有布尔型的，它用数字 0 表示 False，用 1 表示 True。 4）复数（complex ） Python还支持复数，复数由实数部分和虚数部分构成，可以用 a + bj ,或者 complex(a,b) 表示，复数的实部 a 和虚部 b 都是浮点型。 例如： 12345a = 1 + 2jb = 1.1 + 2.2j&gt;&gt;&gt; a = complex(1,2)&gt;&gt;&gt; print(a)(1+2j) 2.1.2 Number 相关运算1）type(object) &amp;&amp; isinstance(objecrt, class_or_tuple) 内置的 type() 函数可以用来查询变量所指的对象类型： 1234&gt;&gt;&gt; a, b, c, d = 20, 5.5, True, 4+3j&gt;&gt;&gt; a,b,c,d = 12, 2.3, True, 1+2j&gt;&gt;&gt; print(type(a), type(b), type(c), type(d))&lt;class 'int'&gt; &lt;class 'float'&gt; &lt;class 'bool'&gt; &lt;class 'complex'&gt; 此外还可以用 isinstance() 来判断： 12345&gt;&gt;&gt; isinstance(a, int)True# 可以看出 bool 型也是 int 型的一种：&gt;&gt;&gt; isinstance(c, int)True isinstance() 和 type() 的区别在于： type() 不会认为子类是一种父类类型。 isinstance() 会认为子类是一种父类类型。 123456&gt;&gt;&gt; type(c) == boolTrue&gt;&gt;&gt; type(c) == intFalse&gt;&gt;&gt; isinstance(c, int)True 2）del 我们知道当变量被赋值时，Number 对象将会被创建： 123&gt;&gt;&gt; number1 = 10&gt;&gt;&gt; number2 = 1.2&gt;&gt;&gt; number3 = True 我们还可以使用 del 语句删除单个或多个对象的引用，例如： 123456&gt;&gt;&gt; del number1&gt;&gt;&gt; del number2, number3&gt;&gt;&gt; print(number1)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;NameError: name 'number2' is not defined 3）数据类型转换 有时候，我们需要对数据内置的类型进行转换。对于数据类型的转换，你只需要将数据类型作为函数名即可。 int(x) 将 x 转换为一个整数。 float(x) 将 x 转换到一个浮点数。 complex(x) 将 x 转换到一个复数，实数部分为 x，虚数部分为 0。 complex(x, y) 将 x 和 y 转换到一个复数，实数部分为 x，虚数部分为 y。x 和 y 是数字表达式。 注意：int、float 和 bool可以相互转换；int、float 和 bool 均可以转换为 complex，但不可以反转。 4）数值计算 Python 解释器可以作为一个简单的计算器，您可以在解释器里输入一个数学表达式，它将输出表达式的值。 1234567891011121314&gt;&gt;&gt;5 + 4 # 加法9&gt;&gt;&gt; 4.3 - 2 # 减法2.3&gt;&gt;&gt; 3 * 7 # 乘法21&gt;&gt;&gt; 2 / 4 # 除法，得到一个浮点数0.5&gt;&gt;&gt; 2 // 4 # 除法，得到一个整数0&gt;&gt;&gt; 17 % 3 # 取余 2&gt;&gt;&gt; 2 ** 5 # 幂运算32 注意：int 和 float 进行混合计算时，会得到浮点数： 123456&gt;&gt;&gt; 15 // 3.05.0&gt;&gt;&gt; 12 / True12.0&gt;&gt;&gt; 13 - 2.011.0 5）常用数学函数 使用数学函数前，我们需要导入想要的模块： 1import math 下面来看常用数学函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 常用数学常量&gt;&gt;&gt; math.pi3.141592653589793&gt;&gt;&gt; math.e2.718281828459045# 返回数字的绝对值&gt;&gt;&gt; a = -10&gt;&gt;&gt; abs(a)10&gt;&gt;&gt; math.fabs(a)10.0# 返回数字的向上取整结果&gt;&gt;&gt; b = 12.3&gt;&gt;&gt; math.ceil(b)13&gt;&gt;&gt; c = 12.8&gt;&gt;&gt; math.ceil(c)13# # 返回数字的向下取整结果&gt;&gt;&gt; math.floor(c)12&gt;&gt;&gt; math.floor(b)12# 返回 e 的 x 次幂&gt;&gt;&gt; math.exp(1)2.718281828459045# log 函数&gt;&gt;&gt; math.log(100)4.605170185988092&gt;&gt;&gt; math.log(100, 10)2.0# 返回以 10 为基数的 x 的对数&gt;&gt;&gt; math.log10(100)2.0# 返回给定参数的最大值，参数可以为序列。&gt;&gt;&gt; max(1, 2.0, 3, 4)4# 返回给定参数的最小值，参数可以为序列。&gt;&gt;&gt; max(1, 2.0, 3, 4)1# pow(x,y):返回 x**y 运算后的值&gt;&gt;&gt; pow(2,4)16# round(x,n):返回浮点数 x 的四舍五入值，如给出 n 值，则代表舍入到小数点后的位数&gt;&gt;&gt; round(2.71828,3)2.718# sqrt(x):返回数字 x 的平方根&gt;&gt;&gt; math.sqrt(100)10.0 三角函数相关方法： 123456789101112131415161718import math# math.sin(x)：返回 x 弧度的正弦值# math.cos(x)：返回 x 弧度的余弦值# math.tan(x)：返回 x 弧度的正切值# math.asin(x)：返回 x 的反正弦弧度值# math.acos(x)：返回 x 的反余弦弧度值# math.atan(x)：返回 x 的反正切弧度值# math.degrees(x)：将弧度转换为角度&gt;&gt;&gt; math.degrees(math.pi/2)90.0# math.radians(x)：将角度转换为弧度&gt;&gt;&gt; math.radians(90)1.5707963267948966&gt;&gt;&gt; math.pi/21.5707963267948966 2.2 String（字符串）字符串是 Python 中最常用的数据类型，是以单引号 &#39; 或双引号 &quot; 括起来的任意文本，同时使用反斜杠 \ 转义特殊字符。 创建字符串很简单，只要为变量分配一个值即可。例如： 12str1 = &apos;abc&apos;str2 = &apos;XYZ&apos; &#39;&#39;或&quot;&quot;本身只是一种表示方式，不是字符串的一部分，因此，字符串&#39;abc&#39;只有a，b，c这3个字符。 注意，Python 不支持单独的字符类型（char），一个字符就是长度为 1 的字符串。 2.2.1 单引号 &#39; 或双引号 &quot; 使用**如果&#39;本身也是字符串中的一个字符，那就可以用&quot;&quot;括起来，比如 &quot;I&#39;m OK&quot;。当然如果 &quot; 也是一个字符的话，可以用&#39;&#39; 括起来，&#39;I&quot;m OK&#39;。但切记不要混合使用： 1&quot;I&apos;m OK&apos; 如果字符串内部既包含&#39;又包含&quot;怎么办？可以用转义字符\来标识，比如： 1&apos;I\&apos;m \&quot;OK\&quot;!&apos; 表示的字符串内容是： 1I&apos;m &quot;OK&quot;! 2.2.2 字符串索引与切片Python 中的字符串有两种索引方式，从左往右以 0 开始，从右往左以 -1 开始。可以使用 索引以及方括号 来截取字符串： 1234567str = 'Welcome to Python world' print (str) # 输出字符串print (str[0:-1]) # 输出第一个到倒数第二个的所有字符print (str[0]) # 输出字符串第一个字符print (str[2:5]) # 输出从第三个开始到第五个的字符print (str[2:]) # 输出从第三个开始的后的所有字符 执行结果如下： 12345Welcome to Python worldWelcome to Python worlWlcolcome to Python world 请注意，字符串是不可变类型，故我们不能对索引到的字符串字符进行修改或删除： 12345678910&gt;&gt;&gt; str = "welcome"&gt;&gt;&gt; str[2] = '3'Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'str' object does not support item assignment&gt;&gt;&gt; del str1[2]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'str' object doesn't support item deletion 2.2.3 字符串连接123456789&gt;&gt;&gt; print("that" "is" "good")thisisgood# 等价于：&gt;&gt;&gt; print("that" + "is" + "good")thisisgood&gt;&gt;&gt; print("good" * 2)goodgood 2.2.4 转义字符转义字符\可以转义很多字符。比如，\n表示换行；\t表示制表符；\r表示回车；字符\本身也要转义，所以\\表示的字符就是\。可以在 Python 的交互式命令行用 print() 打印字符串看看： 12345678&gt;&gt;&gt; print('I\'m ok.')I'm ok.&gt;&gt;&gt; print('I\'m learning\nPython.')I'm learningPython.&gt;&gt;&gt; print('\\\n\\')\\ 转义字符\可以用来转义，而使用 r （R）可以让\不发生转义。。 如 r&quot;this is a line with \n&quot; 则 \n 会显示，并不是换行。 2.2.5 字符串内换行如果字符串内部有很多换行，用 \n 写在一行里不好阅读，为了简化，Python允许用&#39;&#39;&#39;...&#39;&#39;&#39; 或 &quot;&quot;&quot;...&quot;&quot;&quot; 的格式表示多行内容，可以自己试试： 123456&gt;&gt;&gt; print('''line1... line2... line3''')line1line2line3 &#39;&#39;&#39;...&#39;&#39;&#39; 的书写方式让开发人员从引号和特殊字符串的泥潭里面解脱出来，自始至终保持一小块字符串（特殊字符串）的格式，即所见即所得。一个典型的用例是，当你需要一块 HTML 或者 SQL 时，这时用字符串组合，特殊字符串转义将会非常的繁琐。 123456789101112131415errHTML = '''&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;Friends CGI Demo&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;&lt;H3&gt;ERROR&lt;/H3&gt;&lt;B&gt;%s&lt;/B&gt;&lt;P&gt;&lt;FORM&gt;&lt;INPUT TYPE=button VALUE=BackONCLICK="window.history.back()"&gt;&lt;/FORM&gt;&lt;/BODY&gt;&lt;/HTML&gt;'''cursor.execute('''CREATE TABLE users ( login VARCHAR(8), uid INTEGER,prid INTEGER)''') 注意，&#39;&#39;&#39;...&#39;&#39;&#39; 也可以和 r(R) 组合，用于消掉多行字符串中的转义。 2.2.6 字符串运算符假设实例变量 a 值为字符串 “Hello”，b 变量值为 “Python”： 12345678910111213141516171819202122232425262728&gt;&gt;&gt; a = &apos;Hello&apos;&gt;&gt;&gt; b = &apos;Python&apos;# 字符串连接: `+`&gt;&gt;&gt; print(a + b)HelloPython# 重复输出字符串: `*`&gt;&gt;&gt; print(a*2)HelloHello# 通过索引获取字符串中字符: `[]`&gt;&gt;&gt; print(a[3])l# 截取字符串：`[:]`&gt;&gt;&gt; print(a[1:3])el# 用于判断字符串是否包含某个子串：# 如果字符串中包含给定的字符返回 True :`in`&gt;&gt;&gt; print (&apos;H&apos; in a)True&gt;&gt;&gt; print (&apos;Hel&apos; in a)True# 如果字符串中不包含给定的字符返回 True ：`not in`&gt;&gt;&gt; print (&apos;H&apos; not in a)False String 类型转化： str() 函数可以将对象（数字、列表、元组、字典、集合等）转化为适于人阅读的形式。 123456789101112&gt;&gt;&gt; str(True)'True'&gt;&gt;&gt; str(123.32)'123.32'&gt;&gt;&gt; str([1,2,3,4])'[1, 2, 3, 4]'&gt;&gt;&gt; str(("Google","Opera"))"('Google', 'Opera')"&gt;&gt;&gt; str(&#123;"Google":1,"Opera":2&#125;)"&#123;'Google': 1, 'Opera': 2&#125;"&gt;&gt;&gt; str(&#123;"Google", "Opera"&#125;)"&#123;'Google', 'Opera'&#125;" 2.2.7 字符串常用内建函数1）len( s ) len()方法返回对象（字符串、列表、元组等）长度或项目个数。 123&gt;&gt;&gt;str = "python"&gt;&gt;&gt; len(str) # 字符串长度6 2）str.count(sub, start= 0,end=len(string)) count() 方法用于统计字符串里某个子串出现的次数。可选参数为在字符串搜索的开始与结束位置： sub – 搜索的子字符串 start – 字符串开始搜索的位置。默认为第一个字符,第一个字符索引值为0。 end – 字符串中结束搜索的位置。字符中第一个字符的索引为 0。默认为字符串的最后一个位置。 123456str="www.orange.com"sub='o'print ("str.count('o') : ", str.count(sub))sub='run'print ("str.count('run', 0, len(str)) : ", str.count(sub,0,10)) 3）str.encode(encoding=’UTF-8’,errors=’strict’) &amp;&amp; bytes.decode(encoding=”utf-8”, errors=”strict”) str.encode() 会以 encoding 指定的编码格式编码字符串。 bytes.decode（）Python3 中没有 decode 方法，但我们可以使用 bytes 对象的 decode() 方法来解码给定的 bytes 对象，这个 bytes 对象可以由 str.encode() 来编码返回。 12345678910111213141516&gt;&gt;&gt; str = "我想学中文";&gt;&gt;&gt; str_utf8 = str.encode("UTF-8")&gt;&gt;&gt; str_gbk = str.encode("GBK")&gt;&gt;&gt; print(str)我想学中文&gt;&gt;&gt; print("UTF-8 编码：", str_utf8)UTF-8 编码： b'\xe6\x88\x91\xe6\x83\xb3\xe5\xad\xa6\xe4\xb8\xad\xe6\x96\x87'&gt;&gt;&gt; print("GBK 编码：", str_gbk)GBK 编码： b'\xce\xd2\xcf\xeb\xd1\xa7\xd6\xd0\xce\xc4'&gt;&gt;&gt; print("UTF-8 解码：", str_utf8.decode('UTF-8','strict'))UTF-8 解码： 我想学中文&gt;&gt;&gt; print("GBK 解码：", str_gbk.decode('GBK','strict'))GBK 解码： 我想学中文 4）str.join(sequence) join() 方法用于将字符串序列中的元素以指定的字符连接生成一个新的字符串。 1234567&gt;&gt;&gt; s1 = "-"&gt;&gt;&gt; s2 = ""&gt;&gt;&gt; seq = ("w", "e", "l", "c", "o", "m", "e")&gt;&gt;&gt; print (s1.join( seq ))w-e-l-c-o-m-e&gt;&gt;&gt; print (s2.join( seq ))welcome 5）lstrip() &amp;&amp; rstrip() &amp;&amp; strip() lstrip([chars])方法用于截掉字符串左边的字符（默认为空格、\n）`或指定字符。 12345678&gt;&gt;&gt; str = " !!!this is string example....!!! \n";&gt;&gt;&gt; print(str) !!!this is string example....!!!&gt;&gt;&gt; print( str.lstrip() );!!!this is string example....!!!&gt;&gt;&gt; rstrip([chars])方法用于截掉字符串左边的字符（默认为空格、\n）或指定字符。 123&gt;&gt;&gt; print( str.rstrip() ); !!!this is string example....!!!&gt;&gt;&gt; strip() 方法用于移除字符串头尾指定的字符（默认为空格、\n）或字符序列。 12&gt;&gt;&gt; print( str.strip() );!!!this is string example....!!! 6）str.replace(old, new,max) replace() 方法把字符串中的 old（旧字符串） 替换成 new(新字符串)，如果指定第三个参数 max，则替换不超过 max 次。 12&gt;&gt;&gt; print (str.replace("is", "was", 3))thwas was string example....!!! 7）max(str) &amp;&amp; min(str) max() 方法返回字符串中最大的字母。 123&gt;&gt;&gt; str = "welcome"&gt;&gt;&gt; print ("最大字符: " + max(str))最大字符: w min() 方法返回字符串中最小的字母。 123&gt;&gt;&gt; str = "welcome"&gt;&gt;&gt; print ("最小字符: " + min(str))最小字符: c 2.2.8 字符串是否包含某个字串1）in &amp;&amp; not in 123456&gt;&gt;&gt; str1 = "welcome to python world"&gt;&gt;&gt; substr = "come"&gt;&gt;&gt; print(substr in str1)True&gt;&gt;&gt; print(substr not in str1)False 2）str.find(sub_str, beg=0, end=len(string)) find() 方法检测字符串中是否包含子字符串 str ，如果指定 beg（开始） 和 end（结束） 范围，则检查是否包含在指定范围内。如果在指定范围找到 str，返回的是 str 在字符串中的起始位置索引值。如果没有找到，返回 -1。 12345678&gt;&gt;&gt; str1 = "welcome to python world"&gt;&gt;&gt; substr = "come"&gt;&gt;&gt; print(str1.find(substr))3&gt;&gt;&gt; print(str1.find(substr,1))3&gt;&gt;&gt; print(str1.find(substr,5))-1 3）str.index(str, beg=0, end=len(string)) index() 方法检测字符串中是否包含子字符串 str ，如果指定 beg（开始） 和 end（结束） 范围，则检查是否包含在指定范围内。如果在指定范围找到 str，返回的是 str 在字符串中的起始位置索引值。该方法与 python find() 方法一样，只不过如果 str 不在 string 中会报一个异常。 12345678910&gt;&gt;&gt; str1 = "welcome to python world"&gt;&gt;&gt; substr = "come"&gt;&gt;&gt; print(str1.index(substr))3&gt;&gt;&gt; print(str1.index(substr,1))3&gt;&gt;&gt; print(str1.index(substr,5))Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ValueError: substring not found 2.2.9 字符串分割1）str.split(str=””, num=string.count(str)) split()通过指定分隔符对字符串进行切片。如果参数 num 有指定值，则仅分隔 num+1 个子字符串。 1234567&gt;&gt;&gt; str = "this is string example....!!!"&gt;&gt;&gt; print (str.split( )) # 以空格为分隔符['this', 'is', 'string', 'example....!!!']&gt;&gt;&gt; print (str.split(' ',1)) # 以空格为分隔符['this', 'is string example....!!!']&gt;&gt;&gt; print (str.split('i')) # 以 i 为分隔符['th', 's ', 's str', 'ng example....!!!'] 2）str.splitlines([keepends]) splitlines() 按照 行(&#39;\r&#39;, &#39;\r\n&#39;, \n&#39;)对字符串进行分隔，返回一个包含各行作为元素的列表。如果参数 keepends 为 False，不包含换行符（默认）；如果为 True，则保留换行符。 123456&gt;&gt;&gt; 'ab c\n\nde fg\rkl\r\n'.splitlines()['ab c', '', 'de fg', 'kl']&gt;&gt;&gt; 'ab c\n\nde fg\rkl\r\n'.splitlines(False)['ab c', '', 'de fg', 'kl']&gt;&gt;&gt; 'ab c\n\nde fg\rkl\r\n'.splitlines(True)['ab c\n', '\n', 'de fg\r', 'kl\r\n'] 2.2.10 字符串格式化字符串格式化涉及到的内容较多，考虑到篇幅原因我们在 Python 字符串之格式化输出 一文中进行了详细说明。这里是为了保持字符串章节完整性故设立一个链接模块。 2.2.11 Unicode 字符串关于 Python 字符串编码涉及到的内容较多，考虑到篇幅原因我们在 Python 字符串之 Unicode 编码 一文中进行了详细说明。这里是为了保持字符串章节完整性故设立一个链接模块。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>DataType</tag>
        <tag>Variables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础语法]]></title>
    <url>%2FPython%2FPython-%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： 任何一种编程语言都有自己的一套语法规则，编译器或者解释器就是负责把符合语法的程序代码转换成 CPU 能够执行的机器码，然后执行，Python 也不例外。从这一章节开始，我们开始正式学习 Python 程序设计语言。 Python 官方标准库地址：https://docs.python.org/zh-cn/3.7/library/index.html ，从这里你可以获取到关于 Python 程序设计语言的所有说明文档。 Python 基础语法在开始 Python 数据类型和变量的学习之前，我们先给出一个简单的 Python 程序样例，以此来说明一些 Python 编程中需要注意的基本规则： 12345678# print absolute value of an integerimport numpya = 100if a &gt; 0: print (a)else: print (-a) 1.1 编码Python 3 源码文件默认情况下（不明确指定），是以 UTF-8 进行编码的，所有字符串都是 unicode 字符串。 1# -*- coding: utf-8 -*- 当然你也可以为源码文件指定不同的编码： 1# -*- coding: GB2312 -*- 上述定义允许在源文件中使用 GB2312（simplified） 字符集中的字符编码，对应适合语言为中文。 1.2 标识符程序设计语言中，标识符（Identifier）用于给变量、常量、函数、类等命名，以建立起名称与使用之间的关系。 标识符可以由字母、数字和下划线组成; 第一个字符必须是字母表中字母或下划线 _ ; 标识符对大小写敏感。 标识符规范： 见名知义； 大小写：1.包名全小写，2.常量全大写，3，变量、函数名开头首字母小写，后面首字母全大写；4.类名首字母全大写。 注意，以下划线开头的标识符是有特殊意义的。例如，以双下划线开头的 __foo 代表类的私有成员。以双下划线开头和结尾的 __foo__ 代表 Python 里特殊方法专用的标识，如 __init__() 代表类的构造函数。 1.3 Python 保留字保留字即关键字，我们不能把它们用作任何标识符名称。Python 的标准库提供了一个 keyword 模块，可以输出当前版本的所有关键字： 123&gt;&gt;&gt; import keyword&gt;&gt;&gt; keyword.kwlist['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'] 1.4 注释注释是给开发人员看的，可以是任意内容，解释器会忽略掉注释。 Python 中单行注释以 # 开头，实例如下： 1234#!/usr/bin/python3# 第一个注释print ("Hello, Python!") # 第二个注释 多行注释可以用多个 # 号，还有 &#39;&#39;&#39; 或 &quot;&quot;&quot;： 123456789101112131415#!/usr/bin/python3 # 第一个注释# 第二个注释 &apos;&apos;&apos;第三注释第四注释&apos;&apos;&apos; &quot;&quot;&quot;第五注释第六注释&quot;&quot;&quot;print (&quot;Hello, Python!&quot;) 1.5 行与缩进Python 可以在同一行中使用多条语句，语句之间使用分号（;）分割，以下是一个简单的实例： 1name = "Google"; age = 20; job = "IT"; 但注意，通常语句后面不加（;），而且每行只写一个语句。 Python 最具特色的就是使用 缩进 来表示代码块，不再需要使用大括号 {} 。 缩进有利有弊。好处是强迫你写出格式化的代码，但没有规定缩进是几个空格还是 Tab（但要一致，否则会报错）。按照约定俗成的管理，应该始终坚持使用 4 个空格的缩进。注意不要混用 Tab 和 空格。 缩进不一致导致的错误： 1IndentationError: unindent does not match any outer indentation level 缩进的坏处就是 “复制－粘贴” 功能失效了，这是最坑爹的地方。当你重构代码时，粘贴过去的代码必须重新检查缩进是否正确。 1.6 跨行语句Python 通常是一行写完一条语句，但如果语句很长，我们可以使用反斜杠 (\) 来实现多行语句，例如： 123total = item_one + \ item_two + \ item_three 在 [], {}, 或 () 中的多行语句，不需要使用反斜杠(\)，例如： 12total = ['item_one', 'item_two', 'item_three', 'item_four', 'item_five'] 1.7 import &amp;&amp; from … import在 Python 中用 import 或者 from...import 来导入相应的模块从而调用其内置方法： 将整个模块(somemodule)导入，格式为： import somemodule 为方便模块使用，简写格式为：import somemodule as xxx 从某个模块中导入某个函数，格式为： from somemodule import somefunction 从某个模块中导入多个函数，格式为： from somemodule import firstfunc, secondfunc, thirdfunc 将某个模块中的全部函数导入，格式为： from somemodule *]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一个 Python 程序]]></title>
    <url>%2FPython%2F%E7%AC%AC%E4%B8%80%E4%B8%AA-Python-%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 引言： 前面我们已经成功搭建了 Python 学习以及开发环境，这里我们以 HelloWorld.py 来看我们的第一个 Python 程序。 HelloWorld首先我们给出 HelloWorld.py 脚本测试程序代码： 1print(&quot;Welcome to Python world!&quot;) 1. 使用文本编辑器在 Python 的交互式命令行环境下写程序，好处是一下就能得到结果，坏处是没法保存，下次还想运行的时候，还得再敲一遍。所以一般我们使用交互式环境进行测试，而不是开发。 而实际开发时，我们通常使用一个文本编辑器来写代码，写完了，保存为一个文件，这样，程序就可以反复运行了。 那么问题来了：文本编辑器到底哪家强？这里我们推荐两款文本编辑器： Sublime Text &amp;&amp; [Notepad++] 这两款文本编辑器都是免费的，甚至为了方便编码可以内嵌各种插件。 安装好文本编辑器后，我们将脚本内容写入文本文件 HelloWorld.py。然后在命令行（Terminal or Shell）下进行运行： 12$ python HelloWorld.pyWelcome to Python world! 1.1 关于 Python 脚本命名文件名只能是英文字母、数字和下划线的组合，但是必须要以 .py 结尾。 必须在脚本存放目录下执行，如果当前目录下没有 HelloWorld.py 这个文件，运行 python HelloWorld.py 就会报错： 12$ python HelloWorld.pypython: can&apos;t open file &apos;hello.py&apos;: [Errno 2] No such file or directory 1.2 直接运行 py 文件我们能不能像 .exe 文件那样直接运行 .py 文件呢？在 Windows 上是不行的，但是，在 Mac 和 Linux 上是可以的，方法是在 .py 文件的第一行加上一个特殊的注释： 123#!/usr/bin/env python3print(&quot;Welcome to Python world!&quot;) 当然，运行时我们要保证执行脚本的可执行权限。 上面我们完成了如何使用文本编辑器进行 Python 程序的学习和开发，下面我们来看 Python 中的输入输出（Input/Output）： 2. 输入和输出2.1 Output输出是我们接触到的第一个 Python 程序语句。用 print() 在括号中加上字符串，就可以向屏幕上输出指定的文字。比如输出 Welcome to Python world!，用代码实现如下： 1&gt;&gt;&gt; print(&quot;Welcome to Python world!&quot;) print()函数也可以接受多个字符串（可以使用 &#39;&#39; 和 &quot; &quot; 括住，但不可混用），用逗号 “,” 隔开，就可以连成一串输出。并且每遇到一个逗号 “,” 会输出一个空格。输出样式如下： 12&gt;&gt;&gt; print(&apos;Welcome to Python world.&apos;, &apos;It is good.&apos;, &apos;why?&apos;)Welcome to Python world. It is good. why? 2.2 Input现在，我们已经可以用 print() 输出你想要的结果了。但是，如果要让用户从电脑输入一些字符怎么办？Python提供了一个 input()，可以让用户输入字符串，并存放到一个变量里。比如输入用户的名字： 123&gt;&gt;&gt; name = input()Michael&gt;&gt;&gt; 输入完成后，我们查看一下： 12&gt;&gt;&gt; print(name)Michael 通常我们为了程序的友好性，需要提示信息来告诉用户：“嘿，赶紧输入你的名字”。input()可以让你显示一个字符串来提示用户。于是我们编辑一个新的脚本 Helloxxx.py ，内容如下： 12name = input(&quot;Please enter your name:&quot;)print(&apos;Hello,&apos;, name) 运行这个程序，你会发现，程序一运行，会首先打印出 please enter your name: ，这样，用户就可以根据提示进行输入。输入名字后，得到 Hello, xxx 的输出： 123$ python Helloxxx.pyplease enter your name: Michaelhello, Michael]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>HelloWorld</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 简介与开发环境搭建]]></title>
    <url>%2FPython%2FPython-%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 开篇 不管是由于工作需要必须掌握 Python 开发，或是抱着极其浓厚的兴趣想要学习 Python 程序设计语言，亦或是不甘落于人后（身边越来越多的人开始使用 Python），不要困惑，我们已经站在 Python 花园秘境的入口，即将一睹大门背后绮丽的风景。 那句话怎么说来着？人生苦短，快来使用 Python 吧 …… 这一章节我们通过对以下两部分内容的解读，正式开启 Python 的学习之旅的第一站（叩门）： Python 简介 Python 开发环境搭建 1. Python 简介这一小节我们来精简介绍 Python 学习背景知识，你可以将其作为 Python 学习入门。但是要注意，相关介绍没必要深究，在后续正式进入 Python 学习生态环境之后，你会越来越深入理解相关的内容。 1.1 Python 是什么？Python 是一个高层次的解释型的面向对象的脚本语言。 Python 是一种解释型语言： 这意味着开发过程中没有了编译这个环节。类似于 PHP 和 Perl 语言。 Python 是交互式语言： 这意味着，你可以在一个 Python 提示符（Shell），直接互动执行你编写的程序。 Python 是面向对象语言: 这意味着 Python 支持面向对象的风格或代码封装于对象的编程技术。 Python 是初学者的语言：Python 对初级程序员而言，是一种伟大的语言。它支持广泛的应用程序开发，从简单的文字处理到 WWW 浏览器再到游戏。 1.2 Python 能干什么？ 基础编程开发 WEB 开发 运维开发 机器学习 &amp; 深度学习 计算机视觉与自然语言处理 数据挖掘 Spark 大数据系列 ……. 看到这里是不是了解了为什么 Python 经常被和 人工智能 捆绑到一块作为 AI 关键词的原因了。 1.3 Python 发展历史Python 的创始人为荷兰人 Guido van Rossum（吉多·范罗苏姆）。1989年圣诞节期间，吉多·范罗苏姆为了在阿姆斯特丹打发时间， 决心开发一个新的脚本解释程序。之所以选中 Python（蟒蛇）作为程序的名字，是因为他是飞行马戏团的爱好者。 Python 源代码同样遵循 GPL（GNU General Public License）协议。 1991年，第一个 Python 解释器诞生。它是用 C 语言实现的，并能够调用 C 语言的库文件。 Python 2.0 于 2000 年 10 月发布，增加了完整的垃圾回收，并且支持 Unicode。 Python 3.0 于 2008 年 12 月发布，此版不完全兼容之前的 Python 源代码。不过，很多新特性也被移植到旧版本。 Python 2.6/2.7 版本。 1.4 Python 特点 易于学习：Python有相对较少的关键字，结构简单，和明确定义的语法，学习起来更加简单。 易于阅读：Python 代码定义非常清晰。 一个广泛的标准库：Python 的最大的优势之一是丰富的库，跨平台的，在 UNIX，Windows 和Macintosh 兼容很好。 互动模式：互动模式的支持，你可以从终端输入执行代码并获得结果的语言，互动的测试和调试代码片断。 可移植：基于其开放源代码的特性，Python 已经被移植（也就是使其工作）到许多平台。 可扩展：如果你需要一段运行很快的关键代码，或者是想要编写一些不愿开放的算法，你可以使用 C 或 C++ 完成那部分程序，然后从你的 Python 程序中调用。 数据库：Python 提供所有主要的商业数据库的接口。 GUI 编程：Python 支持 GUI 可以创建和移植到许多系统调用。 可嵌入: 你可以将 Python 嵌入到C/C++程序，让你的程序的用户获得”脚本化”的能力。 Python 有缺点么 ？？？一味鼓吹 Python 没有缺陷是伪命题，任何编程语言都有缺点，Python 也不例外。 第一个缺点：运行速度慢 和 C 程序相比非常慢，因为 Python 是解释型语言，代码在被执行时会一行一行地翻译成 CPU 能理解的机器码，这个翻译过程非常耗时，所以很慢。而 C 程序是运行前直接被编译成 CPU 能执行的机器码，所以非常快。 但是大量的应用程序不需要这么快的运行速度，因为用户根本感觉不出来。例如开发一个网络应用程序，C 程序的运行时间需要 0.001 秒，而 Python 程序的运行时间需要 0.1 秒，慢了 100 倍，但由于网络更慢，需要等待 1 秒，你想，用户能感觉到 1.001 秒和 1.1 秒的区别吗？这就好比 F1 赛车和普通的出租车在北京三环路上行驶的道理一样，虽然 F1 赛车理论时速高达 400 公里，但由于三环路堵车的时速只有 20 公里，因此，作为乘客，你感觉的时速永远是 20 公里。 甚至我们可以将需要快速执行的 Python 代码段转成 C 内置于 Python 中以提升性能。 第二个缺点：代码不能加密 如果要发布你的 Python 程序，实际上就是发布源代码，这一点跟 C 语言不同，C语言不用发布源代码，只需要把编译后的机器码（也就是你在 Windows 上常见的 xxx.exe 文件）发布出去。要从机器码反推出 C 代码是不可能的，所以，凡是编译型的语言，都没有这个问题，而解释型的语言，则必须把源码发布出去。 当然，想要加密 Python 代码，也并非无解，网络上也给出了一些 Python 加密方法用以提高其源码保密性。 Python 彩蛋这一小节最后一部分，我们给出一个 Python 彩蛋：“Python 之歌”，它概要地解读了 Python 语言设计的特点。在后续我们介绍完 Python 开发环境搭建之后，你可以重新返回到这一部分“敲开”彩蛋： # Python 解释器执行如下命令： import this # 获取到如下信息： The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren&apos;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you&apos;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it&apos;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let&apos;s do more of those! 2. Python 开发环境搭建工欲善其事，必先利其器。在正式开始 Pthon 程序设计语言学习之前，这一章节我将向大家介绍如何搭建本地 Python 语言开发学习环境。 前面我已经提到过 Python 具有良好的移植性，可应用于多平台包括：Windows、Linux 和 Mac OS X 等等。确实，Python 官网确实也提供了不同平台的安装包以提供下载安装。安装后，你会得到 Python 解释器（就是负责运行 Python 程序的），一个命令行交互环境，还有一个简单的集成开发环境（IDLE）。 关于 Python 安装版本选择，个人建议选择 Python3.X（越来越多的开发者转战到 Python3，已成为主流使用版本）。下面来开始搭建我们的本地 Python 语言开发环境。 环境检查在开始正式安装之前，你可以通过终端窗口（Terminal）输入 “python -V” 命令来查看本地是否已经安装 Python 以及 Python 的安装版本。 如果输出如下信息表示当前系统中已安装有 Python，你已经可以跳过安装环节开始 Python 语言的学习了： 1234Python 3.5.0 (default, Dec 1 2018, 17:15:02) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 否则的话，接下文 2.1 Python 下载Python 最新源码，二进制文档，相关新闻资讯等可以在 Python 的官网查看到： Python 官网：https://www.python.org/ 你可以在以下链接中下载 Python 的文档，你可以下载 HTML、PDF 和 PostScript 等多种格式的文档。 Python 文档下载地址：https://www.python.org/doc/ 2.2 Python 安装我们知道 Python 已经被移植在许多平台上（使它能够工作在不同平台上），类似于 Java，“一次编写，多次运行”。而不同平台平台的安装步骤视使用的操作系统和安装方式而异。安装 Python 的方式：我们既可以从源码安装，同时也可以下载使用已经编译并且打包好的二进制版本安装包进行安装。 补充：如果你选择的平台（例如：Unix/Linux）的二进制代码不可用，你可以使用手动编译源代码进行安装。编译的源代码，功能上有更多的选择性， 为 Python 安装提供了更多的灵活性。 以下是各个平台安装包的下载地址： 以下给出不同平台（Unix/Linux、Windows）上安装 Python 的方法： 2.2.1 Unix &amp; Linux 平台安装 Python:下面给出在 Unix &amp; Linux 平台上安装 Python 的通用步骤。 源码安装 打开 WEB 浏览器访问：https://www.python.org/downloads/source/ 选择适用于 Unix/Linux 的源码压缩包。 cd 到安装目录：/usr/local，下载及解压压缩包。 如果你需要自定义一些选项，可以修改 Modules/Setup 执行 ./configure 脚本 make make install 执行以上操作后，Python会被安装在/usr/local/bin目录中。Python库一般选择安装在/usr/local/lib/pythonXX，XX 为你使用的 Python 的版本号。 注意：在安装过程中使用一些包管理器工具（APT等），可以简化安装过程。另外在安装之前需要准备 Python 编译环境，否则会导致安装报错。 由于 Unix &amp; Linux 平台一般自带相应版本的 Python（Python2/Python3），如果你选择在 Unix &amp; Linux 平台学习 Python，那么你可以直接跳过后续教程（你已经可以开始使用 Python 了！）。 当然，有些同学就是想尝试一下 Python 的编译过程（或者系统原生 Python 版本不是你学习使用所需要版本），那么下面我会给出一个详细的安装过程，注意多个 Python 版本是可以共存的（前提是你需要注意新安装的 Python 环境对原有环境的影响，这里不做详细说明，后续补充章节会进行详细解读）。 我们以 “Python-3.5.0 安装” 为样例介绍其详细安装过程： 1）准备编译环境(环境如果不对的话，可能遇到各种问题，比如：wget 无法下载 https 链接文件等) [root@node1 ~]# yum install zlib-devel bzip2-devel openssl-devel ncurses-devel 2）下载及解压 Python3.5.0 源代码包 [root@node1 ~]# cd /usr/local/ [root@node1 ~]# wget https://www.python.org/ftp/python/3.5.0/Python-3.5.0.tar.xz # 下载完成后进行减压： [root@node1 ~]# tar Jxvf Python-3.5.0.tar.xz # 减压后进入到相应目录，可以看到一个 README 文档。说明了如何进行安装： [root@node1 ~]# cd Python-3.5.0 # 查看安装方法： [root@node1 Python-3.5.0]# cat README 3）编译安装 # 编译后你会在目录 [/usr/local] 下发现新产生的：[python3.5] 目录 [root@node1 Python-3.5.0]# ./configure --prefix=/usr/local/python3.5 [root@node1 Python-3.5.0]# make &amp;&amp; make install ### 日志信息如下： Ignoring indexes: https://pypi.python.org/simple Collecting setuptools Collecting pip Installing collected packages: setuptools, pip Successfully installed pip-7.1.2 setuptools-18.2 # 同时提示成功安装 pip-7.12 与 setuptools！之后你就可以使用 pip 包管理器进行安装 Python 扩展包。（关于 pip，你知道它是用来解决 Python 包依赖（一个扩展包的安装需要其它扩展包的安装支持）的工具即可。） 至此 Unix &amp; Linux 平台 Python 安装已完成！ 2.2.2 Window 平台安装 Python:以下为在 Window 平台上安装 Python 的简单步骤： 二进制版本安装1）Python 2.X 版本安装： 以 “最新 Python 2.7 版本安装” 为样例： 打开 WEB 浏览器访问 https://www.python.org/downloads/windows/ 在下载列表中选择Window平台安装包，包格式为：【python-XYZ.msi】 文件 ， XYZ 为你要安装的版本号。 要使用安装程序 【python-XYZ.msi】, Windows 系统必须支持 Microsoft Installer 2.0 搭配使用。只要保存安装文件到本地计算机，然后运行它即可（安装时，根据提示 Next 即可）。需要看看你的机器是否支持 MSI，Windows XP 和更高版本已经有 MSI，很多老机器也可以安装 MSI。 2）Python 3.X 版本安装： 以 “最新 Python 3.5.1 版本安装” 为样例： 打开 WEB 浏览器访问 https:////www.python.org/ ，点击 Downloads,进入选择下载界面： 这里我选择的是：python3.5.1，会看到如下界面： 这里，我们需要安装的是 Windows 下的 Python 版本，所以在 Operating System 中可以选择对应的 Windows 版本。另外需要注意的是要根据系统位数选择相应位数（64-bit 或 32-bit） Python 安装二进制版本，现在主流的都是 64-bit的。 其中，executable 表示可执行版，需要安装后使用，embeddable 表示嵌入版，就是解压以后就可以使用的版本（免安装版本）。 我们选择安装可执行版本，可执行版安装比较简单，step by step ok。embeddable 版本 Python 安装好后一定需要配置环境变量（往下看）。 2.3 Python 环境变量配置Python 默认安装完成后并不可以直接使用，即通过终端窗口输入 “python” 命令无法查看到 Python 版本信息（系统无法访问到安装好的 Python 可执行程序）如下，说明我们还需要为其配置系统环境变量。 123C:\Users\guoxx&gt;python&apos;python&apos; 不是内部或外部命令，也不是可运行的程序或批处理文件。 扩展：我们想要运行的程序或可执行文件（python.exe）可以位于许多目录，而这些路径很可能不在操作系统提供可执行文件的搜索路径中。故，不可以直接执行这些程序或可执行文件。 而路径（PATH）可以存储到系统环境变量中，这是由操作系统维护的一个命名的字符串。这些变量包含系统中可用的命令行解释器（Python 解释器）和其他程序的位置信息。想要操作系统能够搜索到程序或可执行文件，我们需要将其相应的路径添加到系统环境变量中（注意在添加时，要加上环境变量中原有的路径 :$PATH，否则会覆盖原始路径，导致系统出错）。 因此，我们需要将 Python 相关的可执行文件或程序路径添加到系统环境变量 $PATH 中。 下面我们来看不同平台如何配置其环境变量： 2.3.1 Unix/Linux 平台设置环境变量1）export PATH 方法 在 csh shell: 输入 setenv PATH “$PATH:/usr/local/python” 在 bash shell (Linux): 输入 export PATH=”$PATH:/usr/local/python” 在 sh 或者 ksh shell: 输入 PATH=”$PATH:/usr/local/python” 注意: 【/usr/local/python】 是 Python 的安装目录。 2）构建 Python 使用软链接（推荐） 以上面 “Python-3.5.0 安装 样例为示例： # 产生软连接时要注意系统中已存在的 Python 软链接，防止原生 Python 调用被覆盖掉： # 覆盖掉后只能访问最新版本 python。 [root@node1 Python-3.5.0]# ln -s /usr/local/python3.5/bin/python3.5 /usr/local/bin/python3.5 这是由于 【/usr/local/bin/】 路径已在 PATH 中。 2.3.2 Windows 平台设置环境变量在环境变量中添加 Python 目录： 1）cmd 方法 在命令提示框中(cmd) : 输入 path=%path%;C:\Python 注意: C:\Python 是Python的安装目录。 2) GUI 方法 右键点击”计算机”，然后点击”属性” 然后点击”高级系统设置” 选择 “系统变量” 窗口下面的 “Path”,双击即可！ 然后在“Path”行，添加 python 安装路径即可(C:\Python)，所以在后面，添加该路径即可。 ps：记住，路径直接用分号 “；” 隔开！ 2.3.3 检测环境变量配置是否成功？在 命令行终端 中输入： python 或 python -V 如果配置成功的话，会有 Python 版本信息输出，并进入 Python 交互式环境（可执行 Python 语句）。如下： Python 3.5.0 (default, Dec 1 2018, 17:15:02) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; 至此，我们已经完成了 Python 本地环境的安装，下面我们给出一个 Python Hello World 来验证本地环境已经搭建成功： 2.4 Python Hello World首先，在 命令行终端 中输入：“python”，会进入 Python 交互式解释器环境： [root@node1 ~]# python Python 3.5.0 (default, Dec 1 2018, 17:15:02) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; 然后在命令提示符 &gt;&gt;&gt; 之后输入命令： # python 3 输入: print (&quot;Welcome to Python World&quot;) # python 2 输入: print &quot;Welcome to Python World&quot; 语句执行成功后会输出以下语句，表示 Python 本地环境搭建成功： Welcome to Python World 2.5 运行 Python 方式Python 本地环境搭建成功之后，我们来看如何使用搭建好的 Python 环境运行编写的 Python 程序。通常有三种方式可以运行 Python： 1）交互式解释器： 你可以通过命令行窗口（Command Terminal）进入 Python 交互式解释器中开始编写 Python 代码。你可以在 Unix，DOS 或任何其他提供了命令行或者 shell 的系统进行 python 编码工作。如下： # Unix/Linux &amp; Windows/DOS： Python 3.5.0 (default, Dec 1 2018, 17:15:02) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt;print (&quot;Hello Python World&quot;) 退出交互式环境： 12345# Unix/Linux &amp; Windows/DOS：Python 3.5.0 (default, Dec 1 2018, 17:15:02) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt;exit() 2）命令行脚本： 在你的应用程序中通过 引入解释器 可以在命令行中执行 Python 脚本 【 HelloWorld.py 】，如下所示： # Unix/Linux： [root@node1 ~]# python HelloWorld.py 或者 # Windows/DOS C:&gt; python HelloWorld.py 注意：在执行脚本时，系统会检查脚本是否有可执行权限。 3）集成开发环境（IDE：Integrated Development Environment）: 和大多数程序设计语言一样，一个体验良好的集成开发环境对于 Python 程序开发是必不可缺的。这里我推荐使用 PyCharm（大多数 Python 开发人员的首选）。 以 “PyCharm IDE” 为例： PyCharm 是由 JetBrains 打造的一款 Python IDE，支持 macOS、 Windows、 Linux 系统。 PyCharm 功能 : 支持调试、语法高亮、Project管理、代码跳转、智能提示、自动完成、单元测试、版本控制…… PyCharm 下载地址 : https://www.jetbrains.com/pycharm/download/ 以下展示的是 PyCharm 工作界面样例图（是不看起来相当美观）： Windows 下 PyCharm 安装以及使用教程参见：Pycharm Community In Windows 安装以及使用教程。 注意：Python 交互模式的代码是输入一行，解释执行一行。而命令行模式下直接运行.py文件是一次性执行该文件内的所有代码。可见，Python 交互模式主要是为了调试 Python 代码用的，也便于初学者学习，它不是正式运行 Python 代码的环境！ 何为 Python 解释器？前面我们一直提到 Python 解释器，当我们编写 Python 代码时，我们得到的是一个包含 Python 代码的以.py为扩展名的文本文件。要运行代码，就需要 Python 解释器去执行.py文件。 由于整个 Python 语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写 Python 解释器来执行 Python 代码（当然难度很大）。事实上，确实存在多种 Python 解释器。 1）CPython 当我们从 Python 官方网站下载并安装好 Python 3.x 后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用 C 语言开发的，所以叫 CPython。在命令行下运行 python 就是启动 CPython 解释器。所以 CPython 是使用最广的 Python 解释器。 2）IPython IPython 是基于 CPython 之上的一个交互式解释器，也就是说，IPython 只是在交互方式上有所增强，但是执行Python 代码的功能和 CPython 是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了 IE。 CPython 用 &gt;&gt;&gt;作为提示符，而 IPython 用 In [序号]: 作为提示符。 3）Jython Jython 是运行在 Java 平台上的 Python 解释器，可以直接把 Python 代码编译成 Java 字节码执行。 4）IronPython IronPython 和 Jython 类似，只不过 IronPython 是运行在微软 .Net 平台上的 Python 解释器，可以直接把Python 代码编译成 .Net 的字节码。 注意： Python 的解释器很多，这里我们简单介绍了常见的几种，但使用最广泛的还是 CPython。如果要和 Java 或 .Net 平台交互，最好的办法不是用 Jython 或 IronPython，而是通过网络调用来交互，确保各程序之间的独立性。 ================================================================= 至此，这一章节的主要内容已经解读完成，基于上述内容的介绍你已经可以对照 Python 参考书开始 Python 语言的学习了。但我强烈建议你阅读下面的内容，它可以帮助你快速在 Python 学习中走的更远…… 下面我会补充介绍一部分有利于提升我们在进行 Python 开发过程拥有更好体验的内容，毕竟拥有一个更加良好的学习开发环境可以帮助我们更高效学习 Python 的使用。 3. 多 Python 版本共存环境前面我们提到过一个系统中是可以同时存在多个 Python 版本。 我们给出一种实际情况：原生 Unix &amp; Linux 系统已经自带有一个版本的 Python2，但它不是我们需要的 Python 版本，我们想要编译安装满足我们需要的 Python 版本（Python3）。那么我们面临一种选择：我们是在原有版本基础上 update 呢？还是去安装一个新版本的 Python（原生版本 Python 也保留）？ 事实上，由于 Python 2.X 和 Python 3.X兼容性问题，将 Python 2.X 直接升级到 Python3 会产生一些问题（Centos 系统下的很多工具（yum、iotop 等）都依赖原生 Python2.X 版本），直接升级会无法使用。 所以，一般建议添加安装一个新版本的 Python（与原生版本共存）或使用 virtualenv（后面我们会给出一个虚拟环境工具的使用，这里不用深究） 3.1 多版本共存选择安装一个新版本的 Python（与原生版本共存）时，安装过程中我们需要考虑多版本冲突问题（多 Python 版本开发环境是我们必然要面对的问题），关键在于如何保证多个 Python 版本使用之间互相不产生影响呢？ 其实我们在介绍安装 Python 时，已经给出了解决办法，Unix &amp;&amp; Linux 下可以通过构建多个带有不同版本标识的 Python 软链接（不去覆盖 /usr/local/bin/python ）： ln -s /usr/local/python3.5/bin/python3.5 /usr/local/bin/python3.5 ln -s /usr/local/python2.7/bin/python2.7 /usr/local/bin/python3.7 于是： 在使用 Python 时，我们可以通过启动相应版本 Python： # 启动系统原生环境： python # 启动 python3.5 环境： python3.5 # 启动 python3.7 环境： python3.7 那么 Windows 下如何实现呢？ 我建议大家修改环境变量中设置的 Python 安装目录路径即可，想用那个版本就指向那个版本。 至此，我们就可以在当前系统中同时存在多个 Python 版本供我们选择使用！ 除了上述介绍的方法外，我们还可以使用一些虚拟环境管理工具实现 Python 的多版本共存，例如 virtualenv、Anaconda 等。 4. Anaconda（推荐）通过后续 Python 的正式学习，我们会越来越加深对 “Python 实现其强大功能的前提，就是它具有数量庞大且功能相对完善的标准库和第三方库。通过对库的引用，能够快速实现对不同领域业务的开发。”的理解。你会发现：对于 Python 学习的新手来说，管理以及维护这些数量庞大的标准库和第三方库既复杂又重要。这里我们介绍一种使用 虚拟隔离环境 来解决库管理以及维护问题的方法—Anaconda。 Anaconda 包管理软件是一个不错的选择，可以减少很多后续安装 Python 各种包的麻烦。同时在 Anaconda 自带的 Jupyter notebook 中进行代码的编写要比 IDE 和 Terminal 的体验好得多，Jupyter notebook 中还支持记录代码学习笔记，相当有逼格（强烈推荐使用）。 Anaconda 是 Python 的一个发行版，如果把 Python 比作 Linux，那么 Anancoda 就是 CentOS 或者 Ubuntu。它通过 conda 工具解决了Python开发者的两大痛点： 提供包管理，功能类似于 pip，Windows 平台安装第三方包经常失败的场景得以解决。 提供虚拟环境管理，功能类似于 virtualenv，解决了多版本 Python 并存问题。 通俗的讲：使用 Anaconda 进行 Python 相关开发时，相当于将所有的底层依赖细节全部已经打包封装好了！并且，Anaconda 还能创建自己的计算环境，相当于将 Python 的环境与其他环境做了隔离，这样你就可以在当前环境随便玩，爱怎么玩怎么玩，也不用担心破坏之前的环境！ 同样，Anaconda 提供多平台支持,对 Linux、Mac以及Windows 系统均支持。 这里我们给出 Anaconda 官方下载网址。具体介绍、安装以及使用这里不进行说明，感兴趣同学可以参见：Anaconda 介绍、安装以及使用教程。 5. 在 Cloud Studio 中运行 Python 程序前面我们提到过，Python 是跨平台的，它可以运行在 Windows、Mac 和各种 Linux/Unix 系统上。也就是说在 Windows 上写 Python 程序，放到 Linux 上也是能够运行的。所以，通常开发人员在 Windows 下进行项目开发，最后可以将开发好的项目之间投放到 Liunx 服务器上运行。 通过 Python 开发环境搭建介绍，我们知道，Python 本地环境安装后，你会得到 Python 解释器，一个命令行交互环境，还有一个简单的集成开发环境。 除了上述我们介绍过的 Python 开发环境，这里再推荐一种在线云端开发工具—Coding Cloud Studio。它能提供原生的在线 Linux 命令交互终端环境，Python 运行解释器，在线开发文本编辑器，你可以直接在工作站中创建 Python 文件并在 Cloud Studio 中运行你写的 Python 程序。 同时，Cloud Studio 还支持供有强大的插件系统、一键切换开发环境、实时协同编辑、全功能终端、实时进度保存等丰富的功能实现。 这里给出 Cloud Studio 开发界面： 很多人都说 Cloud Studio 开发项目时很卡顿，这里我们只是提供另一种开发思路。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Setup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu Server 快速配置指南]]></title>
    <url>%2FUbuntu%2FUbuntu-Server-%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[愿你每天欢喜多于悲，孤独有人陪… 写在前面： 本文给出了安装好 Ubuntu 之后的一些基础配置方法，完成以下配置可以帮助新手快速掌握 Ubuntu Server的使用。本文所作目的：一方面作为 Ubuntu 快速配置记录，方便下次使用；另一方作为分享以帮助更多的 Ubuntu 小白快速上手。 补充：如果文中有表述不正确的地方，望各位大佬不吝指正。 1. 配置系统网络以及代理1.1 查看网卡信息1234567891011121314151617181920$ ifconfig# 网卡信息：enp0s31f6: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.0.131 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::87a4:c514:5d1d:2b86 prefixlen 64 scopeid 0x20&lt;link&gt; ether 30:9c:23:76:b9:0f txqueuelen 1000 (Ethernet) RX packets 832257 bytes 1230044351 (1.2 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 371708 bytes 28311409 (28.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 device interrupt 16 memory 0xf7100000-f7120000# 本地回环：lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 354776 bytes 26600434 (26.6 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 354776 bytes 26600434 (26.6 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 1.2 配置本地网络 IP1.2.1 &gt; 图像化界面配置Desktop 依次点击：System Sets -&gt; Network Sets -&gt; Wired -&gt; IPV4 Setting 123456# 示例网络配置如下：ADDRESS = 192.168.0.131NETMASK = 255.255.252.0GATEWAY = 192.168.0.1DNS = xx.xx.xxx.xxx,xxx.xxx.xxx.xx 1.2.2 &gt; 配置文件1 &gt; 网卡配置 通过修改网卡配置文件 /etc/network/interfaces 来为 Ubuntu 设置网络。 1sudo vi /etc/network/interfaces 配置文件默认文件内容如下： 123# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopback 下面修改网卡配置文件来进行网络设置： 1）设置动态获取 IP 12auto enp0s31f6iface enp0s31f6 inet dhcp 注意，上面的网卡名称要依据 ifconfig 中查询到的网卡信息进行设置。 默认情况下，Ubuntu16.04 Desktop 网络配置就是动态 IP。可以通过 右上角网络标识 --&gt; Edit Connections --&gt; Wired connection X --&gt; IPV4 Settings 查看： 可以看到：Method：Automatic（DHCP）。 2）设置静态 IP 事实上，我们更多的想给网卡分配一个固定 IP。这里来看如何进行静态 IP 的设置？包括为网卡设置：IP 地址（address）、子网掩码（netmask）、网关（gateway）等。 首先给出一个可通过 IP 地址获取其掩码以及网关的 在线网络和IP地址计算器。下面我们根据获取到的信息配置网卡： 12345auto enp0s31f6iface enp0s31f6 inet staticaddress 192.168.0.1netmask 255.255.255.0gateway 192.168.0.1 2 &gt; DNS 配置 配置域名解析服务器（DNS）用于域名解析（以阿里 DNS 地址：223.5.5.5 为例）： 1）方法一 通过在网卡配置文件 /etc/network/interfaces 中进行设置： 1234567auto enp0s31f6iface enp0s31f6 inet staticaddress 192.168.0.1netmask 255.255.255.0gateway 192.168.0.1dns-nameservers 223.5.5.5 重启服务器，DNS 生效后会体现在 /etc/resolv.conf 文件中： 1234# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)# DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTENnameserver 127.0.1.1nameserver 223.5.5.5 2）方法二 通过域名解析服务器配置文件 sudo vi /etc/resolvconf/resolv.conf.d/base 来配置 DNS： 1nameserver 223.5.5.5 重启服务器后生效。 3 &gt; 测试 配置网卡和 DNS 之后，我们需要 reboot 使得配置生效。然后检测网络连通性： 1）查看配置后网卡信息 1$ ifconfig 2）外网测试 12$ ping www.baidu.comPING www.a.shifen.com (111.13.100.91) 56(84) bytes of data. 或者: 12$ sudo apt-get install curl$ curl www.baidu.com 1.3 配置网络代理服务器出于信息安全，大部分公司都会存在内、外网以进行网络限制，往往我们需要通过网络代理服务器才可以访问外网。下面我们来看如何配置网络代理： 1.3.1 &gt; 图像化界面配置1）Desktop 依次点击：System Sets -&gt; Network Sets -&gt; Network Proxy 配置系统网络代理： 123# 示例网络代理配置如下：HTTP Proxy：http://proxy.xxxxxx.com:8080HTTPS Proxy：https://proxy.xxxxxx.com:8080 2）配置服务器浏览器代理（浏览器可连接外网） 1# 从浏览器设置中找到网络代理设置相关，设置其为 【使用系统代理设置】 或 【自定义代理】。 1.3.2 &gt; 配置文件1）单独配置 apt-get 与其它应用程序（如：wget）代理 12# 注意我们可以单独为应用程序配置代理：为应用程序单独设置代理方法见网络即可，这里不赘述。# 我们也可以直接配置系统网络代理，其它应用程序使用系统代理设置,见下文。 2）配置系统代理 12345678$ vim ~/.bashrc# 文件尾追加：http(s)_proxy = http(s)://用户名：密码@代理地址:代理端口。如下：export http_proxy = http://用户名:密码@proxy.xxxxxx.com:8080export https_proxy = https://用户名:密码@proxy.xxxxxx.com:8080# 更新bashrc$ source ~/.bashrc 1.3.3 &gt; Proxyenv很多人可能发现，一旦给系统设置代理后会影响其它用户使用，我们更希望能够即开即关，而上述的网络代理设置方法不太灵活。 下面给出一种通过网络代理配置文件实现随时切换网络环境（内、外网）的方法： 1）设置网络代理 创建一个用来进行配置网络代理的文件 ： 1$ vim ~/.proxyenv 模板示例内容如下，可以根据需求进行组合添加： 1234567891011121314151617181920212223#!/bin/sh# For terminalexport proxyserveraddr=123.123.123.123export proxyserverport=8087export HTTP_PROXY="http://$proxyserveraddr:$proxyserverport/"export HTTPS_PROXY="https://$proxyserveraddr:$proxyserverport/"export FTP_PROXY="ftp://$proxyserveraddr:$proxyserverport/"export SOCKS_PROXY="socks://$proxyserveraddr:$proxyserverport/"export NO_PROXY="localhost,127.0.0.1,localaddress,.localdomain.com,200.200..;11.11.0.0;"export http_proxy="http://$proxyserveraddr:$proxyserverport/"export https_proxy="https://$proxyserveraddr:$proxyserverport/"export ftp_proxy="ftp://$proxyserveraddr:$proxyserverport/"export socks_proxy="socks://$proxyserveraddr:$proxyserverport/"export no_proxy="localhost,127.0.0.1,localaddress,.localdomain.com,200.200..;11.11.0.0;"# For apt-getcat &lt;&lt;-EOF| sudo tee /etc/apt/apt.confAcquire::http::proxy "http://$proxyserveraddr:$proxyserverport/";Acquire::https::proxy "https://$proxyserveraddr:$proxyserverport/";Acquire::ftp::proxy "ftp://$proxyserveraddr:$proxyserverport/";Acquire::socks::proxy "socks://$proxyserveraddr:$proxyserverport/";EOF 启用网络代理： 1$ . ~/.proxyenv 2）取消网络代理 创建一个用来取消网络代理配置的文件 ： 1$ vim ~/.unproxyenv 模板示例内容如下，可以根据需求进行组合添加（要保证与 ~/.proxyenv 内容相对应）： 12345678910111213141516#!/bin/shunset proxyserveraddrunset proxyserverportunset HTTP_PROXYunset HTTPS_PROXYunset FTP_PROXY：unset SOCKS_PROXYunset NO_PROXYunset http_proxyunset https_proxyunset ftp_proxyunset socks_proxyunset no_proxyecho -n ""|sudo tee /etc/apt/apt.conf 关闭网络代理： 1$ . ~/.unproxyenv 3）样例 123456789101112131415161718192021222324## 1. 启用代理：$ vim ~/.proxyenv# 添加内容如下：#!/bin/shexport http_proxy="http://210.83.210.xxx:9527"export https_proxy="http://210.83.210.xxx:9527"export ftp_proxy="http://210.83.210.xxx:9527"export no_proxy="127.0.0.1, localhost, 10.1.*.*"$ . ~/.proxyenv## 2. 取消代理：$ vim ~/.unproxyenv# 添加内容如下：unset http_proxyunset https_proxyunset ftp_proxyunset no_proxy$ . ~/.unproxyenv 2. 更换 apt-get 软件源为 apt-get 更换国内源。可以提升应用程序安装速度，下载过慢可能导致某些软件安装失败。 2.1 备份官方源一旦配置出错，或者想还原官方源的时候可以随时还原。 1$ cp /etc/apt/sources.list /etc/apt/sources.list.backups 2.2 更换国内源1$ vim /etc/apt/sources.list 国内常用 Ubuntu 镜像源：教育网的源（比如：清华源、中科大源）、阿里源、网易源等等。 1）教育网推荐加入如下内容（中科大的、清华源） 12345678910111213141516171819202122232425262728293031323334### 1. 清华源：# Help Link：https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse### 2. 中科大源：# Help Link：https://mirrors.ustc.edu.cn/help/ubuntu.html# 默认注释了源码仓库，如有需要可自行取消注释deb https://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiverse# deb-src https://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse# deb-src https://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse# deb-src https://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# deb-src https://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# deb-src https://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse 2）阿里源 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 3）网易源 123456789101112deb http://mirrors.163.com/ubuntu/ precise-updates main restricteddeb-src http://mirrors.163.com/ubuntu/ precise-updates main restricteddeb http://mirrors.163.com/ubuntu/ precise universedeb-src http://mirrors.163.com/ubuntu/ precise universedeb http://mirrors.163.com/ubuntu/ precise-updates universedeb-src http://mirrors.163.com/ubuntu/ precise-updates universedeb http://mirrors.163.com/ubuntu/ precise multiversedeb-src http://mirrors.163.com/ubuntu/ precise multiversedeb http://mirrors.163.com/ubuntu/ precise-updates multiversedeb-src http://mirrors.163.com/ubuntu/ precise-updates multiversedeb http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ precise-backports main restricted universe multiverse 2.3 源更新更新软件源以及更新系统中应用程序： 1234$ sudo apt-get update# 更新系统中的应用程序（可选）$ sudo apt-get upgrade 3. 登陆界面设置 root 用户登录选项1）设置 root 用户登录密码 1$ sudo passwd root 2）登录 root 用户 1$ su root 3）修改配置文件 ../50-ubuntu.conf 12345$ vim /usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf% 添加如下内容：greeter-show-manual-login=trueall-guest=false 重启系统可能会产生报错: Error found when loading /root/.config 此时，需要将 /root/.profile 文件中的 mesg n 替换成 tty -s &amp;&amp; mesg n，其它信息不变！ 4. 配置 SSH 远程登录1）安装 SSH 1$ apt-get ssh 2）远程服务器安装 SSH 后，Xshell 客户端可能启动报错：server responded “Algorithm negotiation failes”。 解决方法如下：编辑 sshd_config 文件末尾添加如下内容： 12345$ vim /etc/ssh/sshd_configCiphers aes128-cbc,aes192-cbc,aes256-cbc,aes128-ctr,aes192-ctr,aes256-ctr,3des-cbc,arcfour128,arcfour256,arcfour,blowfish-cbc,cast128-cbc MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160,hmac-sha1-96,hmac-md5-96 KexAlgorithms diffie-hellman-group1-sha1,diffie-hellman-group14-sha1,diffie-hellman-group-exchange-sha1,diffie-hellman-group-exchange-sha256,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group1-sha1,curve25519-sha256@libssh.org 之后，重启 sshd 服务后即可正常连接: 1234$ /etc/init.d/ssh restart# 运行成功后显示信息如下：[ ok ] Restarting ssh (via systemctl): ssh.service. 3）设置允许 root 用户 SSH 远程登录 SSH 服务可以通过 SSH 协议来远程访问服务器，代替 Telnet 和 Ftp。但是 Ubuntu 默认是不启用 root 用户也不允许 root 远程登录的。所以需要先启用 root 用户： 12345$ sudo vim /etc/ssh/sshd_config% 修改：PermitRootLogin no --&gt; PermitRootLogin yes% 重新启动ssh： $ /etc/init.d/ssh restart 5. 常用软件安装以及配置5.1 Vim1）安装 vim 1$ apt-get vim-gtk 2）Vim 配置 6. Windows 远程访问 Ubuntu 桌面环境访问的一般方法如下: 12345% 1. VNC（推荐）% 2. 第三方桌面：xfce% 3. 第三方软件：TeamViewer 7. Ubuntu 重启或开关机7.1 重启命令12345678# 立刻重启$ reboot$ shutdown -r now# 过 10 分钟自动重启$ shutdown -r 10# 在时间为 20:35 时候重启$ shutdown -r 20:35 7.2 关机命令1234567# 立刻关机（一般加-p 关闭电源）$ halt$ poweroff$ shutdown -h now# 10 分钟后自动关机$ shutdown -h 10]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Configuration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK 下载与安装教程]]></title>
    <url>%2FJava%2FJDK-%E4%B8%8B%E8%BD%BD%E4%B8%8E%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 写在前面： 开始学习 JAVA，首要的就是安装以及配置 JDK（Java Development Kit：Java 开发工具包）。本篇博文所作目的主要是记录一下 Java 开发环境的搭建，即 JDK 的安装以及配置过程。 安装以及配置 JDK这里，首先给出 JDK 各版本下载地址供安装选择： Download –&gt; JDK 官方最新版 Download –&gt; JDK 1.8 官方最新版 [Download –&gt; JDK 官方各历史版本 本文将以 JDK1.8 的安装配置为 Demo，其它 JDK 版本安装一样。 1. JDK 安装包下载根据个人需求从上面选择要访问的 JDK 下载页，选择 Accept License Agreement（被授权） 就可以下载相应的安装包了： 可以发现，JDK 官网提供了不同平台下（Linux、Mac 以及 Windows）JDK 的安装包。 根据需要下载相应平台安装包 1.1 –&gt; For Windows: 对于 Windows 下的安装包，提供了 i586（32 bit） 和 x64（64 bit） 供我们下载使用。 这里我们选择安装包：jdk-8u211-windows-x64.exe 用于后续的安装。 1.2 –&gt; For Linux: 对于 Linux下的安装包，提供了 i586（32 bit） 和 x64（64 bit） 的 RPM 包以及 .tar.gz 包供下载使用。 这里我们选择安装包：jdk-8u211-linux-x64.tar.gz 用于后续的安装。 2. 安装 JDK上面我们已经下载好了 JDK 的安装包，接下来我们来看如何在不同的平台下安装下载好的 JDK 安装包： 2.1 Windows 下 JDK 的安装1）检查当前系统中是否安装有 JDK Windows 命令行（Terminal ）下执行如下指令： 1$ java -version 如果原始系统中已安装有 JDK ，则会输出如下 JDK 版本等信息： 123java version "1.8.0_131"Java(TM) SE Runtime Environment (build 1.8.0_131-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) 否则的话，开始安装 JDK（当然，如果系统中已存在某个版本的 JDK 了，我们还可以安装一个其它版本的 JDK），接下文。 2）安装 JDK 双击 jdk-8u211-windows-x64.exe 安装包开始进行安装…… 安装过程中没有什么难的，Next 即可。需要注意的是，我们可以自定义 JDK 安装路径，不要什么东西都往系统盘安装，这里假设我们安装在：E:\JAVA 目录下。 等待安装完成关闭安装界面即可。 可以发现，JDK 以及被安装到了 E:\JAVA 目录下，包含两个文件： 12jdk1.8.0_211jre1.8.0_211 自此 Windows 下 JDK 的安装就完成了，但此时仍不能使用，我们还需要为 JDK 配置环境变量，接 【3. 环境变量配置】。 2.2 Linux 下 JDK 的安装1）检查当前系统中是否安装有 JDK 1[root@localhost test]# java -version 有些 Linux 系统默认会安装有 openjdk（一种 Linux 开源 JDK），例如会输出如下信息： 123openjdk version "1.8.0_102"OpenJDK Runtime Environment (build 1.8.0_102-b14)OpenJDK 64-Bit Server VM (build 25.102-b14, mixed mode) 我们可以选择先将其卸载掉（当然，不用管也可以）： 12# Centos 下可以使用 yum 进行卸载：[root@localhost test]# yum remove *openjdk* 当然系统中还有可能装有其它版本（区别于 openjdk，例如：oracle）的 JDK，例如： 123java version "1.8.0_131"Java(TM) SE Runtime Environment (build 1.8.0_131-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode) 否则的话，开始安装 JDK（当然，如果系统中已存在某个版本的 JDK 了，我们还可以安装一个其它版本的 JDK），接下文。 2）安装 JDK 进入到 jdk-8u211-linux-x64.tar.gz 安装包存放目录（例如：~/Downloads），并且对安装包进行解压： 12345678910# 查找安装包：[root@localhost test]$ cd Downloads[root@localhost Downloads]$ ls |grep jdk*jdk-8u131-linux-x64.tar.gz# 解压至 /usr/java 目录下：[root@localhost Downloads]$ tar -zxvf jdk-8u131-linux-x64.tar.gz -C /usr/java# 等待解压完成即可。# 解压后，我们可以在 /usr/java 目录下可以查看到解压好的 JDK 目录：jdk1.8.0_131 自此 Linux下 JDK 的安装就完成了，但此时仍不能使用，我们还需要为 JDK 配置环境变量，接 【3. 环境变量配置】。 3. 环境变量配置3.1 Windows 下为 JDK 配置环境变量首先打开环境变量配置界面：右键我的电脑 ---&gt; 属性 ---&gt; 高级系统设置 ---&gt; 环境变量。界面如下： –&gt; 开始设置环境变量: 这里注意，我们既可以设置普通用户变量，也可以设置系统变量。区别在于设置普通用户变量的话，只有该用户下可用，其他用户想要使用的话需要重新配置环境变量。而设置系统变量后，我们可以在所有用户下都可以使用。 –&gt; 故，这里我们选择设置系统变量： 1）点击系统变量下面的新建按钮，变量名设置为：JAVA_HOME（代表JDK安装路径），变量值：E:\JAVA\jdk1.8.0_211（具体的 JDK 的安装路径）。如下： 2）继续在系统变量里面新建一个 CLASSPATH 变量， 其变量值为：.;%JAVA_HOME%\lib\tools.jar;%JAVA_HOME%\lib\dt.jar;。如下： 这里注意变量前面还有一个 .;，表示当前路径，不要漏加！！！ 3）在系统变量里面找一个变量名是 Path 的变量，找到后进行编辑，在它的变量值最后面追加一段如下的代码： 1%JAVA_HOME%\bin; 最后点击确定，此时 Windows 下 JDK 的环境变量配置就完成了，还不圆满，需要测试一下。 4）测试 WIndows Terminal 下分别执行如下指令： –&gt; java -version 1234$ C:\Users\guoji&gt;java -versionjava version "1.8.0_211"Java(TM) SE Runtime Environment (build 1.8.0_211-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) –&gt; java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051C:\Users\guoji&gt;java用法: java [-options] class [args...] (执行类) 或 java [-options] -jar jarfile [args...] (执行 jar 文件)其中选项包括: -d32 使用 32 位数据模型 (如果可用) -d64 使用 64 位数据模型 (如果可用) -server 选择 "server" VM 默认 VM 是 server. -cp &lt;目录和 zip/jar 文件的类搜索路径&gt; -classpath &lt;目录和 zip/jar 文件的类搜索路径&gt; 用 ; 分隔的目录, JAR 档案 和 ZIP 档案列表, 用于搜索类文件。 -D&lt;名称&gt;=&lt;值&gt; 设置系统属性 -verbose:[class|gc|jni] 启用详细输出 -version 输出产品版本并退出 -version:&lt;值&gt; 警告: 此功能已过时, 将在 未来发行版中删除。 需要指定的版本才能运行 -showversion 输出产品版本并继续 -jre-restrict-search | -no-jre-restrict-search 警告: 此功能已过时, 将在 未来发行版中删除。 在版本搜索中包括/排除用户专用 JRE -? -help 输出此帮助消息 -X 输出非标准选项的帮助 -ea[:&lt;packagename&gt;...|:&lt;classname&gt;] -enableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 按指定的粒度启用断言 -da[:&lt;packagename&gt;...|:&lt;classname&gt;] -disableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 禁用具有指定粒度的断言 -esa | -enablesystemassertions 启用系统断言 -dsa | -disablesystemassertions 禁用系统断言 -agentlib:&lt;libname&gt;[=&lt;选项&gt;] 加载本机代理库 &lt;libname&gt;, 例如 -agentlib:hprof 另请参阅 -agentlib:jdwp=help 和 -agentlib:hprof=help -agentpath:&lt;pathname&gt;[=&lt;选项&gt;] 按完整路径名加载本机代理库 -javaagent:&lt;jarpath&gt;[=&lt;选项&gt;] 加载 Java 编程语言代理, 请参阅 java.lang.instrument -splash:&lt;imagepath&gt; 使用指定的图像显示启动屏幕有关详细信息, 请参阅 http://www.oracle.com/technetwork/java/javase/documentation/index.html。 —&gt; javac 12345678910111213141516171819202122232425262728293031323334C:\Users\guoji&gt;javac用法: javac &lt;options&gt; &lt;source files&gt;其中, 可能的选项包括: -g 生成所有调试信息 -g:none 不生成任何调试信息 -g:&#123;lines,vars,source&#125; 只生成某些调试信息 -nowarn 不生成任何警告 -verbose 输出有关编译器正在执行的操作的消息 -deprecation 输出使用已过时的 API 的源位置 -classpath &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -cp &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -sourcepath &lt;路径&gt; 指定查找输入源文件的位置 -bootclasspath &lt;路径&gt; 覆盖引导类文件的位置 -extdirs &lt;目录&gt; 覆盖所安装扩展的位置 -endorseddirs &lt;目录&gt; 覆盖签名的标准路径的位置 -proc:&#123;none,only&#125; 控制是否执行注释处理和/或编译。 -processor &lt;class1&gt;[,&lt;class2&gt;,&lt;class3&gt;...] 要运行的注释处理程序的名称; 绕过默认的搜索进程 -processorpath &lt;路径&gt; 指定查找注释处理程序的位置 -parameters 生成元数据以用于方法参数的反射 -d &lt;目录&gt; 指定放置生成的类文件的位置 -s &lt;目录&gt; 指定放置生成的源文件的位置 -h &lt;目录&gt; 指定放置生成的本机标头文件的位置 -implicit:&#123;none,class&#125; 指定是否为隐式引用文件生成类文件 -encoding &lt;编码&gt; 指定源文件使用的字符编码 -source &lt;发行版&gt; 提供与指定发行版的源兼容性 -target &lt;发行版&gt; 生成特定 VM 版本的类文件 -profile &lt;配置文件&gt; 请确保使用的 API 在指定的配置文件中可用 -version 版本信息 -help 输出标准选项的提要 -A关键字[=值] 传递给注释处理程序的选项 -X 输出非标准选项的提要 -J&lt;标记&gt; 直接将 &lt;标记&gt; 传递给运行时系统 -Werror 出现警告时终止编译 @&lt;文件名&gt; 从文件读取选项和文件名 3.2 Linux下为 JDK 配置环境变量–&gt; 开始设置环境变量: 这里注意，我们既可以设置普通用户变量（局部变量），也可以设置系统变量（全局变量）。区别在于设置普通用户变量的话，只有该用户下可用，其他用户想要使用的话需要重新配置环境变量。而设置系统变量后，我们可以在所有用户下都可以使用。 –&gt; 故，这里我们选择设置系统变量： 1234567# 打开系统变量配置文件：[root@localhost test]# vim /etc/profile# 追加如下内容：export JAVA_HOME=/usr/java/jdk1.8.0_131export CLASSPATH=.;%JAVA_HOME%/lib/tools.jar:%JAVA_HOME%/lib/dt.jarexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin –&gt; 生效 /etc/profile 配置文件： 1[root@localhost test]# source /etc/profile 再来看一下普通用户变量如何设置： 12345# 在 ~/.bashrc 用户配置文件中添加上述内容即可。[root@localhost test]$ vim ~/.bashrc# 追加完配置内容后，需要使配置生效：[root@localhost test]$ source ~/.bashrc 此时 Linux下 JDK 的环境变量配置就完成了，还不圆满，需要测试一下。 –&gt; 测试 12345$ java -version$ java$ javac 篇幅原因，输出信息见 Windows 中测试部分。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java Development Kit</tag>
        <tag>JDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象编程概述]]></title>
    <url>%2FJava%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[抛开喧嚣，卸下繁华，只为一纸清凉，遇到更好的自己… 写在前面： 相信很多程序猿，特别是新手，对面向对象都用过而且可能用的还很熟，却没有去深入理解过，别人一问才发现，入门便是不熟。 说起面向对象，不得不提的就是面向过程。大家应该都清楚，面向过程是具体化的，流程化的，解决一个问题，你需要一步一步的分析，一步一步的实现，这就是面向过程的设计。那什么是面向对象？其实，面向对象是模型化的，你只需抽象出一个类，这是一个封闭的盒子，在这里你拥有数据也拥有解决问题的方法。 话不多说，本文让我们来看看究竟什么是面向对象？？？ 1. 什么是面向对象的语言面向对象语言（Object-Oriented Language）是一类以对象作为程序结构基本单位的程序设计语言，用于描述的设计是以对象为核心，并且对象是程序运行时刻的基本成分。 而面向对象的程序设计把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 面向对象语言很多，如一直高居世界编程语言排行榜的 Java、Python、C++、C#、PHP 等等。 2. 面向对象的理解什么是对象？记住！一切皆是类，一切皆是对象。 2.1 实体、类以及对象先来认识一下面向对象中的基本概念： –&gt; 什么是对象？ 对象是现实世界中某一具体实体在计算机逻辑中的抽象，具有一组属性（也称为状态及行为）。 –&gt; 什么是类？ 具有相同特征的对象的集合，是一种抽象数据类型。属于某类的某一个对象，称为类的一次实例化的结果。 –&gt; 对象属性（状态及行为）的理解 1）对象的状态： 对象的状态也称为对象的静态属性，是指对象内部包含的各种信息，也就是变量。 2）对象的行为： 对象的行为也称为对象的动态属性，也指方法，用来设置或改变对象的状态（完成一定的功能）。 2.2 给一个实例一切都是对象。 举一个例子： 我们用于设计的描述（实际问题）是房子，可以抽象出一个房子类【House：Class】，故可以将一栋房子看作是房子类【House：Class】的一次实例化结果：【HouseA：Object】。 后面我们将基于这个实例描述面向对象的四大特性：抽象、封装、继承以及多态性。 不要疑惑，就是四大特性，其实抽象性是面向对象的基本特性，也应该被包含在内。 3. 面向对象的特性这一部分我们来看面向对象的四大特性抽象、封装、继承以及多态性的理解： 3.1 抽象性从对象定义来看面向对象的抽象性，对象就是现实某一具体实体在计算机逻辑中的抽象 。 在上面的实例中，我们从实际问题中抽象出了房子类【House：Class】以及具体的房子对象【HouseA：Object】。 3.2 封装性封装就是将各个独立功能设计成一个个独立的单元，尽可能的隐藏内部细节，只保留必要的对外接口，与外部联系。 高内聚，低耦合。通过封装可以减小耦合，提高内聚，避免牵一发而动全身，方便对程序的修改。 –&gt; 实例说明 对于【HouseA：Object】，房子空间很大，但如果男男女女有十几人住，而且就一间屋子的话，很不方便，并且吃喝拉撒都一个屋，那就成难民营。 –&gt; 这样，就要想法解决: 工人们出动了，把房子折腾折腾给分成楼上楼下两层，并说明了，一楼吃喝玩乐、二楼休息； 楼上楼下工人们又分别给隔离出几个房间，分别规划修建厨房、卫生间、卧室、娱乐室等等，这样就把房子这样改造好了。然后男男女女们过来了，大家就可以各自的房间，做爱做的事，而且不会影响到其他了。 –&gt; 这样一个简单的封装就完成 【House：Class】是对部分类【房子两层：Partialclass】的封装；而【房子两层：Partialclass】是对【房间：函数/方法-function】的封装。 3.3 继承性继承发生在两个对象之间，是特殊类对象具有一般类对象全部的属性和行为。可以提高软件的开发效率，对软件复用有很大的意义。 –&gt; 继续来看房子 工人们已经给分好楼层、并且根据不同的功能做好了房间。这时屋主觉得，不行啊，得有个地下室来放杂物啊。工人一听，好尼玛房子都折腾的差不多了，又要搞一个地下室，不好搞啊。 屋主发话了，要搞必须搞，不好搞也要搞，而且狠狠的搞。工人们就为难了，难道再去给盖一栋有避难所的房子？ 大家集思广益最终，房子其他都还用着，再给挖个地下室就行了，于是新房子诞生了。 –&gt; 一个简单的继承就出现了 【房子：基类-baseclass】被【新房子：扩展类-ExtensionClass】继承。 3.4 多态性多态是同一消息可以根据发送对象的不同产生多种不同的行为方式，可以说是封装的一个实现。实现多态的方法有：方法重写（覆盖）、方法重载以及接口等。 –&gt; 仍然是房子 房子已经成型，要对房间进行装修了。工人想法很简单，都按照一种样式来，简单方便快捷。屋主不乐意了，有要在承重墙上开扇窗户的，有要在卫生间装摄像头的、要在厨房里装空调的….工人们安装要求对能实现的就开始开工了，对房子的窗户、地板、墙、楼梯等进行装修。 –&gt; “一种多态的展现” 就出来了 【房子：类-class】 通过 【窗户、地板、墙、楼梯等：属性-Property】体现多态。 4. 面向对象和面向过程的比较事实上，面向对象更符合人类的思维，面向过程则是机器的思想。 –&gt; 面向过程： 优点：性能比面向对象好，因为类调用时需要实例化，开销比较大，比较消耗资源。 缺点：不易维护、不易复用、不易扩展。 –&gt; 面向对象： 优点：易维护、易复用、易扩展。由于面向对象有封装、继承、多态性的特性，可以设计出高内聚，低耦合的系统，使系统更加灵活、更加易于维护 。 缺点：性能比面向过程差 5. 面向对象的软件开发过程 面向对象分析（Object Oriented Analysis）：OOA； 面向对象设计（Object Oriented Design）：OOD； 面向对象实现（Object Oriented Programming）：OOP，也就是面向对象的编程。 参考：https://www.cnblogs.com/littlemo/p/4350848.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Java</tag>
        <tag>OOP</tag>
      </tags>
  </entry>
</search>
